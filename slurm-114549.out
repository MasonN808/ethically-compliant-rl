Running script with speed limit: 2
Running script with speed limit: 4
Running script with speed limit: 8
Running script with speed limit: 16
Running script with speed limit: 2
Running script with speed limit: 4
Running script with speed limit: 8
Running script with speed limit: 16
Running script with speed limit: 2
Running script with speed limit: 4
Running script with speed limit: 8
Running script with speed limit: 16
Running script with speed limit: 2
Running script with speed limit: 4
Running script with speed limit: 8
Running script with speed limit: 16
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_225403-9gbrjuiu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16/runs/9gbrjuiu
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_225403-uvp0oa2t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/uvp0oa2t
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_225403-5awc8gxe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/5awc8gxe
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231227_065403-2ymaj5gp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/2ymaj5gp
Using cpu device
-------------------------------------
| reward             | [-0.4305399] |
| time/              |              |
|    fps             | 135          |
|    iterations      | 1            |
|    time_elapsed    | 15           |
|    total_timesteps | 2048         |
-------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.26749164] |
| time/              |               |
|    fps             | 135           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.4346465] |
| time/              |              |
|    fps             | 134          |
|    iterations      | 1            |
|    time_elapsed    | 15           |
|    total_timesteps | 2048         |
-------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.3601935] |
| time/              |              |
|    fps             | 120          |
|    iterations      | 1            |
|    time_elapsed    | 17           |
|    total_timesteps | 2048         |
-------------------------------------
-------------------------------------------
| reward                   | [-0.6473]    |
| time/                    |              |
|    fps                   | 130          |
|    iterations            | 2            |
|    time_elapsed          | 31           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0037476534 |
|    clip_fraction         | 0.0374       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.048        |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0503      |
|    lagrangian_multiplier | 0.0699       |
|    learning_rate         | 0.0003       |
|    loss                  | 31           |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00465     |
|    std                   | 0.989        |
|    value_loss            | 354          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.28185967] |
| time/                    |               |
|    fps                   | 130           |
|    iterations            | 2             |
|    time_elapsed          | 31            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.0037172243  |
|    clip_fraction         | 0.0173        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 66.9          |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.0795       |
|    lagrangian_multiplier | 0.072         |
|    learning_rate         | 0.0003        |
|    loss                  | 41.5          |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00471      |
|    std                   | 1.01          |
|    value_loss            | 332           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.5893997] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 2            |
|    time_elapsed          | 31           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.006198242  |
|    clip_fraction         | 0.0365       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0156       |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0439       |
|    lagrangian_multiplier | 0.0819       |
|    learning_rate         | 0.0003       |
|    loss                  | 43           |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00393     |
|    std                   | 0.999        |
|    value_loss            | 584          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6518711] |
| time/                    |              |
|    fps                   | 117          |
|    iterations            | 2            |
|    time_elapsed          | 34           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0068085743 |
|    clip_fraction         | 0.0563       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0841       |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0355       |
|    lagrangian_multiplier | 0.0689       |
|    learning_rate         | 0.0003       |
|    loss                  | 44.9         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00428     |
|    std                   | 0.997        |
|    value_loss            | 460          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.88006765] |
| time/                    |               |
|    fps                   | 129           |
|    iterations            | 3             |
|    time_elapsed          | 47            |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.0073171756  |
|    clip_fraction         | 0.0653        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 1.12          |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.00962      |
|    lagrangian_multiplier | 0.0735        |
|    learning_rate         | 0.0003        |
|    loss                  | 65.3          |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.0065       |
|    std                   | 0.999         |
|    value_loss            | 838           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.5249295] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 3            |
|    time_elapsed          | 47           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0054110675 |
|    clip_fraction         | 0.0489       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 42.3         |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0405      |
|    lagrangian_multiplier | 0.0693       |
|    learning_rate         | 0.0003       |
|    loss                  | 72.8         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00615     |
|    std                   | 1.02         |
|    value_loss            | 657          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.38855597] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 3             |
|    time_elapsed          | 48            |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.0071940087  |
|    clip_fraction         | 0.0548        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0444        |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.133        |
|    lagrangian_multiplier | 0.0585        |
|    learning_rate         | 0.0003        |
|    loss                  | 79.5          |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00537      |
|    std                   | 1             |
|    value_loss            | 831           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.5706698] |
| time/                    |              |
|    fps                   | 115          |
|    iterations            | 3            |
|    time_elapsed          | 52           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.008071713  |
|    clip_fraction         | 0.0819       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.111        |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.000817    |
|    lagrangian_multiplier | 0.0587       |
|    learning_rate         | 0.0003       |
|    loss                  | 72.2         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00992     |
|    std                   | 1            |
|    value_loss            | 602          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7537057] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 4            |
|    time_elapsed          | 63           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.0063815117 |
|    clip_fraction         | 0.0524       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.156        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.00587      |
|    lagrangian_multiplier | 0.0619       |
|    learning_rate         | 0.0003       |
|    loss                  | 45.2         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00605     |
|    std                   | 0.991        |
|    value_loss            | 389          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8067445] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 4            |
|    time_elapsed          | 63           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.006061061  |
|    clip_fraction         | 0.0469       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 126          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0798      |
|    lagrangian_multiplier | 0.0675       |
|    learning_rate         | 0.0003       |
|    loss                  | 56.2         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.0078      |
|    std                   | 0.991        |
|    value_loss            | 462          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.65946686] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 4             |
|    time_elapsed          | 64            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.0066188932  |
|    clip_fraction         | 0.0579        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.018         |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.158         |
|    lagrangian_multiplier | 0.0603        |
|    learning_rate         | 0.0003        |
|    loss                  | 45.3          |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00473      |
|    std                   | 1.01          |
|    value_loss            | 427           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.63367796] |
| time/                    |               |
|    fps                   | 115           |
|    iterations            | 4             |
|    time_elapsed          | 70            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.0068400847  |
|    clip_fraction         | 0.0637        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0723        |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.00581      |
|    lagrangian_multiplier | 0.0528        |
|    learning_rate         | 0.0003        |
|    loss                  | 56.3          |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00837      |
|    std                   | 1.01          |
|    value_loss            | 450           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2727803] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 5            |
|    time_elapsed          | 79           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0051207985 |
|    clip_fraction         | 0.0338       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.11         |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0228       |
|    lagrangian_multiplier | 0.0471       |
|    learning_rate         | 0.0003       |
|    loss                  | 39.6         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00459     |
|    std                   | 0.975        |
|    value_loss            | 364          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.90701467] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 5             |
|    time_elapsed          | 79            |
|    total_timesteps       | 10240         |
| train/                   |               |
|    approx_kl             | 0.008415532   |
|    clip_fraction         | 0.0781        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 141           |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.0994       |
|    lagrangian_multiplier | 0.0839        |
|    learning_rate         | 0.0003        |
|    loss                  | 75.9          |
|    n_updates             | 40            |
|    policy_gradient_loss  | -0.00825      |
|    std                   | 0.996         |
|    value_loss            | 836           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.74038523] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 5             |
|    time_elapsed          | 80            |
|    total_timesteps       | 10240         |
| train/                   |               |
|    approx_kl             | 0.006941827   |
|    clip_fraction         | 0.0539        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0572        |
|    entropy_loss          | -2.86         |
|    explained_variance    | -0.0255       |
|    lagrangian_multiplier | 0.0521        |
|    learning_rate         | 0.0003        |
|    loss                  | 58            |
|    n_updates             | 40            |
|    policy_gradient_loss  | -0.00508      |
|    std                   | 1.02          |
|    value_loss            | 480           |
--------------------------------------------
------------------------------------------
| reward                   | [-0.764275] |
| time/                    |             |
|    fps                   | 115         |
|    iterations            | 5           |
|    time_elapsed          | 88          |
|    total_timesteps       | 10240       |
| train/                   |             |
|    approx_kl             | 0.006178331 |
|    clip_fraction         | 0.0411      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.00999     |
|    entropy_loss          | -2.85       |
|    explained_variance    | -0.0184     |
|    lagrangian_multiplier | 0.0498      |
|    learning_rate         | 0.0003      |
|    loss                  | 23.7        |
|    n_updates             | 40          |
|    policy_gradient_loss  | -0.00419    |
|    std                   | 1           |
|    value_loss            | 187         |
------------------------------------------
-------------------------------------------
| reward                   | [-2.4528084] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 6            |
|    time_elapsed          | 95           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.008642355  |
|    clip_fraction         | 0.0781       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 167          |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.00227      |
|    lagrangian_multiplier | 0.0573       |
|    learning_rate         | 0.0003       |
|    loss                  | 81.7         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.011       |
|    std                   | 0.978        |
|    value_loss            | 586          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7098528] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 6            |
|    time_elapsed          | 95           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.00777508   |
|    clip_fraction         | 0.0624       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.207        |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0342       |
|    lagrangian_multiplier | 0.0482       |
|    learning_rate         | 0.0003       |
|    loss                  | 53.7         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.0068      |
|    std                   | 0.982        |
|    value_loss            | 502          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.66360945] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 6             |
|    time_elapsed          | 97            |
|    total_timesteps       | 12288         |
| train/                   |               |
|    approx_kl             | 0.0063849594  |
|    clip_fraction         | 0.0589        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.672         |
|    entropy_loss          | -2.87         |
|    explained_variance    | 0.0789        |
|    lagrangian_multiplier | 0.0565        |
|    learning_rate         | 0.0003        |
|    loss                  | 58.5          |
|    n_updates             | 50            |
|    policy_gradient_loss  | -0.00894      |
|    std                   | 1.02          |
|    value_loss            | 596           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.6241739] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 6            |
|    time_elapsed          | 106          |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.005239251  |
|    clip_fraction         | 0.0386       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0107       |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0595      |
|    lagrangian_multiplier | 0.07         |
|    learning_rate         | 0.0003       |
|    loss                  | 9.69         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00496     |
|    std                   | 0.994        |
|    value_loss            | 131          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8005446] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 7            |
|    time_elapsed          | 111          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.005645075  |
|    clip_fraction         | 0.0606       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 130          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.00871     |
|    lagrangian_multiplier | 0.0491       |
|    learning_rate         | 0.0003       |
|    loss                  | 110          |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00757     |
|    std                   | 0.969        |
|    value_loss            | 742          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8375676] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 7            |
|    time_elapsed          | 111          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.0067543164 |
|    clip_fraction         | 0.0747       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.46         |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0108       |
|    lagrangian_multiplier | 0.06         |
|    learning_rate         | 0.0003       |
|    loss                  | 117          |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00783     |
|    std                   | 0.98         |
|    value_loss            | 1.09e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7836225] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 7            |
|    time_elapsed          | 113          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.0053704493 |
|    clip_fraction         | 0.0415       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.46         |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.0345      |
|    lagrangian_multiplier | 0.0711       |
|    learning_rate         | 0.0003       |
|    loss                  | 117          |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00809     |
|    std                   | 1.01         |
|    value_loss            | 1.25e+03     |
-------------------------------------------
srun: Job 114549 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation temporarily disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.0181915] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 7            |
|    time_elapsed          | 125          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.005628321  |
|    clip_fraction         | 0.0582       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0302       |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0225      |
|    lagrangian_multiplier | 0.0581       |
|    learning_rate         | 0.0003       |
|    loss                  | 15.9         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00762     |
|    std                   | 0.991        |
|    value_loss            | 162          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0674522] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 8            |
|    time_elapsed          | 127          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.005278234  |
|    clip_fraction         | 0.0444       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 144          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0345      |
|    lagrangian_multiplier | 0.0656       |
|    learning_rate         | 0.0003       |
|    loss                  | 151          |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00593     |
|    std                   | 0.972        |
|    value_loss            | 1.42e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5590457] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 8            |
|    time_elapsed          | 128          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.004824859  |
|    clip_fraction         | 0.0372       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.189        |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.016        |
|    lagrangian_multiplier | 0.0497       |
|    learning_rate         | 0.0003       |
|    loss                  | 55.9         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00623     |
|    std                   | 0.983        |
|    value_loss            | 508          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5848585] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 8            |
|    time_elapsed          | 129          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.004777053  |
|    clip_fraction         | 0.039        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.715        |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.117       |
|    lagrangian_multiplier | 0.0595       |
|    learning_rate         | 0.0003       |
|    loss                  | 96.8         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00666     |
|    std                   | 1.02         |
|    value_loss            | 758          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.87503505] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 8             |
|    time_elapsed          | 143           |
|    total_timesteps       | 16384         |
| train/                   |               |
|    approx_kl             | 0.0052141855  |
|    clip_fraction         | 0.0382        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0199        |
|    entropy_loss          | -2.82         |
|    explained_variance    | -0.00186      |
|    lagrangian_multiplier | 0.0779        |
|    learning_rate         | 0.0003        |
|    loss                  | 11.4          |
|    n_updates             | 70            |
|    policy_gradient_loss  | -0.0047       |
|    std                   | 0.991         |
|    value_loss            | 136           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.6666225] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 9            |
|    time_elapsed          | 143          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.006683767  |
|    clip_fraction         | 0.0554       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 104          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0249      |
|    lagrangian_multiplier | 0.0595       |
|    learning_rate         | 0.0003       |
|    loss                  | 88.6         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00875     |
|    std                   | 0.967        |
|    value_loss            | 706          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.86490184] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 9             |
|    time_elapsed          | 144           |
|    total_timesteps       | 18432         |
| train/                   |               |
|    approx_kl             | 0.0043157483  |
|    clip_fraction         | 0.0315        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 1.78          |
|    entropy_loss          | -2.79         |
|    explained_variance    | 0.00238       |
|    lagrangian_multiplier | 0.0555        |
|    learning_rate         | 0.0003        |
|    loss                  | 56.3          |
|    n_updates             | 80            |
|    policy_gradient_loss  | -0.00638      |
|    std                   | 0.974         |
|    value_loss            | 436           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.550636]  |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 9            |
|    time_elapsed          | 146          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0049955156 |
|    clip_fraction         | 0.0378       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.306        |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0354       |
|    lagrangian_multiplier | 0.0511       |
|    learning_rate         | 0.0003       |
|    loss                  | 74.2         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.0083      |
|    std                   | 1.02         |
|    value_loss            | 603          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8847477] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 10           |
|    time_elapsed          | 160          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0061427793 |
|    clip_fraction         | 0.0647       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 237          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0209      |
|    lagrangian_multiplier | 0.0635       |
|    learning_rate         | 0.0003       |
|    loss                  | 105          |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00735     |
|    std                   | 0.972        |
|    value_loss            | 860          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8820039] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 10           |
|    time_elapsed          | 160          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0057841362 |
|    clip_fraction         | 0.0493       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.27         |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0176       |
|    lagrangian_multiplier | 0.0523       |
|    learning_rate         | 0.0003       |
|    loss                  | 143          |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00765     |
|    std                   | 0.966        |
|    value_loss            | 1.35e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7909812] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 9            |
|    time_elapsed          | 161          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0048016254 |
|    clip_fraction         | 0.0391       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0226       |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0229      |
|    lagrangian_multiplier | 0.0604       |
|    learning_rate         | 0.0003       |
|    loss                  | 21.3         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00505     |
|    std                   | 0.996        |
|    value_loss            | 197          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5946308] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 10           |
|    time_elapsed          | 162          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0060807825 |
|    clip_fraction         | 0.0426       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.02         |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.00865      |
|    lagrangian_multiplier | 0.0678       |
|    learning_rate         | 0.0003       |
|    loss                  | 126          |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.0082      |
|    std                   | 1.04         |
|    value_loss            | 1.46e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7158314] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 11           |
|    time_elapsed          | 176          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.00582448   |
|    clip_fraction         | 0.0509       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 218          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0176      |
|    lagrangian_multiplier | 0.0645       |
|    learning_rate         | 0.0003       |
|    loss                  | 121          |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00705     |
|    std                   | 0.965        |
|    value_loss            | 1.14e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.365143]  |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 11           |
|    time_elapsed          | 177          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0072630327 |
|    clip_fraction         | 0.0848       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.186        |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0249       |
|    lagrangian_multiplier | 0.0425       |
|    learning_rate         | 0.0003       |
|    loss                  | 129          |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00895     |
|    std                   | 0.969        |
|    value_loss            | 996          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0006075] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 10           |
|    time_elapsed          | 179          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0032481183 |
|    clip_fraction         | 0.0238       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0117       |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0282       |
|    lagrangian_multiplier | 0.0653       |
|    learning_rate         | 0.0003       |
|    loss                  | 25.9         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00242     |
|    std                   | 1            |
|    value_loss            | 235          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6536679] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 11           |
|    time_elapsed          | 178          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0068121213 |
|    clip_fraction         | 0.0583       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.59         |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.0362       |
|    lagrangian_multiplier | 0.0693       |
|    learning_rate         | 0.0003       |
|    loss                  | 140          |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00888     |
|    std                   | 1.03         |
|    value_loss            | 1.45e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.5054286] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 12           |
|    time_elapsed          | 192          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0067194896 |
|    clip_fraction         | 0.052        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 245          |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.0329       |
|    lagrangian_multiplier | 0.0558       |
|    learning_rate         | 0.0003       |
|    loss                  | 162          |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00751     |
|    std                   | 0.962        |
|    value_loss            | 1.24e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9336327] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 12           |
|    time_elapsed          | 193          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0064327354 |
|    clip_fraction         | 0.0552       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.409        |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.013        |
|    lagrangian_multiplier | 0.0628       |
|    learning_rate         | 0.0003       |
|    loss                  | 44.6         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00634     |
|    std                   | 0.978        |
|    value_loss            | 491          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4400159] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 12           |
|    time_elapsed          | 195          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0042242734 |
|    clip_fraction         | 0.0476       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.14         |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.0894       |
|    lagrangian_multiplier | 0.0664       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.5         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00734     |
|    std                   | 1.04         |
|    value_loss            | 555          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6480823] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 11           |
|    time_elapsed          | 197          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.007154892  |
|    clip_fraction         | 0.0613       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0047       |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.00755      |
|    lagrangian_multiplier | 0.0756       |
|    learning_rate         | 0.0003       |
|    loss                  | 15.1         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00613     |
|    std                   | 1            |
|    value_loss            | 179          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.1707141] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 13           |
|    time_elapsed          | 208          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0075049754 |
|    clip_fraction         | 0.0481       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 216          |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0239      |
|    lagrangian_multiplier | 0.068        |
|    learning_rate         | 0.0003       |
|    loss                  | 159          |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00742     |
|    std                   | 0.958        |
|    value_loss            | 1.8e+03      |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5500126] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 13           |
|    time_elapsed          | 209          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0054934304 |
|    clip_fraction         | 0.0372       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.234        |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0348       |
|    lagrangian_multiplier | 0.0525       |
|    learning_rate         | 0.0003       |
|    loss                  | 41.6         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00561     |
|    std                   | 0.958        |
|    value_loss            | 336          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6758829] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 13           |
|    time_elapsed          | 211          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0059211305 |
|    clip_fraction         | 0.0628       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.615        |
|    entropy_loss          | -2.92        |
|    explained_variance    | 0.0575       |
|    lagrangian_multiplier | 0.0547       |
|    learning_rate         | 0.0003       |
|    loss                  | 31.8         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00684     |
|    std                   | 1.04         |
|    value_loss            | 298          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0552076] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 12           |
|    time_elapsed          | 215          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0030286699 |
|    clip_fraction         | 0.0141       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0103       |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0423       |
|    lagrangian_multiplier | 0.0583       |
|    learning_rate         | 0.0003       |
|    loss                  | 20.2         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00118     |
|    std                   | 1.01         |
|    value_loss            | 207          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.5379322] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 14           |
|    time_elapsed          | 224          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0057597347 |
|    clip_fraction         | 0.0375       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 185          |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.00653      |
|    lagrangian_multiplier | 0.065        |
|    learning_rate         | 0.0003       |
|    loss                  | 207          |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00628     |
|    std                   | 0.947        |
|    value_loss            | 1.98e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8382876] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 14           |
|    time_elapsed          | 225          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0050228406 |
|    clip_fraction         | 0.0621       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.915        |
|    entropy_loss          | -2.75        |
|    explained_variance    | 0.0286       |
|    lagrangian_multiplier | 0.0511       |
|    learning_rate         | 0.0003       |
|    loss                  | 121          |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00788     |
|    std                   | 0.963        |
|    value_loss            | 1.03e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0517759] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 14           |
|    time_elapsed          | 228          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0064981496 |
|    clip_fraction         | 0.0544       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.17         |
|    entropy_loss          | -2.92        |
|    explained_variance    | 0.0195       |
|    lagrangian_multiplier | 0.0657       |
|    learning_rate         | 0.0003       |
|    loss                  | 74.9         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00866     |
|    std                   | 1.04         |
|    value_loss            | 631          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3907663] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 13           |
|    time_elapsed          | 232          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0059470814 |
|    clip_fraction         | 0.0491       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0171       |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0228      |
|    lagrangian_multiplier | 0.0512       |
|    learning_rate         | 0.0003       |
|    loss                  | 23.5         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00403     |
|    std                   | 1            |
|    value_loss            | 200          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-3.8021116] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 15           |
|    time_elapsed          | 240          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0058439085 |
|    clip_fraction         | 0.0526       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 239          |
|    entropy_loss          | -2.74        |
|    explained_variance    | -0.0135      |
|    lagrangian_multiplier | 0.0491       |
|    learning_rate         | 0.0003       |
|    loss                  | 191          |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00848     |
|    std                   | 0.959        |
|    value_loss            | 1.38e+03     |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-2.786862] |
| time/                    |             |
|    fps                   | 126         |
|    iterations            | 15          |
|    time_elapsed          | 242         |
|    total_timesteps       | 30720       |
| train/                   |             |
|    approx_kl             | 0.006271641 |
|    clip_fraction         | 0.0426      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 2.75        |
|    entropy_loss          | -2.74       |
|    explained_variance    | 0.0226      |
|    lagrangian_multiplier | 0.043       |
|    learning_rate         | 0.0003      |
|    loss                  | 102         |
|    n_updates             | 140         |
|    policy_gradient_loss  | -0.0074     |
|    std                   | 0.949       |
|    value_loss            | 769         |
------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.9705418] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 15           |
|    time_elapsed          | 244          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.006515197  |
|    clip_fraction         | 0.0737       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 21.2         |
|    entropy_loss          | -2.92        |
|    explained_variance    | 0.00713      |
|    lagrangian_multiplier | 0.0746       |
|    learning_rate         | 0.0003       |
|    loss                  | 69           |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00843     |
|    std                   | 1.04         |
|    value_loss            | 846          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2953565] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 14           |
|    time_elapsed          | 251          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.00674792   |
|    clip_fraction         | 0.0585       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0769       |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.00893     |
|    lagrangian_multiplier | 0.0437       |
|    learning_rate         | 0.0003       |
|    loss                  | 65.2         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00878     |
|    std                   | 0.993        |
|    value_loss            | 473          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.3019247] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 16           |
|    time_elapsed          | 256          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0050330553 |
|    clip_fraction         | 0.0356       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 217          |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0054      |
|    lagrangian_multiplier | 0.0661       |
|    learning_rate         | 0.0003       |
|    loss                  | 151          |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00688     |
|    std                   | 0.965        |
|    value_loss            | 1.28e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6789185] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 16           |
|    time_elapsed          | 258          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.007561103  |
|    clip_fraction         | 0.0495       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.7          |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.0285       |
|    lagrangian_multiplier | 0.0464       |
|    learning_rate         | 0.0003       |
|    loss                  | 114          |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00269     |
|    std                   | 0.953        |
|    value_loss            | 907          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.93371207] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 16            |
|    time_elapsed          | 261           |
|    total_timesteps       | 32768         |
| train/                   |               |
|    approx_kl             | 0.007195508   |
|    clip_fraction         | 0.0562        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 1.35          |
|    entropy_loss          | -2.93         |
|    explained_variance    | 0.0882        |
|    lagrangian_multiplier | 0.0564        |
|    learning_rate         | 0.0003        |
|    loss                  | 36.1          |
|    n_updates             | 150           |
|    policy_gradient_loss  | -0.00919      |
|    std                   | 1.05          |
|    value_loss            | 324           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.216723]  |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 15           |
|    time_elapsed          | 269          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0061648814 |
|    clip_fraction         | 0.0619       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0411       |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0136       |
|    lagrangian_multiplier | 0.0588       |
|    learning_rate         | 0.0003       |
|    loss                  | 33.3         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00687     |
|    std                   | 1            |
|    value_loss            | 323          |
-------------------------------------------
-------------------------------------------
| reward                   | [-4.0292044] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 17           |
|    time_elapsed          | 272          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.00679903   |
|    clip_fraction         | 0.0539       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 237          |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0109      |
|    lagrangian_multiplier | 0.0717       |
|    learning_rate         | 0.0003       |
|    loss                  | 217          |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00895     |
|    std                   | 0.962        |
|    value_loss            | 2.1e+03      |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9683763] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 17           |
|    time_elapsed          | 274          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.007107161  |
|    clip_fraction         | 0.068        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 5.97         |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.0233       |
|    lagrangian_multiplier | 0.0645       |
|    learning_rate         | 0.0003       |
|    loss                  | 67.3         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00848     |
|    std                   | 0.951        |
|    value_loss            | 684          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6847894] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 17           |
|    time_elapsed          | 277          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0069152294 |
|    clip_fraction         | 0.0573       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.73         |
|    entropy_loss          | -2.93        |
|    explained_variance    | -0.0259      |
|    lagrangian_multiplier | 0.0572       |
|    learning_rate         | 0.0003       |
|    loss                  | 64.9         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00874     |
|    std                   | 1.05         |
|    value_loss            | 608          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7919589] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 16           |
|    time_elapsed          | 287          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0066227997 |
|    clip_fraction         | 0.0525       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0742       |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.00588     |
|    lagrangian_multiplier | 0.0436       |
|    learning_rate         | 0.0003       |
|    loss                  | 63.6         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00825     |
|    std                   | 0.998        |
|    value_loss            | 501          |
-------------------------------------------
-------------------------------------------
| reward                   | [-4.6889486] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 18           |
|    time_elapsed          | 288          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0075086085 |
|    clip_fraction         | 0.0586       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 227          |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0174      |
|    lagrangian_multiplier | 0.0848       |
|    learning_rate         | 0.0003       |
|    loss                  | 195          |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00788     |
|    std                   | 0.968        |
|    value_loss            | 2.35e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0726116] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 18           |
|    time_elapsed          | 290          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0062463116 |
|    clip_fraction         | 0.0556       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.79         |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.00366     |
|    lagrangian_multiplier | 0.0646       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.6         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00931     |
|    std                   | 0.963        |
|    value_loss            | 541          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.84008735] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 18            |
|    time_elapsed          | 293           |
|    total_timesteps       | 36864         |
| train/                   |               |
|    approx_kl             | 0.005990453   |
|    clip_fraction         | 0.0428        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0316        |
|    entropy_loss          | -2.94         |
|    explained_variance    | 0.0179        |
|    lagrangian_multiplier | 0.0531        |
|    learning_rate         | 0.0003        |
|    loss                  | 32.5          |
|    n_updates             | 170           |
|    policy_gradient_loss  | -0.00687      |
|    std                   | 1.06          |
|    value_loss            | 327           |
--------------------------------------------
-------------------------------------------
| reward                   | [-4.9920926] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 19           |
|    time_elapsed          | 304          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.005352295  |
|    clip_fraction         | 0.054        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 209          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.00481     |
|    lagrangian_multiplier | 0.095        |
|    learning_rate         | 0.0003       |
|    loss                  | 255          |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00619     |
|    std                   | 0.976        |
|    value_loss            | 3.33e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7343017] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 17           |
|    time_elapsed          | 305          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0071444195 |
|    clip_fraction         | 0.0735       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.119        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0213       |
|    lagrangian_multiplier | 0.0544       |
|    learning_rate         | 0.0003       |
|    loss                  | 27.9         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00727     |
|    std                   | 0.997        |
|    value_loss            | 272          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5327743] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 19           |
|    time_elapsed          | 307          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0067527653 |
|    clip_fraction         | 0.0379       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 8.68         |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.000395     |
|    lagrangian_multiplier | 0.057        |
|    learning_rate         | 0.0003       |
|    loss                  | 39.2         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00688     |
|    std                   | 0.97         |
|    value_loss            | 384          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8377278] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 19           |
|    time_elapsed          | 310          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0055354685 |
|    clip_fraction         | 0.0445       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.403        |
|    entropy_loss          | -2.95        |
|    explained_variance    | -0.00315     |
|    lagrangian_multiplier | 0.0588       |
|    learning_rate         | 0.0003       |
|    loss                  | 35.8         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00917     |
|    std                   | 1.06         |
|    value_loss            | 305          |
-------------------------------------------
-------------------------------------------
| reward                   | [-5.140275]  |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 20           |
|    time_elapsed          | 320          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0062767984 |
|    clip_fraction         | 0.0611       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 235          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0392      |
|    lagrangian_multiplier | 0.0636       |
|    learning_rate         | 0.0003       |
|    loss                  | 225          |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00926     |
|    std                   | 0.971        |
|    value_loss            | 2.18e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0614836] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 18           |
|    time_elapsed          | 323          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0051079895 |
|    clip_fraction         | 0.048        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0579       |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.000942    |
|    lagrangian_multiplier | 0.0566       |
|    learning_rate         | 0.0003       |
|    loss                  | 34.6         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00483     |
|    std                   | 0.99         |
|    value_loss            | 334          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2590454] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 20           |
|    time_elapsed          | 323          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.006693528  |
|    clip_fraction         | 0.0543       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 5.47         |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.00677      |
|    lagrangian_multiplier | 0.0601       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.4         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00971     |
|    std                   | 0.96         |
|    value_loss            | 395          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0758511] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 20           |
|    time_elapsed          | 326          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0066338424 |
|    clip_fraction         | 0.0553       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.616        |
|    entropy_loss          | -2.94        |
|    explained_variance    | 0.0374       |
|    lagrangian_multiplier | 0.0642       |
|    learning_rate         | 0.0003       |
|    loss                  | 27.7         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00631     |
|    std                   | 1.05         |
|    value_loss            | 303          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9225942] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 21           |
|    time_elapsed          | 337          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.006913458  |
|    clip_fraction         | 0.0583       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 224          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.00253     |
|    lagrangian_multiplier | 0.0901       |
|    learning_rate         | 0.0003       |
|    loss                  | 266          |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00799     |
|    std                   | 0.967        |
|    value_loss            | 3.5e+03      |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8499955] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 21           |
|    time_elapsed          | 340          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0061531253 |
|    clip_fraction         | 0.0341       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.236        |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0412       |
|    lagrangian_multiplier | 0.046        |
|    learning_rate         | 0.0003       |
|    loss                  | 72.5         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00519     |
|    std                   | 0.931        |
|    value_loss            | 480          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.68096304] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 19            |
|    time_elapsed          | 341           |
|    total_timesteps       | 38912         |
| train/                   |               |
|    approx_kl             | 0.005026666   |
|    clip_fraction         | 0.0432        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0076        |
|    entropy_loss          | -2.81         |
|    explained_variance    | -0.0156       |
|    lagrangian_multiplier | 0.0675        |
|    learning_rate         | 0.0003        |
|    loss                  | 15            |
|    n_updates             | 180           |
|    policy_gradient_loss  | -0.00501      |
|    std                   | 0.988         |
|    value_loss            | 148           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.6390128] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 21           |
|    time_elapsed          | 342          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.004266095  |
|    clip_fraction         | 0.0279       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.021        |
|    entropy_loss          | -2.92        |
|    explained_variance    | 0.0725       |
|    lagrangian_multiplier | 0.0718       |
|    learning_rate         | 0.0003       |
|    loss                  | 20.5         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00465     |
|    std                   | 1.04         |
|    value_loss            | 221          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.47505313] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 22            |
|    time_elapsed          | 353           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.0065235617  |
|    clip_fraction         | 0.0673        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 244           |
|    entropy_loss          | -2.77         |
|    explained_variance    | -0.0287       |
|    lagrangian_multiplier | 0.0602        |
|    learning_rate         | 0.0003        |
|    loss                  | 235           |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.00894      |
|    std                   | 0.967         |
|    value_loss            | 2.29e+03      |
--------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.46776143] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 22            |
|    time_elapsed          | 356           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.0067201364  |
|    clip_fraction         | 0.0501        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.33          |
|    entropy_loss          | -2.67         |
|    explained_variance    | 0.0564        |
|    lagrangian_multiplier | 0.0612        |
|    learning_rate         | 0.0003        |
|    loss                  | 43.7          |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.00628      |
|    std                   | 0.916         |
|    value_loss            | 451           |
--------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2967068] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 20           |
|    time_elapsed          | 359          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0062665436 |
|    clip_fraction         | 0.0489       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00534      |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0517      |
|    lagrangian_multiplier | 0.0756       |
|    learning_rate         | 0.0003       |
|    loss                  | 13           |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00625     |
|    std                   | 0.996        |
|    value_loss            | 154          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.41791996] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 22            |
|    time_elapsed          | 359           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.005469545   |
|    clip_fraction         | 0.0443        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0129        |
|    entropy_loss          | -2.92         |
|    explained_variance    | -0.0631       |
|    lagrangian_multiplier | 0.0634        |
|    learning_rate         | 0.0003        |
|    loss                  | 21.7          |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.00584      |
|    std                   | 1.05          |
|    value_loss            | 225           |
--------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.38515884] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 23            |
|    time_elapsed          | 369           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.007212337   |
|    clip_fraction         | 0.0833        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 245           |
|    entropy_loss          | -2.79         |
|    explained_variance    | 0.0351        |
|    lagrangian_multiplier | 0.0563        |
|    learning_rate         | 0.0003        |
|    loss                  | 202           |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.012        |
|    std                   | 0.982         |
|    value_loss            | 1.54e+03      |
--------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.3395258] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 23           |
|    time_elapsed          | 373          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.008390191  |
|    clip_fraction         | 0.0885       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.12         |
|    entropy_loss          | -2.66        |
|    explained_variance    | 0.0391       |
|    lagrangian_multiplier | 0.0672       |
|    learning_rate         | 0.0003       |
|    loss                  | 54.9         |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.0085      |
|    std                   | 0.914        |
|    value_loss            | 568          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.5280058] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 23           |
|    time_elapsed          | 375          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.0025750007 |
|    clip_fraction         | 0.0143       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0155       |
|    entropy_loss          | -2.92        |
|    explained_variance    | 0.0218       |
|    lagrangian_multiplier | 0.0812       |
|    learning_rate         | 0.0003       |
|    loss                  | 21.3         |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.00432     |
|    std                   | 1.03         |
|    value_loss            | 254          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2795082] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 21           |
|    time_elapsed          | 377          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.004016526  |
|    clip_fraction         | 0.0333       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.018        |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0473      |
|    lagrangian_multiplier | 0.0564       |
|    learning_rate         | 0.0003       |
|    loss                  | 23.8         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00301     |
|    std                   | 1.01         |
|    value_loss            | 206          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8130583] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 24           |
|    time_elapsed          | 385          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.00753233   |
|    clip_fraction         | 0.0718       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 191          |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0201      |
|    lagrangian_multiplier | 0.0617       |
|    learning_rate         | 0.0003       |
|    loss                  | 158          |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.0111      |
|    std                   | 0.985        |
|    value_loss            | 1.49e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5185811] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 24           |
|    time_elapsed          | 389          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.0079244375 |
|    clip_fraction         | 0.0802       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.69         |
|    entropy_loss          | -2.66        |
|    explained_variance    | 0.025        |
|    lagrangian_multiplier | 0.0605       |
|    learning_rate         | 0.0003       |
|    loss                  | 69.2         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00802     |
|    std                   | 0.92         |
|    value_loss            | 660          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5223095] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 24           |
|    time_elapsed          | 392          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.0034484197 |
|    clip_fraction         | 0.021        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0221       |
|    entropy_loss          | -2.9         |
|    explained_variance    | -0.00886     |
|    lagrangian_multiplier | 0.072        |
|    learning_rate         | 0.0003       |
|    loss                  | 11.5         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00501     |
|    std                   | 1.03         |
|    value_loss            | 132          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8759964] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 22           |
|    time_elapsed          | 395          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.005796496  |
|    clip_fraction         | 0.0686       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0279       |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0109      |
|    lagrangian_multiplier | 0.0423       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.6         |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00488     |
|    std                   | 1.01         |
|    value_loss            | 362          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4883186] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 25           |
|    time_elapsed          | 401          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.006120307  |
|    clip_fraction         | 0.0593       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 252          |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0367       |
|    lagrangian_multiplier | 0.0555       |
|    learning_rate         | 0.0003       |
|    loss                  | 113          |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00693     |
|    std                   | 0.989        |
|    value_loss            | 869          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.47867373] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 25            |
|    time_elapsed          | 405           |
|    total_timesteps       | 51200         |
| train/                   |               |
|    approx_kl             | 0.0057478235  |
|    clip_fraction         | 0.0598        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 13.4          |
|    entropy_loss          | -2.67         |
|    explained_variance    | 0.0225        |
|    lagrangian_multiplier | 0.073         |
|    learning_rate         | 0.0003        |
|    loss                  | 54.9          |
|    n_updates             | 240           |
|    policy_gradient_loss  | -0.00736      |
|    std                   | 0.921         |
|    value_loss            | 604           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.46399605] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 23            |
|    time_elapsed          | 413           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.005149155   |
|    clip_fraction         | 0.051         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0746        |
|    entropy_loss          | -2.86         |
|    explained_variance    | 0.0103        |
|    lagrangian_multiplier | 0.0568        |
|    learning_rate         | 0.0003        |
|    loss                  | 30.3          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00646      |
|    std                   | 1.02          |
|    value_loss            | 327           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.8047734] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 26           |
|    time_elapsed          | 417          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0063923346 |
|    clip_fraction         | 0.0752       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 171          |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0247       |
|    lagrangian_multiplier | 0.0552       |
|    learning_rate         | 0.0003       |
|    loss                  | 136          |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00932     |
|    std                   | 0.986        |
|    value_loss            | 1.03e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6485202] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 26           |
|    time_elapsed          | 422          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0065398654 |
|    clip_fraction         | 0.0576       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.271        |
|    entropy_loss          | -2.66        |
|    explained_variance    | 0.0471       |
|    lagrangian_multiplier | 0.0756       |
|    learning_rate         | 0.0003       |
|    loss                  | 23.7         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00612     |
|    std                   | 0.912        |
|    value_loss            | 268          |
-------------------------------------------
------------------------------------------
| reward                   | [-0.647927] |
| time/                    |             |
|    fps                   | 114         |
|    iterations            | 24          |
|    time_elapsed          | 431         |
|    total_timesteps       | 49152       |
| train/                   |             |
|    approx_kl             | 0.006187443 |
|    clip_fraction         | 0.0633      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0801      |
|    entropy_loss          | -2.89       |
|    explained_variance    | 0.012       |
|    lagrangian_multiplier | 0.0584      |
|    learning_rate         | 0.0003      |
|    loss                  | 47.7        |
|    n_updates             | 230         |
|    policy_gradient_loss  | -0.00869    |
|    std                   | 1.03        |
|    value_loss            | 383         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.7180837] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 27           |
|    time_elapsed          | 434          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.0065004947 |
|    clip_fraction         | 0.0582       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 260          |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0376       |
|    lagrangian_multiplier | 0.0554       |
|    learning_rate         | 0.0003       |
|    loss                  | 103          |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00846     |
|    std                   | 0.972        |
|    value_loss            | 660          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9675225] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 27           |
|    time_elapsed          | 438          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.0061969315 |
|    clip_fraction         | 0.0542       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.8          |
|    entropy_loss          | -2.65        |
|    explained_variance    | 0.0244       |
|    lagrangian_multiplier | 0.0795       |
|    learning_rate         | 0.0003       |
|    loss                  | 21.4         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00777     |
|    std                   | 0.914        |
|    value_loss            | 297          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6305393] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 25           |
|    time_elapsed          | 449          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.0065873857 |
|    clip_fraction         | 0.0635       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.06         |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.00932      |
|    lagrangian_multiplier | 0.0514       |
|    learning_rate         | 0.0003       |
|    loss                  | 68.7         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.0105      |
|    std                   | 1.03         |
|    value_loss            | 558          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2219447] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 28           |
|    time_elapsed          | 450          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.010802494  |
|    clip_fraction         | 0.0871       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 234          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0177      |
|    lagrangian_multiplier | 0.0568       |
|    learning_rate         | 0.0003       |
|    loss                  | 245          |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.0169      |
|    std                   | 0.974        |
|    value_loss            | 1.91e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2491703] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 28           |
|    time_elapsed          | 454          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.0074443636 |
|    clip_fraction         | 0.078        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.975        |
|    entropy_loss          | -2.64        |
|    explained_variance    | 0.051        |
|    lagrangian_multiplier | 0.0755       |
|    learning_rate         | 0.0003       |
|    loss                  | 21.6         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00837     |
|    std                   | 0.903        |
|    value_loss            | 263          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.84146076] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 29            |
|    time_elapsed          | 466           |
|    total_timesteps       | 59392         |
| train/                   |               |
|    approx_kl             | 0.008486351   |
|    clip_fraction         | 0.0684        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 249           |
|    entropy_loss          | -2.76         |
|    explained_variance    | 0.0179        |
|    lagrangian_multiplier | 0.0539        |
|    learning_rate         | 0.0003        |
|    loss                  | 87.8          |
|    n_updates             | 280           |
|    policy_gradient_loss  | -0.00974      |
|    std                   | 0.956         |
|    value_loss            | 573           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.894761]  |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 26           |
|    time_elapsed          | 467          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0045088585 |
|    clip_fraction         | 0.0326       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0446       |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.0309       |
|    lagrangian_multiplier | 0.0678       |
|    learning_rate         | 0.0003       |
|    loss                  | 19.4         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00529     |
|    std                   | 1.04         |
|    value_loss            | 203          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6080414] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 29           |
|    time_elapsed          | 471          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.0061363373 |
|    clip_fraction         | 0.0591       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.83         |
|    entropy_loss          | -2.64        |
|    explained_variance    | 0.0346       |
|    lagrangian_multiplier | 0.0619       |
|    learning_rate         | 0.0003       |
|    loss                  | 51.5         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00894     |
|    std                   | 0.91         |
|    value_loss            | 500          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.96644163] |
| time/                    |               |
|    fps                   | 107           |
|    iterations            | 25            |
|    time_elapsed          | 477           |
|    total_timesteps       | 51200         |
| train/                   |               |
|    approx_kl             | 0.00531632    |
|    clip_fraction         | 0.0371        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0085        |
|    entropy_loss          | -2.9          |
|    explained_variance    | 0.113         |
|    lagrangian_multiplier | 0.0627        |
|    learning_rate         | 0.0003        |
|    loss                  | 17            |
|    n_updates             | 240           |
|    policy_gradient_loss  | -0.0032       |
|    std                   | 1.03          |
|    value_loss            | 165           |
--------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.2167733] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 30           |
|    time_elapsed          | 482          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.006761029  |
|    clip_fraction         | 0.0664       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 207          |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0173      |
|    lagrangian_multiplier | 0.0813       |
|    learning_rate         | 0.0003       |
|    loss                  | 98.1         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.0104      |
|    std                   | 0.962        |
|    value_loss            | 1.16e+03     |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.5924378] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 27           |
|    time_elapsed          | 485          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.006618757  |
|    clip_fraction         | 0.0709       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0507       |
|    entropy_loss          | -2.9         |
|    explained_variance    | -0.000956    |
|    lagrangian_multiplier | 0.0724       |
|    learning_rate         | 0.0003       |
|    loss                  | 14.7         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 1.03         |
|    value_loss            | 165          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7157879] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 30           |
|    time_elapsed          | 487          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.005479219  |
|    clip_fraction         | 0.0462       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 7.12         |
|    entropy_loss          | -2.65        |
|    explained_variance    | 0.0334       |
|    lagrangian_multiplier | 0.0595       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.2         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00701     |
|    std                   | 0.916        |
|    value_loss            | 554          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2436068] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 31           |
|    time_elapsed          | 498          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0055624433 |
|    clip_fraction         | 0.0587       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 102          |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.00988      |
|    lagrangian_multiplier | 0.0817       |
|    learning_rate         | 0.0003       |
|    loss                  | 73.5         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00935     |
|    std                   | 0.957        |
|    value_loss            | 907          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-0.935097] |
| time/                    |             |
|    fps                   | 113         |
|    iterations            | 28          |
|    time_elapsed          | 503         |
|    total_timesteps       | 57344       |
| train/                   |             |
|    approx_kl             | 0.006100239 |
|    clip_fraction         | 0.0403      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0106      |
|    entropy_loss          | -2.89       |
|    explained_variance    | -0.000967   |
|    lagrangian_multiplier | 0.0765      |
|    learning_rate         | 0.0003      |
|    loss                  | 11.4        |
|    n_updates             | 270         |
|    policy_gradient_loss  | -0.00378    |
|    std                   | 1.02        |
|    value_loss            | 135         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.63438773] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 31            |
|    time_elapsed          | 503           |
|    total_timesteps       | 63488         |
| train/                   |               |
|    approx_kl             | 0.007534028   |
|    clip_fraction         | 0.0685        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 5.28          |
|    entropy_loss          | -2.67         |
|    explained_variance    | 0.0343        |
|    lagrangian_multiplier | 0.0604        |
|    learning_rate         | 0.0003        |
|    loss                  | 49.5          |
|    n_updates             | 300           |
|    policy_gradient_loss  | -0.00896      |
|    std                   | 0.929         |
|    value_loss            | 492           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.80666614] |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 26            |
|    time_elapsed          | 503           |
|    total_timesteps       | 53248         |
| train/                   |               |
|    approx_kl             | 0.0036636177  |
|    clip_fraction         | 0.0272        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0105        |
|    entropy_loss          | -2.9          |
|    explained_variance    | 0.0325        |
|    lagrangian_multiplier | 0.0732        |
|    learning_rate         | 0.0003        |
|    loss                  | 8.89          |
|    n_updates             | 250           |
|    policy_gradient_loss  | -0.00402      |
|    std                   | 1.03          |
|    value_loss            | 115           |
--------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.72391415] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 32            |
|    time_elapsed          | 514           |
|    total_timesteps       | 65536         |
| train/                   |               |
|    approx_kl             | 0.0066110585  |
|    clip_fraction         | 0.0709        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 203           |
|    entropy_loss          | -2.77         |
|    explained_variance    | -0.0385       |
|    lagrangian_multiplier | 0.0694        |
|    learning_rate         | 0.0003        |
|    loss                  | 120           |
|    n_updates             | 310           |
|    policy_gradient_loss  | -0.00966      |
|    std                   | 0.968         |
|    value_loss            | 944           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.79936033] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 32            |
|    time_elapsed          | 519           |
|    total_timesteps       | 65536         |
| train/                   |               |
|    approx_kl             | 0.007613858   |
|    clip_fraction         | 0.0639        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 4.02          |
|    entropy_loss          | -2.7          |
|    explained_variance    | 0.0366        |
|    lagrangian_multiplier | 0.063         |
|    learning_rate         | 0.0003        |
|    loss                  | 31.7          |
|    n_updates             | 310           |
|    policy_gradient_loss  | -0.00741      |
|    std                   | 0.943         |
|    value_loss            | 348           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.55784494] |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 27            |
|    time_elapsed          | 520           |
|    total_timesteps       | 55296         |
| train/                   |               |
|    approx_kl             | 0.0063669644  |
|    clip_fraction         | 0.0619        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 12.2          |
|    entropy_loss          | -2.92         |
|    explained_variance    | -0.0776       |
|    lagrangian_multiplier | 0.0722        |
|    learning_rate         | 0.0003        |
|    loss                  | 54.3          |
|    n_updates             | 260           |
|    policy_gradient_loss  | -0.0118       |
|    std                   | 1.05          |
|    value_loss            | 501           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.8885951] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 29           |
|    time_elapsed          | 521          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.0050032334 |
|    clip_fraction         | 0.0431       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00882      |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.00812      |
|    lagrangian_multiplier | 0.0707       |
|    learning_rate         | 0.0003       |
|    loss                  | 15.7         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00639     |
|    std                   | 1.02         |
|    value_loss            | 190          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9829981] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 33           |
|    time_elapsed          | 530          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0072906977 |
|    clip_fraction         | 0.0814       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 132          |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.00195      |
|    lagrangian_multiplier | 0.0722       |
|    learning_rate         | 0.0003       |
|    loss                  | 78.6         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00859     |
|    std                   | 0.978        |
|    value_loss            | 700          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.393889] |
| time/                    |             |
|    fps                   | 126         |
|    iterations            | 33          |
|    time_elapsed          | 535         |
|    total_timesteps       | 67584       |
| train/                   |             |
|    approx_kl             | 0.007498121 |
|    clip_fraction         | 0.0593      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.135       |
|    entropy_loss          | -2.71       |
|    explained_variance    | 0.0188      |
|    lagrangian_multiplier | 0.0699      |
|    learning_rate         | 0.0003      |
|    loss                  | 22.2        |
|    n_updates             | 320         |
|    policy_gradient_loss  | -0.0073     |
|    std                   | 0.94        |
|    value_loss            | 216         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.64357984] |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 28            |
|    time_elapsed          | 536           |
|    total_timesteps       | 57344         |
| train/                   |               |
|    approx_kl             | 0.006933822   |
|    clip_fraction         | 0.0721        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 1             |
|    entropy_loss          | -2.95         |
|    explained_variance    | -0.107        |
|    lagrangian_multiplier | 0.063         |
|    learning_rate         | 0.0003        |
|    loss                  | 22.7          |
|    n_updates             | 270           |
|    policy_gradient_loss  | -0.0124       |
|    std                   | 1.07          |
|    value_loss            | 242           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.6001104] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 30           |
|    time_elapsed          | 539          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.004153095  |
|    clip_fraction         | 0.0404       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00688      |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.0224      |
|    lagrangian_multiplier | 0.0738       |
|    learning_rate         | 0.0003       |
|    loss                  | 17.3         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00303     |
|    std                   | 1.02         |
|    value_loss            | 204          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0901275] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 34           |
|    time_elapsed          | 547          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0067248293 |
|    clip_fraction         | 0.0764       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 261          |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0228       |
|    lagrangian_multiplier | 0.0541       |
|    learning_rate         | 0.0003       |
|    loss                  | 68.3         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00859     |
|    std                   | 0.98         |
|    value_loss            | 374          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6806443] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 34           |
|    time_elapsed          | 552          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.005262327  |
|    clip_fraction         | 0.0514       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.2          |
|    entropy_loss          | -2.71        |
|    explained_variance    | 0.00533      |
|    lagrangian_multiplier | 0.0713       |
|    learning_rate         | 0.0003       |
|    loss                  | 25           |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00728     |
|    std                   | 0.943        |
|    value_loss            | 248          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.4966855] |
| time/                    |              |
|    fps                   | 107          |
|    iterations            | 29           |
|    time_elapsed          | 553          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.005720546  |
|    clip_fraction         | 0.0547       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.298        |
|    entropy_loss          | -2.98        |
|    explained_variance    | -0.00338     |
|    lagrangian_multiplier | 0.0764       |
|    learning_rate         | 0.0003       |
|    loss                  | 24.6         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00941     |
|    std                   | 1.08         |
|    value_loss            | 249          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.755139]  |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 31           |
|    time_elapsed          | 557          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0046955133 |
|    clip_fraction         | 0.044        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0109       |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0302       |
|    lagrangian_multiplier | 0.0735       |
|    learning_rate         | 0.0003       |
|    loss                  | 10           |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00481     |
|    std                   | 1.03         |
|    value_loss            | 112          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.110255]  |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 35           |
|    time_elapsed          | 563          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0082850475 |
|    clip_fraction         | 0.0792       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 253          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0299      |
|    lagrangian_multiplier | 0.0563       |
|    learning_rate         | 0.0003       |
|    loss                  | 75.3         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.0112      |
|    std                   | 0.968        |
|    value_loss            | 382          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7210935] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 35           |
|    time_elapsed          | 568          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.005513191  |
|    clip_fraction         | 0.0623       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.41         |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0327       |
|    lagrangian_multiplier | 0.061        |
|    learning_rate         | 0.0003       |
|    loss                  | 32.8         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00745     |
|    std                   | 0.949        |
|    value_loss            | 293          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.40204385] |
| time/                    |               |
|    fps                   | 107           |
|    iterations            | 30            |
|    time_elapsed          | 569           |
|    total_timesteps       | 61440         |
| train/                   |               |
|    approx_kl             | 0.006208678   |
|    clip_fraction         | 0.0539        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 7.28          |
|    entropy_loss          | -2.99         |
|    explained_variance    | -0.0546       |
|    lagrangian_multiplier | 0.0865        |
|    learning_rate         | 0.0003        |
|    loss                  | 30.6          |
|    n_updates             | 290           |
|    policy_gradient_loss  | -0.00963      |
|    std                   | 1.08          |
|    value_loss            | 425           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.8563244] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 32           |
|    time_elapsed          | 575          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.004329074  |
|    clip_fraction         | 0.0272       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0136       |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0125       |
|    lagrangian_multiplier | 0.0759       |
|    learning_rate         | 0.0003       |
|    loss                  | 8.99         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00409     |
|    std                   | 1.02         |
|    value_loss            | 107          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0520487] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 36           |
|    time_elapsed          | 579          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.00530003   |
|    clip_fraction         | 0.0619       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 214          |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0957      |
|    lagrangian_multiplier | 0.0695       |
|    learning_rate         | 0.0003       |
|    loss                  | 75.9         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00852     |
|    std                   | 0.962        |
|    value_loss            | 658          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0326903] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 36           |
|    time_elapsed          | 584          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0070337537 |
|    clip_fraction         | 0.0783       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.08         |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0316       |
|    lagrangian_multiplier | 0.0556       |
|    learning_rate         | 0.0003       |
|    loss                  | 60.2         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0103      |
|    std                   | 0.948        |
|    value_loss            | 606          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.31262767] |
| time/                    |               |
|    fps                   | 108           |
|    iterations            | 31            |
|    time_elapsed          | 585           |
|    total_timesteps       | 63488         |
| train/                   |               |
|    approx_kl             | 0.004557461   |
|    clip_fraction         | 0.0205        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00334       |
|    entropy_loss          | -2.99         |
|    explained_variance    | 0.0372        |
|    lagrangian_multiplier | 0.104         |
|    learning_rate         | 0.0003        |
|    loss                  | 6.11          |
|    n_updates             | 300           |
|    policy_gradient_loss  | -0.00181      |
|    std                   | 1.08          |
|    value_loss            | 86.5          |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.6840985] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 33           |
|    time_elapsed          | 593          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0042097718 |
|    clip_fraction         | 0.0326       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00529      |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.0111      |
|    lagrangian_multiplier | 0.078        |
|    learning_rate         | 0.0003       |
|    loss                  | 9.68         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00339     |
|    std                   | 1.01         |
|    value_loss            | 114          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3851826] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 37           |
|    time_elapsed          | 595          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.0069587147 |
|    clip_fraction         | 0.0705       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 189          |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0696      |
|    lagrangian_multiplier | 0.0691       |
|    learning_rate         | 0.0003       |
|    loss                  | 81.2         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.0108      |
|    std                   | 0.95         |
|    value_loss            | 670          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.95228815] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 37            |
|    time_elapsed          | 600           |
|    total_timesteps       | 75776         |
| train/                   |               |
|    approx_kl             | 0.007600109   |
|    clip_fraction         | 0.0759        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 13.1          |
|    entropy_loss          | -2.71         |
|    explained_variance    | 0.00977       |
|    lagrangian_multiplier | 0.0532        |
|    learning_rate         | 0.0003        |
|    loss                  | 69.8          |
|    n_updates             | 360           |
|    policy_gradient_loss  | -0.0101       |
|    std                   | 0.939         |
|    value_loss            | 587           |
--------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8080141] |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 32           |
|    time_elapsed          | 602          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.0044997754 |
|    clip_fraction         | 0.0298       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00641      |
|    entropy_loss          | -3           |
|    explained_variance    | -0.0306      |
|    lagrangian_multiplier | 0.0814       |
|    learning_rate         | 0.0003       |
|    loss                  | 9.41         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00265     |
|    std                   | 1.08         |
|    value_loss            | 116          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.73115045] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 34            |
|    time_elapsed          | 611           |
|    total_timesteps       | 69632         |
| train/                   |               |
|    approx_kl             | 0.007336777   |
|    clip_fraction         | 0.0777        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00599       |
|    entropy_loss          | -2.86         |
|    explained_variance    | -0.0231       |
|    lagrangian_multiplier | 0.075         |
|    learning_rate         | 0.0003        |
|    loss                  | 8.47          |
|    n_updates             | 330           |
|    policy_gradient_loss  | -0.00618      |
|    std                   | 1.01          |
|    value_loss            | 100           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2099584] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 38           |
|    time_elapsed          | 611          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.006521807  |
|    clip_fraction         | 0.0577       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 90.2         |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.0343      |
|    lagrangian_multiplier | 0.0761       |
|    learning_rate         | 0.0003       |
|    loss                  | 49.8         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00513     |
|    std                   | 0.949        |
|    value_loss            | 448          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.3412294] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 38           |
|    time_elapsed          | 616          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0040022293 |
|    clip_fraction         | 0.0417       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 2.64         |
|    entropy_loss          | -2.7         |
|    explained_variance    | 0.044        |
|    lagrangian_multiplier | 0.0774       |
|    learning_rate         | 0.0003       |
|    loss                  | 11.4         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00804     |
|    std                   | 0.94         |
|    value_loss            | 123          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8391498] |
| time/                    |              |
|    fps                   | 109          |
|    iterations            | 33           |
|    time_elapsed          | 619          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.006140958  |
|    clip_fraction         | 0.0507       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00512      |
|    entropy_loss          | -2.99        |
|    explained_variance    | 0.0183       |
|    lagrangian_multiplier | 0.094        |
|    learning_rate         | 0.0003       |
|    loss                  | 6.76         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00382     |
|    std                   | 1.07         |
|    value_loss            | 94.9         |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8342837] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 39           |
|    time_elapsed          | 627          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.007611421  |
|    clip_fraction         | 0.076        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 260          |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.0454      |
|    lagrangian_multiplier | 0.0558       |
|    learning_rate         | 0.0003       |
|    loss                  | 65.6         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00851     |
|    std                   | 0.936        |
|    value_loss            | 339          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5519213] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 35           |
|    time_elapsed          | 629          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.006307832  |
|    clip_fraction         | 0.0646       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00633      |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0733      |
|    lagrangian_multiplier | 0.0812       |
|    learning_rate         | 0.0003       |
|    loss                  | 8.01         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00589     |
|    std                   | 1            |
|    value_loss            | 103          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.39387366] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 39            |
|    time_elapsed          | 633           |
|    total_timesteps       | 79872         |
| train/                   |               |
|    approx_kl             | 0.0077295415  |
|    clip_fraction         | 0.0785        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 8.42          |
|    entropy_loss          | -2.71         |
|    explained_variance    | 0.0227        |
|    lagrangian_multiplier | 0.0574        |
|    learning_rate         | 0.0003        |
|    loss                  | 46.4          |
|    n_updates             | 380           |
|    policy_gradient_loss  | -0.00776      |
|    std                   | 0.944         |
|    value_loss            | 442           |
--------------------------------------------
------------------------------------------
| reward                   | [-0.475933] |
| time/                    |             |
|    fps                   | 109         |
|    iterations            | 34          |
|    time_elapsed          | 635         |
|    total_timesteps       | 69632       |
| train/                   |             |
|    approx_kl             | 0.006247934 |
|    clip_fraction         | 0.0467      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.00683     |
|    entropy_loss          | -2.96       |
|    explained_variance    | -0.00931    |
|    lagrangian_multiplier | 0.0861      |
|    learning_rate         | 0.0003      |
|    loss                  | 11.6        |
|    n_updates             | 330         |
|    policy_gradient_loss  | -0.00498    |
|    std                   | 1.05        |
|    value_loss            | 156         |
------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8338482] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 40           |
|    time_elapsed          | 643          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.00638828   |
|    clip_fraction         | 0.0615       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 158          |
|    entropy_loss          | -2.7         |
|    explained_variance    | -0.0758      |
|    lagrangian_multiplier | 0.0645       |
|    learning_rate         | 0.0003       |
|    loss                  | 66.2         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00702     |
|    std                   | 0.931        |
|    value_loss            | 578          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.63058025] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 36            |
|    time_elapsed          | 647           |
|    total_timesteps       | 73728         |
| train/                   |               |
|    approx_kl             | 0.006109897   |
|    clip_fraction         | 0.0388        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00329       |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.0262       |
|    lagrangian_multiplier | 0.085         |
|    learning_rate         | 0.0003        |
|    loss                  | 7.06          |
|    n_updates             | 350           |
|    policy_gradient_loss  | -0.00332      |
|    std                   | 0.991         |
|    value_loss            | 90            |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3763292] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 40           |
|    time_elapsed          | 649          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.008116831  |
|    clip_fraction         | 0.0935       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.43         |
|    entropy_loss          | -2.71        |
|    explained_variance    | 0.00264      |
|    lagrangian_multiplier | 0.0626       |
|    learning_rate         | 0.0003       |
|    loss                  | 28.2         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.0104      |
|    std                   | 0.947        |
|    value_loss            | 228          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.4706391] |
| time/                    |              |
|    fps                   | 109          |
|    iterations            | 35           |
|    time_elapsed          | 652          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0022705933 |
|    clip_fraction         | 0.0345       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00536      |
|    entropy_loss          | -2.94        |
|    explained_variance    | 0.133        |
|    lagrangian_multiplier | 0.0926       |
|    learning_rate         | 0.0003       |
|    loss                  | 9.04         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.0029      |
|    std                   | 1.05         |
|    value_loss            | 123          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.7798402] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 41           |
|    time_elapsed          | 659          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0071481112 |
|    clip_fraction         | 0.0836       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 264          |
|    entropy_loss          | -2.71        |
|    explained_variance    | -0.0265      |
|    lagrangian_multiplier | 0.0613       |
|    learning_rate         | 0.0003       |
|    loss                  | 60.1         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00754     |
|    std                   | 0.941        |
|    value_loss            | 324          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.35834986] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 37            |
|    time_elapsed          | 665           |
|    total_timesteps       | 75776         |
| train/                   |               |
|    approx_kl             | 0.0038421722  |
|    clip_fraction         | 0.0335        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00426       |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.0533        |
|    lagrangian_multiplier | 0.0904        |
|    learning_rate         | 0.0003        |
|    loss                  | 5.86          |
|    n_updates             | 360           |
|    policy_gradient_loss  | -0.00288      |
|    std                   | 0.993         |
|    value_loss            | 72.7          |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.9394597] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 41           |
|    time_elapsed          | 666          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.006348145  |
|    clip_fraction         | 0.0652       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.82         |
|    entropy_loss          | -2.71        |
|    explained_variance    | -0.00558     |
|    lagrangian_multiplier | 0.0633       |
|    learning_rate         | 0.0003       |
|    loss                  | 38.1         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00802     |
|    std                   | 0.934        |
|    value_loss            | 395          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5137732] |
| time/                    |              |
|    fps                   | 110          |
|    iterations            | 36           |
|    time_elapsed          | 668          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0070563983 |
|    clip_fraction         | 0.054        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00248      |
|    entropy_loss          | -2.93        |
|    explained_variance    | -0.169       |
|    lagrangian_multiplier | 0.0969       |
|    learning_rate         | 0.0003       |
|    loss                  | 4.75         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00433     |
|    std                   | 1.04         |
|    value_loss            | 59.3         |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.91726196] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 42            |
|    time_elapsed          | 675           |
|    total_timesteps       | 86016         |
| train/                   |               |
|    approx_kl             | 0.0047143856  |
|    clip_fraction         | 0.0291        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 131           |
|    entropy_loss          | -2.71         |
|    explained_variance    | -0.0268       |
|    lagrangian_multiplier | 0.0855        |
|    learning_rate         | 0.0003        |
|    loss                  | 98            |
|    n_updates             | 410           |
|    policy_gradient_loss  | -0.00606      |
|    std                   | 0.937         |
|    value_loss            | 1.05e+03      |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.70155853] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 42            |
|    time_elapsed          | 682           |
|    total_timesteps       | 86016         |
| train/                   |               |
|    approx_kl             | 0.004566596   |
|    clip_fraction         | 0.0403        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.365         |
|    entropy_loss          | -2.68         |
|    explained_variance    | 0.0422        |
|    lagrangian_multiplier | 0.0518        |
|    learning_rate         | 0.0003        |
|    loss                  | 54.7          |
|    n_updates             | 410           |
|    policy_gradient_loss  | -0.00634      |
|    std                   | 0.927         |
|    value_loss            | 476           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.48168382] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 38            |
|    time_elapsed          | 683           |
|    total_timesteps       | 77824         |
| train/                   |               |
|    approx_kl             | 0.0043291114  |
|    clip_fraction         | 0.0253        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00247       |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.00621       |
|    lagrangian_multiplier | 0.0869        |
|    learning_rate         | 0.0003        |
|    loss                  | 8.21          |
|    n_updates             | 370           |
|    policy_gradient_loss  | -0.00219      |
|    std                   | 0.993         |
|    value_loss            | 105           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.48377326] |
| time/                    |               |
|    fps                   | 110           |
|    iterations            | 37            |
|    time_elapsed          | 685           |
|    total_timesteps       | 75776         |
| train/                   |               |
|    approx_kl             | 0.0032894742  |
|    clip_fraction         | 0.028         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00354       |
|    entropy_loss          | -2.93         |
|    explained_variance    | -0.00707      |
|    lagrangian_multiplier | 0.0835        |
|    learning_rate         | 0.0003        |
|    loss                  | 5.61          |
|    n_updates             | 360           |
|    policy_gradient_loss  | -0.00151      |
|    std                   | 1.05          |
|    value_loss            | 75.2          |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.6987253] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 43           |
|    time_elapsed          | 691          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0070549194 |
|    clip_fraction         | 0.0648       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 156          |
|    entropy_loss          | -2.7         |
|    explained_variance    | -0.0487      |
|    lagrangian_multiplier | 0.0675       |
|    learning_rate         | 0.0003       |
|    loss                  | 99.2         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00653     |
|    std                   | 0.928        |
|    value_loss            | 908          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.119103]  |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 43           |
|    time_elapsed          | 698          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0031941554 |
|    clip_fraction         | 0.0391       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.233        |
|    entropy_loss          | -2.69        |
|    explained_variance    | 0.0394       |
|    lagrangian_multiplier | 0.066        |
|    learning_rate         | 0.0003       |
|    loss                  | 38.1         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00621     |
|    std                   | 0.938        |
|    value_loss            | 380          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.36829627] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 39            |
|    time_elapsed          | 701           |
|    total_timesteps       | 79872         |
| train/                   |               |
|    approx_kl             | 0.0051220935  |
|    clip_fraction         | 0.0284        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00198       |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.00364       |
|    lagrangian_multiplier | 0.0949        |
|    learning_rate         | 0.0003        |
|    loss                  | 3.3           |
|    n_updates             | 380           |
|    policy_gradient_loss  | -0.00248      |
|    std                   | 1             |
|    value_loss            | 45.1          |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.46969134] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 44            |
|    time_elapsed          | 707           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.0073411013  |
|    clip_fraction         | 0.0784        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 223           |
|    entropy_loss          | -2.71         |
|    explained_variance    | -0.0587       |
|    lagrangian_multiplier | 0.0636        |
|    learning_rate         | 0.0003        |
|    loss                  | 70.7          |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.0106       |
|    std                   | 0.942         |
|    value_loss            | 501           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.4635016] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 44           |
|    time_elapsed          | 714          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.008059321  |
|    clip_fraction         | 0.0705       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 5.91         |
|    entropy_loss          | -2.71        |
|    explained_variance    | 0.0181       |
|    lagrangian_multiplier | 0.0619       |
|    learning_rate         | 0.0003       |
|    loss                  | 29.9         |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.0106      |
|    std                   | 0.945        |
|    value_loss            | 288          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.50456727] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 40            |
|    time_elapsed          | 719           |
|    total_timesteps       | 81920         |
| train/                   |               |
|    approx_kl             | 0.0027877023  |
|    clip_fraction         | 0.0129        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00163       |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.0211        |
|    lagrangian_multiplier | 0.0951        |
|    learning_rate         | 0.0003        |
|    loss                  | 4.72          |
|    n_updates             | 390           |
|    policy_gradient_loss  | -0.00211      |
|    std                   | 1.01          |
|    value_loss            | 61.4          |
--------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.9457706] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 45           |
|    time_elapsed          | 723          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.0062021976 |
|    clip_fraction         | 0.0529       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 255          |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.0309      |
|    lagrangian_multiplier | 0.0752       |
|    learning_rate         | 0.0003       |
|    loss                  | 120          |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00849     |
|    std                   | 0.946        |
|    value_loss            | 1.23e+03     |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2343369] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 45           |
|    time_elapsed          | 731          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.0053975764 |
|    clip_fraction         | 0.0486       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 5.21         |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0201       |
|    lagrangian_multiplier | 0.0804       |
|    learning_rate         | 0.0003       |
|    loss                  | 21.6         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.01        |
|    std                   | 0.953        |
|    value_loss            | 248          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.5419539] |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 38           |
|    time_elapsed          | 736          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0054172575 |
|    clip_fraction         | 0.0325       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00432      |
|    entropy_loss          | -2.91        |
|    explained_variance    | -0.276       |
|    lagrangian_multiplier | 0.0959       |
|    learning_rate         | 0.0003       |
|    loss                  | 7.06         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00212     |
|    std                   | 1.03         |
|    value_loss            | 95.4         |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5941749] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 41           |
|    time_elapsed          | 737          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0034503234 |
|    clip_fraction         | 0.0265       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00217      |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.00295     |
|    lagrangian_multiplier | 0.0954       |
|    learning_rate         | 0.0003       |
|    loss                  | 3.67         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.0017      |
|    std                   | 0.997        |
|    value_loss            | 52.4         |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9357858] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 46           |
|    time_elapsed          | 740          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0051325257 |
|    clip_fraction         | 0.0492       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 260          |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.028       |
|    lagrangian_multiplier | 0.0654       |
|    learning_rate         | 0.0003       |
|    loss                  | 117          |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.00658     |
|    std                   | 0.946        |
|    value_loss            | 980          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.5413445] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 46           |
|    time_elapsed          | 747          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0063168406 |
|    clip_fraction         | 0.0703       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.356        |
|    entropy_loss          | -2.75        |
|    explained_variance    | 0.0635       |
|    lagrangian_multiplier | 0.0754       |
|    learning_rate         | 0.0003       |
|    loss                  | 14.7         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.0126      |
|    std                   | 0.967        |
|    value_loss            | 130          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.5334996] |
| time/                    |              |
|    fps                   | 106          |
|    iterations            | 39           |
|    time_elapsed          | 752          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0076958914 |
|    clip_fraction         | 0.0883       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.753        |
|    entropy_loss          | -2.92        |
|    explained_variance    | -0.0228      |
|    lagrangian_multiplier | 0.0838       |
|    learning_rate         | 0.0003       |
|    loss                  | 55.9         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.0104      |
|    std                   | 1.05         |
|    value_loss            | 653          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.45254955] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 42            |
|    time_elapsed          | 755           |
|    total_timesteps       | 86016         |
| train/                   |               |
|    approx_kl             | 0.005163407   |
|    clip_fraction         | 0.0439        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0042        |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.0501        |
|    lagrangian_multiplier | 0.0962        |
|    learning_rate         | 0.0003        |
|    loss                  | 5.44          |
|    n_updates             | 410           |
|    policy_gradient_loss  | -0.00403      |
|    std                   | 0.985         |
|    value_loss            | 73.4          |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.7328551] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 47           |
|    time_elapsed          | 756          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.009302018  |
|    clip_fraction         | 0.0888       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 265          |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.045       |
|    lagrangian_multiplier | 0.0553       |
|    learning_rate         | 0.0003       |
|    loss                  | 147          |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.0138      |
|    std                   | 0.944        |
|    value_loss            | 1.15e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.49107045] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 47            |
|    time_elapsed          | 763           |
|    total_timesteps       | 96256         |
| train/                   |               |
|    approx_kl             | 0.0069441693  |
|    clip_fraction         | 0.0731        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 5.94          |
|    entropy_loss          | -2.76         |
|    explained_variance    | 0.00114       |
|    lagrangian_multiplier | 0.0609        |
|    learning_rate         | 0.0003        |
|    loss                  | 42.7          |
|    n_updates             | 460           |
|    policy_gradient_loss  | -0.00823      |
|    std                   | 0.963         |
|    value_loss            | 383           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.6668501] |
| time/                    |              |
|    fps                   | 106          |
|    iterations            | 40           |
|    time_elapsed          | 769          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.006173088  |
|    clip_fraction         | 0.069        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.75         |
|    entropy_loss          | -2.92        |
|    explained_variance    | -0.214       |
|    lagrangian_multiplier | 0.0735       |
|    learning_rate         | 0.0003       |
|    loss                  | 38.9         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00659     |
|    std                   | 1.04         |
|    value_loss            | 410          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-2.098947] |
| time/                    |             |
|    fps                   | 127         |
|    iterations            | 48          |
|    time_elapsed          | 772         |
|    total_timesteps       | 98304       |
| train/                   |             |
|    approx_kl             | 0.008647297 |
|    clip_fraction         | 0.0879      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 264         |
|    entropy_loss          | -2.72       |
|    explained_variance    | -0.144      |
|    lagrangian_multiplier | 0.0572      |
|    learning_rate         | 0.0003      |
|    loss                  | 52.8        |
|    n_updates             | 470         |
|    policy_gradient_loss  | -0.00941    |
|    std                   | 0.938       |
|    value_loss            | 234         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.29612923] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 43            |
|    time_elapsed          | 774           |
|    total_timesteps       | 88064         |
| train/                   |               |
|    approx_kl             | 0.003995846   |
|    clip_fraction         | 0.0239        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00487       |
|    entropy_loss          | -2.8          |
|    explained_variance    | 0.0535        |
|    lagrangian_multiplier | 0.0915        |
|    learning_rate         | 0.0003        |
|    loss                  | 6.57          |
|    n_updates             | 420           |
|    policy_gradient_loss  | -0.0015       |
|    std                   | 0.978         |
|    value_loss            | 84.9          |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.28560126] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 48            |
|    time_elapsed          | 779           |
|    total_timesteps       | 98304         |
| train/                   |               |
|    approx_kl             | 0.007859306   |
|    clip_fraction         | 0.0811        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 1.83          |
|    entropy_loss          | -2.75         |
|    explained_variance    | 0.0259        |
|    lagrangian_multiplier | 0.0424        |
|    learning_rate         | 0.0003        |
|    loss                  | 104           |
|    n_updates             | 470           |
|    policy_gradient_loss  | -0.0105       |
|    std                   | 0.961         |
|    value_loss            | 757           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.5699774] |
| time/                    |              |
|    fps                   | 106          |
|    iterations            | 41           |
|    time_elapsed          | 785          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0064428104 |
|    clip_fraction         | 0.0803       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.26         |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.105       |
|    lagrangian_multiplier | 0.0646       |
|    learning_rate         | 0.0003       |
|    loss                  | 54.8         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00781     |
|    std                   | 1.02         |
|    value_loss            | 529          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.71469593] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 49            |
|    time_elapsed          | 788           |
|    total_timesteps       | 100352        |
| train/                   |               |
|    approx_kl             | 0.006728268   |
|    clip_fraction         | 0.0726        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 263           |
|    entropy_loss          | -2.71         |
|    explained_variance    | -0.0979       |
|    lagrangian_multiplier | 0.063         |
|    learning_rate         | 0.0003        |
|    loss                  | 52.8          |
|    n_updates             | 480           |
|    policy_gradient_loss  | -0.00967      |
|    std                   | 0.943         |
|    value_loss            | 252           |
--------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
--------------------------------------------
| reward                   | [-0.48867667] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 44            |
|    time_elapsed          | 792           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.005192586   |
|    clip_fraction         | 0.0416        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00183       |
|    entropy_loss          | -2.79         |
|    explained_variance    | -0.0108       |
|    lagrangian_multiplier | 0.0866        |
|    learning_rate         | 0.0003        |
|    loss                  | 7.06          |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.00408      |
|    std                   | 0.979         |
|    value_loss            | 88.8          |
--------------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñÖ‚ñà‚ñá‚ñá‚ñá‚ñà
wandb:             train/approx_kl ‚ñÅ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñá‚ñÜ‚ñÑ
wandb:         train/clip_fraction ‚ñÅ‚ñÑ‚ñÑ‚ñá‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÇ‚ñÜ‚ñá‚ñÑ‚ñà‚ñà‚ñÜ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÖ‚ñà‚ñá‚ñÜ‚ñÉ‚ñÜ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÉ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:          train/entropy_loss ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá
wandb:    train/explained_variance ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÅ‚ñÉ
wandb: train/lagrangian_multiplier ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÉ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñá‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÅ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÖ
wandb:                   train/std ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:            train/value_loss ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                      reward -0.7147
wandb:             train/approx_kl 0.00673
wandb:         train/clip_fraction 0.07256
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 262.69117
wandb:          train/entropy_loss -2.7125
wandb:    train/explained_variance -0.09791
wandb: train/lagrangian_multiplier 0.063
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 52.81827
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00967
wandb:                   train/std 0.9427
wandb:            train/value_loss 251.88591
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/5awc8gxe
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_225403-5awc8gxe/logs
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/usr/lib/python3.9/threading.py", line 954, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.9/threading.py", line 892, in run
    self._target(*self._args, **self._kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 277, in check_stop_status
    self._loop_check_status(
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 215, in _loop_check_status
    local_handle = request()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface.py", line 787, in deliver_stop_status
    return self._deliver_stop_status(status)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py", line 585, in _deliver_stop_status
    return self._deliver_record(record)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py", line 560, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
srun: error: gail.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-0.8666956] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 49           |
|    time_elapsed          | 796          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.0040358    |
|    clip_fraction         | 0.0351       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.25         |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.0304       |
|    lagrangian_multiplier | 0.0779       |
|    learning_rate         | 0.0003       |
|    loss                  | 36.1         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.00638     |
|    std                   | 0.969        |
|    value_loss            | 400          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.027 MB of 0.027 MB uploaded (0.000 MB deduped)wandb: \ 0.027 MB of 0.027 MB uploaded (0.000 MB deduped)wandb: | 0.076 MB of 0.076 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñá‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñà‚ñá‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñà‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñÜ
wandb:             train/approx_kl ‚ñÇ‚ñá‚ñÖ‚ñÑ‚ñá‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÑ‚ñá‚ñá‚ñá‚ñÑ‚ñÜ‚ñá‚ñÇ‚ñá‚ñà‚ñÉ‚ñÅ‚ñà‚ñÑ‚ñÜ‚ñá‚ñÇ
wandb:         train/clip_fraction ‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÇ‚ñÜ‚ñà‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÅ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÜ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñà‚ñÅ‚ñÉ‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ
wandb:          train/entropy_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ
wandb:    train/explained_variance ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÜ
wandb: train/lagrangian_multiplier ‚ñÜ‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñá‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÑ‚ñÅ‚ñà
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñá‚ñÉ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñÇ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñà‚ñÉ‚ñÑ‚ñÇ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÖ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÖ
wandb:                   train/std ‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ
wandb:            train/value_loss ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÜ‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÉ
wandb: 
wandb: Run summary:
wandb:                      reward -0.8667
wandb:             train/approx_kl 0.00404
wandb:         train/clip_fraction 0.03511
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 4.24554
wandb:          train/entropy_loss -2.75621
wandb:    train/explained_variance 0.03036
wandb: train/lagrangian_multiplier 0.07788
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 36.06934
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00638
wandb:                   train/std 0.96929
wandb:            train/value_loss 400.4942
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/uvp0oa2t
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_225403-uvp0oa2t/logs
srun: error: dqn.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-1.5877124] |
| time/                    |              |
|    fps                   | 107          |
|    iterations            | 42           |
|    time_elapsed          | 802          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.01022693   |
|    clip_fraction         | 0.0941       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.21         |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.107       |
|    lagrangian_multiplier | 0.0507       |
|    learning_rate         | 0.0003       |
|    loss                  | 50.3         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.0111      |
|    std                   | 0.998        |
|    value_loss            | 404          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.4165883] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 45           |
|    time_elapsed          | 810          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.00397395   |
|    clip_fraction         | 0.024        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0023       |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.026        |
|    lagrangian_multiplier | 0.0982       |
|    learning_rate         | 0.0003       |
|    loss                  | 3.32         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00198     |
|    std                   | 0.977        |
|    value_loss            | 48.1         |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8367469] |
| time/                    |              |
|    fps                   | 107          |
|    iterations            | 43           |
|    time_elapsed          | 818          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.00852592   |
|    clip_fraction         | 0.0742       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 9.73         |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0789      |
|    lagrangian_multiplier | 0.0678       |
|    learning_rate         | 0.0003       |
|    loss                  | 50.3         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00756     |
|    std                   | 0.981        |
|    value_loss            | 502          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.3252627] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 46           |
|    time_elapsed          | 828          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0050849267 |
|    clip_fraction         | 0.0359       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00356      |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0649       |
|    lagrangian_multiplier | 0.0936       |
|    learning_rate         | 0.0003       |
|    loss                  | 6.19         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.00268     |
|    std                   | 0.97         |
|    value_loss            | 79.7         |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.48878333] |
| time/                    |               |
|    fps                   | 107           |
|    iterations            | 44            |
|    time_elapsed          | 835           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.007888114   |
|    clip_fraction         | 0.0641        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 11            |
|    entropy_loss          | -2.81         |
|    explained_variance    | -0.039        |
|    lagrangian_multiplier | 0.0621        |
|    learning_rate         | 0.0003        |
|    loss                  | 56.4          |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.00927      |
|    std                   | 0.992         |
|    value_loss            | 573           |
--------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114549.4
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_230804-wy5oyvhe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/wy5oyvhe
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114549.5
--------------------------------------------
| reward                   | [-0.31401184] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 47            |
|    time_elapsed          | 846           |
|    total_timesteps       | 96256         |
| train/                   |               |
|    approx_kl             | 0.005362414   |
|    clip_fraction         | 0.047         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00105       |
|    entropy_loss          | -2.78         |
|    explained_variance    | -0.0324       |
|    lagrangian_multiplier | 0.0938        |
|    learning_rate         | 0.0003        |
|    loss                  | 5.47          |
|    n_updates             | 460           |
|    policy_gradient_loss  | -0.00394      |
|    std                   | 0.976         |
|    value_loss            | 76.2          |
--------------------------------------------
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_230811-zrkjmj9p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2/runs/zrkjmj9p
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.7817741] |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 45           |
|    time_elapsed          | 851          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.008452678  |
|    clip_fraction         | 0.0656       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.43         |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0425      |
|    lagrangian_multiplier | 0.0652       |
|    learning_rate         | 0.0003       |
|    loss                  | 45.4         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00798     |
|    std                   | 0.99         |
|    value_loss            | 499          |
-------------------------------------------
Using cpu device
------------------------------------
| reward             | [-0.668335] |
| time/              |             |
|    fps             | 136         |
|    iterations      | 1           |
|    time_elapsed    | 15          |
|    total_timesteps | 2048        |
------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
Using cpu device
-------------------------------------
| reward             | [-0.3547912] |
| time/              |              |
|    fps             | 135          |
|    iterations      | 1            |
|    time_elapsed    | 15           |
|    total_timesteps | 2048         |
-------------------------------------
--------------------------------------------
| reward                   | [-0.47320026] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 48            |
|    time_elapsed          | 864           |
|    total_timesteps       | 98304         |
| train/                   |               |
|    approx_kl             | 0.006249495   |
|    clip_fraction         | 0.0545        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0033        |
|    entropy_loss          | -2.77         |
|    explained_variance    | -0.0293       |
|    lagrangian_multiplier | 0.0865        |
|    learning_rate         | 0.0003        |
|    loss                  | 5.39          |
|    n_updates             | 470           |
|    policy_gradient_loss  | -0.00383      |
|    std                   | 0.969         |
|    value_loss            | 69.2          |
--------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.4622754] |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 46           |
|    time_elapsed          | 867          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0061184424 |
|    clip_fraction         | 0.0524       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.069        |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.233       |
|    lagrangian_multiplier | 0.0807       |
|    learning_rate         | 0.0003       |
|    loss                  | 15.7         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.00557     |
|    std                   | 0.988        |
|    value_loss            | 225          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.6816455] |
| time/                    |              |
|    fps                   | 131          |
|    iterations            | 2            |
|    time_elapsed          | 31           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0075263525 |
|    clip_fraction         | 0.0672       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 43.2         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0117       |
|    lagrangian_multiplier | 0.0645       |
|    learning_rate         | 0.0003       |
|    loss                  | 43.5         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00649     |
|    std                   | 1.01         |
|    value_loss            | 413          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.38073632] |
| time/                    |               |
|    fps                   | 131           |
|    iterations            | 2             |
|    time_elapsed          | 31            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.006224297   |
|    clip_fraction         | 0.0445        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 224           |
|    entropy_loss          | -2.82         |
|    explained_variance    | -0.0133       |
|    lagrangian_multiplier | 0.0529        |
|    learning_rate         | 0.0003        |
|    loss                  | 98.1          |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00659      |
|    std                   | 0.983         |
|    value_loss            | 635           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.37555242] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 49            |
|    time_elapsed          | 882           |
|    total_timesteps       | 100352        |
| train/                   |               |
|    approx_kl             | 0.005856149   |
|    clip_fraction         | 0.0628        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00237       |
|    entropy_loss          | -2.77         |
|    explained_variance    | 0.00531       |
|    lagrangian_multiplier | 0.0958        |
|    learning_rate         | 0.0003        |
|    loss                  | 4.38          |
|    n_updates             | 480           |
|    policy_gradient_loss  | -0.00523      |
|    std                   | 0.97          |
|    value_loss            | 61.5          |
--------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
--------------------------------------------
| reward                   | [-0.80786324] |
| time/                    |               |
|    fps                   | 108           |
|    iterations            | 47            |
|    time_elapsed          | 884           |
|    total_timesteps       | 96256         |
| train/                   |               |
|    approx_kl             | 0.0065833637  |
|    clip_fraction         | 0.071         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 6.69          |
|    entropy_loss          | -2.81         |
|    explained_variance    | -0.0879       |
|    lagrangian_multiplier | 0.0714        |
|    learning_rate         | 0.0003        |
|    loss                  | 42.1          |
|    n_updates             | 460           |
|    policy_gradient_loss  | -0.01         |
|    std                   | 0.987         |
|    value_loss            | 464           |
--------------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÅ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:             train/approx_kl ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñá‚ñÅ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÖ
wandb:         train/clip_fraction ‚ñÖ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñÅ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÉ‚ñá‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÜ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÜ‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÜ‚ñÉ‚ñÖ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:          train/entropy_loss ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:    train/explained_variance ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÅ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñà‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÖ
wandb: train/lagrangian_multiplier ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÖ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñá‚ñÑ‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÜ‚ñÅ‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñà‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÅ‚ñÖ‚ñÅ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ
wandb:                   train/std ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:            train/value_loss ‚ñÜ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñá‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                      reward -0.37555
wandb:             train/approx_kl 0.00586
wandb:         train/clip_fraction 0.06279
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 0.00237
wandb:          train/entropy_loss -2.7661
wandb:    train/explained_variance 0.00531
wandb: train/lagrangian_multiplier 0.09582
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 4.38458
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00523
wandb:                   train/std 0.96994
wandb:            train/value_loss 61.52171
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16/runs/9gbrjuiu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_225403-9gbrjuiu/logs
-------------------------------------------
| reward                   | [-1.2695674] |
| time/                    |              |
|    fps                   | 130          |
|    iterations            | 3            |
|    time_elapsed          | 47           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0078098415 |
|    clip_fraction         | 0.0574       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 63.5         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0161      |
|    lagrangian_multiplier | 0.0513       |
|    learning_rate         | 0.0003       |
|    loss                  | 101          |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00716     |
|    std                   | 0.995        |
|    value_loss            | 644          |
-------------------------------------------
srun: error: ddpg.ist.berkeley.edu: task 0: Exited with exit code 1
--------------------------------------------
| reward                   | [-0.80768406] |
| time/                    |               |
|    fps                   | 130           |
|    iterations            | 3             |
|    time_elapsed          | 47            |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.003939513   |
|    clip_fraction         | 0.0254        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 43.5          |
|    entropy_loss          | -2.8          |
|    explained_variance    | -0.0166       |
|    lagrangian_multiplier | 0.0668        |
|    learning_rate         | 0.0003        |
|    loss                  | 39.5          |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00289      |
|    std                   | 0.98          |
|    value_loss            | 403           |
--------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114549.6
-------------------------------------------
| reward                   | [-1.1269515] |
| time/                    |              |
|    fps                   | 109          |
|    iterations            | 48           |
|    time_elapsed          | 900          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.0071191085 |
|    clip_fraction         | 0.0722       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.43         |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.197       |
|    lagrangian_multiplier | 0.0761       |
|    learning_rate         | 0.0003       |
|    loss                  | 26           |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.0104      |
|    std                   | 1.01         |
|    value_loss            | 292          |
-------------------------------------------
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_230907-o2arbqkn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2/runs/o2arbqkn
-------------------------------------------
| reward                   | [-0.635933]  |
| time/                    |              |
|    fps                   | 130          |
|    iterations            | 4            |
|    time_elapsed          | 62           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.0050983685 |
|    clip_fraction         | 0.0488       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 207          |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.00543     |
|    lagrangian_multiplier | 0.0591       |
|    learning_rate         | 0.0003       |
|    loss                  | 113          |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00685     |
|    std                   | 0.992        |
|    value_loss            | 754          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.992321]  |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 4            |
|    time_elapsed          | 63           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.0048660054 |
|    clip_fraction         | 0.0423       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 236          |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0145       |
|    lagrangian_multiplier | 0.0509       |
|    learning_rate         | 0.0003       |
|    loss                  | 154          |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00557     |
|    std                   | 0.98         |
|    value_loss            | 898          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0973903] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 5            |
|    time_elapsed          | 78           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0050472235 |
|    clip_fraction         | 0.0335       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 104          |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0167       |
|    lagrangian_multiplier | 0.0702       |
|    learning_rate         | 0.0003       |
|    loss                  | 42           |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00541     |
|    std                   | 0.983        |
|    value_loss            | 315          |
-------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.44683725] |
| time/              |               |
|    fps             | 119           |
|    iterations      | 1             |
|    time_elapsed    | 17            |
|    total_timesteps | 2048          |
--------------------------------------
-------------------------------------------
| reward                   | [-1.4195193] |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 49           |
|    time_elapsed          | 923          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.00613794   |
|    clip_fraction         | 0.0562       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.638        |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.144       |
|    lagrangian_multiplier | 0.0707       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.7         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.00723     |
|    std                   | 1.01         |
|    value_loss            | 494          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
-------------------------------------------
| reward                   | [-1.1371247] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 5            |
|    time_elapsed          | 79           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0058272067 |
|    clip_fraction         | 0.0487       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 106          |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.02         |
|    lagrangian_multiplier | 0.0718       |
|    learning_rate         | 0.0003       |
|    loss                  | 40.2         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00624     |
|    std                   | 0.982        |
|    value_loss            | 393          |
-------------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÑ
wandb:             train/approx_kl ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÜ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ
wandb:         train/clip_fraction ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÖ‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÅ
wandb:          train/entropy_loss ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ
wandb:    train/explained_variance ‚ñÜ‚ñÉ‚ñà‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÉ
wandb: train/lagrangian_multiplier ‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñà‚ñÖ‚ñá‚ñÜ‚ñá‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñà‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñà‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÑ
wandb:                   train/std ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ
wandb:            train/value_loss ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ
wandb: 
wandb: Run summary:
wandb:                      reward -1.41952
wandb:             train/approx_kl 0.00614
wandb:         train/clip_fraction 0.0562
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 0.63817
wandb:          train/entropy_loss -2.85313
wandb:    train/explained_variance -0.14354
wandb: train/lagrangian_multiplier 0.07073
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 46.71583
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00723
wandb:                   train/std 1.00768
wandb:            train/value_loss 494.12198
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/2ymaj5gp
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231227_065403-2ymaj5gp/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/usr/lib/python3.9/threading.py", line 954, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.9/threading.py", line 892, in run
    self._target(*self._args, **self._kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 259, in check_network_status
    self._loop_check_status(
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 215, in _loop_check_status
    local_handle = request()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface.py", line 795, in deliver_network_status
    return self._deliver_network_status(status)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py", line 601, in _deliver_network_status
    return self._deliver_record(record)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py", line 560, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
srun: error: airl.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-0.4353696] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 6            |
|    time_elapsed          | 94           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0050554485 |
|    clip_fraction         | 0.0517       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 98.8         |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.00846     |
|    lagrangian_multiplier | 0.0738       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.7         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.0069      |
|    std                   | 0.992        |
|    value_loss            | 485          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5869776] |
| time/                    |              |
|    fps                   | 116          |
|    iterations            | 2            |
|    time_elapsed          | 35           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0061940216 |
|    clip_fraction         | 0.0431       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 203          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0178      |
|    lagrangian_multiplier | 0.0554       |
|    learning_rate         | 0.0003       |
|    loss                  | 91.7         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00529     |
|    std                   | 1            |
|    value_loss            | 673          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.44860652] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 6             |
|    time_elapsed          | 95            |
|    total_timesteps       | 12288         |
| train/                   |               |
|    approx_kl             | 0.0061910856  |
|    clip_fraction         | 0.0524        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 205           |
|    entropy_loss          | -2.79         |
|    explained_variance    | -0.0244       |
|    lagrangian_multiplier | 0.0685        |
|    learning_rate         | 0.0003        |
|    loss                  | 92.4          |
|    n_updates             | 50            |
|    policy_gradient_loss  | -0.00839      |
|    std                   | 0.972         |
|    value_loss            | 811           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2467268] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 7            |
|    time_elapsed          | 110          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.0063997777 |
|    clip_fraction         | 0.0618       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 180          |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0115      |
|    lagrangian_multiplier | 0.0721       |
|    learning_rate         | 0.0003       |
|    loss                  | 59.9         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00846     |
|    std                   | 0.984        |
|    value_loss            | 454          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2490355] |
| time/                    |              |
|    fps                   | 115          |
|    iterations            | 3            |
|    time_elapsed          | 53           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.006688306  |
|    clip_fraction         | 0.0425       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 116          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0408      |
|    lagrangian_multiplier | 0.0643       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.7         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00532     |
|    std                   | 1            |
|    value_loss            | 389          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6427355] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 7            |
|    time_elapsed          | 111          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.0039024109 |
|    clip_fraction         | 0.0207       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 193          |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0225       |
|    lagrangian_multiplier | 0.0573       |
|    learning_rate         | 0.0003       |
|    loss                  | 63.5         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00307     |
|    std                   | 0.961        |
|    value_loss            | 468          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0421965] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 8            |
|    time_elapsed          | 126          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.006110252  |
|    clip_fraction         | 0.054        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 53.1         |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.000414     |
|    lagrangian_multiplier | 0.0766       |
|    learning_rate         | 0.0003       |
|    loss                  | 24.6         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00743     |
|    std                   | 0.992        |
|    value_loss            | 249          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114549.7
--------------------------------------------
| reward                   | [-0.91762716] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 4             |
|    time_elapsed          | 71            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.0048996876  |
|    clip_fraction         | 0.0381        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 109           |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.131        |
|    lagrangian_multiplier | 0.0793        |
|    learning_rate         | 0.0003        |
|    loss                  | 30            |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00498      |
|    std                   | 0.991         |
|    value_loss            | 313           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.0246825] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 8            |
|    time_elapsed          | 127          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0069717    |
|    clip_fraction         | 0.0631       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 181          |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.00258      |
|    lagrangian_multiplier | 0.0623       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.9         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00742     |
|    std                   | 0.959        |
|    value_loss            | 309          |
-------------------------------------------
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231227_071020-fnc3q7d4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16/runs/fnc3q7d4
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.6450269] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 9            |
|    time_elapsed          | 142          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.005606557  |
|    clip_fraction         | 0.0449       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 201          |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0235      |
|    lagrangian_multiplier | 0.0591       |
|    learning_rate         | 0.0003       |
|    loss                  | 78.7         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00655     |
|    std                   | 0.987        |
|    value_loss            | 539          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.0460405] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 9            |
|    time_elapsed          | 143          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.005103362  |
|    clip_fraction         | 0.0383       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 200          |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.00496     |
|    lagrangian_multiplier | 0.0564       |
|    learning_rate         | 0.0003       |
|    loss                  | 109          |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00501     |
|    std                   | 0.96         |
|    value_loss            | 831          |
-------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.62525284] |
| time/              |               |
|    fps             | 136           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
-------------------------------------------
| reward                   | [-0.7546792] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 5            |
|    time_elapsed          | 89           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.005798717  |
|    clip_fraction         | 0.051        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 144          |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0838      |
|    lagrangian_multiplier | 0.0736       |
|    learning_rate         | 0.0003       |
|    loss                  | 81.3         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00782     |
|    std                   | 1            |
|    value_loss            | 745          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.0390348] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 10           |
|    time_elapsed          | 158          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0053559626 |
|    clip_fraction         | 0.0483       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 138          |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0131       |
|    lagrangian_multiplier | 0.0598       |
|    learning_rate         | 0.0003       |
|    loss                  | 78.4         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00731     |
|    std                   | 0.989        |
|    value_loss            | 634          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.6208736] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 10           |
|    time_elapsed          | 159          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.004867631  |
|    clip_fraction         | 0.0405       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 194          |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.0182       |
|    lagrangian_multiplier | 0.0535       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.1         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00473     |
|    std                   | 0.947        |
|    value_loss            | 230          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8501167] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 6            |
|    time_elapsed          | 107          |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.005484277  |
|    clip_fraction         | 0.0401       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 86.9         |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0411      |
|    lagrangian_multiplier | 0.052        |
|    learning_rate         | 0.0003       |
|    loss                  | 71.1         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00615     |
|    std                   | 0.993        |
|    value_loss            | 605          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7882195] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 11           |
|    time_elapsed          | 174          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.006653423  |
|    clip_fraction         | 0.0579       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 36.7         |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0268      |
|    lagrangian_multiplier | 0.0502       |
|    learning_rate         | 0.0003       |
|    loss                  | 91.7         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00912     |
|    std                   | 0.962        |
|    value_loss            | 706          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3151171] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 11           |
|    time_elapsed          | 175          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0050756605 |
|    clip_fraction         | 0.0427       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 178          |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.0185       |
|    lagrangian_multiplier | 0.0622       |
|    learning_rate         | 0.0003       |
|    loss                  | 118          |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00699     |
|    std                   | 0.947        |
|    value_loss            | 941          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.49513352] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 7             |
|    time_elapsed          | 125           |
|    total_timesteps       | 14336         |
| train/                   |               |
|    approx_kl             | 0.007017321   |
|    clip_fraction         | 0.0732        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 98.3          |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.0195        |
|    lagrangian_multiplier | 0.0571        |
|    learning_rate         | 0.0003        |
|    loss                  | 31.6          |
|    n_updates             | 60            |
|    policy_gradient_loss  | -0.00721      |
|    std                   | 0.994         |
|    value_loss            | 215           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3504546] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 12           |
|    time_elapsed          | 190          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0066692764 |
|    clip_fraction         | 0.0466       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 133          |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0262      |
|    lagrangian_multiplier | 0.0552       |
|    learning_rate         | 0.0003       |
|    loss                  | 137          |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00767     |
|    std                   | 0.961        |
|    value_loss            | 1.11e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.7818806] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 12           |
|    time_elapsed          | 191          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0051606325 |
|    clip_fraction         | 0.0466       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 240          |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.00779      |
|    lagrangian_multiplier | 0.0626       |
|    learning_rate         | 0.0003       |
|    loss                  | 135          |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00729     |
|    std                   | 0.94         |
|    value_loss            | 970          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5270585] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 13           |
|    time_elapsed          | 206          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.006032137  |
|    clip_fraction         | 0.0483       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 192          |
|    entropy_loss          | -2.75        |
|    explained_variance    | 0.0215       |
|    lagrangian_multiplier | 0.0553       |
|    learning_rate         | 0.0003       |
|    loss                  | 73.6         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00661     |
|    std                   | 0.958        |
|    value_loss            | 553          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.52154577] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 8             |
|    time_elapsed          | 143           |
|    total_timesteps       | 16384         |
| train/                   |               |
|    approx_kl             | 0.0059530195  |
|    clip_fraction         | 0.0554        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 113           |
|    entropy_loss          | -2.81         |
|    explained_variance    | -0.0948       |
|    lagrangian_multiplier | 0.0662        |
|    learning_rate         | 0.0003        |
|    loss                  | 48.9          |
|    n_updates             | 70            |
|    policy_gradient_loss  | -0.0084       |
|    std                   | 0.98          |
|    value_loss            | 407           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.0008192] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 13           |
|    time_elapsed          | 207          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0049758255 |
|    clip_fraction         | 0.0454       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 83.3         |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.026        |
|    lagrangian_multiplier | 0.0724       |
|    learning_rate         | 0.0003       |
|    loss                  | 50.7         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00514     |
|    std                   | 0.942        |
|    value_loss            | 492          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9721694] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 14           |
|    time_elapsed          | 222          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0048419125 |
|    clip_fraction         | 0.0413       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 33.6         |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0261      |
|    lagrangian_multiplier | 0.0791       |
|    learning_rate         | 0.0003       |
|    loss                  | 31.2         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00428     |
|    std                   | 0.952        |
|    value_loss            | 339          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.71738243] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 9             |
|    time_elapsed          | 161           |
|    total_timesteps       | 18432         |
| train/                   |               |
|    approx_kl             | 0.0064832373  |
|    clip_fraction         | 0.0575        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 174           |
|    entropy_loss          | -2.81         |
|    explained_variance    | 0.00821       |
|    lagrangian_multiplier | 0.0712        |
|    learning_rate         | 0.0003        |
|    loss                  | 43            |
|    n_updates             | 80            |
|    policy_gradient_loss  | -0.00557      |
|    std                   | 0.991         |
|    value_loss            | 358           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.5743328] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 14           |
|    time_elapsed          | 223          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.006126104  |
|    clip_fraction         | 0.0641       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 232          |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0119       |
|    lagrangian_multiplier | 0.0629       |
|    learning_rate         | 0.0003       |
|    loss                  | 149          |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.0108      |
|    std                   | 0.948        |
|    value_loss            | 1.13e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.3593027] |
| time/                    |              |
|    fps                   | 41           |
|    iterations            | 2            |
|    time_elapsed          | 98           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0054678344 |
|    clip_fraction         | 0.0455       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0215       |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0534      |
|    lagrangian_multiplier | 0.065        |
|    learning_rate         | 0.0003       |
|    loss                  | 35.1         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00578     |
|    std                   | 1            |
|    value_loss            | 415          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5288227] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 15           |
|    time_elapsed          | 238          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.006695476  |
|    clip_fraction         | 0.0569       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 182          |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.00265      |
|    lagrangian_multiplier | 0.0591       |
|    learning_rate         | 0.0003       |
|    loss                  | 86.5         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00793     |
|    std                   | 0.947        |
|    value_loss            | 660          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5183225] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 10           |
|    time_elapsed          | 179          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.006371421  |
|    clip_fraction         | 0.0632       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 155          |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.00947      |
|    lagrangian_multiplier | 0.0616       |
|    learning_rate         | 0.0003       |
|    loss                  | 47           |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00754     |
|    std                   | 0.985        |
|    value_loss            | 322          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.650092]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 15           |
|    time_elapsed          | 239          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0073630894 |
|    clip_fraction         | 0.0655       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 152          |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.0118      |
|    lagrangian_multiplier | 0.0662       |
|    learning_rate         | 0.0003       |
|    loss                  | 59.5         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00793     |
|    std                   | 0.948        |
|    value_loss            | 485          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8236469] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 16           |
|    time_elapsed          | 254          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0067719566 |
|    clip_fraction         | 0.0627       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 194          |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.0182      |
|    lagrangian_multiplier | 0.0573       |
|    learning_rate         | 0.0003       |
|    loss                  | 72.1         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00815     |
|    std                   | 0.946        |
|    value_loss            | 483          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.74655336] |
| time/                    |               |
|    fps                   | 49            |
|    iterations            | 3             |
|    time_elapsed          | 122           |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.0067168735  |
|    clip_fraction         | 0.0524        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0375        |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.0116       |
|    lagrangian_multiplier | 0.047         |
|    learning_rate         | 0.0003        |
|    loss                  | 47.7          |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00635      |
|    std                   | 1.01          |
|    value_loss            | 356           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3549135] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 11           |
|    time_elapsed          | 197          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.005521153  |
|    clip_fraction         | 0.0464       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 98.8         |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0272      |
|    lagrangian_multiplier | 0.0553       |
|    learning_rate         | 0.0003       |
|    loss                  | 35.4         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00647     |
|    std                   | 0.988        |
|    value_loss            | 280          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6519848] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 16           |
|    time_elapsed          | 255          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0051582465 |
|    clip_fraction         | 0.0374       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 170          |
|    entropy_loss          | -2.74        |
|    explained_variance    | -0.0475      |
|    lagrangian_multiplier | 0.0622       |
|    learning_rate         | 0.0003       |
|    loss                  | 65.9         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00468     |
|    std                   | 0.954        |
|    value_loss            | 464          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.0292846] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 17           |
|    time_elapsed          | 270          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.007378698  |
|    clip_fraction         | 0.0853       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 243          |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.0236      |
|    lagrangian_multiplier | 0.0596       |
|    learning_rate         | 0.0003       |
|    loss                  | 98.7         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.0091      |
|    std                   | 0.942        |
|    value_loss            | 619          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.85630053] |
| time/                    |               |
|    fps                   | 58            |
|    iterations            | 4             |
|    time_elapsed          | 139           |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.003523199   |
|    clip_fraction         | 0.025         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0294        |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.0341       |
|    lagrangian_multiplier | 0.0683        |
|    learning_rate         | 0.0003        |
|    loss                  | 32.4          |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00456      |
|    std                   | 1             |
|    value_loss            | 348           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.0093782] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 17           |
|    time_elapsed          | 271          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.007164608  |
|    clip_fraction         | 0.0769       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 225          |
|    entropy_loss          | -2.74        |
|    explained_variance    | -0.0227      |
|    lagrangian_multiplier | 0.0556       |
|    learning_rate         | 0.0003       |
|    loss                  | 81.2         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.012       |
|    std                   | 0.951        |
|    value_loss            | 532          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3106811] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 12           |
|    time_elapsed          | 215          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.005230773  |
|    clip_fraction         | 0.0416       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 65.8         |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0303      |
|    lagrangian_multiplier | 0.0569       |
|    learning_rate         | 0.0003       |
|    loss                  | 60.8         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00879     |
|    std                   | 0.997        |
|    value_loss            | 477          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.0174303] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 18           |
|    time_elapsed          | 286          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0071482062 |
|    clip_fraction         | 0.0856       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 136          |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.00893     |
|    lagrangian_multiplier | 0.0691       |
|    learning_rate         | 0.0003       |
|    loss                  | 82.9         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.0109      |
|    std                   | 0.948        |
|    value_loss            | 857          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.277473]  |
| time/                    |              |
|    fps                   | 65           |
|    iterations            | 5            |
|    time_elapsed          | 155          |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0055321557 |
|    clip_fraction         | 0.0567       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0364       |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.022       |
|    lagrangian_multiplier | 0.0625       |
|    learning_rate         | 0.0003       |
|    loss                  | 38.7         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00703     |
|    std                   | 1            |
|    value_loss            | 397          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.358019]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 18           |
|    time_elapsed          | 287          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0064674458 |
|    clip_fraction         | 0.0673       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 244          |
|    entropy_loss          | -2.74        |
|    explained_variance    | -0.0243      |
|    lagrangian_multiplier | 0.0571       |
|    learning_rate         | 0.0003       |
|    loss                  | 72.3         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00939     |
|    std                   | 0.95         |
|    value_loss            | 381          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.99037236] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 13            |
|    time_elapsed          | 233           |
|    total_timesteps       | 26624         |
| train/                   |               |
|    approx_kl             | 0.0048795626  |
|    clip_fraction         | 0.0471        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 82.4          |
|    entropy_loss          | -2.83         |
|    explained_variance    | 0.0564        |
|    lagrangian_multiplier | 0.0478        |
|    learning_rate         | 0.0003        |
|    loss                  | 50.2          |
|    n_updates             | 120           |
|    policy_gradient_loss  | -0.00806      |
|    std                   | 0.993         |
|    value_loss            | 336           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.1532307] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 19           |
|    time_elapsed          | 302          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0072366474 |
|    clip_fraction         | 0.0803       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 140          |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.00923     |
|    lagrangian_multiplier | 0.0492       |
|    learning_rate         | 0.0003       |
|    loss                  | 77.6         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00986     |
|    std                   | 0.94         |
|    value_loss            | 571          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9983996] |
| time/                    |              |
|    fps                   | 71           |
|    iterations            | 6            |
|    time_elapsed          | 171          |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.005721681  |
|    clip_fraction         | 0.0407       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0691       |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0404      |
|    lagrangian_multiplier | 0.0487       |
|    learning_rate         | 0.0003       |
|    loss                  | 69.5         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.0054      |
|    std                   | 0.997        |
|    value_loss            | 570          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4484403] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 19           |
|    time_elapsed          | 303          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.004995472  |
|    clip_fraction         | 0.0471       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 127          |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.0113       |
|    lagrangian_multiplier | 0.0701       |
|    learning_rate         | 0.0003       |
|    loss                  | 59.8         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00842     |
|    std                   | 0.949        |
|    value_loss            | 543          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.801913]  |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 14           |
|    time_elapsed          | 252          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0062122354 |
|    clip_fraction         | 0.0539       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 108          |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.11        |
|    lagrangian_multiplier | 0.0469       |
|    learning_rate         | 0.0003       |
|    loss                  | 40.8         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00657     |
|    std                   | 0.996        |
|    value_loss            | 253          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.6786628] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 20           |
|    time_elapsed          | 318          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.004933914  |
|    clip_fraction         | 0.0553       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 85.6         |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.0133      |
|    lagrangian_multiplier | 0.0453       |
|    learning_rate         | 0.0003       |
|    loss                  | 146          |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.0077      |
|    std                   | 0.94         |
|    value_loss            | 1.13e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9512935] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 20           |
|    time_elapsed          | 318          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.005846954  |
|    clip_fraction         | 0.0509       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 189          |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0158       |
|    lagrangian_multiplier | 0.0674       |
|    learning_rate         | 0.0003       |
|    loss                  | 77.3         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00764     |
|    std                   | 0.939        |
|    value_loss            | 718          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.52010065] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 15            |
|    time_elapsed          | 270           |
|    total_timesteps       | 30720         |
| train/                   |               |
|    approx_kl             | 0.005238204   |
|    clip_fraction         | 0.0395        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 80.1          |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.152         |
|    lagrangian_multiplier | 0.0502        |
|    learning_rate         | 0.0003        |
|    loss                  | 38.8          |
|    n_updates             | 140           |
|    policy_gradient_loss  | -0.00547      |
|    std                   | 0.983         |
|    value_loss            | 250           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.64646715] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 21            |
|    time_elapsed          | 334           |
|    total_timesteps       | 43008         |
| train/                   |               |
|    approx_kl             | 0.0051319906  |
|    clip_fraction         | 0.0399        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 41.2          |
|    entropy_loss          | -2.71         |
|    explained_variance    | 0.00462       |
|    lagrangian_multiplier | 0.0547        |
|    learning_rate         | 0.0003        |
|    loss                  | 57.4          |
|    n_updates             | 200           |
|    policy_gradient_loss  | -0.0054       |
|    std                   | 0.939         |
|    value_loss            | 583           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.2889302] |
| time/                    |              |
|    fps                   | 70           |
|    iterations            | 7            |
|    time_elapsed          | 203          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.008465217  |
|    clip_fraction         | 0.0885       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0511       |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0567      |
|    lagrangian_multiplier | 0.0783       |
|    learning_rate         | 0.0003       |
|    loss                  | 32           |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.0104      |
|    std                   | 1            |
|    value_loss            | 371          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.277189]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 21           |
|    time_elapsed          | 335          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0064889817 |
|    clip_fraction         | 0.062        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 225          |
|    entropy_loss          | -2.7         |
|    explained_variance    | -0.00772     |
|    lagrangian_multiplier | 0.0519       |
|    learning_rate         | 0.0003       |
|    loss                  | 130          |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00957     |
|    std                   | 0.931        |
|    value_loss            | 966          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.554494]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 22           |
|    time_elapsed          | 350          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.0060324436 |
|    clip_fraction         | 0.0492       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 74.4         |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0013       |
|    lagrangian_multiplier | 0.0664       |
|    learning_rate         | 0.0003       |
|    loss                  | 95.7         |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00739     |
|    std                   | 0.948        |
|    value_loss            | 909          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8401861] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 16           |
|    time_elapsed          | 288          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.005820776  |
|    clip_fraction         | 0.0619       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 34           |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0994      |
|    lagrangian_multiplier | 0.0668       |
|    learning_rate         | 0.0003       |
|    loss                  | 19.2         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00838     |
|    std                   | 0.979        |
|    value_loss            | 174          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8275772] |
| time/                    |              |
|    fps                   | 74           |
|    iterations            | 8            |
|    time_elapsed          | 219          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.005854943  |
|    clip_fraction         | 0.0533       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0655       |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.00403     |
|    lagrangian_multiplier | 0.0593       |
|    learning_rate         | 0.0003       |
|    loss                  | 82.5         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00587     |
|    std                   | 1.01         |
|    value_loss            | 930          |
-------------------------------------------
-------------------------------------------
| reward                   | [-4.3543997] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 22           |
|    time_elapsed          | 351          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.006102443  |
|    clip_fraction         | 0.0727       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 207          |
|    entropy_loss          | -2.7         |
|    explained_variance    | -0.0278      |
|    lagrangian_multiplier | 0.065        |
|    learning_rate         | 0.0003       |
|    loss                  | 83           |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00825     |
|    std                   | 0.937        |
|    value_loss            | 627          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.29916725] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 23            |
|    time_elapsed          | 366           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.0066737114  |
|    clip_fraction         | 0.0582        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 73.2          |
|    entropy_loss          | -2.73         |
|    explained_variance    | -0.0183       |
|    lagrangian_multiplier | 0.0444        |
|    learning_rate         | 0.0003        |
|    loss                  | 160           |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00762      |
|    std                   | 0.944         |
|    value_loss            | 1.22e+03      |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2109624] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 17           |
|    time_elapsed          | 306          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0055020805 |
|    clip_fraction         | 0.0603       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 67.7         |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.106       |
|    lagrangian_multiplier | 0.0636       |
|    learning_rate         | 0.0003       |
|    loss                  | 31.2         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00683     |
|    std                   | 0.97         |
|    value_loss            | 254          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5283253] |
| time/                    |              |
|    fps                   | 78           |
|    iterations            | 9            |
|    time_elapsed          | 235          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.004709797  |
|    clip_fraction         | 0.0494       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.089        |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.000934     |
|    lagrangian_multiplier | 0.0743       |
|    learning_rate         | 0.0003       |
|    loss                  | 87.8         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00733     |
|    std                   | 1.02         |
|    value_loss            | 1.06e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.7921622] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 23           |
|    time_elapsed          | 367          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.0049193054 |
|    clip_fraction         | 0.0488       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 198          |
|    entropy_loss          | -2.71        |
|    explained_variance    | -0.0101      |
|    lagrangian_multiplier | 0.0505       |
|    learning_rate         | 0.0003       |
|    loss                  | 270          |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.00758     |
|    std                   | 0.939        |
|    value_loss            | 1.9e+03      |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.2905107] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 24           |
|    time_elapsed          | 382          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.00386653   |
|    clip_fraction         | 0.0354       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 89.5         |
|    entropy_loss          | -2.71        |
|    explained_variance    | -0.0189      |
|    lagrangian_multiplier | 0.0448       |
|    learning_rate         | 0.0003       |
|    loss                  | 130          |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00636     |
|    std                   | 0.935        |
|    value_loss            | 970          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.7614087] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 18           |
|    time_elapsed          | 324          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.004882006  |
|    clip_fraction         | 0.0439       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 60.8         |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.18        |
|    lagrangian_multiplier | 0.0669       |
|    learning_rate         | 0.0003       |
|    loss                  | 21.9         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00616     |
|    std                   | 0.981        |
|    value_loss            | 183          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.1829183] |
| time/                    |              |
|    fps                   | 81           |
|    iterations            | 10           |
|    time_elapsed          | 251          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.006967678  |
|    clip_fraction         | 0.0535       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0831       |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.0232      |
|    lagrangian_multiplier | 0.0725       |
|    learning_rate         | 0.0003       |
|    loss                  | 97.4         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.0082      |
|    std                   | 1.01         |
|    value_loss            | 990          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.4690636] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 24           |
|    time_elapsed          | 383          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.0056106867 |
|    clip_fraction         | 0.0528       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 212          |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.00649     |
|    lagrangian_multiplier | 0.0621       |
|    learning_rate         | 0.0003       |
|    loss                  | 89.8         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.0106      |
|    std                   | 0.949        |
|    value_loss            | 790          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-1.459863] |
| time/                    |             |
|    fps                   | 128         |
|    iterations            | 25          |
|    time_elapsed          | 398         |
|    total_timesteps       | 51200       |
| train/                   |             |
|    approx_kl             | 0.004692723 |
|    clip_fraction         | 0.0312      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 11.2        |
|    entropy_loss          | -2.7        |
|    explained_variance    | -0.0125     |
|    lagrangian_multiplier | 0.0822      |
|    learning_rate         | 0.0003      |
|    loss                  | 12.8        |
|    n_updates             | 240         |
|    policy_gradient_loss  | -0.00593    |
|    std                   | 0.935       |
|    value_loss            | 126         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.7973845] |
| time/                    |              |
|    fps                   | 84           |
|    iterations            | 11           |
|    time_elapsed          | 268          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0059243664 |
|    clip_fraction         | 0.0482       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.049        |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.017       |
|    lagrangian_multiplier | 0.0677       |
|    learning_rate         | 0.0003       |
|    loss                  | 63.3         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00676     |
|    std                   | 1.01         |
|    value_loss            | 715          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.72352266] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 19            |
|    time_elapsed          | 342           |
|    total_timesteps       | 38912         |
| train/                   |               |
|    approx_kl             | 0.0066616917  |
|    clip_fraction         | 0.0525        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 75            |
|    entropy_loss          | -2.79         |
|    explained_variance    | 0.0274        |
|    lagrangian_multiplier | 0.0617        |
|    learning_rate         | 0.0003        |
|    loss                  | 51.5          |
|    n_updates             | 180           |
|    policy_gradient_loss  | -0.00772      |
|    std                   | 0.971         |
|    value_loss            | 396           |
--------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.6251851] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 25           |
|    time_elapsed          | 399          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.0059497003 |
|    clip_fraction         | 0.0437       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 210          |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.0153      |
|    lagrangian_multiplier | 0.0556       |
|    learning_rate         | 0.0003       |
|    loss                  | 95.6         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00776     |
|    std                   | 0.939        |
|    value_loss            | 613          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5577573] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 26           |
|    time_elapsed          | 414          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0066656675 |
|    clip_fraction         | 0.0468       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00612      |
|    entropy_loss          | -2.7         |
|    explained_variance    | 0.00891      |
|    lagrangian_multiplier | 0.0771       |
|    learning_rate         | 0.0003       |
|    loss                  | 13.4         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.0047      |
|    std                   | 0.932        |
|    value_loss            | 141          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.4127977] |
| time/                    |              |
|    fps                   | 86           |
|    iterations            | 12           |
|    time_elapsed          | 284          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.006003838  |
|    clip_fraction         | 0.0365       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0415       |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.00189     |
|    lagrangian_multiplier | 0.0524       |
|    learning_rate         | 0.0003       |
|    loss                  | 109          |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00493     |
|    std                   | 0.995        |
|    value_loss            | 829          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0936154] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 26           |
|    time_elapsed          | 415          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.005332681  |
|    clip_fraction         | 0.0455       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 180          |
|    entropy_loss          | -2.69        |
|    explained_variance    | -0.0304      |
|    lagrangian_multiplier | 0.0739       |
|    learning_rate         | 0.0003       |
|    loss                  | 66.2         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00551     |
|    std                   | 0.922        |
|    value_loss            | 537          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7653666] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 20           |
|    time_elapsed          | 360          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.005635741  |
|    clip_fraction         | 0.0628       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 30.9         |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0884      |
|    lagrangian_multiplier | 0.0707       |
|    learning_rate         | 0.0003       |
|    loss                  | 14.6         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00833     |
|    std                   | 0.981        |
|    value_loss            | 131          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7096678] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 27           |
|    time_elapsed          | 430          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.008867668  |
|    clip_fraction         | 0.0685       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 41.3         |
|    entropy_loss          | -2.67        |
|    explained_variance    | -0.000737    |
|    lagrangian_multiplier | 0.0508       |
|    learning_rate         | 0.0003       |
|    loss                  | 101          |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00945     |
|    std                   | 0.907        |
|    value_loss            | 837          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4711744] |
| time/                    |              |
|    fps                   | 88           |
|    iterations            | 13           |
|    time_elapsed          | 300          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0066439626 |
|    clip_fraction         | 0.0542       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0542       |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0102      |
|    lagrangian_multiplier | 0.0731       |
|    learning_rate         | 0.0003       |
|    loss                  | 65.6         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00831     |
|    std                   | 0.99         |
|    value_loss            | 741          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5336982] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 27           |
|    time_elapsed          | 431          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.005816914  |
|    clip_fraction         | 0.053        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 163          |
|    entropy_loss          | -2.68        |
|    explained_variance    | -0.00839     |
|    lagrangian_multiplier | 0.0688       |
|    learning_rate         | 0.0003       |
|    loss                  | 65.6         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00696     |
|    std                   | 0.924        |
|    value_loss            | 554          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.18713]  |
| time/                    |             |
|    fps                   | 113         |
|    iterations            | 21          |
|    time_elapsed          | 378         |
|    total_timesteps       | 43008       |
| train/                   |             |
|    approx_kl             | 0.006765534 |
|    clip_fraction         | 0.0732      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 135         |
|    entropy_loss          | -2.79       |
|    explained_variance    | 0.101       |
|    lagrangian_multiplier | 0.0468      |
|    learning_rate         | 0.0003      |
|    loss                  | 79.5        |
|    n_updates             | 200         |
|    policy_gradient_loss  | -0.0101     |
|    std                   | 0.975       |
|    value_loss            | 529         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.5954778] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 28           |
|    time_elapsed          | 446          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.005284029  |
|    clip_fraction         | 0.0511       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 75.4         |
|    entropy_loss          | -2.64        |
|    explained_variance    | -0.00367     |
|    lagrangian_multiplier | 0.0471       |
|    learning_rate         | 0.0003       |
|    loss                  | 115          |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00774     |
|    std                   | 0.904        |
|    value_loss            | 769          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7258857] |
| time/                    |              |
|    fps                   | 90           |
|    iterations            | 14           |
|    time_elapsed          | 317          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0051813126 |
|    clip_fraction         | 0.0509       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0694       |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0108      |
|    lagrangian_multiplier | 0.0606       |
|    learning_rate         | 0.0003       |
|    loss                  | 72.6         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.0067      |
|    std                   | 0.992        |
|    value_loss            | 661          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1685188] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 28           |
|    time_elapsed          | 447          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.006201793  |
|    clip_fraction         | 0.0519       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 227          |
|    entropy_loss          | -2.7         |
|    explained_variance    | -0.00898     |
|    lagrangian_multiplier | 0.0591       |
|    learning_rate         | 0.0003       |
|    loss                  | 97.7         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.0127      |
|    std                   | 0.941        |
|    value_loss            | 691          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.37472978] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 22            |
|    time_elapsed          | 396           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.005569933   |
|    clip_fraction         | 0.048         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 124           |
|    entropy_loss          | -2.79         |
|    explained_variance    | -0.112        |
|    lagrangian_multiplier | 0.0712        |
|    learning_rate         | 0.0003        |
|    loss                  | 32.3          |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.00786      |
|    std                   | 0.976         |
|    value_loss            | 265           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.51546144] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 29            |
|    time_elapsed          | 462           |
|    total_timesteps       | 59392         |
| train/                   |               |
|    approx_kl             | 0.0050014965  |
|    clip_fraction         | 0.0466        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 18.8          |
|    entropy_loss          | -2.63         |
|    explained_variance    | -0.00312      |
|    lagrangian_multiplier | 0.0487        |
|    learning_rate         | 0.0003        |
|    loss                  | 60.8          |
|    n_updates             | 280           |
|    policy_gradient_loss  | -0.00459      |
|    std                   | 0.899         |
|    value_loss            | 471           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.3532815] |
| time/                    |              |
|    fps                   | 92           |
|    iterations            | 15           |
|    time_elapsed          | 333          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.005885284  |
|    clip_fraction         | 0.0522       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0392       |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.00513     |
|    lagrangian_multiplier | 0.0769       |
|    learning_rate         | 0.0003       |
|    loss                  | 93.2         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00741     |
|    std                   | 0.989        |
|    value_loss            | 1.09e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.116067]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 29           |
|    time_elapsed          | 463          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.0059343316 |
|    clip_fraction         | 0.0634       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 193          |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.054       |
|    lagrangian_multiplier | 0.0666       |
|    learning_rate         | 0.0003       |
|    loss                  | 48.4         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00566     |
|    std                   | 0.946        |
|    value_loss            | 320          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7051167] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 23           |
|    time_elapsed          | 414          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.00902731   |
|    clip_fraction         | 0.0818       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 62.5         |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.139        |
|    lagrangian_multiplier | 0.0686       |
|    learning_rate         | 0.0003       |
|    loss                  | 24.4         |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.00911     |
|    std                   | 0.981        |
|    value_loss            | 187          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2336625] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 30           |
|    time_elapsed          | 478          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0060310857 |
|    clip_fraction         | 0.053        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 21.1         |
|    entropy_loss          | -2.62        |
|    explained_variance    | 0.0146       |
|    lagrangian_multiplier | 0.0502       |
|    learning_rate         | 0.0003       |
|    loss                  | 68.9         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00613     |
|    std                   | 0.899        |
|    value_loss            | 515          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.413745] |
| time/                    |             |
|    fps                   | 128         |
|    iterations            | 30          |
|    time_elapsed          | 479         |
|    total_timesteps       | 61440       |
| train/                   |             |
|    approx_kl             | 0.00709386  |
|    clip_fraction         | 0.0679      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 239         |
|    entropy_loss          | -2.74       |
|    explained_variance    | -0.00483    |
|    lagrangian_multiplier | 0.0525      |
|    learning_rate         | 0.0003      |
|    loss                  | 177         |
|    n_updates             | 290         |
|    policy_gradient_loss  | -0.0104     |
|    std                   | 0.961       |
|    value_loss            | 1.45e+03    |
------------------------------------------
-------------------------------------------
| reward                   | [-2.4365108] |
| time/                    |              |
|    fps                   | 93           |
|    iterations            | 16           |
|    time_elapsed          | 349          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.007991219  |
|    clip_fraction         | 0.0788       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0988       |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.016       |
|    lagrangian_multiplier | 0.0722       |
|    learning_rate         | 0.0003       |
|    loss                  | 71.7         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.0119      |
|    std                   | 0.982        |
|    value_loss            | 862          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9185432] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 31           |
|    time_elapsed          | 494          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.005696832  |
|    clip_fraction         | 0.0455       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.27         |
|    entropy_loss          | -2.63        |
|    explained_variance    | -0.0183      |
|    lagrangian_multiplier | 0.0846       |
|    learning_rate         | 0.0003       |
|    loss                  | 12.2         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00539     |
|    std                   | 0.902        |
|    value_loss            | 156          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6521093] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 24           |
|    time_elapsed          | 432          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.005663625  |
|    clip_fraction         | 0.0487       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 79.7         |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0632      |
|    lagrangian_multiplier | 0.0565       |
|    learning_rate         | 0.0003       |
|    loss                  | 33.7         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00894     |
|    std                   | 0.999        |
|    value_loss            | 211          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.264552] |
| time/                    |             |
|    fps                   | 128         |
|    iterations            | 31          |
|    time_elapsed          | 494         |
|    total_timesteps       | 63488       |
| train/                   |             |
|    approx_kl             | 0.005596556 |
|    clip_fraction         | 0.0513      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 210         |
|    entropy_loss          | -2.76       |
|    explained_variance    | -0.0416     |
|    lagrangian_multiplier | 0.0638      |
|    learning_rate         | 0.0003      |
|    loss                  | 59.8        |
|    n_updates             | 300         |
|    policy_gradient_loss  | -0.00634    |
|    std                   | 0.967       |
|    value_loss            | 404         |
------------------------------------------
------------------------------------------
| reward                   | [-1.573914] |
| time/                    |             |
|    fps                   | 95          |
|    iterations            | 17          |
|    time_elapsed          | 365         |
|    total_timesteps       | 34816       |
| train/                   |             |
|    approx_kl             | 0.008223344 |
|    clip_fraction         | 0.0958      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0619      |
|    entropy_loss          | -2.79       |
|    explained_variance    | 0.00678     |
|    lagrangian_multiplier | 0.0565      |
|    learning_rate         | 0.0003      |
|    loss                  | 86.1        |
|    n_updates             | 160         |
|    policy_gradient_loss  | -0.0103     |
|    std                   | 0.972       |
|    value_loss            | 949         |
------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.98221296] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 32            |
|    time_elapsed          | 510           |
|    total_timesteps       | 65536         |
| train/                   |               |
|    approx_kl             | 0.006559034   |
|    clip_fraction         | 0.0446        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 94.7          |
|    entropy_loss          | -2.62         |
|    explained_variance    | -0.0144       |
|    lagrangian_multiplier | 0.0546        |
|    learning_rate         | 0.0003        |
|    loss                  | 60.3          |
|    n_updates             | 310           |
|    policy_gradient_loss  | -0.00719      |
|    std                   | 0.894         |
|    value_loss            | 424           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.63044167] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 25            |
|    time_elapsed          | 450           |
|    total_timesteps       | 51200         |
| train/                   |               |
|    approx_kl             | 0.0100836     |
|    clip_fraction         | 0.0768        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 170           |
|    entropy_loss          | -2.82         |
|    explained_variance    | -0.105        |
|    lagrangian_multiplier | 0.0511        |
|    learning_rate         | 0.0003        |
|    loss                  | 50.5          |
|    n_updates             | 240           |
|    policy_gradient_loss  | -0.0113       |
|    std                   | 0.989         |
|    value_loss            | 269           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.1938499] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 32           |
|    time_elapsed          | 510          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.005718616  |
|    clip_fraction         | 0.0496       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 203          |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0211      |
|    lagrangian_multiplier | 0.051        |
|    learning_rate         | 0.0003       |
|    loss                  | 83.4         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00719     |
|    std                   | 0.954        |
|    value_loss            | 535          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5307167] |
| time/                    |              |
|    fps                   | 96           |
|    iterations            | 18           |
|    time_elapsed          | 382          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0057566585 |
|    clip_fraction         | 0.0533       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0398       |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0179      |
|    lagrangian_multiplier | 0.0497       |
|    learning_rate         | 0.0003       |
|    loss                  | 74.4         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00806     |
|    std                   | 0.975        |
|    value_loss            | 633          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.0485128] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 33           |
|    time_elapsed          | 526          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.007510818  |
|    clip_fraction         | 0.0573       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 68.7         |
|    entropy_loss          | -2.63        |
|    explained_variance    | 0.00329      |
|    lagrangian_multiplier | 0.0614       |
|    learning_rate         | 0.0003       |
|    loss                  | 37.7         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00782     |
|    std                   | 0.905        |
|    value_loss            | 290          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.91148114] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 26            |
|    time_elapsed          | 468           |
|    total_timesteps       | 53248         |
| train/                   |               |
|    approx_kl             | 0.0061817816  |
|    clip_fraction         | 0.0614        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 154           |
|    entropy_loss          | -2.82         |
|    explained_variance    | -0.0532       |
|    lagrangian_multiplier | 0.0592        |
|    learning_rate         | 0.0003        |
|    loss                  | 56.3          |
|    n_updates             | 250           |
|    policy_gradient_loss  | -0.00914      |
|    std                   | 0.988         |
|    value_loss            | 417           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.4715207] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 33           |
|    time_elapsed          | 526          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0075833187 |
|    clip_fraction         | 0.0691       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 265          |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.03        |
|    lagrangian_multiplier | 0.0611       |
|    learning_rate         | 0.0003       |
|    loss                  | 75.5         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00748     |
|    std                   | 0.941        |
|    value_loss            | 518          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8202455] |
| time/                    |              |
|    fps                   | 97           |
|    iterations            | 19           |
|    time_elapsed          | 398          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.005118522  |
|    clip_fraction         | 0.032        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0891       |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0148      |
|    lagrangian_multiplier | 0.0716       |
|    learning_rate         | 0.0003       |
|    loss                  | 44           |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00389     |
|    std                   | 0.972        |
|    value_loss            | 513          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8386902] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 34           |
|    time_elapsed          | 542          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.009325232  |
|    clip_fraction         | 0.0905       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 109          |
|    entropy_loss          | -2.63        |
|    explained_variance    | 0.00483      |
|    lagrangian_multiplier | 0.0613       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.4         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.0125      |
|    std                   | 0.901        |
|    value_loss            | 543          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0234135] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 34           |
|    time_elapsed          | 543          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0053236815 |
|    clip_fraction         | 0.0559       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 263          |
|    entropy_loss          | -2.71        |
|    explained_variance    | -0.0494      |
|    lagrangian_multiplier | 0.0585       |
|    learning_rate         | 0.0003       |
|    loss                  | 76.3         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00822     |
|    std                   | 0.94         |
|    value_loss            | 475          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5062939] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 27           |
|    time_elapsed          | 487          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.0054107616 |
|    clip_fraction         | 0.0491       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 141          |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0111      |
|    lagrangian_multiplier | 0.0662       |
|    learning_rate         | 0.0003       |
|    loss                  | 44           |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.0108      |
|    std                   | 0.99         |
|    value_loss            | 312          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.1220617] |
| time/                    |              |
|    fps                   | 98           |
|    iterations            | 20           |
|    time_elapsed          | 414          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.007078844  |
|    clip_fraction         | 0.09         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0704       |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.00682      |
|    lagrangian_multiplier | 0.0451       |
|    learning_rate         | 0.0003       |
|    loss                  | 101          |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.0113      |
|    std                   | 0.965        |
|    value_loss            | 715          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3740306] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 35           |
|    time_elapsed          | 558          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.00803355   |
|    clip_fraction         | 0.0789       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 57.5         |
|    entropy_loss          | -2.62        |
|    explained_variance    | 0.0165       |
|    lagrangian_multiplier | 0.0626       |
|    learning_rate         | 0.0003       |
|    loss                  | 29.9         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 0.897        |
|    value_loss            | 255          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9645917] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 35           |
|    time_elapsed          | 559          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0051890127 |
|    clip_fraction         | 0.0401       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 114          |
|    entropy_loss          | -2.71        |
|    explained_variance    | -0.00996     |
|    lagrangian_multiplier | 0.0552       |
|    learning_rate         | 0.0003       |
|    loss                  | 67.7         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00894     |
|    std                   | 0.943        |
|    value_loss            | 522          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6260971] |
| time/                    |              |
|    fps                   | 99           |
|    iterations            | 21           |
|    time_elapsed          | 431          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.007310104  |
|    clip_fraction         | 0.0684       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.108        |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.0269       |
|    lagrangian_multiplier | 0.0552       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.2         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00884     |
|    std                   | 0.943        |
|    value_loss            | 537          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3404459] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 28           |
|    time_elapsed          | 505          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.0067672376 |
|    clip_fraction         | 0.0706       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 99.2         |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.00728     |
|    lagrangian_multiplier | 0.054        |
|    learning_rate         | 0.0003       |
|    loss                  | 33           |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00829     |
|    std                   | 0.984        |
|    value_loss            | 168          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0988809] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 36           |
|    time_elapsed          | 574          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.004933305  |
|    clip_fraction         | 0.0602       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 25.1         |
|    entropy_loss          | -2.61        |
|    explained_variance    | -0.0172      |
|    lagrangian_multiplier | 0.056        |
|    learning_rate         | 0.0003       |
|    loss                  | 33.5         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00642     |
|    std                   | 0.889        |
|    value_loss            | 332          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4620962] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 36           |
|    time_elapsed          | 576          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.007036172  |
|    clip_fraction         | 0.0668       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 170          |
|    entropy_loss          | -2.71        |
|    explained_variance    | 0.000645     |
|    lagrangian_multiplier | 0.0521       |
|    learning_rate         | 0.0003       |
|    loss                  | 64.6         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0104      |
|    std                   | 0.939        |
|    value_loss            | 473          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.41505092] |
| time/                    |               |
|    fps                   | 100           |
|    iterations            | 22            |
|    time_elapsed          | 447           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.0077329427  |
|    clip_fraction         | 0.0848        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0731        |
|    entropy_loss          | -2.71         |
|    explained_variance    | 0.0146        |
|    lagrangian_multiplier | 0.059         |
|    learning_rate         | 0.0003        |
|    loss                  | 83.8          |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.0101       |
|    std                   | 0.939         |
|    value_loss            | 730           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.66177696] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 29            |
|    time_elapsed          | 523           |
|    total_timesteps       | 59392         |
| train/                   |               |
|    approx_kl             | 0.0062209074  |
|    clip_fraction         | 0.0502        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 80.6          |
|    entropy_loss          | -2.81         |
|    explained_variance    | 0.0559        |
|    lagrangian_multiplier | 0.0574        |
|    learning_rate         | 0.0003        |
|    loss                  | 34.8          |
|    n_updates             | 280           |
|    policy_gradient_loss  | -0.00844      |
|    std                   | 0.984         |
|    value_loss            | 232           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3174579] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 37           |
|    time_elapsed          | 590          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.00641513   |
|    clip_fraction         | 0.065        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 66.6         |
|    entropy_loss          | -2.6         |
|    explained_variance    | -0.00536     |
|    lagrangian_multiplier | 0.0459       |
|    learning_rate         | 0.0003       |
|    loss                  | 54.4         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00909     |
|    std                   | 0.883        |
|    value_loss            | 367          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1018149] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 37           |
|    time_elapsed          | 592          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.0068956506 |
|    clip_fraction         | 0.0559       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 218          |
|    entropy_loss          | -2.7         |
|    explained_variance    | -0.0355      |
|    lagrangian_multiplier | 0.059        |
|    learning_rate         | 0.0003       |
|    loss                  | 72.4         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00817     |
|    std                   | 0.931        |
|    value_loss            | 428          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5491446] |
| time/                    |              |
|    fps                   | 101          |
|    iterations            | 23           |
|    time_elapsed          | 463          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.0073086843 |
|    clip_fraction         | 0.0655       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0785       |
|    entropy_loss          | -2.69        |
|    explained_variance    | -0.0146      |
|    lagrangian_multiplier | 0.0557       |
|    learning_rate         | 0.0003       |
|    loss                  | 70.7         |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.00742     |
|    std                   | 0.927        |
|    value_loss            | 658          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2652243] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 30           |
|    time_elapsed          | 541          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0054632346 |
|    clip_fraction         | 0.0726       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 211          |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0712      |
|    lagrangian_multiplier | 0.0546       |
|    learning_rate         | 0.0003       |
|    loss                  | 110          |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 1            |
|    value_loss            | 822          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.90247893] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 38            |
|    time_elapsed          | 606           |
|    total_timesteps       | 77824         |
| train/                   |               |
|    approx_kl             | 0.0058710948  |
|    clip_fraction         | 0.0476        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 52.9          |
|    entropy_loss          | -2.57         |
|    explained_variance    | -0.00979      |
|    lagrangian_multiplier | 0.0643        |
|    learning_rate         | 0.0003        |
|    loss                  | 35.6          |
|    n_updates             | 370           |
|    policy_gradient_loss  | -0.00576      |
|    std                   | 0.87          |
|    value_loss            | 331           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2174829] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 38           |
|    time_elapsed          | 608          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.007496205  |
|    clip_fraction         | 0.0686       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 250          |
|    entropy_loss          | -2.68        |
|    explained_variance    | -0.0159      |
|    lagrangian_multiplier | 0.0611       |
|    learning_rate         | 0.0003       |
|    loss                  | 66.7         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00818     |
|    std                   | 0.926        |
|    value_loss            | 366          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7607817] |
| time/                    |              |
|    fps                   | 102          |
|    iterations            | 24           |
|    time_elapsed          | 479          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.007951656  |
|    clip_fraction         | 0.068        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0755       |
|    entropy_loss          | -2.68        |
|    explained_variance    | -0.0311      |
|    lagrangian_multiplier | 0.0564       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.2         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.924        |
|    value_loss            | 566          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9694763] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 31           |
|    time_elapsed          | 559          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.00664554   |
|    clip_fraction         | 0.063        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 210          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0561       |
|    lagrangian_multiplier | 0.0505       |
|    learning_rate         | 0.0003       |
|    loss                  | 95.2         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.0104      |
|    std                   | 0.992        |
|    value_loss            | 792          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.067552] |
| time/                    |             |
|    fps                   | 128         |
|    iterations            | 39          |
|    time_elapsed          | 622         |
|    total_timesteps       | 79872       |
| train/                   |             |
|    approx_kl             | 0.004941752 |
|    clip_fraction         | 0.0413      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 71.2        |
|    entropy_loss          | -2.55       |
|    explained_variance    | 0.0131      |
|    lagrangian_multiplier | 0.0775      |
|    learning_rate         | 0.0003      |
|    loss                  | 37.3        |
|    n_updates             | 380         |
|    policy_gradient_loss  | -0.00628    |
|    std                   | 0.867       |
|    value_loss            | 381         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.2356493] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 39           |
|    time_elapsed          | 624          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.006834005  |
|    clip_fraction         | 0.0604       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 199          |
|    entropy_loss          | -2.67        |
|    explained_variance    | -0.0265      |
|    lagrangian_multiplier | 0.056        |
|    learning_rate         | 0.0003       |
|    loss                  | 53.2         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00712     |
|    std                   | 0.914        |
|    value_loss            | 257          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.68897414] |
| time/                    |               |
|    fps                   | 103           |
|    iterations            | 25            |
|    time_elapsed          | 495           |
|    total_timesteps       | 51200         |
| train/                   |               |
|    approx_kl             | 0.0072480077  |
|    clip_fraction         | 0.0574        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0695        |
|    entropy_loss          | -2.68         |
|    explained_variance    | -0.0265       |
|    lagrangian_multiplier | 0.0551        |
|    learning_rate         | 0.0003        |
|    loss                  | 54.4          |
|    n_updates             | 240           |
|    policy_gradient_loss  | -0.00982      |
|    std                   | 0.923         |
|    value_loss            | 513           |
--------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8758077] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 40           |
|    time_elapsed          | 638          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.0060148546 |
|    clip_fraction         | 0.0585       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 32.5         |
|    entropy_loss          | -2.55        |
|    explained_variance    | -0.0186      |
|    lagrangian_multiplier | 0.06         |
|    learning_rate         | 0.0003       |
|    loss                  | 37           |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00671     |
|    std                   | 0.867        |
|    value_loss            | 285          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.74432737] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 32            |
|    time_elapsed          | 577           |
|    total_timesteps       | 65536         |
| train/                   |               |
|    approx_kl             | 0.0072528436  |
|    clip_fraction         | 0.079         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 199           |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.0676        |
|    lagrangian_multiplier | 0.0494        |
|    learning_rate         | 0.0003        |
|    loss                  | 59.5          |
|    n_updates             | 310           |
|    policy_gradient_loss  | -0.0119       |
|    std                   | 0.989         |
|    value_loss            | 326           |
--------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.94542813] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 40            |
|    time_elapsed          | 640           |
|    total_timesteps       | 81920         |
| train/                   |               |
|    approx_kl             | 0.0065178126  |
|    clip_fraction         | 0.0462        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 139           |
|    entropy_loss          | -2.64         |
|    explained_variance    | -0.00578      |
|    lagrangian_multiplier | 0.0656        |
|    learning_rate         | 0.0003        |
|    loss                  | 58.8          |
|    n_updates             | 390           |
|    policy_gradient_loss  | -0.00773      |
|    std                   | 0.899         |
|    value_loss            | 444           |
--------------------------------------------
------------------------------------------
| reward                   | [-1.121149] |
| time/                    |             |
|    fps                   | 104         |
|    iterations            | 26          |
|    time_elapsed          | 511         |
|    total_timesteps       | 53248       |
| train/                   |             |
|    approx_kl             | 0.008518091 |
|    clip_fraction         | 0.1         |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.075       |
|    entropy_loss          | -2.68       |
|    explained_variance    | -0.0156     |
|    lagrangian_multiplier | 0.0527      |
|    learning_rate         | 0.0003      |
|    loss                  | 49.9        |
|    n_updates             | 250         |
|    policy_gradient_loss  | -0.0114     |
|    std                   | 0.923       |
|    value_loss            | 482         |
------------------------------------------
-------------------------------------------
| reward                   | [-0.8269945] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 41           |
|    time_elapsed          | 654          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0073627587 |
|    clip_fraction         | 0.0687       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 9.03         |
|    entropy_loss          | -2.55        |
|    explained_variance    | -0.003       |
|    lagrangian_multiplier | 0.0652       |
|    learning_rate         | 0.0003       |
|    loss                  | 35.5         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00845     |
|    std                   | 0.865        |
|    value_loss            | 321          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8830682] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 33           |
|    time_elapsed          | 595          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.006785108  |
|    clip_fraction         | 0.0655       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 245          |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0741       |
|    lagrangian_multiplier | 0.0473       |
|    learning_rate         | 0.0003       |
|    loss                  | 104          |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.0116      |
|    std                   | 0.994        |
|    value_loss            | 604          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9314022] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 41           |
|    time_elapsed          | 656          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.00766766   |
|    clip_fraction         | 0.0785       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 128          |
|    entropy_loss          | -2.62        |
|    explained_variance    | 0.00955      |
|    lagrangian_multiplier | 0.061        |
|    learning_rate         | 0.0003       |
|    loss                  | 52.5         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00878     |
|    std                   | 0.897        |
|    value_loss            | 354          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4793853] |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 27           |
|    time_elapsed          | 527          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.007458727  |
|    clip_fraction         | 0.0835       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0536       |
|    entropy_loss          | -2.68        |
|    explained_variance    | -0.014       |
|    lagrangian_multiplier | 0.0701       |
|    learning_rate         | 0.0003       |
|    loss                  | 41.1         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.014       |
|    std                   | 0.924        |
|    value_loss            | 396          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.6710805] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 42           |
|    time_elapsed          | 670          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0057617566 |
|    clip_fraction         | 0.0509       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 40.2         |
|    entropy_loss          | -2.54        |
|    explained_variance    | -0.0286      |
|    lagrangian_multiplier | 0.0562       |
|    learning_rate         | 0.0003       |
|    loss                  | 32.9         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00646     |
|    std                   | 0.858        |
|    value_loss            | 268          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9883449] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 34           |
|    time_elapsed          | 613          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.006702112  |
|    clip_fraction         | 0.0644       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 227          |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0919      |
|    lagrangian_multiplier | 0.0584       |
|    learning_rate         | 0.0003       |
|    loss                  | 64.1         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.986        |
|    value_loss            | 370          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.1057467] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 42           |
|    time_elapsed          | 672          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.005718383  |
|    clip_fraction         | 0.0444       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 232          |
|    entropy_loss          | -2.63        |
|    explained_variance    | -0.0176      |
|    lagrangian_multiplier | 0.0559       |
|    learning_rate         | 0.0003       |
|    loss                  | 76.4         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.0122      |
|    std                   | 0.911        |
|    value_loss            | 502          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4946021] |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 28           |
|    time_elapsed          | 543          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.007516198  |
|    clip_fraction         | 0.091        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0342       |
|    entropy_loss          | -2.69        |
|    explained_variance    | -0.051       |
|    lagrangian_multiplier | 0.0677       |
|    learning_rate         | 0.0003       |
|    loss                  | 25.9         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.0107      |
|    std                   | 0.93         |
|    value_loss            | 311          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.77721375] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 43            |
|    time_elapsed          | 686           |
|    total_timesteps       | 88064         |
| train/                   |               |
|    approx_kl             | 0.007128433   |
|    clip_fraction         | 0.0675        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 96.7          |
|    entropy_loss          | -2.55         |
|    explained_variance    | -0.00364      |
|    lagrangian_multiplier | 0.0643        |
|    learning_rate         | 0.0003        |
|    loss                  | 44.6          |
|    n_updates             | 420           |
|    policy_gradient_loss  | -0.0108       |
|    std                   | 0.874         |
|    value_loss            | 368           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.4464344] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 35           |
|    time_elapsed          | 631          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0071865562 |
|    clip_fraction         | 0.0836       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 213          |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0607       |
|    lagrangian_multiplier | 0.0464       |
|    learning_rate         | 0.0003       |
|    loss                  | 73.1         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.0115      |
|    std                   | 0.983        |
|    value_loss            | 347          |
-------------------------------------------
-------------------------------------------
| reward                   | [-5.059963]  |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 43           |
|    time_elapsed          | 688          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0065282835 |
|    clip_fraction         | 0.0703       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 247          |
|    entropy_loss          | -2.65        |
|    explained_variance    | -0.0452      |
|    lagrangian_multiplier | 0.0545       |
|    learning_rate         | 0.0003       |
|    loss                  | 75.7         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.915        |
|    value_loss            | 371          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6311632] |
| time/                    |              |
|    fps                   | 106          |
|    iterations            | 29           |
|    time_elapsed          | 560          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.006517903  |
|    clip_fraction         | 0.0803       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0268       |
|    entropy_loss          | -2.7         |
|    explained_variance    | -0.0238      |
|    lagrangian_multiplier | 0.0618       |
|    learning_rate         | 0.0003       |
|    loss                  | 30.5         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00799     |
|    std                   | 0.937        |
|    value_loss            | 313          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8121065] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 44           |
|    time_elapsed          | 702          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.009844559  |
|    clip_fraction         | 0.113        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 118          |
|    entropy_loss          | -2.55        |
|    explained_variance    | -0.00487     |
|    lagrangian_multiplier | 0.0651       |
|    learning_rate         | 0.0003       |
|    loss                  | 45.8         |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.0149      |
|    std                   | 0.863        |
|    value_loss            | 349          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.7387578] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 44           |
|    time_elapsed          | 704          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.007152733  |
|    clip_fraction         | 0.0699       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 223          |
|    entropy_loss          | -2.66        |
|    explained_variance    | -0.00273     |
|    lagrangian_multiplier | 0.0666       |
|    learning_rate         | 0.0003       |
|    loss                  | 176          |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.0104      |
|    std                   | 0.913        |
|    value_loss            | 2e+03        |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3012223] |
| time/                    |              |
|    fps                   | 106          |
|    iterations            | 30           |
|    time_elapsed          | 576          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0054734815 |
|    clip_fraction         | 0.0678       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0743       |
|    entropy_loss          | -2.71        |
|    explained_variance    | -0.0349      |
|    lagrangian_multiplier | 0.0713       |
|    learning_rate         | 0.0003       |
|    loss                  | 41.5         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.0087      |
|    std                   | 0.943        |
|    value_loss            | 479          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.786519]  |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 36           |
|    time_elapsed          | 649          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0064381044 |
|    clip_fraction         | 0.0541       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 264          |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0067      |
|    lagrangian_multiplier | 0.0712       |
|    learning_rate         | 0.0003       |
|    loss                  | 119          |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00979     |
|    std                   | 0.977        |
|    value_loss            | 942          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2449315] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 45           |
|    time_elapsed          | 718          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.006961505  |
|    clip_fraction         | 0.0803       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 78.4         |
|    entropy_loss          | -2.55        |
|    explained_variance    | -0.023       |
|    lagrangian_multiplier | 0.0585       |
|    learning_rate         | 0.0003       |
|    loss                  | 44.1         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 0.867        |
|    value_loss            | 313          |
-------------------------------------------
------------------------------------------
| reward                   | [-2.003523] |
| time/                    |             |
|    fps                   | 127         |
|    iterations            | 45          |
|    time_elapsed          | 720         |
|    total_timesteps       | 92160       |
| train/                   |             |
|    approx_kl             | 0.005390511 |
|    clip_fraction         | 0.0404      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 167         |
|    entropy_loss          | -2.67       |
|    explained_variance    | 4.85e-05    |
|    lagrangian_multiplier | 0.0655      |
|    learning_rate         | 0.0003      |
|    loss                  | 112         |
|    n_updates             | 440         |
|    policy_gradient_loss  | -0.00806    |
|    std                   | 0.924       |
|    value_loss            | 834         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.7754189] |
| time/                    |              |
|    fps                   | 107          |
|    iterations            | 31           |
|    time_elapsed          | 592          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0074066357 |
|    clip_fraction         | 0.0706       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0265       |
|    entropy_loss          | -2.7         |
|    explained_variance    | -0.084       |
|    lagrangian_multiplier | 0.0883       |
|    learning_rate         | 0.0003       |
|    loss                  | 18.1         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00902     |
|    std                   | 0.932        |
|    value_loss            | 239          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.93606985] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 37            |
|    time_elapsed          | 667           |
|    total_timesteps       | 75776         |
| train/                   |               |
|    approx_kl             | 0.0075058127  |
|    clip_fraction         | 0.101         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 259           |
|    entropy_loss          | -2.79         |
|    explained_variance    | -0.0558       |
|    lagrangian_multiplier | 0.0621        |
|    learning_rate         | 0.0003        |
|    loss                  | 84.9          |
|    n_updates             | 360           |
|    policy_gradient_loss  | -0.0113       |
|    std                   | 0.98          |
|    value_loss            | 595           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.8267401] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 46           |
|    time_elapsed          | 734          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.007263727  |
|    clip_fraction         | 0.0687       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 136          |
|    entropy_loss          | -2.56        |
|    explained_variance    | -0.0139      |
|    lagrangian_multiplier | 0.058        |
|    learning_rate         | 0.0003       |
|    loss                  | 59.9         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.0125      |
|    std                   | 0.873        |
|    value_loss            | 379          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6121135] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 46           |
|    time_elapsed          | 736          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0055175405 |
|    clip_fraction         | 0.0519       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 135          |
|    entropy_loss          | -2.68        |
|    explained_variance    | -0.0132      |
|    lagrangian_multiplier | 0.0579       |
|    learning_rate         | 0.0003       |
|    loss                  | 83.1         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.00986     |
|    std                   | 0.929        |
|    value_loss            | 562          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.94050294] |
| time/                    |               |
|    fps                   | 107           |
|    iterations            | 32            |
|    time_elapsed          | 608           |
|    total_timesteps       | 65536         |
| train/                   |               |
|    approx_kl             | 0.006619515   |
|    clip_fraction         | 0.068         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0721        |
|    entropy_loss          | -2.7          |
|    explained_variance    | -0.0305       |
|    lagrangian_multiplier | 0.0617        |
|    learning_rate         | 0.0003        |
|    loss                  | 42.6          |
|    n_updates             | 310           |
|    policy_gradient_loss  | -0.00784      |
|    std                   | 0.941         |
|    value_loss            | 411           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.81539494] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 38            |
|    time_elapsed          | 686           |
|    total_timesteps       | 77824         |
| train/                   |               |
|    approx_kl             | 0.0066061104  |
|    clip_fraction         | 0.0616        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 214           |
|    entropy_loss          | -2.79         |
|    explained_variance    | -0.00428      |
|    lagrangian_multiplier | 0.0571        |
|    learning_rate         | 0.0003        |
|    loss                  | 89.4          |
|    n_updates             | 370           |
|    policy_gradient_loss  | -0.0087       |
|    std                   | 0.97          |
|    value_loss            | 475           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3711959] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 47           |
|    time_elapsed          | 750          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.008124014  |
|    clip_fraction         | 0.0659       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 194          |
|    entropy_loss          | -2.55        |
|    explained_variance    | -0.00515     |
|    lagrangian_multiplier | 0.0611       |
|    learning_rate         | 0.0003       |
|    loss                  | 59.7         |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.0084      |
|    std                   | 0.863        |
|    value_loss            | 434          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.2830973] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 47           |
|    time_elapsed          | 752          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.005678654  |
|    clip_fraction         | 0.05         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 158          |
|    entropy_loss          | -2.7         |
|    explained_variance    | -0.0174      |
|    lagrangian_multiplier | 0.0565       |
|    learning_rate         | 0.0003       |
|    loss                  | 48.6         |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.00974     |
|    std                   | 0.94         |
|    value_loss            | 315          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3723112] |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 33           |
|    time_elapsed          | 624          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0069334423 |
|    clip_fraction         | 0.0689       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0711       |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.0186      |
|    lagrangian_multiplier | 0.0684       |
|    learning_rate         | 0.0003       |
|    loss                  | 71.4         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00956     |
|    std                   | 0.95         |
|    value_loss            | 757          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5351279] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 48           |
|    time_elapsed          | 766          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.0072797164 |
|    clip_fraction         | 0.0737       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 154          |
|    entropy_loss          | -2.54        |
|    explained_variance    | -0.00961     |
|    lagrangian_multiplier | 0.0709       |
|    learning_rate         | 0.0003       |
|    loss                  | 85.1         |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00759     |
|    std                   | 0.864        |
|    value_loss            | 766          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1384511] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 39           |
|    time_elapsed          | 704          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.007871216  |
|    clip_fraction         | 0.0725       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 187          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0111       |
|    lagrangian_multiplier | 0.044        |
|    learning_rate         | 0.0003       |
|    loss                  | 75.4         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.0109      |
|    std                   | 0.975        |
|    value_loss            | 401          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.5913679] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 48           |
|    time_elapsed          | 768          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.0072165383 |
|    clip_fraction         | 0.0644       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 183          |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.00862     |
|    lagrangian_multiplier | 0.0501       |
|    learning_rate         | 0.0003       |
|    loss                  | 122          |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.0123      |
|    std                   | 0.969        |
|    value_loss            | 679          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0143032] |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 34           |
|    time_elapsed          | 641          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0060725296 |
|    clip_fraction         | 0.068        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0863       |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.0171      |
|    lagrangian_multiplier | 0.0612       |
|    learning_rate         | 0.0003       |
|    loss                  | 59.5         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00873     |
|    std                   | 0.947        |
|    value_loss            | 590          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.2868629] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 49           |
|    time_elapsed          | 782          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.0068160295 |
|    clip_fraction         | 0.0728       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 144          |
|    entropy_loss          | -2.54        |
|    explained_variance    | -0.00105     |
|    lagrangian_multiplier | 0.0741       |
|    learning_rate         | 0.0003       |
|    loss                  | 73.7         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.00715     |
|    std                   | 0.862        |
|    value_loss            | 696          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
------------------------------------------
| reward                   | [-1.396338] |
| time/                    |             |
|    fps                   | 113         |
|    iterations            | 40          |
|    time_elapsed          | 722         |
|    total_timesteps       | 81920       |
| train/                   |             |
|    approx_kl             | 0.009024095 |
|    clip_fraction         | 0.0752      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 216         |
|    entropy_loss          | -2.79       |
|    explained_variance    | 0.00644     |
|    lagrangian_multiplier | 0.0443      |
|    learning_rate         | 0.0003      |
|    loss                  | 120         |
|    n_updates             | 390         |
|    policy_gradient_loss  | -0.0138     |
|    std                   | 0.974       |
|    value_loss            | 608         |
------------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÅ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñá‚ñà‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÉ
wandb:             train/approx_kl ‚ñÖ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñá‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñà‚ñÑ‚ñÜ‚ñÖ‚ñÑ
wandb:         train/clip_fraction ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÖ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÇ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñá‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñÜ‚ñá‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñá‚ñÖ‚ñÖ
wandb:          train/entropy_loss ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:    train/explained_variance ‚ñá‚ñÉ‚ñÖ‚ñà‚ñÑ‚ñÖ‚ñÇ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñá‚ñÖ‚ñÖ‚ñà‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñá‚ñÉ‚ñÅ‚ñÖ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÖ
wandb: train/lagrangian_multiplier ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÅ‚ñà‚ñá‚ñÇ‚ñÅ‚ñÇ‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÑ‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÜ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÇ‚ñÖ‚ñÜ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÉ‚ñÖ‚ñà‚ñÅ‚ñÅ‚ñÖ‚ñÜ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÉ‚ñá‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÜ
wandb:                   train/std ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            train/value_loss ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñà‚ñÅ‚ñÅ‚ñÜ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb:                      reward -2.28686
wandb:             train/approx_kl 0.00682
wandb:         train/clip_fraction 0.07275
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 144.12672
wandb:          train/entropy_loss -2.54096
wandb:    train/explained_variance -0.00105
wandb: train/lagrangian_multiplier 0.07414
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 73.71511
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00715
wandb:                   train/std 0.86159
wandb:            train/value_loss 696.01307
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/wy5oyvhe
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_230804-wy5oyvhe/logs
srun: error: dqn.ist.berkeley.edu: task 0: Exited with exit code 1
--------------------------------------------
| reward                   | [-0.41818213] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 49            |
|    time_elapsed          | 784           |
|    total_timesteps       | 100352        |
| train/                   |               |
|    approx_kl             | 0.007257593   |
|    clip_fraction         | 0.0612        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 143           |
|    entropy_loss          | -2.77         |
|    explained_variance    | -0.000913     |
|    lagrangian_multiplier | 0.0582        |
|    learning_rate         | 0.0003        |
|    loss                  | 126           |
|    n_updates             | 480           |
|    policy_gradient_loss  | -0.0112       |
|    std                   | 0.967         |
|    value_loss            | 993           |
--------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
-------------------------------------------
| reward                   | [-2.10164]   |
| time/                    |              |
|    fps                   | 109          |
|    iterations            | 35           |
|    time_elapsed          | 657          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0070041483 |
|    clip_fraction         | 0.065        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0846       |
|    entropy_loss          | -2.74        |
|    explained_variance    | -0.0163      |
|    lagrangian_multiplier | 0.0635       |
|    learning_rate         | 0.0003       |
|    loss                  | 57.7         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.958        |
|    value_loss            | 606          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114549.8
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÅ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñà‚ñà
wandb:             train/approx_kl ‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÉ‚ñá‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñá‚ñá
wandb:         train/clip_fraction ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÉ‚ñà‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñá‚ñÉ‚ñÑ‚ñÜ‚ñÜ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñá‚ñÅ‚ñá‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñà‚ñÖ‚ñá‚ñà‚ñÜ‚ñÑ‚ñá‚ñá‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÑ
wandb:          train/entropy_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÖ‚ñÑ‚ñÉ
wandb:    train/explained_variance ‚ñÖ‚ñÑ‚ñá‚ñà‚ñÑ‚ñÜ‚ñÖ‚ñà‚ñà‚ñá‚ñá‚ñÖ‚ñÅ‚ñÑ‚ñá‚ñà‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÜ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÅ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÜ
wandb: train/lagrangian_multiplier ‚ñÇ‚ñÜ‚ñÅ‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñá‚ñÜ‚ñÇ‚ñÖ‚ñÅ‚ñÉ‚ñà‚ñá‚ñÑ‚ñÇ‚ñÖ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÜ‚ñÜ‚ñÉ‚ñÅ‚ñÉ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÉ‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÑ‚ñÑ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÇ‚ñÑ‚ñá‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÇ
wandb:                   train/std ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñá‚ñá
wandb:            train/value_loss ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÉ‚ñÅ‚ñÉ‚ñÑ
wandb: 
wandb: Run summary:
wandb:                      reward -0.41818
wandb:             train/approx_kl 0.00726
wandb:         train/clip_fraction 0.06123
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 143.47116
wandb:          train/entropy_loss -2.76665
wandb:    train/explained_variance -0.00091
wandb: train/lagrangian_multiplier 0.05822
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 125.61754
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.01118
wandb:                   train/std 0.9671
wandb:            train/value_loss 993.24639
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2/runs/zrkjmj9p
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_230811-zrkjmj9p/logs
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_232122-6weznita
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16/runs/6weznita
srun: error: gail.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-3.158516]  |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 41           |
|    time_elapsed          | 740          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0073462324 |
|    clip_fraction         | 0.0786       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 221          |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.00691     |
|    lagrangian_multiplier | 0.054        |
|    learning_rate         | 0.0003       |
|    loss                  | 126          |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.0124      |
|    std                   | 0.975        |
|    value_loss            | 817          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8869675] |
| time/                    |              |
|    fps                   | 109          |
|    iterations            | 36           |
|    time_elapsed          | 673          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.006784006  |
|    clip_fraction         | 0.0601       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.103        |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0182      |
|    lagrangian_multiplier | 0.0508       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.9         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00782     |
|    std                   | 0.964        |
|    value_loss            | 566          |
-------------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.5448293] |
| time/              |              |
|    fps             | 135          |
|    iterations      | 1            |
|    time_elapsed    | 15           |
|    total_timesteps | 2048         |
-------------------------------------
-------------------------------------------
| reward                   | [-2.5856512] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 42           |
|    time_elapsed          | 758          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0077301506 |
|    clip_fraction         | 0.0772       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 258          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0331      |
|    lagrangian_multiplier | 0.0509       |
|    learning_rate         | 0.0003       |
|    loss                  | 165          |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.0114      |
|    std                   | 0.97         |
|    value_loss            | 1.26e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.41431463] |
| time/                    |               |
|    fps                   | 109           |
|    iterations            | 37            |
|    time_elapsed          | 689           |
|    total_timesteps       | 75776         |
| train/                   |               |
|    approx_kl             | 0.008848149   |
|    clip_fraction         | 0.0986        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0214        |
|    entropy_loss          | -2.75         |
|    explained_variance    | -0.000391     |
|    lagrangian_multiplier | 0.0589        |
|    learning_rate         | 0.0003        |
|    loss                  | 46.8          |
|    n_updates             | 360           |
|    policy_gradient_loss  | -0.0106       |
|    std                   | 0.961         |
|    value_loss            | 439           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.47416008] |
| time/                    |               |
|    fps                   | 131           |
|    iterations            | 2             |
|    time_elapsed          | 31            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.00477913    |
|    clip_fraction         | 0.0349        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.579         |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.0479       |
|    lagrangian_multiplier | 0.0666        |
|    learning_rate         | 0.0003        |
|    loss                  | 45.9          |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00598      |
|    std                   | 0.993         |
|    value_loss            | 437           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3460808] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 43           |
|    time_elapsed          | 776          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.008052807  |
|    clip_fraction         | 0.0754       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 210          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0266      |
|    lagrangian_multiplier | 0.047        |
|    learning_rate         | 0.0003       |
|    loss                  | 176          |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.012       |
|    std                   | 0.969        |
|    value_loss            | 1.11e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.76179194] |
| time/                    |               |
|    fps                   | 130           |
|    iterations            | 3             |
|    time_elapsed          | 47            |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.005833324   |
|    clip_fraction         | 0.05          |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.165         |
|    entropy_loss          | -2.83         |
|    explained_variance    | 0.0372        |
|    lagrangian_multiplier | 0.0566        |
|    learning_rate         | 0.0003        |
|    loss                  | 21.1          |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00585      |
|    std                   | 0.998         |
|    value_loss            | 198           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.7490131] |
| time/                    |              |
|    fps                   | 109          |
|    iterations            | 38           |
|    time_elapsed          | 709          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.008074525  |
|    clip_fraction         | 0.072        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0834       |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.00255      |
|    lagrangian_multiplier | 0.0562       |
|    learning_rate         | 0.0003       |
|    loss                  | 117          |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.0135      |
|    std                   | 0.951        |
|    value_loss            | 1.07e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5849399] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 44           |
|    time_elapsed          | 794          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.008535644  |
|    clip_fraction         | 0.0896       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 252          |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0871       |
|    lagrangian_multiplier | 0.0479       |
|    learning_rate         | 0.0003       |
|    loss                  | 81.5         |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.0118      |
|    std                   | 0.964        |
|    value_loss            | 438          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.34892085] |
| time/                    |               |
|    fps                   | 129           |
|    iterations            | 4             |
|    time_elapsed          | 63            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.0060039833  |
|    clip_fraction         | 0.0608        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.239         |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.0243       |
|    lagrangian_multiplier | 0.0582        |
|    learning_rate         | 0.0003        |
|    loss                  | 34.1          |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00676      |
|    std                   | 1             |
|    value_loss            | 343           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.3275895] |
| time/                    |              |
|    fps                   | 110          |
|    iterations            | 39           |
|    time_elapsed          | 725          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0072681634 |
|    clip_fraction         | 0.0632       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0168       |
|    entropy_loss          | -2.71        |
|    explained_variance    | -0.0245      |
|    lagrangian_multiplier | 0.0756       |
|    learning_rate         | 0.0003       |
|    loss                  | 22.9         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00854     |
|    std                   | 0.929        |
|    value_loss            | 242          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0897046] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 45           |
|    time_elapsed          | 812          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.006267268  |
|    clip_fraction         | 0.0754       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 257          |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.000668    |
|    lagrangian_multiplier | 0.0459       |
|    learning_rate         | 0.0003       |
|    loss                  | 159          |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.0104      |
|    std                   | 0.971        |
|    value_loss            | 871          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9689111] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 5            |
|    time_elapsed          | 79           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.004062117  |
|    clip_fraction         | 0.0319       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.579        |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.00152      |
|    lagrangian_multiplier | 0.0542       |
|    learning_rate         | 0.0003       |
|    loss                  | 60.8         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00526     |
|    std                   | 1.01         |
|    value_loss            | 545          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114549.9
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.50182503] |
| time/                    |               |
|    fps                   | 110           |
|    iterations            | 40            |
|    time_elapsed          | 741           |
|    total_timesteps       | 81920         |
| train/                   |               |
|    approx_kl             | 0.007031086   |
|    clip_fraction         | 0.0512        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.148         |
|    entropy_loss          | -2.69         |
|    explained_variance    | 0.000218      |
|    lagrangian_multiplier | 0.0717        |
|    learning_rate         | 0.0003        |
|    loss                  | 122           |
|    n_updates             | 390           |
|    policy_gradient_loss  | -0.00519      |
|    std                   | 0.929         |
|    value_loss            | 1.27e+03      |
--------------------------------------------
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_232247-ol3grgtc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/ol3grgtc
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.9249237] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 46           |
|    time_elapsed          | 830          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0060632583 |
|    clip_fraction         | 0.0552       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 271          |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0284      |
|    lagrangian_multiplier | 0.0679       |
|    learning_rate         | 0.0003       |
|    loss                  | 117          |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.0105      |
|    std                   | 0.98         |
|    value_loss            | 1.03e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1004139] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 6            |
|    time_elapsed          | 95           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.004118237  |
|    clip_fraction         | 0.0424       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.139        |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0162       |
|    lagrangian_multiplier | 0.0663       |
|    learning_rate         | 0.0003       |
|    loss                  | 45.1         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00404     |
|    std                   | 1.01         |
|    value_loss            | 401          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9936782] |
| time/                    |              |
|    fps                   | 110          |
|    iterations            | 41           |
|    time_elapsed          | 758          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.006799961  |
|    clip_fraction         | 0.0568       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0838       |
|    entropy_loss          | -2.69        |
|    explained_variance    | -0.00325     |
|    lagrangian_multiplier | 0.0666       |
|    learning_rate         | 0.0003       |
|    loss                  | 77           |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00906     |
|    std                   | 0.932        |
|    value_loss            | 826          |
-------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.49280506] |
| time/              |               |
|    fps             | 135           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.255554]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 7            |
|    time_elapsed          | 111          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.0042523677 |
|    clip_fraction         | 0.0308       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.342        |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0101       |
|    lagrangian_multiplier | 0.0716       |
|    learning_rate         | 0.0003       |
|    loss                  | 93.8         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00443     |
|    std                   | 1.01         |
|    value_loss            | 1.05e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.875061]  |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 42           |
|    time_elapsed          | 774          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0089254165 |
|    clip_fraction         | 0.0815       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0248       |
|    entropy_loss          | -2.7         |
|    explained_variance    | 0.0341       |
|    lagrangian_multiplier | 0.07         |
|    learning_rate         | 0.0003       |
|    loss                  | 19.9         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.0105      |
|    std                   | 0.941        |
|    value_loss            | 221          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.3982559] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 47           |
|    time_elapsed          | 849          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.007234944  |
|    clip_fraction         | 0.0639       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 239          |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0465       |
|    lagrangian_multiplier | 0.0478       |
|    learning_rate         | 0.0003       |
|    loss                  | 97.8         |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.00866     |
|    std                   | 0.981        |
|    value_loss            | 505          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5870189] |
| time/                    |              |
|    fps                   | 131          |
|    iterations            | 2            |
|    time_elapsed          | 31           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0048002144 |
|    clip_fraction         | 0.0401       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0504       |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0988      |
|    lagrangian_multiplier | 0.0641       |
|    learning_rate         | 0.0003       |
|    loss                  | 26           |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00516     |
|    std                   | 0.997        |
|    value_loss            | 265          |
-------------------------------------------
srun: Job 114549 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.0249708] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 8            |
|    time_elapsed          | 128          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.005919438  |
|    clip_fraction         | 0.0563       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.228        |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0337       |
|    lagrangian_multiplier | 0.0471       |
|    learning_rate         | 0.0003       |
|    loss                  | 144          |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00668     |
|    std                   | 0.999        |
|    value_loss            | 1.34e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7543957] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 43           |
|    time_elapsed          | 791          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.008034123  |
|    clip_fraction         | 0.0717       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.156        |
|    entropy_loss          | -2.7         |
|    explained_variance    | -0.0196      |
|    lagrangian_multiplier | 0.0668       |
|    learning_rate         | 0.0003       |
|    loss                  | 64.9         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00733     |
|    std                   | 0.936        |
|    value_loss            | 653          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.36620435] |
| time/                    |               |
|    fps                   | 130           |
|    iterations            | 3             |
|    time_elapsed          | 47            |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.004943465   |
|    clip_fraction         | 0.0479        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.324         |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.0524        |
|    lagrangian_multiplier | 0.0583        |
|    learning_rate         | 0.0003        |
|    loss                  | 60.2          |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00553      |
|    std                   | 0.987         |
|    value_loss            | 541           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.8909424] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 48           |
|    time_elapsed          | 867          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.005393145  |
|    clip_fraction         | 0.0583       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 261          |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0439      |
|    lagrangian_multiplier | 0.0614       |
|    learning_rate         | 0.0003       |
|    loss                  | 193          |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00867     |
|    std                   | 0.989        |
|    value_loss            | 1.41e+03     |
-------------------------------------------
slurmstepd: error: *** STEP 114549.9 ON gail.ist.berkeley.edu CANCELLED AT 2023-12-26T23:23:36 ***
slurmstepd: error: *** STEP 114549.8 ON dqn.ist.berkeley.edu CANCELLED AT 2023-12-26T23:23:36 ***
slurmstepd: error: *** STEP 114549.6 ON ddpg.ist.berkeley.edu CANCELLED AT 2023-12-26T23:23:36 ***
slurmstepd: error: *** JOB 114549 ON airl.ist.berkeley.edu CANCELLED AT 2023-12-27T07:23:36 ***
slurmstepd: error: *** STEP 114549.7 ON airl.ist.berkeley.edu CANCELLED AT 2023-12-27T07:23:36 ***
