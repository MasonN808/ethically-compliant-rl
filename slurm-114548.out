Running script with speed limit: 2
Running script with speed limit: 4
Running script with speed limit: 8
Running script with speed limit: 16
Running script with speed limit: 2
Running script with speed limit: 4
Running script with speed limit: 8
Running script with speed limit: 16
Running script with speed limit: 2
Running script with speed limit: 4
Running script with speed limit: 8
Running script with speed limit: 16
Running script with speed limit: 2
Running script with speed limit: 4
Running script with speed limit: 8
Running script with speed limit: 16
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_194359-3cdf4gu3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16/runs/3cdf4gu3
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_194359-gg2qhntv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2/runs/gg2qhntv
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231227_034359-9s8lka6w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2/runs/9s8lka6w
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_194359-9wbc9ht6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/9wbc9ht6
Using cpu device
--------------------------------------
| reward             | [-0.31438392] |
| time/              |               |
|    fps             | 135           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.45121235] |
| time/              |               |
|    fps             | 132           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.34132895] |
| time/              |               |
|    fps             | 134           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.49265492] |
| time/              |               |
|    fps             | 118           |
|    iterations      | 1             |
|    time_elapsed    | 17            |
|    total_timesteps | 2048          |
--------------------------------------
-------------------------------------------
| reward                   | [-1.0004226] |
| time/                    |              |
|    fps                   | 132          |
|    iterations            | 2            |
|    time_elapsed          | 30           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.007231254  |
|    clip_fraction         | 0.0588       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 186          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.00203      |
|    lagrangian_multiplier | 0.0607       |
|    learning_rate         | 0.0003       |
|    loss                  | 84.5         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00716     |
|    std                   | 0.994        |
|    value_loss            | 716          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6713551] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 2            |
|    time_elapsed          | 31           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0051030503 |
|    clip_fraction         | 0.0319       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.104        |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0276      |
|    lagrangian_multiplier | 0.0671       |
|    learning_rate         | 0.0003       |
|    loss                  | 21.9         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00405     |
|    std                   | 0.99         |
|    value_loss            | 229          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.55031335] |
| time/                    |               |
|    fps                   | 115           |
|    iterations            | 2             |
|    time_elapsed          | 35            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.006634167   |
|    clip_fraction         | 0.0599        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0278        |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.0548        |
|    lagrangian_multiplier | 0.0681        |
|    learning_rate         | 0.0003        |
|    loss                  | 43.8          |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00674      |
|    std                   | 1.01          |
|    value_loss            | 472           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.8384087] |
| time/                    |              |
|    fps                   | 131          |
|    iterations            | 3            |
|    time_elapsed          | 46           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0054177903 |
|    clip_fraction         | 0.0442       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 142          |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0314       |
|    lagrangian_multiplier | 0.0575       |
|    learning_rate         | 0.0003       |
|    loss                  | 49.7         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00633     |
|    std                   | 1            |
|    value_loss            | 324          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6705364] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 3            |
|    time_elapsed          | 48           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.005716819  |
|    clip_fraction         | 0.0562       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.149        |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.038        |
|    lagrangian_multiplier | 0.0633       |
|    learning_rate         | 0.0003       |
|    loss                  | 43.1         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00703     |
|    std                   | 0.995        |
|    value_loss            | 410          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7535415] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 3            |
|    time_elapsed          | 53           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0061309445 |
|    clip_fraction         | 0.0614       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.148        |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0299      |
|    lagrangian_multiplier | 0.0535       |
|    learning_rate         | 0.0003       |
|    loss                  | 81.3         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00569     |
|    std                   | 1            |
|    value_loss            | 675          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.76664126] |
| time/                    |               |
|    fps                   | 130           |
|    iterations            | 4             |
|    time_elapsed          | 62            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.0061627626  |
|    clip_fraction         | 0.0636        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 249           |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.00656      |
|    lagrangian_multiplier | 0.0524        |
|    learning_rate         | 0.0003        |
|    loss                  | 116           |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00897      |
|    std                   | 0.993         |
|    value_loss            | 780           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.8404843] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 4            |
|    time_elapsed          | 64           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.0049870927 |
|    clip_fraction         | 0.0421       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.057        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0441       |
|    lagrangian_multiplier | 0.0731       |
|    learning_rate         | 0.0003       |
|    loss                  | 33.7         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00501     |
|    std                   | 0.995        |
|    value_loss            | 309          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8716782] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 4            |
|    time_elapsed          | 72           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.005670105  |
|    clip_fraction         | 0.0505       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.03         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.00741     |
|    lagrangian_multiplier | 0.051        |
|    learning_rate         | 0.0003       |
|    loss                  | 62.9         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00644     |
|    std                   | 1            |
|    value_loss            | 455          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6198992] |
| time/                    |              |
|    fps                   | 52           |
|    iterations            | 2            |
|    time_elapsed          | 77           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0063060396 |
|    clip_fraction         | 0.0498       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 217          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0103       |
|    lagrangian_multiplier | 0.0693       |
|    learning_rate         | 0.0003       |
|    loss                  | 61.7         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.0064      |
|    std                   | 0.995        |
|    value_loss            | 387          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.190556] |
| time/                    |             |
|    fps                   | 130         |
|    iterations            | 5           |
|    time_elapsed          | 78          |
|    total_timesteps       | 10240       |
| train/                   |             |
|    approx_kl             | 0.004049261 |
|    clip_fraction         | 0.0333      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 105         |
|    entropy_loss          | -2.83       |
|    explained_variance    | -0.0451     |
|    lagrangian_multiplier | 0.0552      |
|    learning_rate         | 0.0003      |
|    loss                  | 54.4        |
|    n_updates             | 40          |
|    policy_gradient_loss  | -0.0057     |
|    std                   | 0.997       |
|    value_loss            | 347         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.3094513] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 5            |
|    time_elapsed          | 80           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0055736057 |
|    clip_fraction         | 0.0355       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0476       |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.00227     |
|    lagrangian_multiplier | 0.058        |
|    learning_rate         | 0.0003       |
|    loss                  | 26.5         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00354     |
|    std                   | 1            |
|    value_loss            | 253          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.90995055] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 5             |
|    time_elapsed          | 90            |
|    total_timesteps       | 10240         |
| train/                   |               |
|    approx_kl             | 0.0057447916  |
|    clip_fraction         | 0.0493        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 4.35          |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.0144        |
|    lagrangian_multiplier | 0.0545        |
|    learning_rate         | 0.0003        |
|    loss                  | 56.5          |
|    n_updates             | 40            |
|    policy_gradient_loss  | -0.00671      |
|    std                   | 0.997         |
|    value_loss            | 563           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.8246123] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 6            |
|    time_elapsed          | 94           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.005330858  |
|    clip_fraction         | 0.0341       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 144          |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.00924      |
|    lagrangian_multiplier | 0.0571       |
|    learning_rate         | 0.0003       |
|    loss                  | 100          |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00609     |
|    std                   | 0.985        |
|    value_loss            | 816          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8193213] |
| time/                    |              |
|    fps                   | 65           |
|    iterations            | 3            |
|    time_elapsed          | 94           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0062884316 |
|    clip_fraction         | 0.048        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 176          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0177      |
|    lagrangian_multiplier | 0.0621       |
|    learning_rate         | 0.0003       |
|    loss                  | 54.7         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00477     |
|    std                   | 1            |
|    value_loss            | 377          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2204001] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 6            |
|    time_elapsed          | 96           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.005923317  |
|    clip_fraction         | 0.0506       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.142        |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0192       |
|    lagrangian_multiplier | 0.0477       |
|    learning_rate         | 0.0003       |
|    loss                  | 75.8         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00728     |
|    std                   | 1            |
|    value_loss            | 638          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.407153] |
| time/                    |             |
|    fps                   | 113         |
|    iterations            | 6           |
|    time_elapsed          | 108         |
|    total_timesteps       | 12288       |
| train/                   |             |
|    approx_kl             | 0.005607065 |
|    clip_fraction         | 0.0428      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 1.63        |
|    entropy_loss          | -2.83       |
|    explained_variance    | -0.0128     |
|    lagrangian_multiplier | 0.0597      |
|    learning_rate         | 0.0003      |
|    loss                  | 34          |
|    n_updates             | 50          |
|    policy_gradient_loss  | -0.00671    |
|    std                   | 0.995       |
|    value_loss            | 286         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.341879]  |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 7            |
|    time_elapsed          | 110          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.0063725766 |
|    clip_fraction         | 0.0448       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 96.6         |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.017       |
|    lagrangian_multiplier | 0.0716       |
|    learning_rate         | 0.0003       |
|    loss                  | 37.1         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00554     |
|    std                   | 0.984        |
|    value_loss            | 348          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1160172] |
| time/                    |              |
|    fps                   | 74           |
|    iterations            | 4            |
|    time_elapsed          | 110          |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.004111096  |
|    clip_fraction         | 0.0325       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 140          |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0825       |
|    lagrangian_multiplier | 0.0637       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.1         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00448     |
|    std                   | 1            |
|    value_loss            | 350          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3781345] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 7            |
|    time_elapsed          | 113          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.0027344306 |
|    clip_fraction         | 0.0276       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0432       |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0155      |
|    lagrangian_multiplier | 0.0555       |
|    learning_rate         | 0.0003       |
|    loss                  | 46           |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00239     |
|    std                   | 1            |
|    value_loss            | 425          |
-------------------------------------------
srun: Job 114548 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation temporarily disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-1.890958] |
| time/                    |             |
|    fps                   | 129         |
|    iterations            | 8           |
|    time_elapsed          | 126         |
|    total_timesteps       | 16384       |
| train/                   |             |
|    approx_kl             | 0.006545199 |
|    clip_fraction         | 0.0688      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 227         |
|    entropy_loss          | -2.81       |
|    explained_variance    | -0.014      |
|    lagrangian_multiplier | 0.0669      |
|    learning_rate         | 0.0003      |
|    loss                  | 119         |
|    n_updates             | 70          |
|    policy_gradient_loss  | -0.00847    |
|    std                   | 0.987       |
|    value_loss            | 929         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.82867247] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 7             |
|    time_elapsed          | 126           |
|    total_timesteps       | 14336         |
| train/                   |               |
|    approx_kl             | 0.0046036453  |
|    clip_fraction         | 0.0564        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.237         |
|    entropy_loss          | -2.82         |
|    explained_variance    | -0.0312       |
|    lagrangian_multiplier | 0.0468        |
|    learning_rate         | 0.0003        |
|    loss                  | 65.6          |
|    n_updates             | 60            |
|    policy_gradient_loss  | -0.00626      |
|    std                   | 0.989         |
|    value_loss            | 553           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.82454044] |
| time/                    |               |
|    fps                   | 80            |
|    iterations            | 5             |
|    time_elapsed          | 126           |
|    total_timesteps       | 10240         |
| train/                   |               |
|    approx_kl             | 0.0044435654  |
|    clip_fraction         | 0.0377        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 41.6          |
|    entropy_loss          | -2.82         |
|    explained_variance    | -0.134        |
|    lagrangian_multiplier | 0.0762        |
|    learning_rate         | 0.0003        |
|    loss                  | 55.4          |
|    n_updates             | 40            |
|    policy_gradient_loss  | -0.0052       |
|    std                   | 0.98          |
|    value_loss            | 575           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.69380903] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 8             |
|    time_elapsed          | 129           |
|    total_timesteps       | 16384         |
| train/                   |               |
|    approx_kl             | 0.0068369927  |
|    clip_fraction         | 0.0494        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.111         |
|    entropy_loss          | -2.86         |
|    explained_variance    | 0.0154        |
|    lagrangian_multiplier | 0.0383        |
|    learning_rate         | 0.0003        |
|    loss                  | 97            |
|    n_updates             | 70            |
|    policy_gradient_loss  | -0.00643      |
|    std                   | 1.01          |
|    value_loss            | 644           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.9577506] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 9            |
|    time_elapsed          | 142          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0042522205 |
|    clip_fraction         | 0.0351       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 154          |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.00198      |
|    lagrangian_multiplier | 0.0699       |
|    learning_rate         | 0.0003       |
|    loss                  | 61.6         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00455     |
|    std                   | 0.985        |
|    value_loss            | 464          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8990372] |
| time/                    |              |
|    fps                   | 85           |
|    iterations            | 6            |
|    time_elapsed          | 143          |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.004540896  |
|    clip_fraction         | 0.0249       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 121          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.00817     |
|    lagrangian_multiplier | 0.0722       |
|    learning_rate         | 0.0003       |
|    loss                  | 74.8         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00417     |
|    std                   | 0.968        |
|    value_loss            | 598          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1975651] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 8            |
|    time_elapsed          | 145          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0057225227 |
|    clip_fraction         | 0.0471       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.48         |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0737      |
|    lagrangian_multiplier | 0.0594       |
|    learning_rate         | 0.0003       |
|    loss                  | 37.5         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00761     |
|    std                   | 0.987        |
|    value_loss            | 396          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5884225] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 9            |
|    time_elapsed          | 145          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0043819845 |
|    clip_fraction         | 0.0386       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.127        |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0353       |
|    lagrangian_multiplier | 0.0604       |
|    learning_rate         | 0.0003       |
|    loss                  | 29.8         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00588     |
|    std                   | 1.01         |
|    value_loss            | 317          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8702072] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 10           |
|    time_elapsed          | 158          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.006747592  |
|    clip_fraction         | 0.0593       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 252          |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.00226     |
|    lagrangian_multiplier | 0.0643       |
|    learning_rate         | 0.0003       |
|    loss                  | 103          |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00736     |
|    std                   | 0.975        |
|    value_loss            | 876          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6919502] |
| time/                    |              |
|    fps                   | 90           |
|    iterations            | 7            |
|    time_elapsed          | 159          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.006255617  |
|    clip_fraction         | 0.0689       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 123          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.00722     |
|    lagrangian_multiplier | 0.0857       |
|    learning_rate         | 0.0003       |
|    loss                  | 49.7         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.0107      |
|    std                   | 0.972        |
|    value_loss            | 529          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9250852] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 10           |
|    time_elapsed          | 162          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.005286146  |
|    clip_fraction         | 0.0685       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.16         |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0132       |
|    lagrangian_multiplier | 0.0572       |
|    learning_rate         | 0.0003       |
|    loss                  | 99           |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00915     |
|    std                   | 1.02         |
|    value_loss            | 811          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6626966] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 9            |
|    time_elapsed          | 163          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0062041366 |
|    clip_fraction         | 0.0586       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.856        |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0271      |
|    lagrangian_multiplier | 0.0496       |
|    learning_rate         | 0.0003       |
|    loss                  | 24           |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00648     |
|    std                   | 0.995        |
|    value_loss            | 199          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.233331] |
| time/                    |             |
|    fps                   | 128         |
|    iterations            | 11          |
|    time_elapsed          | 174         |
|    total_timesteps       | 22528       |
| train/                   |             |
|    approx_kl             | 0.006765771 |
|    clip_fraction         | 0.0612      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 254         |
|    entropy_loss          | -2.78       |
|    explained_variance    | 0.00456     |
|    lagrangian_multiplier | 0.0618      |
|    learning_rate         | 0.0003      |
|    loss                  | 122         |
|    n_updates             | 100         |
|    policy_gradient_loss  | -0.00628    |
|    std                   | 0.972       |
|    value_loss            | 924         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.0347857] |
| time/                    |              |
|    fps                   | 93           |
|    iterations            | 8            |
|    time_elapsed          | 175          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.009273496  |
|    clip_fraction         | 0.0823       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 163          |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0602      |
|    lagrangian_multiplier | 0.0765       |
|    learning_rate         | 0.0003       |
|    loss                  | 43           |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.0105      |
|    std                   | 0.992        |
|    value_loss            | 362          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7881064] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 11           |
|    time_elapsed          | 178          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0072563356 |
|    clip_fraction         | 0.0726       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.161        |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0421       |
|    lagrangian_multiplier | 0.0614       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.8         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00899     |
|    std                   | 1.02         |
|    value_loss            | 483          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6593747] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 10           |
|    time_elapsed          | 181          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.005647869  |
|    clip_fraction         | 0.0423       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0486       |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0684      |
|    lagrangian_multiplier | 0.0581       |
|    learning_rate         | 0.0003       |
|    loss                  | 43.1         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00575     |
|    std                   | 0.994        |
|    value_loss            | 403          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3461074] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 12           |
|    time_elapsed          | 190          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.005619139  |
|    clip_fraction         | 0.0525       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 216          |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.00908      |
|    lagrangian_multiplier | 0.0719       |
|    learning_rate         | 0.0003       |
|    loss                  | 96.8         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00729     |
|    std                   | 0.982        |
|    value_loss            | 858          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.717245]  |
| time/                    |              |
|    fps                   | 96           |
|    iterations            | 9            |
|    time_elapsed          | 191          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0044371285 |
|    clip_fraction         | 0.0253       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 172          |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.00524      |
|    lagrangian_multiplier | 0.0808       |
|    learning_rate         | 0.0003       |
|    loss                  | 26.9         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.0046      |
|    std                   | 0.985        |
|    value_loss            | 232          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0800152] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 12           |
|    time_elapsed          | 194          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0075760977 |
|    clip_fraction         | 0.0789       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.148        |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0287       |
|    lagrangian_multiplier | 0.0509       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.6         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.0112      |
|    std                   | 1.01         |
|    value_loss            | 494          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.80998456] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 11            |
|    time_elapsed          | 200           |
|    total_timesteps       | 22528         |
| train/                   |               |
|    approx_kl             | 0.005792219   |
|    clip_fraction         | 0.061         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 4             |
|    entropy_loss          | -2.83         |
|    explained_variance    | 0.0201        |
|    lagrangian_multiplier | 0.058         |
|    learning_rate         | 0.0003        |
|    loss                  | 56.6          |
|    n_updates             | 100           |
|    policy_gradient_loss  | -0.00775      |
|    std                   | 0.994         |
|    value_loss            | 506           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.6467632] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 13           |
|    time_elapsed          | 206          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0068531036 |
|    clip_fraction         | 0.0698       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 174          |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0153       |
|    lagrangian_multiplier | 0.0628       |
|    learning_rate         | 0.0003       |
|    loss                  | 75           |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00752     |
|    std                   | 0.991        |
|    value_loss            | 577          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4462228] |
| time/                    |              |
|    fps                   | 98           |
|    iterations            | 10           |
|    time_elapsed          | 207          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.007651965  |
|    clip_fraction         | 0.0704       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 193          |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0424       |
|    lagrangian_multiplier | 0.0746       |
|    learning_rate         | 0.0003       |
|    loss                  | 86.1         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.0109      |
|    std                   | 0.979        |
|    value_loss            | 959          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8723599] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 13           |
|    time_elapsed          | 210          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.007246876  |
|    clip_fraction         | 0.0725       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.168        |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0344       |
|    lagrangian_multiplier | 0.05         |
|    learning_rate         | 0.0003       |
|    loss                  | 75.6         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00813     |
|    std                   | 1.01         |
|    value_loss            | 648          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9487948] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 12           |
|    time_elapsed          | 218          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0065850387 |
|    clip_fraction         | 0.0655       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0674       |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.017       |
|    lagrangian_multiplier | 0.0528       |
|    learning_rate         | 0.0003       |
|    loss                  | 55.4         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00728     |
|    std                   | 0.995        |
|    value_loss            | 408          |
-------------------------------------------
------------------------------------------
| reward                   | [-2.131901] |
| time/                    |             |
|    fps                   | 128         |
|    iterations            | 14          |
|    time_elapsed          | 222         |
|    total_timesteps       | 28672       |
| train/                   |             |
|    approx_kl             | 0.007289312 |
|    clip_fraction         | 0.0718      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 192         |
|    entropy_loss          | -2.83       |
|    explained_variance    | 0.00908     |
|    lagrangian_multiplier | 0.0694      |
|    learning_rate         | 0.0003      |
|    loss                  | 78.5        |
|    n_updates             | 130         |
|    policy_gradient_loss  | -0.0103     |
|    std                   | 0.996       |
|    value_loss            | 698         |
------------------------------------------
-------------------------------------------
| reward                   | [-0.6468954] |
| time/                    |              |
|    fps                   | 100          |
|    iterations            | 11           |
|    time_elapsed          | 224          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.00502358   |
|    clip_fraction         | 0.0378       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 29.5         |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0191      |
|    lagrangian_multiplier | 0.0521       |
|    learning_rate         | 0.0003       |
|    loss                  | 24.1         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00643     |
|    std                   | 0.992        |
|    value_loss            | 200          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.1493642] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 14           |
|    time_elapsed          | 227          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.008347033  |
|    clip_fraction         | 0.0925       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.146        |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0286       |
|    lagrangian_multiplier | 0.0575       |
|    learning_rate         | 0.0003       |
|    loss                  | 106          |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 1            |
|    value_loss            | 1.09e+03     |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.6590205] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 13           |
|    time_elapsed          | 237          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0061362595 |
|    clip_fraction         | 0.0447       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0388       |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.033       |
|    lagrangian_multiplier | 0.0676       |
|    learning_rate         | 0.0003       |
|    loss                  | 9.74         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.0068      |
|    std                   | 1.01         |
|    value_loss            | 114          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8185358] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 15           |
|    time_elapsed          | 238          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0066286963 |
|    clip_fraction         | 0.0599       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 218          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0198       |
|    lagrangian_multiplier | 0.0628       |
|    learning_rate         | 0.0003       |
|    loss                  | 83.3         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00999     |
|    std                   | 0.993        |
|    value_loss            | 551          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.6018877] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 15           |
|    time_elapsed          | 243          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.007970593  |
|    clip_fraction         | 0.0697       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.173        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0334       |
|    lagrangian_multiplier | 0.0508       |
|    learning_rate         | 0.0003       |
|    loss                  | 109          |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00728     |
|    std                   | 0.991        |
|    value_loss            | 891          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.5133566] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 16           |
|    time_elapsed          | 254          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.005921351  |
|    clip_fraction         | 0.0533       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 200          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.00964      |
|    lagrangian_multiplier | 0.0543       |
|    learning_rate         | 0.0003       |
|    loss                  | 117          |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00733     |
|    std                   | 0.996        |
|    value_loss            | 974          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0433009] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 14           |
|    time_elapsed          | 255          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0047338046 |
|    clip_fraction         | 0.0404       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0783       |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0388      |
|    lagrangian_multiplier | 0.0633       |
|    learning_rate         | 0.0003       |
|    loss                  | 23           |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00415     |
|    std                   | 0.994        |
|    value_loss            | 227          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1541411] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 16           |
|    time_elapsed          | 259          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0071119643 |
|    clip_fraction         | 0.0618       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0981       |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0407       |
|    lagrangian_multiplier | 0.0446       |
|    learning_rate         | 0.0003       |
|    loss                  | 89.8         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00947     |
|    std                   | 0.968        |
|    value_loss            | 667          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.65897554] |
| time/                    |               |
|    fps                   | 91            |
|    iterations            | 12            |
|    time_elapsed          | 269           |
|    total_timesteps       | 24576         |
| train/                   |               |
|    approx_kl             | 0.004424791   |
|    clip_fraction         | 0.0617        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 214           |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.0445        |
|    lagrangian_multiplier | 0.0673        |
|    learning_rate         | 0.0003        |
|    loss                  | 77.3          |
|    n_updates             | 110           |
|    policy_gradient_loss  | -0.00717      |
|    std                   | 0.99          |
|    value_loss            | 595           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.1319766] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 17           |
|    time_elapsed          | 270          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0070386473 |
|    clip_fraction         | 0.0523       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 168          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0142      |
|    lagrangian_multiplier | 0.0713       |
|    learning_rate         | 0.0003       |
|    loss                  | 56.9         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00759     |
|    std                   | 1.01         |
|    value_loss            | 474          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.92550653] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 15            |
|    time_elapsed          | 273           |
|    total_timesteps       | 30720         |
| train/                   |               |
|    approx_kl             | 0.004002182   |
|    clip_fraction         | 0.0333        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.032         |
|    entropy_loss          | -2.82         |
|    explained_variance    | -0.0568       |
|    lagrangian_multiplier | 0.0632        |
|    learning_rate         | 0.0003        |
|    loss                  | 28.5          |
|    n_updates             | 140           |
|    policy_gradient_loss  | -0.00303      |
|    std                   | 0.989         |
|    value_loss            | 254           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.7019143] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 17           |
|    time_elapsed          | 275          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.010889289  |
|    clip_fraction         | 0.0981       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.165        |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.00863      |
|    lagrangian_multiplier | 0.0508       |
|    learning_rate         | 0.0003       |
|    loss                  | 72.4         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.012       |
|    std                   | 0.964        |
|    value_loss            | 658          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.5298812] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 18           |
|    time_elapsed          | 286          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0064774044 |
|    clip_fraction         | 0.0507       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 219          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.00288     |
|    lagrangian_multiplier | 0.0657       |
|    learning_rate         | 0.0003       |
|    loss                  | 86.4         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00792     |
|    std                   | 1            |
|    value_loss            | 833          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2113391] |
| time/                    |              |
|    fps                   | 93           |
|    iterations            | 13           |
|    time_elapsed          | 285          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0053441157 |
|    clip_fraction         | 0.0432       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 116          |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0626       |
|    lagrangian_multiplier | 0.0687       |
|    learning_rate         | 0.0003       |
|    loss                  | 63.9         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.005       |
|    std                   | 0.983        |
|    value_loss            | 456          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.6000476] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 18           |
|    time_elapsed          | 292          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.006099926  |
|    clip_fraction         | 0.0539       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.147        |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.00623     |
|    lagrangian_multiplier | 0.0552       |
|    learning_rate         | 0.0003       |
|    loss                  | 157          |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00912     |
|    std                   | 0.975        |
|    value_loss            | 1.46e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2060028] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 16           |
|    time_elapsed          | 292          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.005912102  |
|    clip_fraction         | 0.0449       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0705       |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0839      |
|    lagrangian_multiplier | 0.067        |
|    learning_rate         | 0.0003       |
|    loss                  | 18.7         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00477     |
|    std                   | 0.991        |
|    value_loss            | 213          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.42756972] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 19            |
|    time_elapsed          | 301           |
|    total_timesteps       | 38912         |
| train/                   |               |
|    approx_kl             | 0.006381546   |
|    clip_fraction         | 0.0628        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 241           |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.00267       |
|    lagrangian_multiplier | 0.0687        |
|    learning_rate         | 0.0003        |
|    loss                  | 146           |
|    n_updates             | 180           |
|    policy_gradient_loss  | -0.00784      |
|    std                   | 1.01          |
|    value_loss            | 1.31e+03      |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.5172112] |
| time/                    |              |
|    fps                   | 95           |
|    iterations            | 14           |
|    time_elapsed          | 301          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0056550056 |
|    clip_fraction         | 0.0525       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 35.4         |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.00322      |
|    lagrangian_multiplier | 0.0516       |
|    learning_rate         | 0.0003       |
|    loss                  | 39.9         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00373     |
|    std                   | 0.974        |
|    value_loss            | 277          |
-------------------------------------------
------------------------------------------
| reward                   | [-3.547273] |
| time/                    |             |
|    fps                   | 126         |
|    iterations            | 19          |
|    time_elapsed          | 308         |
|    total_timesteps       | 38912       |
| train/                   |             |
|    approx_kl             | 0.005765886 |
|    clip_fraction         | 0.0418      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.165       |
|    entropy_loss          | -2.78       |
|    explained_variance    | 0.0183      |
|    lagrangian_multiplier | 0.0574      |
|    learning_rate         | 0.0003      |
|    loss                  | 99.6        |
|    n_updates             | 180         |
|    policy_gradient_loss  | -0.00599    |
|    std                   | 0.967       |
|    value_loss            | 1.03e+03    |
------------------------------------------
-------------------------------------------
| reward                   | [-2.1465142] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 17           |
|    time_elapsed          | 310          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0047264323 |
|    clip_fraction         | 0.0448       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0336       |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.049       |
|    lagrangian_multiplier | 0.0658       |
|    learning_rate         | 0.0003       |
|    loss                  | 17.6         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00478     |
|    std                   | 0.99         |
|    value_loss            | 196          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.9408402] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 20           |
|    time_elapsed          | 317          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0063391007 |
|    clip_fraction         | 0.0673       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 217          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0164      |
|    lagrangian_multiplier | 0.063        |
|    learning_rate         | 0.0003       |
|    loss                  | 90.9         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00678     |
|    std                   | 1.01         |
|    value_loss            | 716          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0770047] |
| time/                    |              |
|    fps                   | 96           |
|    iterations            | 15           |
|    time_elapsed          | 318          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0058229654 |
|    clip_fraction         | 0.0579       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 23           |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0549       |
|    lagrangian_multiplier | 0.0661       |
|    learning_rate         | 0.0003       |
|    loss                  | 21.4         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00499     |
|    std                   | 0.963        |
|    value_loss            | 208          |
-------------------------------------------
------------------------------------------
| reward                   | [-3.098762] |
| time/                    |             |
|    fps                   | 126         |
|    iterations            | 20          |
|    time_elapsed          | 324         |
|    total_timesteps       | 40960       |
| train/                   |             |
|    approx_kl             | 0.005332064 |
|    clip_fraction         | 0.0542      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.198       |
|    entropy_loss          | -2.76       |
|    explained_variance    | 0.0139      |
|    lagrangian_multiplier | 0.0664      |
|    learning_rate         | 0.0003      |
|    loss                  | 141         |
|    n_updates             | 190         |
|    policy_gradient_loss  | -0.00741    |
|    std                   | 0.961       |
|    value_loss            | 1.45e+03    |
------------------------------------------
--------------------------------------------
| reward                   | [-0.94204944] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 18            |
|    time_elapsed          | 328           |
|    total_timesteps       | 36864         |
| train/                   |               |
|    approx_kl             | 0.0066013485  |
|    clip_fraction         | 0.0466        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0391        |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.0529       |
|    lagrangian_multiplier | 0.0622        |
|    learning_rate         | 0.0003        |
|    loss                  | 46.1          |
|    n_updates             | 170           |
|    policy_gradient_loss  | -0.00563      |
|    std                   | 1             |
|    value_loss            | 421           |
--------------------------------------------
-------------------------------------------
| reward                   | [-3.0962727] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 21           |
|    time_elapsed          | 333          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.00783945   |
|    clip_fraction         | 0.0903       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 227          |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0085       |
|    lagrangian_multiplier | 0.0575       |
|    learning_rate         | 0.0003       |
|    loss                  | 132          |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00968     |
|    std                   | 1.01         |
|    value_loss            | 965          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3325301] |
| time/                    |              |
|    fps                   | 98           |
|    iterations            | 16           |
|    time_elapsed          | 334          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.006354928  |
|    clip_fraction         | 0.0461       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 15.8         |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0261      |
|    lagrangian_multiplier | 0.0848       |
|    learning_rate         | 0.0003       |
|    loss                  | 16           |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00481     |
|    std                   | 0.956        |
|    value_loss            | 190          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.0896301] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 21           |
|    time_elapsed          | 340          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0058115013 |
|    clip_fraction         | 0.0448       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.181        |
|    entropy_loss          | -2.75        |
|    explained_variance    | 0.0136       |
|    lagrangian_multiplier | 0.0562       |
|    learning_rate         | 0.0003       |
|    loss                  | 156          |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00756     |
|    std                   | 0.951        |
|    value_loss            | 1.4e+03      |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2358392] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 19           |
|    time_elapsed          | 347          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.005355704  |
|    clip_fraction         | 0.0334       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.133        |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0363      |
|    lagrangian_multiplier | 0.0509       |
|    learning_rate         | 0.0003       |
|    loss                  | 40.7         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00549     |
|    std                   | 0.99         |
|    value_loss            | 341          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.29837996] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 22            |
|    time_elapsed          | 349           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.0058506327  |
|    clip_fraction         | 0.0502        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 213           |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.0077        |
|    lagrangian_multiplier | 0.0678        |
|    learning_rate         | 0.0003        |
|    loss                  | 111           |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.00751      |
|    std                   | 1             |
|    value_loss            | 985           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.7012126] |
| time/                    |              |
|    fps                   | 99           |
|    iterations            | 17           |
|    time_elapsed          | 350          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0042637307 |
|    clip_fraction         | 0.0335       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 22.2         |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0971      |
|    lagrangian_multiplier | 0.089        |
|    learning_rate         | 0.0003       |
|    loss                  | 42.3         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00345     |
|    std                   | 0.961        |
|    value_loss            | 495          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.5540017] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 22           |
|    time_elapsed          | 357          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.007826053  |
|    clip_fraction         | 0.0741       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.232        |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.0243       |
|    lagrangian_multiplier | 0.0495       |
|    learning_rate         | 0.0003       |
|    loss                  | 240          |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00731     |
|    std                   | 0.947        |
|    value_loss            | 1.99e+03     |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.3911049] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 20           |
|    time_elapsed          | 365          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0066502206 |
|    clip_fraction         | 0.0676       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0377       |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0357      |
|    lagrangian_multiplier | 0.0636       |
|    learning_rate         | 0.0003       |
|    loss                  | 39           |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00833     |
|    std                   | 0.972        |
|    value_loss            | 397          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.46566948] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 23            |
|    time_elapsed          | 365           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.006694492   |
|    clip_fraction         | 0.0565        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 227           |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.00562       |
|    lagrangian_multiplier | 0.0707        |
|    learning_rate         | 0.0003        |
|    loss                  | 127           |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00805      |
|    std                   | 1.01          |
|    value_loss            | 1.06e+03      |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2980853] |
| time/                    |              |
|    fps                   | 100          |
|    iterations            | 18           |
|    time_elapsed          | 366          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.008286329  |
|    clip_fraction         | 0.0796       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 240          |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0759      |
|    lagrangian_multiplier | 0.0598       |
|    learning_rate         | 0.0003       |
|    loss                  | 78.5         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.0106      |
|    std                   | 0.969        |
|    value_loss            | 522          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.62199]   |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 23           |
|    time_elapsed          | 373          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.0068510664 |
|    clip_fraction         | 0.0682       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.213        |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0128       |
|    lagrangian_multiplier | 0.0682       |
|    learning_rate         | 0.0003       |
|    loss                  | 125          |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.00867     |
|    std                   | 0.941        |
|    value_loss            | 1.36e+03     |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.49409252] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 24            |
|    time_elapsed          | 381           |
|    total_timesteps       | 49152         |
| train/                   |               |
|    approx_kl             | 0.0068777716  |
|    clip_fraction         | 0.0731        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 258           |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.00569       |
|    lagrangian_multiplier | 0.0593        |
|    learning_rate         | 0.0003        |
|    loss                  | 173           |
|    n_updates             | 230           |
|    policy_gradient_loss  | -0.00871      |
|    std                   | 1             |
|    value_loss            | 1.48e+03      |
--------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.5511811] |
| time/                    |              |
|    fps                   | 101          |
|    iterations            | 19           |
|    time_elapsed          | 383          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.006786293  |
|    clip_fraction         | 0.0611       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 61.7         |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0831      |
|    lagrangian_multiplier | 0.0754       |
|    learning_rate         | 0.0003       |
|    loss                  | 38.3         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00712     |
|    std                   | 0.979        |
|    value_loss            | 361          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1740048] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 21           |
|    time_elapsed          | 383          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0070308996 |
|    clip_fraction         | 0.0568       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0443       |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0379      |
|    lagrangian_multiplier | 0.0634       |
|    learning_rate         | 0.0003       |
|    loss                  | 25.7         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00551     |
|    std                   | 0.963        |
|    value_loss            | 283          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8212872] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 24           |
|    time_elapsed          | 389          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.006007478  |
|    clip_fraction         | 0.0732       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.277        |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.033        |
|    lagrangian_multiplier | 0.0503       |
|    learning_rate         | 0.0003       |
|    loss                  | 143          |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.0084      |
|    std                   | 0.939        |
|    value_loss            | 1.31e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3475032] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 25           |
|    time_elapsed          | 397          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.00743899   |
|    clip_fraction         | 0.0583       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 203          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0096      |
|    lagrangian_multiplier | 0.0636       |
|    learning_rate         | 0.0003       |
|    loss                  | 127          |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00745     |
|    std                   | 0.998        |
|    value_loss            | 1.19e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0704607] |
| time/                    |              |
|    fps                   | 102          |
|    iterations            | 20           |
|    time_elapsed          | 399          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.00813949   |
|    clip_fraction         | 0.0755       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 83.7         |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.019        |
|    lagrangian_multiplier | 0.0765       |
|    learning_rate         | 0.0003       |
|    loss                  | 38.9         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00596     |
|    std                   | 0.984        |
|    value_loss            | 382          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.48507372] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 22            |
|    time_elapsed          | 401           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.0055999113  |
|    clip_fraction         | 0.0611        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0903        |
|    entropy_loss          | -2.77         |
|    explained_variance    | -0.0202       |
|    lagrangian_multiplier | 0.0626        |
|    learning_rate         | 0.0003        |
|    loss                  | 25.1          |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.00519      |
|    std                   | 0.97          |
|    value_loss            | 268           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2228501] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 25           |
|    time_elapsed          | 405          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.006111391  |
|    clip_fraction         | 0.0562       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.25         |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0319       |
|    lagrangian_multiplier | 0.0508       |
|    learning_rate         | 0.0003       |
|    loss                  | 151          |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.0073      |
|    std                   | 0.941        |
|    value_loss            | 1.25e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2876227] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 26           |
|    time_elapsed          | 413          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0068387534 |
|    clip_fraction         | 0.0637       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 255          |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0126       |
|    lagrangian_multiplier | 0.0646       |
|    learning_rate         | 0.0003       |
|    loss                  | 136          |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00767     |
|    std                   | 0.987        |
|    value_loss            | 1.16e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.89683867] |
| time/                    |               |
|    fps                   | 103           |
|    iterations            | 21            |
|    time_elapsed          | 415           |
|    total_timesteps       | 43008         |
| train/                   |               |
|    approx_kl             | 0.004602588   |
|    clip_fraction         | 0.0472        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 51.4          |
|    entropy_loss          | -2.81         |
|    explained_variance    | -0.0358       |
|    lagrangian_multiplier | 0.071         |
|    learning_rate         | 0.0003        |
|    loss                  | 23.5          |
|    n_updates             | 200           |
|    policy_gradient_loss  | -0.00458      |
|    std                   | 0.986         |
|    value_loss            | 224           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.5857392] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 23           |
|    time_elapsed          | 420          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.0048084497 |
|    clip_fraction         | 0.0465       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0512       |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0246      |
|    lagrangian_multiplier | 0.0593       |
|    learning_rate         | 0.0003       |
|    loss                  | 35.4         |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.00421     |
|    std                   | 0.962        |
|    value_loss            | 325          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.632477]  |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 26           |
|    time_elapsed          | 422          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0070818877 |
|    clip_fraction         | 0.0629       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.212        |
|    entropy_loss          | -2.71        |
|    explained_variance    | 0.0297       |
|    lagrangian_multiplier | 0.062        |
|    learning_rate         | 0.0003       |
|    loss                  | 137          |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00769     |
|    std                   | 0.935        |
|    value_loss            | 1.37e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4253535] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 27           |
|    time_elapsed          | 429          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.007062953  |
|    clip_fraction         | 0.0624       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 232          |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.00264     |
|    lagrangian_multiplier | 0.0523       |
|    learning_rate         | 0.0003       |
|    loss                  | 179          |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00964     |
|    std                   | 0.983        |
|    value_loss            | 1.25e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.39215684] |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 22            |
|    time_elapsed          | 431           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.00641612    |
|    clip_fraction         | 0.0796        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 130           |
|    entropy_loss          | -2.82         |
|    explained_variance    | -0.0729       |
|    lagrangian_multiplier | 0.0719        |
|    learning_rate         | 0.0003        |
|    loss                  | 78.2          |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.00928      |
|    std                   | 0.999         |
|    value_loss            | 693           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.4354553] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 24           |
|    time_elapsed          | 438          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.005571895  |
|    clip_fraction         | 0.0366       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0654       |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0272      |
|    lagrangian_multiplier | 0.0538       |
|    learning_rate         | 0.0003       |
|    loss                  | 55.8         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00616     |
|    std                   | 0.954        |
|    value_loss            | 488          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4224397] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 27           |
|    time_elapsed          | 438          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.008212247  |
|    clip_fraction         | 0.0796       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.213        |
|    entropy_loss          | -2.7         |
|    explained_variance    | 0.0308       |
|    lagrangian_multiplier | 0.0709       |
|    learning_rate         | 0.0003       |
|    loss                  | 105          |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.0103      |
|    std                   | 0.937        |
|    value_loss            | 1.38e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3779236] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 28           |
|    time_elapsed          | 445          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.006246186  |
|    clip_fraction         | 0.0605       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 264          |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0108       |
|    lagrangian_multiplier | 0.0641       |
|    learning_rate         | 0.0003       |
|    loss                  | 186          |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00984     |
|    std                   | 0.977        |
|    value_loss            | 1.55e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.31011495] |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 23            |
|    time_elapsed          | 448           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.005137772   |
|    clip_fraction         | 0.0452        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 28.3          |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.0135        |
|    lagrangian_multiplier | 0.0653        |
|    learning_rate         | 0.0003        |
|    loss                  | 17.4          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00616      |
|    std                   | 1.01          |
|    value_loss            | 177           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3824079] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 28           |
|    time_elapsed          | 454          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.008657496  |
|    clip_fraction         | 0.0823       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.236        |
|    entropy_loss          | -2.71        |
|    explained_variance    | 0.0169       |
|    lagrangian_multiplier | 0.0549       |
|    learning_rate         | 0.0003       |
|    loss                  | 148          |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.0089      |
|    std                   | 0.935        |
|    value_loss            | 1.43e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6907467] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 25           |
|    time_elapsed          | 456          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.004674142  |
|    clip_fraction         | 0.0315       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.117        |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.0514      |
|    lagrangian_multiplier | 0.0464       |
|    learning_rate         | 0.0003       |
|    loss                  | 34.9         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00663     |
|    std                   | 0.94         |
|    value_loss            | 284          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5554328] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 29           |
|    time_elapsed          | 461          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.006495163  |
|    clip_fraction         | 0.0618       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 229          |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0069      |
|    lagrangian_multiplier | 0.0674       |
|    learning_rate         | 0.0003       |
|    loss                  | 126          |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00927     |
|    std                   | 0.981        |
|    value_loss            | 1.1e+03      |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.95639753] |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 24            |
|    time_elapsed          | 464           |
|    total_timesteps       | 49152         |
| train/                   |               |
|    approx_kl             | 0.0044664834  |
|    clip_fraction         | 0.0469        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 66.1          |
|    entropy_loss          | -2.86         |
|    explained_variance    | 0.000809      |
|    lagrangian_multiplier | 0.0681        |
|    learning_rate         | 0.0003        |
|    loss                  | 42.4          |
|    n_updates             | 230           |
|    policy_gradient_loss  | -0.00635      |
|    std                   | 1.01          |
|    value_loss            | 517           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.8698068] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 29           |
|    time_elapsed          | 471          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.0065855854 |
|    clip_fraction         | 0.0743       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.249        |
|    entropy_loss          | -2.71        |
|    explained_variance    | 0.017        |
|    lagrangian_multiplier | 0.0539       |
|    learning_rate         | 0.0003       |
|    loss                  | 135          |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00892     |
|    std                   | 0.942        |
|    value_loss            | 1.27e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.51859826] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 26            |
|    time_elapsed          | 474           |
|    total_timesteps       | 53248         |
| train/                   |               |
|    approx_kl             | 0.0044787354  |
|    clip_fraction         | 0.0363        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.037         |
|    entropy_loss          | -2.7          |
|    explained_variance    | -0.0286       |
|    lagrangian_multiplier | 0.0661        |
|    learning_rate         | 0.0003        |
|    loss                  | 43.3          |
|    n_updates             | 250           |
|    policy_gradient_loss  | -0.00436      |
|    std                   | 0.923         |
|    value_loss            | 473           |
--------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.5366628] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 30           |
|    time_elapsed          | 477          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0079284655 |
|    clip_fraction         | 0.0652       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 231          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0102       |
|    lagrangian_multiplier | 0.059        |
|    learning_rate         | 0.0003       |
|    loss                  | 155          |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00885     |
|    std                   | 0.967        |
|    value_loss            | 1.41e+03     |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8385079] |
| time/                    |              |
|    fps                   | 106          |
|    iterations            | 25           |
|    time_elapsed          | 480          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.0051192245 |
|    clip_fraction         | 0.05         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 46.5         |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0816      |
|    lagrangian_multiplier | 0.0611       |
|    learning_rate         | 0.0003       |
|    loss                  | 26.7         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00771     |
|    std                   | 1.01         |
|    value_loss            | 214          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.4227362] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 30           |
|    time_elapsed          | 487          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.008817794  |
|    clip_fraction         | 0.0793       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.177        |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.0183       |
|    lagrangian_multiplier | 0.0546       |
|    learning_rate         | 0.0003       |
|    loss                  | 97           |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.0109      |
|    std                   | 0.96         |
|    value_loss            | 829          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.8153913] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 31           |
|    time_elapsed          | 493          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.007260659  |
|    clip_fraction         | 0.0627       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 260          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0096       |
|    lagrangian_multiplier | 0.0532       |
|    learning_rate         | 0.0003       |
|    loss                  | 202          |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00965     |
|    std                   | 0.975        |
|    value_loss            | 1.42e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.36159408] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 27            |
|    time_elapsed          | 493           |
|    total_timesteps       | 55296         |
| train/                   |               |
|    approx_kl             | 0.008109903   |
|    clip_fraction         | 0.0861        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0297        |
|    entropy_loss          | -2.66         |
|    explained_variance    | -0.0717       |
|    lagrangian_multiplier | 0.0513        |
|    learning_rate         | 0.0003        |
|    loss                  | 59.9          |
|    n_updates             | 260           |
|    policy_gradient_loss  | -0.00889      |
|    std                   | 0.912         |
|    value_loss            | 508           |
--------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2706497] |
| time/                    |              |
|    fps                   | 107          |
|    iterations            | 26           |
|    time_elapsed          | 496          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0075822584 |
|    clip_fraction         | 0.0834       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 155          |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0447      |
|    lagrangian_multiplier | 0.0702       |
|    learning_rate         | 0.0003       |
|    loss                  | 57.9         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 1            |
|    value_loss            | 487          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.4738991] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 31           |
|    time_elapsed          | 503          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.006374554  |
|    clip_fraction         | 0.0576       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.194        |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.0432       |
|    lagrangian_multiplier | 0.0593       |
|    learning_rate         | 0.0003       |
|    loss                  | 105          |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00643     |
|    std                   | 0.963        |
|    value_loss            | 944          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.3379126] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 32           |
|    time_elapsed          | 509          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.006892249  |
|    clip_fraction         | 0.0529       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 256          |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.000925     |
|    lagrangian_multiplier | 0.0639       |
|    learning_rate         | 0.0003       |
|    loss                  | 164          |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00711     |
|    std                   | 0.974        |
|    value_loss            | 1.28e+03     |
-------------------------------------------
------------------------------------------
| reward                   | [-1.261222] |
| time/                    |             |
|    fps                   | 112         |
|    iterations            | 28          |
|    time_elapsed          | 511         |
|    total_timesteps       | 57344       |
| train/                   |             |
|    approx_kl             | 0.005515813 |
|    clip_fraction         | 0.0418      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0453      |
|    entropy_loss          | -2.64       |
|    explained_variance    | -0.062      |
|    lagrangian_multiplier | 0.0702      |
|    learning_rate         | 0.0003      |
|    loss                  | 26.6        |
|    n_updates             | 270         |
|    policy_gradient_loss  | -0.00441    |
|    std                   | 0.898       |
|    value_loss            | 304         |
------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.48937428] |
| time/                    |               |
|    fps                   | 107           |
|    iterations            | 27            |
|    time_elapsed          | 513           |
|    total_timesteps       | 55296         |
| train/                   |               |
|    approx_kl             | 0.0074724774  |
|    clip_fraction         | 0.0791        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 139           |
|    entropy_loss          | -2.82         |
|    explained_variance    | -0.00122      |
|    lagrangian_multiplier | 0.0616        |
|    learning_rate         | 0.0003        |
|    loss                  | 72.5          |
|    n_updates             | 260           |
|    policy_gradient_loss  | -0.00867      |
|    std                   | 0.985         |
|    value_loss            | 478           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.8305877] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 32           |
|    time_elapsed          | 520          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.007273447  |
|    clip_fraction         | 0.0804       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.171        |
|    entropy_loss          | -2.75        |
|    explained_variance    | 0.0369       |
|    lagrangian_multiplier | 0.045        |
|    learning_rate         | 0.0003       |
|    loss                  | 110          |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 0.955        |
|    value_loss            | 904          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.3153353] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 33           |
|    time_elapsed          | 525          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.008140661  |
|    clip_fraction         | 0.0785       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 237          |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.00461      |
|    lagrangian_multiplier | 0.0581       |
|    learning_rate         | 0.0003       |
|    loss                  | 122          |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.0117      |
|    std                   | 0.988        |
|    value_loss            | 1.06e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1485376] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 29           |
|    time_elapsed          | 529          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.004849236  |
|    clip_fraction         | 0.04         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.062        |
|    entropy_loss          | -2.62        |
|    explained_variance    | -0.055       |
|    lagrangian_multiplier | 0.0733       |
|    learning_rate         | 0.0003       |
|    loss                  | 11.9         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00524     |
|    std                   | 0.897        |
|    value_loss            | 122          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7167518] |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 28           |
|    time_elapsed          | 529          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.00655292   |
|    clip_fraction         | 0.0609       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 201          |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0015      |
|    lagrangian_multiplier | 0.0642       |
|    learning_rate         | 0.0003       |
|    loss                  | 61.5         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00813     |
|    std                   | 0.979        |
|    value_loss            | 460          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7535939] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 33           |
|    time_elapsed          | 536          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.007111454  |
|    clip_fraction         | 0.0773       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.228        |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.0391       |
|    lagrangian_multiplier | 0.0528       |
|    learning_rate         | 0.0003       |
|    loss                  | 151          |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.0116      |
|    std                   | 0.95         |
|    value_loss            | 1.21e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.4731257] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 34           |
|    time_elapsed          | 541          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0067102797 |
|    clip_fraction         | 0.0582       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 265          |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.00945      |
|    lagrangian_multiplier | 0.0579       |
|    learning_rate         | 0.0003       |
|    loss                  | 180          |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.0111      |
|    std                   | 0.988        |
|    value_loss            | 1.55e+03     |
-------------------------------------------
------------------------------------------
| reward                   | [-1.571789] |
| time/                    |             |
|    fps                   | 108         |
|    iterations            | 29          |
|    time_elapsed          | 545         |
|    total_timesteps       | 59392       |
| train/                   |             |
|    approx_kl             | 0.004778861 |
|    clip_fraction         | 0.0427      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 39.8        |
|    entropy_loss          | -2.8        |
|    explained_variance    | 0.00346     |
|    lagrangian_multiplier | 0.0763      |
|    learning_rate         | 0.0003      |
|    loss                  | 44.7        |
|    n_updates             | 280         |
|    policy_gradient_loss  | -0.00638    |
|    std                   | 0.983       |
|    value_loss            | 439         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.61461073] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 30            |
|    time_elapsed          | 548           |
|    total_timesteps       | 61440         |
| train/                   |               |
|    approx_kl             | 0.006908602   |
|    clip_fraction         | 0.0624        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0555        |
|    entropy_loss          | -2.61         |
|    explained_variance    | -0.0639       |
|    lagrangian_multiplier | 0.051         |
|    learning_rate         | 0.0003        |
|    loss                  | 34.9          |
|    n_updates             | 290           |
|    policy_gradient_loss  | -0.00527      |
|    std                   | 0.89          |
|    value_loss            | 364           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.8428047] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 34           |
|    time_elapsed          | 552          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0082049    |
|    clip_fraction         | 0.0875       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.155        |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0526       |
|    lagrangian_multiplier | 0.0566       |
|    learning_rate         | 0.0003       |
|    loss                  | 68.8         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.0112      |
|    std                   | 0.94         |
|    value_loss            | 642          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2932178] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 35           |
|    time_elapsed          | 557          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.006978634  |
|    clip_fraction         | 0.0615       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 246          |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0159       |
|    lagrangian_multiplier | 0.0646       |
|    learning_rate         | 0.0003       |
|    loss                  | 156          |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.0109      |
|    std                   | 0.984        |
|    value_loss            | 1.4e+03      |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6514781] |
| time/                    |              |
|    fps                   | 109          |
|    iterations            | 30           |
|    time_elapsed          | 562          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0075299954 |
|    clip_fraction         | 0.0709       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 155          |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0225       |
|    lagrangian_multiplier | 0.066        |
|    learning_rate         | 0.0003       |
|    loss                  | 62.8         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00834     |
|    std                   | 0.976        |
|    value_loss            | 472          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.42132297] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 31            |
|    time_elapsed          | 566           |
|    total_timesteps       | 63488         |
| train/                   |               |
|    approx_kl             | 0.0059922114  |
|    clip_fraction         | 0.0618        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.444         |
|    entropy_loss          | -2.6          |
|    explained_variance    | -0.0657       |
|    lagrangian_multiplier | 0.0657        |
|    learning_rate         | 0.0003        |
|    loss                  | 18            |
|    n_updates             | 300           |
|    policy_gradient_loss  | -0.00756      |
|    std                   | 0.887         |
|    value_loss            | 224           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.4448501] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 35           |
|    time_elapsed          | 568          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0066109085 |
|    clip_fraction         | 0.0497       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.114        |
|    entropy_loss          | -2.7         |
|    explained_variance    | 0.0387       |
|    lagrangian_multiplier | 0.0583       |
|    learning_rate         | 0.0003       |
|    loss                  | 72.3         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00612     |
|    std                   | 0.928        |
|    value_loss            | 708          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2409074] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 36           |
|    time_elapsed          | 572          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0068803085 |
|    clip_fraction         | 0.0533       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 246          |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.00137      |
|    lagrangian_multiplier | 0.0613       |
|    learning_rate         | 0.0003       |
|    loss                  | 147          |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0077      |
|    std                   | 0.984        |
|    value_loss            | 1.35e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3034357] |
| time/                    |              |
|    fps                   | 109          |
|    iterations            | 31           |
|    time_elapsed          | 578          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0073356032 |
|    clip_fraction         | 0.08         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 124          |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0289       |
|    lagrangian_multiplier | 0.0777       |
|    learning_rate         | 0.0003       |
|    loss                  | 40           |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 0.983        |
|    value_loss            | 395          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8122419] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 32           |
|    time_elapsed          | 584          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.006395586  |
|    clip_fraction         | 0.0564       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0263       |
|    entropy_loss          | -2.59        |
|    explained_variance    | -0.0362      |
|    lagrangian_multiplier | 0.088        |
|    learning_rate         | 0.0003       |
|    loss                  | 6.83         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00548     |
|    std                   | 0.882        |
|    value_loss            | 82.6         |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2506254] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 36           |
|    time_elapsed          | 585          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.00579735   |
|    clip_fraction         | 0.0467       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.155        |
|    entropy_loss          | -2.69        |
|    explained_variance    | 0.0502       |
|    lagrangian_multiplier | 0.0578       |
|    learning_rate         | 0.0003       |
|    loss                  | 73.8         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00575     |
|    std                   | 0.93         |
|    value_loss            | 750          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.6824896] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 37           |
|    time_elapsed          | 588          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.008378735  |
|    clip_fraction         | 0.0766       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 245          |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.00774     |
|    lagrangian_multiplier | 0.0736       |
|    learning_rate         | 0.0003       |
|    loss                  | 106          |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.983        |
|    value_loss            | 982          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2810445] |
| time/                    |              |
|    fps                   | 110          |
|    iterations            | 32           |
|    time_elapsed          | 594          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.005556912  |
|    clip_fraction         | 0.04         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 29.2         |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0842      |
|    lagrangian_multiplier | 0.0765       |
|    learning_rate         | 0.0003       |
|    loss                  | 41.6         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00445     |
|    std                   | 0.98         |
|    value_loss            | 453          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.8350637] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 37           |
|    time_elapsed          | 601          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.009450037  |
|    clip_fraction         | 0.121        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.185        |
|    entropy_loss          | -2.69        |
|    explained_variance    | 0.0241       |
|    lagrangian_multiplier | 0.0531       |
|    learning_rate         | 0.0003       |
|    loss                  | 95.3         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.0128      |
|    std                   | 0.931        |
|    value_loss            | 816          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0358974] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 33           |
|    time_elapsed          | 602          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.010250821  |
|    clip_fraction         | 0.0987       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.751        |
|    entropy_loss          | -2.58        |
|    explained_variance    | -0.0182      |
|    lagrangian_multiplier | 0.0488       |
|    learning_rate         | 0.0003       |
|    loss                  | 32.9         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 0.881        |
|    value_loss            | 279          |
-------------------------------------------
------------------------------------------
| reward                   | [-2.540802] |
| time/                    |             |
|    fps                   | 128         |
|    iterations            | 38          |
|    time_elapsed          | 604         |
|    total_timesteps       | 77824       |
| train/                   |             |
|    approx_kl             | 0.00887762  |
|    clip_fraction         | 0.091       |
|    clip_range            | 0.2         |
|    cost_value_loss       | 238         |
|    entropy_loss          | -2.81       |
|    explained_variance    | 0.00949     |
|    lagrangian_multiplier | 0.0742      |
|    learning_rate         | 0.0003      |
|    loss                  | 130         |
|    n_updates             | 370         |
|    policy_gradient_loss  | -0.012      |
|    std                   | 0.987       |
|    value_loss            | 1.27e+03    |
------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-1.718305] |
| time/                    |             |
|    fps                   | 126         |
|    iterations            | 38          |
|    time_elapsed          | 617         |
|    total_timesteps       | 77824       |
| train/                   |             |
|    approx_kl             | 0.008586488 |
|    clip_fraction         | 0.106       |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.121       |
|    entropy_loss          | -2.69       |
|    explained_variance    | 0.0175      |
|    lagrangian_multiplier | 0.0531      |
|    learning_rate         | 0.0003      |
|    loss                  | 92.6        |
|    n_updates             | 370         |
|    policy_gradient_loss  | -0.0127     |
|    std                   | 0.923       |
|    value_loss            | 835         |
------------------------------------------
------------------------------------------
| reward                   | [-3.37253]  |
| time/                    |             |
|    fps                   | 128         |
|    iterations            | 39          |
|    time_elapsed          | 620         |
|    total_timesteps       | 79872       |
| train/                   |             |
|    approx_kl             | 0.007616162 |
|    clip_fraction         | 0.0607      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 235         |
|    entropy_loss          | -2.81       |
|    explained_variance    | 0.00947     |
|    lagrangian_multiplier | 0.0751      |
|    learning_rate         | 0.0003      |
|    loss                  | 109         |
|    n_updates             | 380         |
|    policy_gradient_loss  | -0.0084     |
|    std                   | 0.983       |
|    value_loss            | 973         |
------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.9342739] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 34           |
|    time_elapsed          | 621          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.008403399  |
|    clip_fraction         | 0.0744       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.8          |
|    entropy_loss          | -2.55        |
|    explained_variance    | -0.0195      |
|    lagrangian_multiplier | 0.0541       |
|    learning_rate         | 0.0003       |
|    loss                  | 35.5         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.0106      |
|    std                   | 0.856        |
|    value_loss            | 326          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0799029] |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 33           |
|    time_elapsed          | 623          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.008349651  |
|    clip_fraction         | 0.0674       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 97.3         |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0102       |
|    lagrangian_multiplier | 0.0804       |
|    learning_rate         | 0.0003       |
|    loss                  | 44.9         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.0088      |
|    std                   | 0.963        |
|    value_loss            | 447          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-1.093627] |
| time/                    |             |
|    fps                   | 126         |
|    iterations            | 39          |
|    time_elapsed          | 633         |
|    total_timesteps       | 79872       |
| train/                   |             |
|    approx_kl             | 0.008600647 |
|    clip_fraction         | 0.0879      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.103       |
|    entropy_loss          | -2.68       |
|    explained_variance    | 0.0229      |
|    lagrangian_multiplier | 0.0567      |
|    learning_rate         | 0.0003      |
|    loss                  | 92.7        |
|    n_updates             | 380         |
|    policy_gradient_loss  | -0.00898    |
|    std                   | 0.921       |
|    value_loss            | 1.07e+03    |
------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.3883506] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 40           |
|    time_elapsed          | 636          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.0072212685 |
|    clip_fraction         | 0.0548       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 247          |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.00227      |
|    lagrangian_multiplier | 0.0599       |
|    learning_rate         | 0.0003       |
|    loss                  | 144          |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00727     |
|    std                   | 0.993        |
|    value_loss            | 1.24e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1895219] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 35           |
|    time_elapsed          | 639          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.007696449  |
|    clip_fraction         | 0.0752       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.132        |
|    entropy_loss          | -2.52        |
|    explained_variance    | -0.0615      |
|    lagrangian_multiplier | 0.0626       |
|    learning_rate         | 0.0003       |
|    loss                  | 19.6         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00937     |
|    std                   | 0.854        |
|    value_loss            | 200          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7575897] |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 34           |
|    time_elapsed          | 639          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.004587994  |
|    clip_fraction         | 0.0528       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 133          |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0448       |
|    lagrangian_multiplier | 0.0801       |
|    learning_rate         | 0.0003       |
|    loss                  | 67.9         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00609     |
|    std                   | 0.967        |
|    value_loss            | 776          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.1826422] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 40           |
|    time_elapsed          | 650          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.0071812393 |
|    clip_fraction         | 0.0712       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.138        |
|    entropy_loss          | -2.67        |
|    explained_variance    | 0.0377       |
|    lagrangian_multiplier | 0.0582       |
|    learning_rate         | 0.0003       |
|    loss                  | 79.4         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00955     |
|    std                   | 0.92         |
|    value_loss            | 719          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.9025025] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 41           |
|    time_elapsed          | 652          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0069945687 |
|    clip_fraction         | 0.0599       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 220          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.00752      |
|    lagrangian_multiplier | 0.0658       |
|    learning_rate         | 0.0003       |
|    loss                  | 130          |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00754     |
|    std                   | 0.999        |
|    value_loss            | 981          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3584931] |
| time/                    |              |
|    fps                   | 109          |
|    iterations            | 35           |
|    time_elapsed          | 656          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0057213674 |
|    clip_fraction         | 0.0625       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 106          |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0329      |
|    lagrangian_multiplier | 0.0663       |
|    learning_rate         | 0.0003       |
|    loss                  | 110          |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00693     |
|    std                   | 0.99         |
|    value_loss            | 875          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8173822] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 36           |
|    time_elapsed          | 657          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.005819234  |
|    clip_fraction         | 0.0482       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.49         |
|    entropy_loss          | -2.51        |
|    explained_variance    | -0.0606      |
|    lagrangian_multiplier | 0.0524       |
|    learning_rate         | 0.0003       |
|    loss                  | 34.4         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0075      |
|    std                   | 0.844        |
|    value_loss            | 305          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2637413] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 41           |
|    time_elapsed          | 666          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0068102726 |
|    clip_fraction         | 0.0536       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.111        |
|    entropy_loss          | -2.66        |
|    explained_variance    | 0.0416       |
|    lagrangian_multiplier | 0.0709       |
|    learning_rate         | 0.0003       |
|    loss                  | 81.3         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00649     |
|    std                   | 0.91         |
|    value_loss            | 804          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.633488]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 42           |
|    time_elapsed          | 668          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0060439506 |
|    clip_fraction         | 0.0563       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 238          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.00174     |
|    lagrangian_multiplier | 0.0732       |
|    learning_rate         | 0.0003       |
|    loss                  | 90.3         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.0108      |
|    std                   | 1            |
|    value_loss            | 970          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7091203] |
| time/                    |              |
|    fps                   | 109          |
|    iterations            | 36           |
|    time_elapsed          | 672          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0049112104 |
|    clip_fraction         | 0.0505       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 74.9         |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0211       |
|    lagrangian_multiplier | 0.0642       |
|    learning_rate         | 0.0003       |
|    loss                  | 44.3         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00721     |
|    std                   | 0.994        |
|    value_loss            | 320          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7678838] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 37           |
|    time_elapsed          | 675          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.0072997534 |
|    clip_fraction         | 0.061        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.258        |
|    entropy_loss          | -2.49        |
|    explained_variance    | -0.0124      |
|    lagrangian_multiplier | 0.0498       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.9         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00751     |
|    std                   | 0.841        |
|    value_loss            | 452          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.5485137] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 42           |
|    time_elapsed          | 682          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0062622605 |
|    clip_fraction         | 0.053        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00977      |
|    entropy_loss          | -2.66        |
|    explained_variance    | 0.0576       |
|    lagrangian_multiplier | 0.0979       |
|    learning_rate         | 0.0003       |
|    loss                  | 50.7         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00521     |
|    std                   | 0.914        |
|    value_loss            | 764          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.3068545] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 43           |
|    time_elapsed          | 684          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0071939817 |
|    clip_fraction         | 0.07         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 225          |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0188       |
|    lagrangian_multiplier | 0.0657       |
|    learning_rate         | 0.0003       |
|    loss                  | 115          |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.0134      |
|    std                   | 0.999        |
|    value_loss            | 829          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6908232] |
| time/                    |              |
|    fps                   | 110          |
|    iterations            | 37           |
|    time_elapsed          | 688          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.00592566   |
|    clip_fraction         | 0.0481       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 105          |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0875      |
|    lagrangian_multiplier | 0.0693       |
|    learning_rate         | 0.0003       |
|    loss                  | 88.9         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00373     |
|    std                   | 0.997        |
|    value_loss            | 882          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5123441] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 38           |
|    time_elapsed          | 694          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0058985623 |
|    clip_fraction         | 0.0682       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.31         |
|    entropy_loss          | -2.48        |
|    explained_variance    | -0.000696    |
|    lagrangian_multiplier | 0.0644       |
|    learning_rate         | 0.0003       |
|    loss                  | 30.2         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.839        |
|    value_loss            | 288          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.3264656] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 43           |
|    time_elapsed          | 698          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0037214765 |
|    clip_fraction         | 0.0534       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0859       |
|    entropy_loss          | -2.65        |
|    explained_variance    | 0.0456       |
|    lagrangian_multiplier | 0.0694       |
|    learning_rate         | 0.0003       |
|    loss                  | 85.1         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00466     |
|    std                   | 0.908        |
|    value_loss            | 981          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.30736667] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 44            |
|    time_elapsed          | 700           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.008015128   |
|    clip_fraction         | 0.0715        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 230           |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.00952       |
|    lagrangian_multiplier | 0.0738        |
|    learning_rate         | 0.0003        |
|    loss                  | 87.3          |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.0104       |
|    std                   | 1             |
|    value_loss            | 734           |
--------------------------------------------
------------------------------------------
| reward                   | [-1.026836] |
| time/                    |             |
|    fps                   | 110         |
|    iterations            | 38          |
|    time_elapsed          | 704         |
|    total_timesteps       | 77824       |
| train/                   |             |
|    approx_kl             | 0.007817611 |
|    clip_fraction         | 0.0767      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 176         |
|    entropy_loss          | -2.84       |
|    explained_variance    | -0.055      |
|    lagrangian_multiplier | 0.0673      |
|    learning_rate         | 0.0003      |
|    loss                  | 88.3        |
|    n_updates             | 370         |
|    policy_gradient_loss  | -0.0119     |
|    std                   | 1.01        |
|    value_loss            | 727         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.1877627] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 39           |
|    time_elapsed          | 712          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.006917497  |
|    clip_fraction         | 0.0743       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 5.51         |
|    entropy_loss          | -2.48        |
|    explained_variance    | -0.0242      |
|    lagrangian_multiplier | 0.0548       |
|    learning_rate         | 0.0003       |
|    loss                  | 34.8         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.0111      |
|    std                   | 0.833        |
|    value_loss            | 322          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.34776345] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 44            |
|    time_elapsed          | 714           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.0053815264  |
|    clip_fraction         | 0.0412        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0925        |
|    entropy_loss          | -2.62         |
|    explained_variance    | 0.0402        |
|    lagrangian_multiplier | 0.067         |
|    learning_rate         | 0.0003        |
|    loss                  | 85.6          |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.00836      |
|    std                   | 0.891         |
|    value_loss            | 898           |
--------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8626888] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 45           |
|    time_elapsed          | 716          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.00777343   |
|    clip_fraction         | 0.0645       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 244          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0011       |
|    lagrangian_multiplier | 0.071        |
|    learning_rate         | 0.0003       |
|    loss                  | 78.9         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00967     |
|    std                   | 1            |
|    value_loss            | 656          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2545896] |
| time/                    |              |
|    fps                   | 110          |
|    iterations            | 39           |
|    time_elapsed          | 721          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.006456692  |
|    clip_fraction         | 0.064        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 173          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0581      |
|    lagrangian_multiplier | 0.0647       |
|    learning_rate         | 0.0003       |
|    loss                  | 66.7         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.0105      |
|    std                   | 1.01         |
|    value_loss            | 514          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.9080547] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 45           |
|    time_elapsed          | 730          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.005646966  |
|    clip_fraction         | 0.0597       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0394       |
|    entropy_loss          | -2.62        |
|    explained_variance    | 0.0444       |
|    lagrangian_multiplier | 0.0659       |
|    learning_rate         | 0.0003       |
|    loss                  | 95.3         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00514     |
|    std                   | 0.903        |
|    value_loss            | 986          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2789422] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 40           |
|    time_elapsed          | 730          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.006597182  |
|    clip_fraction         | 0.0582       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.64         |
|    entropy_loss          | -2.46        |
|    explained_variance    | -0.0376      |
|    lagrangian_multiplier | 0.0509       |
|    learning_rate         | 0.0003       |
|    loss                  | 44.8         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00617     |
|    std                   | 0.828        |
|    value_loss            | 415          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9363376] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 46           |
|    time_elapsed          | 732          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.008658102  |
|    clip_fraction         | 0.0813       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 195          |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0316      |
|    lagrangian_multiplier | 0.0609       |
|    learning_rate         | 0.0003       |
|    loss                  | 71.6         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.0106      |
|    std                   | 0.995        |
|    value_loss            | 504          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2230474] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 40           |
|    time_elapsed          | 737          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.0064493455 |
|    clip_fraction         | 0.0556       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 76.8         |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0743       |
|    lagrangian_multiplier | 0.0632       |
|    learning_rate         | 0.0003       |
|    loss                  | 31.1         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00635     |
|    std                   | 1.01         |
|    value_loss            | 235          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1492383] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 46           |
|    time_elapsed          | 747          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0056946753 |
|    clip_fraction         | 0.0583       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0914       |
|    entropy_loss          | -2.63        |
|    explained_variance    | 0.0512       |
|    lagrangian_multiplier | 0.0638       |
|    learning_rate         | 0.0003       |
|    loss                  | 99.8         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.00699     |
|    std                   | 0.898        |
|    value_loss            | 1e+03        |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8103401] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 47           |
|    time_elapsed          | 748          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.007412899  |
|    clip_fraction         | 0.0646       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 228          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0532       |
|    lagrangian_multiplier | 0.0641       |
|    learning_rate         | 0.0003       |
|    loss                  | 81.2         |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.00928     |
|    std                   | 1            |
|    value_loss            | 517          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0024248] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 41           |
|    time_elapsed          | 749          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.007435511  |
|    clip_fraction         | 0.0795       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 9.3          |
|    entropy_loss          | -2.46        |
|    explained_variance    | -0.0471      |
|    lagrangian_multiplier | 0.0611       |
|    learning_rate         | 0.0003       |
|    loss                  | 42.4         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.0118      |
|    std                   | 0.829        |
|    value_loss            | 384          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.8047237] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 41           |
|    time_elapsed          | 753          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0074243885 |
|    clip_fraction         | 0.0771       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 89.1         |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.01        |
|    lagrangian_multiplier | 0.0708       |
|    learning_rate         | 0.0003       |
|    loss                  | 31.2         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.0136      |
|    std                   | 1            |
|    value_loss            | 230          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2792763] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 47           |
|    time_elapsed          | 763          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.0065333694 |
|    clip_fraction         | 0.0655       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0635       |
|    entropy_loss          | -2.63        |
|    explained_variance    | 0.0153       |
|    lagrangian_multiplier | 0.072        |
|    learning_rate         | 0.0003       |
|    loss                  | 131          |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.00663     |
|    std                   | 0.908        |
|    value_loss            | 1.46e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0407281] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 48           |
|    time_elapsed          | 764          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.005541388  |
|    clip_fraction         | 0.0578       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 181          |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0337       |
|    lagrangian_multiplier | 0.0671       |
|    learning_rate         | 0.0003       |
|    loss                  | 40.4         |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00811     |
|    std                   | 1.01         |
|    value_loss            | 292          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8184028] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 42           |
|    time_elapsed          | 767          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.007510052  |
|    clip_fraction         | 0.0728       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 11           |
|    entropy_loss          | -2.45        |
|    explained_variance    | -0.0243      |
|    lagrangian_multiplier | 0.0572       |
|    learning_rate         | 0.0003       |
|    loss                  | 33.6         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00941     |
|    std                   | 0.819        |
|    value_loss            | 317          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-3.1789868] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 42           |
|    time_elapsed          | 769          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0067057405 |
|    clip_fraction         | 0.0583       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 119          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0455      |
|    lagrangian_multiplier | 0.0788       |
|    learning_rate         | 0.0003       |
|    loss                  | 73.2         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00934     |
|    std                   | 1            |
|    value_loss            | 812          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6038597] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 48           |
|    time_elapsed          | 779          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.0073357834 |
|    clip_fraction         | 0.0701       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0278       |
|    entropy_loss          | -2.65        |
|    explained_variance    | 0.0282       |
|    lagrangian_multiplier | 0.0626       |
|    learning_rate         | 0.0003       |
|    loss                  | 108          |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00566     |
|    std                   | 0.91         |
|    value_loss            | 1.09e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8288058] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 49           |
|    time_elapsed          | 779          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.0065525724 |
|    clip_fraction         | 0.0687       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 249          |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0158       |
|    lagrangian_multiplier | 0.0665       |
|    learning_rate         | 0.0003       |
|    loss                  | 112          |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.0119      |
|    std                   | 1.02         |
|    value_loss            | 947          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
-------------------------------------------
| reward                   | [-0.8814389] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 43           |
|    time_elapsed          | 785          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0065940656 |
|    clip_fraction         | 0.0643       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 2.07         |
|    entropy_loss          | -2.44        |
|    explained_variance    | -0.0415      |
|    lagrangian_multiplier | 0.0524       |
|    learning_rate         | 0.0003       |
|    loss                  | 29.2         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00751     |
|    std                   | 0.824        |
|    value_loss            | 261          |
-------------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÅ‚ñà‚ñÇ‚ñÉ‚ñà‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÖ‚ñÅ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñá‚ñá‚ñÜ‚ñá
wandb:             train/approx_kl ‚ñÜ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÉ‚ñÖ
wandb:         train/clip_fraction ‚ñÑ‚ñÇ‚ñÖ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÖ‚ñÉ‚ñá‚ñÅ‚ñÉ‚ñÜ‚ñÉ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñá
wandb:          train/entropy_loss ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÅ
wandb:    train/explained_variance ‚ñÑ‚ñÜ‚ñÑ‚ñÅ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñà‚ñá‚ñÖ
wandb: train/lagrangian_multiplier ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÑ‚ñÇ‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÅ‚ñÖ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñà‚ñà‚ñÉ‚ñá‚ñÖ‚ñà‚ñá‚ñÖ‚ñÜ‚ñÖ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÑ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñÖ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÇ‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÇ
wandb:                   train/std ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñà
wandb:            train/value_loss ‚ñÉ‚ñÅ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñá‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñà‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÖ
wandb: 
wandb: Run summary:
wandb:                      reward -0.82881
wandb:             train/approx_kl 0.00655
wandb:         train/clip_fraction 0.0687
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 249.34418
wandb:          train/entropy_loss -2.87013
wandb:    train/explained_variance 0.01581
wandb: train/lagrangian_multiplier 0.06652
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 112.45954
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.01193
wandb:                   train/std 1.02132
wandb:            train/value_loss 947.46099
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2/runs/gg2qhntv
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_194359-gg2qhntv/logs
-------------------------------------------
| reward                   | [-2.1977582] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 43           |
|    time_elapsed          | 786          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0065134997 |
|    clip_fraction         | 0.0584       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 215          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0345      |
|    lagrangian_multiplier | 0.0783       |
|    learning_rate         | 0.0003       |
|    loss                  | 107          |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00975     |
|    std                   | 0.997        |
|    value_loss            | 1.01e+03     |
-------------------------------------------
srun: error: gail.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-1.7350872] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 49           |
|    time_elapsed          | 795          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.006077314  |
|    clip_fraction         | 0.0782       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0629       |
|    entropy_loss          | -2.66        |
|    explained_variance    | 0.027        |
|    lagrangian_multiplier | 0.0771       |
|    learning_rate         | 0.0003       |
|    loss                  | 156          |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.00918     |
|    std                   | 0.915        |
|    value_loss            | 1.63e+03     |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ
wandb:             train/approx_kl ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÉ
wandb:         train/clip_fraction ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÑ‚ñÖ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ
wandb:          train/entropy_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá
wandb:    train/explained_variance ‚ñÅ‚ñÜ‚ñá‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñÖ‚ñÜ‚ñÖ
wandb: train/lagrangian_multiplier ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÜ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñà‚ñÖ‚ñá‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÜ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñá‚ñá‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÑ
wandb:                   train/std ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:            train/value_loss ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñá
wandb: 
wandb: Run summary:
wandb:                      reward -1.73509
wandb:             train/approx_kl 0.00608
wandb:         train/clip_fraction 0.07822
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 0.06288
wandb:          train/entropy_loss -2.65627
wandb:    train/explained_variance 0.02698
wandb: train/lagrangian_multiplier 0.0771
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 156.01625
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00918
wandb:                   train/std 0.91469
wandb:            train/value_loss 1631.46388
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16/runs/3cdf4gu3
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_194359-3cdf4gu3/logs
-------------------------------------------
| reward                   | [-0.5111563] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 44           |
|    time_elapsed          | 802          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.005214143  |
|    clip_fraction         | 0.037        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 223          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.000325    |
|    lagrangian_multiplier | 0.074        |
|    learning_rate         | 0.0003       |
|    loss                  | 64.5         |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.0056      |
|    std                   | 1.01         |
|    value_loss            | 455          |
-------------------------------------------
srun: error: dqn.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-0.5145731] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 44           |
|    time_elapsed          | 804          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.0051091546 |
|    clip_fraction         | 0.0712       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0363       |
|    entropy_loss          | -2.46        |
|    explained_variance    | -0.0383      |
|    lagrangian_multiplier | 0.0735       |
|    learning_rate         | 0.0003       |
|    loss                  | 15.4         |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.00616     |
|    std                   | 0.831        |
|    value_loss            | 181          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.3743519] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 45           |
|    time_elapsed          | 818          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.0057129227 |
|    clip_fraction         | 0.0687       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 171          |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.00581     |
|    lagrangian_multiplier | 0.0712       |
|    learning_rate         | 0.0003       |
|    loss                  | 60.1         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00989     |
|    std                   | 1.02         |
|    value_loss            | 505          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.53714126] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 45            |
|    time_elapsed          | 822           |
|    total_timesteps       | 92160         |
| train/                   |               |
|    approx_kl             | 0.0075941244  |
|    clip_fraction         | 0.0713        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 15.5          |
|    entropy_loss          | -2.46         |
|    explained_variance    | -0.0136       |
|    lagrangian_multiplier | 0.0569        |
|    learning_rate         | 0.0003        |
|    loss                  | 42.6          |
|    n_updates             | 440           |
|    policy_gradient_loss  | -0.00725      |
|    std                   | 0.828         |
|    value_loss            | 402           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.5644517] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 46           |
|    time_elapsed          | 834          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0066661704 |
|    clip_fraction         | 0.0617       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 152          |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0501       |
|    lagrangian_multiplier | 0.066        |
|    learning_rate         | 0.0003       |
|    loss                  | 66.1         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.00961     |
|    std                   | 1.03         |
|    value_loss            | 589          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114548.4
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_195759-4p6nmkvx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/4p6nmkvx
--------------------------------------------
| reward                   | [-0.83160937] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 46            |
|    time_elapsed          | 840           |
|    total_timesteps       | 94208         |
| train/                   |               |
|    approx_kl             | 0.0071566757  |
|    clip_fraction         | 0.0749        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 15            |
|    entropy_loss          | -2.44         |
|    explained_variance    | -0.0415       |
|    lagrangian_multiplier | 0.0637        |
|    learning_rate         | 0.0003        |
|    loss                  | 30.5          |
|    n_updates             | 450           |
|    policy_gradient_loss  | -0.00846      |
|    std                   | 0.816         |
|    value_loss            | 301           |
--------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114548.5
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_195806-m6hu2kdv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/m6hu2kdv
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.66680825] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 47            |
|    time_elapsed          | 851           |
|    total_timesteps       | 96256         |
| train/                   |               |
|    approx_kl             | 0.007278044   |
|    clip_fraction         | 0.0572        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 167           |
|    entropy_loss          | -2.89         |
|    explained_variance    | 0.00399       |
|    lagrangian_multiplier | 0.0805        |
|    learning_rate         | 0.0003        |
|    loss                  | 35.6          |
|    n_updates             | 460           |
|    policy_gradient_loss  | -0.00866      |
|    std                   | 1.02          |
|    value_loss            | 288           |
--------------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.5781737] |
| time/              |              |
|    fps             | 134          |
|    iterations      | 1            |
|    time_elapsed    | 15           |
|    total_timesteps | 2048         |
-------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.26543325] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 47            |
|    time_elapsed          | 858           |
|    total_timesteps       | 96256         |
| train/                   |               |
|    approx_kl             | 0.00875899    |
|    clip_fraction         | 0.0994        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.328         |
|    entropy_loss          | -2.45         |
|    explained_variance    | -0.0356       |
|    lagrangian_multiplier | 0.0549        |
|    learning_rate         | 0.0003        |
|    loss                  | 54.2          |
|    n_updates             | 460           |
|    policy_gradient_loss  | -0.0135       |
|    std                   | 0.834         |
|    value_loss            | 478           |
--------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.37060106] |
| time/              |               |
|    fps             | 134           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.0091287] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 48           |
|    time_elapsed          | 867          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.0069500334 |
|    clip_fraction         | 0.0642       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 240          |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0494       |
|    lagrangian_multiplier | 0.0852       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.5         |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00756     |
|    std                   | 1.02         |
|    value_loss            | 369          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.6114159] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 2            |
|    time_elapsed          | 31           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.005954187  |
|    clip_fraction         | 0.0452       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0298       |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0102      |
|    lagrangian_multiplier | 0.0727       |
|    learning_rate         | 0.0003       |
|    loss                  | 40.3         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00421     |
|    std                   | 1.01         |
|    value_loss            | 488          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0238299] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 48           |
|    time_elapsed          | 876          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.008241812  |
|    clip_fraction         | 0.0669       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 32           |
|    entropy_loss          | -2.49        |
|    explained_variance    | -0.0277      |
|    lagrangian_multiplier | 0.0567       |
|    learning_rate         | 0.0003       |
|    loss                  | 48.6         |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.846        |
|    value_loss            | 499          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.3651611] |
| time/                    |              |
|    fps                   | 130          |
|    iterations            | 2            |
|    time_elapsed          | 31           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0055730836 |
|    clip_fraction         | 0.0334       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.172        |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.029        |
|    lagrangian_multiplier | 0.0559       |
|    learning_rate         | 0.0003       |
|    loss                  | 56.8         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00489     |
|    std                   | 0.997        |
|    value_loss            | 532          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0554574] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 49           |
|    time_elapsed          | 883          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.0062546497 |
|    clip_fraction         | 0.0656       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 228          |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.0595      |
|    lagrangian_multiplier | 0.0808       |
|    learning_rate         | 0.0003       |
|    loss                  | 99.3         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.00998     |
|    std                   | 1.03         |
|    value_loss            | 762          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
--------------------------------------------
| reward                   | [-0.96811587] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 3             |
|    time_elapsed          | 47            |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.004858153   |
|    clip_fraction         | 0.0409        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 106           |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.0206        |
|    lagrangian_multiplier | 0.0627        |
|    learning_rate         | 0.0003        |
|    loss                  | 54            |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00511      |
|    std                   | 1             |
|    value_loss            | 435           |
--------------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñÖ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÜ
wandb:             train/approx_kl ‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñà‚ñÅ‚ñÜ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÜ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñá‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÑ
wandb:         train/clip_fraction ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñà‚ñÅ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñá‚ñÑ‚ñà‚ñÉ‚ñÑ‚ñà‚ñá‚ñÖ‚ñá‚ñà‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñÜ‚ñÖ‚ñÜ‚ñÜ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñá‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÅ‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñà
wandb:          train/entropy_loss ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb:    train/explained_variance ‚ñÜ‚ñÖ‚ñà‚ñÅ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÖ‚ñá‚ñÑ‚ñÇ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñá‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÉ
wandb: train/lagrangian_multiplier ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÅ‚ñÑ‚ñÅ‚ñÑ‚ñá‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÜ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñá‚ñá‚ñÖ‚ñÇ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñá
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÇ‚ñá‚ñÇ‚ñÜ‚ñÖ‚ñà‚ñá‚ñá‚ñà‚ñÖ‚ñÜ‚ñá‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñà‚ñÅ‚ñÇ‚ñÜ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÉ
wandb:                   train/std ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà
wandb:            train/value_loss ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÅ‚ñà‚ñÅ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÇ‚ñá‚ñÜ‚ñÑ‚ñÅ‚ñÜ‚ñà‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÜ
wandb: 
wandb: Run summary:
wandb:                      reward -1.05546
wandb:             train/approx_kl 0.00625
wandb:         train/clip_fraction 0.06563
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 227.74024
wandb:          train/entropy_loss -2.89096
wandb:    train/explained_variance -0.05954
wandb: train/lagrangian_multiplier 0.08081
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 99.27787
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00998
wandb:                   train/std 1.03214
wandb:            train/value_loss 762.36237
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2/runs/9s8lka6w
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231227_034359-9s8lka6w/logs
srun: error: airl.ist.berkeley.edu: task 0: Exited with exit code 1
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114548.6
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.60983604] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 49            |
|    time_elapsed          | 895           |
|    total_timesteps       | 100352        |
| train/                   |               |
|    approx_kl             | 0.008348269   |
|    clip_fraction         | 0.0631        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0733        |
|    entropy_loss          | -2.5          |
|    explained_variance    | -0.0122       |
|    lagrangian_multiplier | 0.0691        |
|    learning_rate         | 0.0003        |
|    loss                  | 11.8          |
|    n_updates             | 480           |
|    policy_gradient_loss  | -0.0075       |
|    std                   | 0.85          |
|    value_loss            | 118           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.57495064] |
| time/                    |               |
|    fps                   | 129           |
|    iterations            | 3             |
|    time_elapsed          | 47            |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.006291179   |
|    clip_fraction         | 0.0483        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.101         |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.0142        |
|    lagrangian_multiplier | 0.044         |
|    learning_rate         | 0.0003        |
|    loss                  | 43.7          |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00459      |
|    std                   | 0.99          |
|    value_loss            | 368           |
--------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231227_035855-31jz1zo6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16/runs/31jz1zo6
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
wandb: 
wandb: Run history:
wandb:                      reward ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÜ‚ñÅ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ
wandb:             train/approx_kl ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñà‚ñÜ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñÜ‚ñÜ
wandb:         train/clip_fraction ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñá‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÑ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñà‚ñÅ
wandb:          train/entropy_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá
wandb:    train/explained_variance ‚ñà‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÖ
wandb: train/lagrangian_multiplier ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÖ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÅ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñÉ‚ñÖ
wandb:                   train/std ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:            train/value_loss ‚ñÜ‚ñà‚ñÖ‚ñá‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÜ‚ñÜ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                      reward -0.60984
wandb:             train/approx_kl 0.00835
wandb:         train/clip_fraction 0.06313
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 0.07327
wandb:          train/entropy_loss -2.50111
wandb:    train/explained_variance -0.01221
wandb: train/lagrangian_multiplier 0.06913
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 11.772
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.0075
wandb:                   train/std 0.8496
wandb:            train/value_loss 118.19388
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/9wbc9ht6
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_194359-9wbc9ht6/logs
srun: error: ddpg.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-1.4708285] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 4            |
|    time_elapsed          | 64           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.0058042393 |
|    clip_fraction         | 0.0441       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 65.5         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.022       |
|    lagrangian_multiplier | 0.0799       |
|    learning_rate         | 0.0003       |
|    loss                  | 56.3         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00543     |
|    std                   | 1            |
|    value_loss            | 597          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.49250817] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 4             |
|    time_elapsed          | 63            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.0067242417  |
|    clip_fraction         | 0.0396        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.947         |
|    entropy_loss          | -2.8          |
|    explained_variance    | 0.029         |
|    lagrangian_multiplier | 0.0501        |
|    learning_rate         | 0.0003        |
|    loss                  | 92            |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00423      |
|    std                   | 0.971         |
|    value_loss            | 799           |
--------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.49151182] |
| time/              |               |
|    fps             | 135           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
-------------------------------------------
| reward                   | [-1.6281278] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 5            |
|    time_elapsed          | 80           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.004263337  |
|    clip_fraction         | 0.0436       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 74.8         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0125      |
|    lagrangian_multiplier | 0.0521       |
|    learning_rate         | 0.0003       |
|    loss                  | 95.8         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00719     |
|    std                   | 1            |
|    value_loss            | 695          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0551394] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 5            |
|    time_elapsed          | 79           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.005303989  |
|    clip_fraction         | 0.0385       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.168        |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0614       |
|    lagrangian_multiplier | 0.0469       |
|    learning_rate         | 0.0003       |
|    loss                  | 67.1         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00513     |
|    std                   | 0.959        |
|    value_loss            | 544          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.97184134] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 6             |
|    time_elapsed          | 96            |
|    total_timesteps       | 12288         |
| train/                   |               |
|    approx_kl             | 0.004584199   |
|    clip_fraction         | 0.0418        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 26.5          |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.0506       |
|    lagrangian_multiplier | 0.0588        |
|    learning_rate         | 0.0003        |
|    loss                  | 77.4          |
|    n_updates             | 50            |
|    policy_gradient_loss  | -0.00554      |
|    std                   | 0.999         |
|    value_loss            | 736           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.4346201] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 6            |
|    time_elapsed          | 95           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0036601187 |
|    clip_fraction         | 0.0186       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.183        |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.0577       |
|    lagrangian_multiplier | 0.0449       |
|    learning_rate         | 0.0003       |
|    loss                  | 39.2         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00242     |
|    std                   | 0.968        |
|    value_loss            | 298          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9057405] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 7            |
|    time_elapsed          | 113          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.0051190453 |
|    clip_fraction         | 0.0345       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 95.7         |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.023        |
|    lagrangian_multiplier | 0.0413       |
|    learning_rate         | 0.0003       |
|    loss                  | 77.5         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00499     |
|    std                   | 1            |
|    value_loss            | 493          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5692657] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 7            |
|    time_elapsed          | 111          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.006148228  |
|    clip_fraction         | 0.043        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0603       |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0442       |
|    lagrangian_multiplier | 0.0567       |
|    learning_rate         | 0.0003       |
|    loss                  | 95.5         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00362     |
|    std                   | 0.969        |
|    value_loss            | 744          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9536574] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 8            |
|    time_elapsed          | 129          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.005772843  |
|    clip_fraction         | 0.0468       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 14.5         |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0129       |
|    lagrangian_multiplier | 0.0406       |
|    learning_rate         | 0.0003       |
|    loss                  | 71.7         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00453     |
|    std                   | 0.996        |
|    value_loss            | 508          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114548.7
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.6371756] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 8            |
|    time_elapsed          | 128          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.005367535  |
|    clip_fraction         | 0.0453       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.039        |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.106        |
|    lagrangian_multiplier | 0.053        |
|    learning_rate         | 0.0003       |
|    loss                  | 20.6         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00783     |
|    std                   | 0.967        |
|    value_loss            | 148          |
-------------------------------------------
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_200016-gwna5x1f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/gwna5x1f
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.7707571] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 9            |
|    time_elapsed          | 146          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.005975757  |
|    clip_fraction         | 0.0484       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 85.1         |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0512       |
|    lagrangian_multiplier | 0.0418       |
|    learning_rate         | 0.0003       |
|    loss                  | 106          |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00764     |
|    std                   | 0.999        |
|    value_loss            | 678          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.5017163] |
| time/                    |              |
|    fps                   | 43           |
|    iterations            | 2            |
|    time_elapsed          | 94           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.00629982   |
|    clip_fraction         | 0.0574       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0466       |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0234      |
|    lagrangian_multiplier | 0.0724       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.9         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00706     |
|    std                   | 1            |
|    value_loss            | 506          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.623743]  |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 9            |
|    time_elapsed          | 144          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0041658655 |
|    clip_fraction         | 0.0335       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0156       |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.058        |
|    lagrangian_multiplier | 0.0576       |
|    learning_rate         | 0.0003       |
|    loss                  | 19.1         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00502     |
|    std                   | 0.978        |
|    value_loss            | 167          |
-------------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.6460912] |
| time/              |              |
|    fps             | 120          |
|    iterations      | 1            |
|    time_elapsed    | 16           |
|    total_timesteps | 2048         |
-------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-0.627103] |
| time/                    |             |
|    fps                   | 125         |
|    iterations            | 10          |
|    time_elapsed          | 162         |
|    total_timesteps       | 20480       |
| train/                   |             |
|    approx_kl             | 0.005468458 |
|    clip_fraction         | 0.0623      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 72          |
|    entropy_loss          | -2.84       |
|    explained_variance    | -0.0214     |
|    lagrangian_multiplier | 0.0566      |
|    learning_rate         | 0.0003      |
|    loss                  | 44.9        |
|    n_updates             | 90          |
|    policy_gradient_loss  | -0.00812    |
|    std                   | 1           |
|    value_loss            | 325         |
------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.6334079] |
| time/                    |              |
|    fps                   | 55           |
|    iterations            | 3            |
|    time_elapsed          | 110          |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0061649866 |
|    clip_fraction         | 0.0427       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0948       |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.00913      |
|    lagrangian_multiplier | 0.0544       |
|    learning_rate         | 0.0003       |
|    loss                  | 66           |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00492     |
|    std                   | 0.995        |
|    value_loss            | 523          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.60636395] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 10            |
|    time_elapsed          | 160           |
|    total_timesteps       | 20480         |
| train/                   |               |
|    approx_kl             | 0.006111537   |
|    clip_fraction         | 0.0347        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0336        |
|    entropy_loss          | -2.8          |
|    explained_variance    | 0.0794        |
|    lagrangian_multiplier | 0.0559        |
|    learning_rate         | 0.0003        |
|    loss                  | 23.7          |
|    n_updates             | 90            |
|    policy_gradient_loss  | -0.0036       |
|    std                   | 0.983         |
|    value_loss            | 221           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.59449023] |
| time/                    |               |
|    fps                   | 117           |
|    iterations            | 2             |
|    time_elapsed          | 34            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.0046753753  |
|    clip_fraction         | 0.0421        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 51.9          |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.0104       |
|    lagrangian_multiplier | 0.0557        |
|    learning_rate         | 0.0003        |
|    loss                  | 58.4          |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.0056       |
|    std                   | 1             |
|    value_loss            | 465           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.0533835] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 11           |
|    time_elapsed          | 179          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0070369635 |
|    clip_fraction         | 0.0729       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 35.3         |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.021       |
|    lagrangian_multiplier | 0.0463       |
|    learning_rate         | 0.0003       |
|    loss                  | 46           |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.0094      |
|    std                   | 1.02         |
|    value_loss            | 377          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.90791935] |
| time/                    |               |
|    fps                   | 64            |
|    iterations            | 4             |
|    time_elapsed          | 126           |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.0065838834  |
|    clip_fraction         | 0.0584        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0299        |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.0721       |
|    lagrangian_multiplier | 0.0529        |
|    learning_rate         | 0.0003        |
|    loss                  | 35.7          |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00624      |
|    std                   | 0.996         |
|    value_loss            | 314           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.8493322] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 11           |
|    time_elapsed          | 177          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.006409231  |
|    clip_fraction         | 0.0417       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0164       |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0978       |
|    lagrangian_multiplier | 0.0637       |
|    learning_rate         | 0.0003       |
|    loss                  | 12.8         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00583     |
|    std                   | 0.981        |
|    value_loss            | 135          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.4525607] |
| time/                    |              |
|    fps                   | 116          |
|    iterations            | 3            |
|    time_elapsed          | 52           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.004967577  |
|    clip_fraction         | 0.0405       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.24         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0339      |
|    lagrangian_multiplier | 0.0544       |
|    learning_rate         | 0.0003       |
|    loss                  | 30.3         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00483     |
|    std                   | 1            |
|    value_loss            | 300          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.40247354] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 12            |
|    time_elapsed          | 195           |
|    total_timesteps       | 24576         |
| train/                   |               |
|    approx_kl             | 0.004369884   |
|    clip_fraction         | 0.0399        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 108           |
|    entropy_loss          | -2.87         |
|    explained_variance    | -0.0116       |
|    lagrangian_multiplier | 0.0529        |
|    learning_rate         | 0.0003        |
|    loss                  | 59            |
|    n_updates             | 110           |
|    policy_gradient_loss  | -0.007        |
|    std                   | 1.02          |
|    value_loss            | 367           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3790938] |
| time/                    |              |
|    fps                   | 71           |
|    iterations            | 5            |
|    time_elapsed          | 142          |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0064161187 |
|    clip_fraction         | 0.0561       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.039        |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0211      |
|    lagrangian_multiplier | 0.0727       |
|    learning_rate         | 0.0003       |
|    loss                  | 65.7         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00759     |
|    std                   | 0.997        |
|    value_loss            | 557          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3376038] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 12           |
|    time_elapsed          | 193          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0052310284 |
|    clip_fraction         | 0.0456       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0794       |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.00323      |
|    lagrangian_multiplier | 0.0524       |
|    learning_rate         | 0.0003       |
|    loss                  | 28.6         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00636     |
|    std                   | 0.976        |
|    value_loss            | 272          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.43547082] |
| time/                    |               |
|    fps                   | 116           |
|    iterations            | 4             |
|    time_elapsed          | 70            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.006456008   |
|    clip_fraction         | 0.0749        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 137           |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.00337      |
|    lagrangian_multiplier | 0.061         |
|    learning_rate         | 0.0003        |
|    loss                  | 46.5          |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00785      |
|    std                   | 0.998         |
|    value_loss            | 316           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.1057647] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 13           |
|    time_elapsed          | 212          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0065323445 |
|    clip_fraction         | 0.0478       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 85.6         |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.00137      |
|    lagrangian_multiplier | 0.0541       |
|    learning_rate         | 0.0003       |
|    loss                  | 49.2         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00788     |
|    std                   | 1.01         |
|    value_loss            | 372          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.8132975] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 13           |
|    time_elapsed          | 209          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.006092232  |
|    clip_fraction         | 0.0418       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 5.79         |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0153       |
|    lagrangian_multiplier | 0.0568       |
|    learning_rate         | 0.0003       |
|    loss                  | 218          |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00558     |
|    std                   | 0.974        |
|    value_loss            | 1.84e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6836674] |
| time/                    |              |
|    fps                   | 115          |
|    iterations            | 5            |
|    time_elapsed          | 88           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0066384147 |
|    clip_fraction         | 0.0536       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 113          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0229      |
|    lagrangian_multiplier | 0.0693       |
|    learning_rate         | 0.0003       |
|    loss                  | 95.9         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.006       |
|    std                   | 1            |
|    value_loss            | 978          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8317648] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 14           |
|    time_elapsed          | 228          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.004255104  |
|    clip_fraction         | 0.026        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 105          |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.038        |
|    lagrangian_multiplier | 0.0492       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.9         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00394     |
|    std                   | 1.01         |
|    value_loss            | 386          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2603339] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 14           |
|    time_elapsed          | 225          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.00514077   |
|    clip_fraction         | 0.041        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 2.71         |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.023        |
|    lagrangian_multiplier | 0.0441       |
|    learning_rate         | 0.0003       |
|    loss                  | 115          |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00542     |
|    std                   | 0.976        |
|    value_loss            | 724          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7986687] |
| time/                    |              |
|    fps                   | 67           |
|    iterations            | 6            |
|    time_elapsed          | 183          |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0037684685 |
|    clip_fraction         | 0.0274       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0556       |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0182      |
|    lagrangian_multiplier | 0.0577       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.3         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00303     |
|    std                   | 1            |
|    value_loss            | 528          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9119902] |
| time/                    |              |
|    fps                   | 115          |
|    iterations            | 6            |
|    time_elapsed          | 106          |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.004355139  |
|    clip_fraction         | 0.0571       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 179          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0618      |
|    lagrangian_multiplier | 0.0581       |
|    learning_rate         | 0.0003       |
|    loss                  | 78.3         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00861     |
|    std                   | 1            |
|    value_loss            | 632          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9147658] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 15           |
|    time_elapsed          | 244          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0054180343 |
|    clip_fraction         | 0.0388       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 17.6         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0151       |
|    lagrangian_multiplier | 0.0537       |
|    learning_rate         | 0.0003       |
|    loss                  | 32.6         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00555     |
|    std                   | 1            |
|    value_loss            | 278          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.1765597] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 15           |
|    time_elapsed          | 242          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0071019335 |
|    clip_fraction         | 0.0506       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.49         |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0371       |
|    lagrangian_multiplier | 0.0555       |
|    learning_rate         | 0.0003       |
|    loss                  | 195          |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00754     |
|    std                   | 0.966        |
|    value_loss            | 1.7e+03      |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.94812024] |
| time/                    |               |
|    fps                   | 71            |
|    iterations            | 7             |
|    time_elapsed          | 199           |
|    total_timesteps       | 14336         |
| train/                   |               |
|    approx_kl             | 0.0039706887  |
|    clip_fraction         | 0.0472        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.115         |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.00616       |
|    lagrangian_multiplier | 0.0629        |
|    learning_rate         | 0.0003        |
|    loss                  | 34.9          |
|    n_updates             | 60            |
|    policy_gradient_loss  | -0.00605      |
|    std                   | 1             |
|    value_loss            | 330           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.1135938] |
| time/                    |              |
|    fps                   | 115          |
|    iterations            | 7            |
|    time_elapsed          | 124          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.0056970953 |
|    clip_fraction         | 0.0543       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 155          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0454      |
|    lagrangian_multiplier | 0.0604       |
|    learning_rate         | 0.0003       |
|    loss                  | 65.3         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00744     |
|    std                   | 1.02         |
|    value_loss            | 463          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0353439] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 16           |
|    time_elapsed          | 261          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.004329904  |
|    clip_fraction         | 0.0403       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 45           |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0397       |
|    lagrangian_multiplier | 0.0528       |
|    learning_rate         | 0.0003       |
|    loss                  | 37.2         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.0063      |
|    std                   | 1.01         |
|    value_loss            | 296          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.8892975] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 16           |
|    time_elapsed          | 258          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.006478479  |
|    clip_fraction         | 0.053        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.09         |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.00632      |
|    lagrangian_multiplier | 0.0455       |
|    learning_rate         | 0.0003       |
|    loss                  | 75.3         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.0071      |
|    std                   | 0.971        |
|    value_loss            | 602          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.9838533] |
| time/                    |              |
|    fps                   | 75           |
|    iterations            | 8            |
|    time_elapsed          | 215          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0065699955 |
|    clip_fraction         | 0.0575       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0295       |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0588      |
|    lagrangian_multiplier | 0.0692       |
|    learning_rate         | 0.0003       |
|    loss                  | 36.6         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00806     |
|    std                   | 0.988        |
|    value_loss            | 387          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9538474] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 17           |
|    time_elapsed          | 278          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0046747234 |
|    clip_fraction         | 0.0411       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 79.6         |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0207      |
|    lagrangian_multiplier | 0.0537       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.1         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00655     |
|    std                   | 1.01         |
|    value_loss            | 458          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5510976] |
| time/                    |              |
|    fps                   | 115          |
|    iterations            | 8            |
|    time_elapsed          | 141          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.006460605  |
|    clip_fraction         | 0.0476       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 167          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0586      |
|    lagrangian_multiplier | 0.0763       |
|    learning_rate         | 0.0003       |
|    loss                  | 85           |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00876     |
|    std                   | 0.983        |
|    value_loss            | 737          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.7253547] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 17           |
|    time_elapsed          | 274          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0064609973 |
|    clip_fraction         | 0.0571       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.97         |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0107       |
|    lagrangian_multiplier | 0.0455       |
|    learning_rate         | 0.0003       |
|    loss                  | 55.3         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.0108      |
|    std                   | 0.986        |
|    value_loss            | 449          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6533]    |
| time/                    |              |
|    fps                   | 79           |
|    iterations            | 9            |
|    time_elapsed          | 231          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0047624186 |
|    clip_fraction         | 0.0362       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0373       |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0306      |
|    lagrangian_multiplier | 0.062        |
|    learning_rate         | 0.0003       |
|    loss                  | 163          |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00581     |
|    std                   | 1            |
|    value_loss            | 1.44e+03     |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.56273353] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 18            |
|    time_elapsed          | 294           |
|    total_timesteps       | 36864         |
| train/                   |               |
|    approx_kl             | 0.005387122   |
|    clip_fraction         | 0.053         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 55.3          |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.0329       |
|    lagrangian_multiplier | 0.0528        |
|    learning_rate         | 0.0003        |
|    loss                  | 31.6          |
|    n_updates             | 170           |
|    policy_gradient_loss  | -0.00658      |
|    std                   | 0.999         |
|    value_loss            | 266           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.8056956] |
| time/                    |              |
|    fps                   | 115          |
|    iterations            | 9            |
|    time_elapsed          | 160          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.004460871  |
|    clip_fraction         | 0.0419       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 137          |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0831      |
|    lagrangian_multiplier | 0.0692       |
|    learning_rate         | 0.0003       |
|    loss                  | 77.8         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00509     |
|    std                   | 0.985        |
|    value_loss            | 681          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.1609554] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 18           |
|    time_elapsed          | 291          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.007839097  |
|    clip_fraction         | 0.0563       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.8          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0101      |
|    lagrangian_multiplier | 0.0405       |
|    learning_rate         | 0.0003       |
|    loss                  | 144          |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00976     |
|    std                   | 1.02         |
|    value_loss            | 873          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8492843] |
| time/                    |              |
|    fps                   | 82           |
|    iterations            | 10           |
|    time_elapsed          | 247          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.005161985  |
|    clip_fraction         | 0.0507       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0486       |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.00924     |
|    lagrangian_multiplier | 0.064        |
|    learning_rate         | 0.0003       |
|    loss                  | 105          |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00885     |
|    std                   | 1            |
|    value_loss            | 1.18e+03     |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.59618723] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 19            |
|    time_elapsed          | 311           |
|    total_timesteps       | 38912         |
| train/                   |               |
|    approx_kl             | 0.004673269   |
|    clip_fraction         | 0.0386        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 41.7          |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.007        |
|    lagrangian_multiplier | 0.0507        |
|    learning_rate         | 0.0003        |
|    loss                  | 29.1          |
|    n_updates             | 180           |
|    policy_gradient_loss  | -0.0079       |
|    std                   | 1             |
|    value_loss            | 253           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.0505003] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 19           |
|    time_elapsed          | 307          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0062894863 |
|    clip_fraction         | 0.0574       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.136        |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0137       |
|    lagrangian_multiplier | 0.0533       |
|    learning_rate         | 0.0003       |
|    loss                  | 97.8         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00737     |
|    std                   | 1.01         |
|    value_loss            | 784          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.682194] |
| time/                    |             |
|    fps                   | 114         |
|    iterations            | 10          |
|    time_elapsed          | 178         |
|    total_timesteps       | 20480       |
| train/                   |             |
|    approx_kl             | 0.007341073 |
|    clip_fraction         | 0.0632      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 132         |
|    entropy_loss          | -2.8        |
|    explained_variance    | -0.0744     |
|    lagrangian_multiplier | 0.0673      |
|    learning_rate         | 0.0003      |
|    loss                  | 79.5        |
|    n_updates             | 90          |
|    policy_gradient_loss  | -0.00793    |
|    std                   | 0.978       |
|    value_loss            | 732         |
------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.8480399] |
| time/                    |              |
|    fps                   | 85           |
|    iterations            | 11           |
|    time_elapsed          | 264          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.005773133  |
|    clip_fraction         | 0.0543       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.134        |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0477      |
|    lagrangian_multiplier | 0.0496       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.9         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00617     |
|    std                   | 1.01         |
|    value_loss            | 546          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0766367] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 20           |
|    time_elapsed          | 327          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.005383362  |
|    clip_fraction         | 0.0482       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 44.2         |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0116      |
|    lagrangian_multiplier | 0.0474       |
|    learning_rate         | 0.0003       |
|    loss                  | 53.7         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00802     |
|    std                   | 0.994        |
|    value_loss            | 345          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1877403] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 20           |
|    time_elapsed          | 323          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0066732005 |
|    clip_fraction         | 0.0483       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 7.27         |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0348       |
|    lagrangian_multiplier | 0.0554       |
|    learning_rate         | 0.0003       |
|    loss                  | 177          |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00813     |
|    std                   | 1.02         |
|    value_loss            | 1.62e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.91608155] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 11            |
|    time_elapsed          | 196           |
|    total_timesteps       | 22528         |
| train/                   |               |
|    approx_kl             | 0.0054942425  |
|    clip_fraction         | 0.0763        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 89.5          |
|    entropy_loss          | -2.8          |
|    explained_variance    | -0.0708       |
|    lagrangian_multiplier | 0.0729        |
|    learning_rate         | 0.0003        |
|    loss                  | 53.5          |
|    n_updates             | 100           |
|    policy_gradient_loss  | -0.00824      |
|    std                   | 0.988         |
|    value_loss            | 521           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.4618809] |
| time/                    |              |
|    fps                   | 87           |
|    iterations            | 12           |
|    time_elapsed          | 280          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.006638159  |
|    clip_fraction         | 0.0603       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.131        |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0182      |
|    lagrangian_multiplier | 0.0433       |
|    learning_rate         | 0.0003       |
|    loss                  | 170          |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00867     |
|    std                   | 0.992        |
|    value_loss            | 1.15e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8344547] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 21           |
|    time_elapsed          | 344          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0067908727 |
|    clip_fraction         | 0.0619       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 54.8         |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0246       |
|    lagrangian_multiplier | 0.0494       |
|    learning_rate         | 0.0003       |
|    loss                  | 39.2         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00882     |
|    std                   | 0.995        |
|    value_loss            | 268          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2542117] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 21           |
|    time_elapsed          | 339          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0026368774 |
|    clip_fraction         | 0.021        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0825       |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0497       |
|    lagrangian_multiplier | 0.0548       |
|    learning_rate         | 0.0003       |
|    loss                  | 37.2         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00231     |
|    std                   | 1.02         |
|    value_loss            | 321          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.608915] |
| time/                    |             |
|    fps                   | 114         |
|    iterations            | 12          |
|    time_elapsed          | 213         |
|    total_timesteps       | 24576       |
| train/                   |             |
|    approx_kl             | 0.005272215 |
|    clip_fraction         | 0.0447      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 72.2        |
|    entropy_loss          | -2.82       |
|    explained_variance    | -0.0756     |
|    lagrangian_multiplier | 0.0643      |
|    learning_rate         | 0.0003      |
|    loss                  | 46          |
|    n_updates             | 110         |
|    policy_gradient_loss  | -0.00636    |
|    std                   | 0.991       |
|    value_loss            | 398         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.0447298] |
| time/                    |              |
|    fps                   | 89           |
|    iterations            | 13           |
|    time_elapsed          | 296          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0074492926 |
|    clip_fraction         | 0.0653       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.118        |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0185      |
|    lagrangian_multiplier | 0.0578       |
|    learning_rate         | 0.0003       |
|    loss                  | 89.6         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.0113      |
|    std                   | 1            |
|    value_loss            | 968          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3773843] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 22           |
|    time_elapsed          | 360          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.004620294  |
|    clip_fraction         | 0.0373       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 16.3         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.00992     |
|    lagrangian_multiplier | 0.0445       |
|    learning_rate         | 0.0003       |
|    loss                  | 24           |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00518     |
|    std                   | 1            |
|    value_loss            | 181          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8872781] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 22           |
|    time_elapsed          | 356          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.00437274   |
|    clip_fraction         | 0.0389       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0599       |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.00303     |
|    lagrangian_multiplier | 0.0476       |
|    learning_rate         | 0.0003       |
|    loss                  | 36.1         |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00599     |
|    std                   | 1.02         |
|    value_loss            | 296          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9815488] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 13           |
|    time_elapsed          | 231          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.006314736  |
|    clip_fraction         | 0.0548       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 157          |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0724      |
|    lagrangian_multiplier | 0.0686       |
|    learning_rate         | 0.0003       |
|    loss                  | 65.6         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00741     |
|    std                   | 0.985        |
|    value_loss            | 546          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.7231162] |
| time/                    |              |
|    fps                   | 91           |
|    iterations            | 14           |
|    time_elapsed          | 312          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0061877426 |
|    clip_fraction         | 0.0703       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.108        |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0246      |
|    lagrangian_multiplier | 0.0592       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.3         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.0107      |
|    std                   | 1.01         |
|    value_loss            | 475          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8707264] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 23           |
|    time_elapsed          | 376          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.006276652  |
|    clip_fraction         | 0.0481       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 85.7         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0385      |
|    lagrangian_multiplier | 0.0496       |
|    learning_rate         | 0.0003       |
|    loss                  | 39           |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.00706     |
|    std                   | 1            |
|    value_loss            | 244          |
-------------------------------------------
------------------------------------------
| reward                   | [-0.538634] |
| time/                    |             |
|    fps                   | 126         |
|    iterations            | 23          |
|    time_elapsed          | 373         |
|    total_timesteps       | 47104       |
| train/                   |             |
|    approx_kl             | 0.00622135  |
|    clip_fraction         | 0.0513      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0846      |
|    entropy_loss          | -2.87       |
|    explained_variance    | 0.0435      |
|    lagrangian_multiplier | 0.0486      |
|    learning_rate         | 0.0003      |
|    loss                  | 73.1        |
|    n_updates             | 220         |
|    policy_gradient_loss  | -0.00978    |
|    std                   | 1.02        |
|    value_loss            | 666         |
------------------------------------------
-------------------------------------------
| reward                   | [-0.9756054] |
| time/                    |              |
|    fps                   | 93           |
|    iterations            | 15           |
|    time_elapsed          | 329          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0057149767 |
|    clip_fraction         | 0.057        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0422       |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.016        |
|    lagrangian_multiplier | 0.0714       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.1         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 1.02         |
|    value_loss            | 647          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7456548] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 14           |
|    time_elapsed          | 249          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.007958537  |
|    clip_fraction         | 0.0915       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 115          |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0646      |
|    lagrangian_multiplier | 0.0628       |
|    learning_rate         | 0.0003       |
|    loss                  | 120          |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.0121      |
|    std                   | 0.974        |
|    value_loss            | 1.07e+03     |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.4540955] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 24           |
|    time_elapsed          | 393          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.006175095  |
|    clip_fraction         | 0.0521       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 52.5         |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0126      |
|    lagrangian_multiplier | 0.0484       |
|    learning_rate         | 0.0003       |
|    loss                  | 36.6         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00917     |
|    std                   | 1.01         |
|    value_loss            | 209          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.66849786] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 24            |
|    time_elapsed          | 389           |
|    total_timesteps       | 49152         |
| train/                   |               |
|    approx_kl             | 0.0045864955  |
|    clip_fraction         | 0.0283        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0472        |
|    entropy_loss          | -2.87         |
|    explained_variance    | 0.0779        |
|    lagrangian_multiplier | 0.0568        |
|    learning_rate         | 0.0003        |
|    loss                  | 34.3          |
|    n_updates             | 230           |
|    policy_gradient_loss  | -0.00383      |
|    std                   | 1.01          |
|    value_loss            | 349           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.1966342] |
| time/                    |              |
|    fps                   | 94           |
|    iterations            | 16           |
|    time_elapsed          | 345          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.00676105   |
|    clip_fraction         | 0.0519       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0485       |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.019        |
|    lagrangian_multiplier | 0.0548       |
|    learning_rate         | 0.0003       |
|    loss                  | 74.7         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00606     |
|    std                   | 1.01         |
|    value_loss            | 611          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9328689] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 15           |
|    time_elapsed          | 267          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0046570487 |
|    clip_fraction         | 0.0377       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 62.1         |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0284       |
|    lagrangian_multiplier | 0.0769       |
|    learning_rate         | 0.0003       |
|    loss                  | 33.4         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00509     |
|    std                   | 0.965        |
|    value_loss            | 478          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.5961449] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 25           |
|    time_elapsed          | 410          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.0077222437 |
|    clip_fraction         | 0.0863       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 54.5         |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0151      |
|    lagrangian_multiplier | 0.0458       |
|    learning_rate         | 0.0003       |
|    loss                  | 29.3         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.0114      |
|    std                   | 1            |
|    value_loss            | 181          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6085659] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 25           |
|    time_elapsed          | 405          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.0048491573 |
|    clip_fraction         | 0.055        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0313       |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0753       |
|    lagrangian_multiplier | 0.06         |
|    learning_rate         | 0.0003       |
|    loss                  | 15.2         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00612     |
|    std                   | 1            |
|    value_loss            | 197          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-3.0360665] |
| time/                    |              |
|    fps                   | 96           |
|    iterations            | 17           |
|    time_elapsed          | 361          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0070567913 |
|    clip_fraction         | 0.0681       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0214       |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.00893      |
|    lagrangian_multiplier | 0.0521       |
|    learning_rate         | 0.0003       |
|    loss                  | 109          |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00791     |
|    std                   | 1            |
|    value_loss            | 944          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4118618] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 16           |
|    time_elapsed          | 285          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.006468121  |
|    clip_fraction         | 0.0567       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 57           |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0217      |
|    lagrangian_multiplier | 0.0617       |
|    learning_rate         | 0.0003       |
|    loss                  | 40.4         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00902     |
|    std                   | 0.962        |
|    value_loss            | 325          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.82382995] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 26            |
|    time_elapsed          | 426           |
|    total_timesteps       | 53248         |
| train/                   |               |
|    approx_kl             | 0.0056130467  |
|    clip_fraction         | 0.0354        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 38.6          |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.0506        |
|    lagrangian_multiplier | 0.0508        |
|    learning_rate         | 0.0003        |
|    loss                  | 23.7          |
|    n_updates             | 250           |
|    policy_gradient_loss  | -0.00682      |
|    std                   | 1             |
|    value_loss            | 154           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.5068223] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 26           |
|    time_elapsed          | 421          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0069806725 |
|    clip_fraction         | 0.0715       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 5.14         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0383       |
|    lagrangian_multiplier | 0.0518       |
|    learning_rate         | 0.0003       |
|    loss                  | 136          |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.01        |
|    std                   | 1.02         |
|    value_loss            | 1.01e+03     |
-------------------------------------------
------------------------------------------
| reward                   | [-3.38953]  |
| time/                    |             |
|    fps                   | 97          |
|    iterations            | 18          |
|    time_elapsed          | 377         |
|    total_timesteps       | 36864       |
| train/                   |             |
|    approx_kl             | 0.007432581 |
|    clip_fraction         | 0.0576      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.06        |
|    entropy_loss          | -2.84       |
|    explained_variance    | 0.0188      |
|    lagrangian_multiplier | 0.0711      |
|    learning_rate         | 0.0003      |
|    loss                  | 76.1        |
|    n_updates             | 170         |
|    policy_gradient_loss  | -0.00747    |
|    std                   | 1           |
|    value_loss            | 872         |
------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.4460692] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 17           |
|    time_elapsed          | 303          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.005483191  |
|    clip_fraction         | 0.0413       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.26         |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0185       |
|    lagrangian_multiplier | 0.0691       |
|    learning_rate         | 0.0003       |
|    loss                  | 22.1         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00485     |
|    std                   | 0.971        |
|    value_loss            | 242          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.63424015] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 27            |
|    time_elapsed          | 443           |
|    total_timesteps       | 55296         |
| train/                   |               |
|    approx_kl             | 0.007350267   |
|    clip_fraction         | 0.0643        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 9.34          |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.00953      |
|    lagrangian_multiplier | 0.051         |
|    learning_rate         | 0.0003        |
|    loss                  | 17.7          |
|    n_updates             | 260           |
|    policy_gradient_loss  | -0.00776      |
|    std                   | 1.01          |
|    value_loss            | 146           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.7539861] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 27           |
|    time_elapsed          | 438          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.003910122  |
|    clip_fraction         | 0.0288       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0302       |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0788       |
|    lagrangian_multiplier | 0.0556       |
|    learning_rate         | 0.0003       |
|    loss                  | 21.6         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00434     |
|    std                   | 1            |
|    value_loss            | 219          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-3.5881596] |
| time/                    |              |
|    fps                   | 98           |
|    iterations            | 19           |
|    time_elapsed          | 394          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0046562767 |
|    clip_fraction         | 0.0291       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0205       |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.00565     |
|    lagrangian_multiplier | 0.0771       |
|    learning_rate         | 0.0003       |
|    loss                  | 145          |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00472     |
|    std                   | 1            |
|    value_loss            | 1.74e+03     |
-------------------------------------------
------------------------------------------
| reward                   | [-2.008773] |
| time/                    |             |
|    fps                   | 114         |
|    iterations            | 18          |
|    time_elapsed          | 321         |
|    total_timesteps       | 36864       |
| train/                   |             |
|    approx_kl             | 0.006357677 |
|    clip_fraction         | 0.0571      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 133         |
|    entropy_loss          | -2.78       |
|    explained_variance    | -0.0932     |
|    lagrangian_multiplier | 0.0715      |
|    learning_rate         | 0.0003      |
|    loss                  | 67.9        |
|    n_updates             | 170         |
|    policy_gradient_loss  | -0.00998    |
|    std                   | 0.971       |
|    value_loss            | 558         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.1227661] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 28           |
|    time_elapsed          | 459          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.006139601  |
|    clip_fraction         | 0.0544       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.178        |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0339       |
|    lagrangian_multiplier | 0.0551       |
|    learning_rate         | 0.0003       |
|    loss                  | 13.1         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00674     |
|    std                   | 1            |
|    value_loss            | 116          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.4843618] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 28           |
|    time_elapsed          | 454          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.0044312    |
|    clip_fraction         | 0.0255       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.053        |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0639       |
|    lagrangian_multiplier | 0.0522       |
|    learning_rate         | 0.0003       |
|    loss                  | 19.6         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00268     |
|    std                   | 0.998        |
|    value_loss            | 166          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.3342342] |
| time/                    |              |
|    fps                   | 99           |
|    iterations            | 20           |
|    time_elapsed          | 410          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.008197822  |
|    clip_fraction         | 0.0718       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0456       |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.00145     |
|    lagrangian_multiplier | 0.0602       |
|    learning_rate         | 0.0003       |
|    loss                  | 213          |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 0.993        |
|    value_loss            | 2.21e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6867207] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 29           |
|    time_elapsed          | 475          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.00784163   |
|    clip_fraction         | 0.0746       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 83.2         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0153       |
|    lagrangian_multiplier | 0.0578       |
|    learning_rate         | 0.0003       |
|    loss                  | 36.7         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.0117      |
|    std                   | 1            |
|    value_loss            | 214          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2070794] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 19           |
|    time_elapsed          | 339          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0057116286 |
|    clip_fraction         | 0.0647       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 181          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0328       |
|    lagrangian_multiplier | 0.0701       |
|    learning_rate         | 0.0003       |
|    loss                  | 71.7         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.0105      |
|    std                   | 0.973        |
|    value_loss            | 672          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5613035] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 29           |
|    time_elapsed          | 470          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.0057327    |
|    clip_fraction         | 0.0394       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0654       |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.012        |
|    lagrangian_multiplier | 0.0557       |
|    learning_rate         | 0.0003       |
|    loss                  | 27           |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00461     |
|    std                   | 0.981        |
|    value_loss            | 257          |
-------------------------------------------
------------------------------------------
| reward                   | [-3.153062] |
| time/                    |             |
|    fps                   | 100         |
|    iterations            | 21          |
|    time_elapsed          | 426         |
|    total_timesteps       | 43008       |
| train/                   |             |
|    approx_kl             | 0.005169737 |
|    clip_fraction         | 0.053       |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0204      |
|    entropy_loss          | -2.82       |
|    explained_variance    | -0.0065     |
|    lagrangian_multiplier | 0.0801      |
|    learning_rate         | 0.0003      |
|    loss                  | 126         |
|    n_updates             | 200         |
|    policy_gradient_loss  | -0.00774    |
|    std                   | 0.99        |
|    value_loss            | 1.62e+03    |
------------------------------------------
--------------------------------------------
| reward                   | [-0.43550804] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 30            |
|    time_elapsed          | 492           |
|    total_timesteps       | 61440         |
| train/                   |               |
|    approx_kl             | 0.0061413804  |
|    clip_fraction         | 0.0569        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 35.2          |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.00421      |
|    lagrangian_multiplier | 0.0615        |
|    learning_rate         | 0.0003        |
|    loss                  | 21            |
|    n_updates             | 290           |
|    policy_gradient_loss  | -0.00766      |
|    std                   | 0.999         |
|    value_loss            | 158           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.4853222] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 30           |
|    time_elapsed          | 486          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.004331302  |
|    clip_fraction         | 0.0315       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0147       |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0814       |
|    lagrangian_multiplier | 0.06         |
|    learning_rate         | 0.0003       |
|    loss                  | 26.3         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00424     |
|    std                   | 0.989        |
|    value_loss            | 238          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.291529]  |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 20           |
|    time_elapsed          | 357          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0066975304 |
|    clip_fraction         | 0.052        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 184          |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.00942      |
|    lagrangian_multiplier | 0.0614       |
|    learning_rate         | 0.0003       |
|    loss                  | 94           |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00902     |
|    std                   | 0.957        |
|    value_loss            | 733          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.95852435] |
| time/                    |               |
|    fps                   | 101           |
|    iterations            | 22            |
|    time_elapsed          | 442           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.004372776   |
|    clip_fraction         | 0.0397        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0222        |
|    entropy_loss          | -2.83         |
|    explained_variance    | 0.0263        |
|    lagrangian_multiplier | 0.0615        |
|    learning_rate         | 0.0003        |
|    loss                  | 74.6          |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.00537      |
|    std                   | 1             |
|    value_loss            | 885           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.6481638] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 31           |
|    time_elapsed          | 508          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0054917787 |
|    clip_fraction         | 0.0555       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 12.4         |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0315       |
|    lagrangian_multiplier | 0.0482       |
|    learning_rate         | 0.0003       |
|    loss                  | 17.1         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00637     |
|    std                   | 0.992        |
|    value_loss            | 130          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.5025091] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 31           |
|    time_elapsed          | 503          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0056180404 |
|    clip_fraction         | 0.036        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0226       |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0958       |
|    lagrangian_multiplier | 0.059        |
|    learning_rate         | 0.0003       |
|    loss                  | 10.7         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00463     |
|    std                   | 0.991        |
|    value_loss            | 109          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.0915189] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 21           |
|    time_elapsed          | 375          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0066051283 |
|    clip_fraction         | 0.071        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 158          |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0143      |
|    lagrangian_multiplier | 0.0712       |
|    learning_rate         | 0.0003       |
|    loss                  | 80.3         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00803     |
|    std                   | 0.959        |
|    value_loss            | 666          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.529407]  |
| time/                    |              |
|    fps                   | 102          |
|    iterations            | 23           |
|    time_elapsed          | 459          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.0050264755 |
|    clip_fraction         | 0.0396       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0198       |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.000136     |
|    lagrangian_multiplier | 0.0589       |
|    learning_rate         | 0.0003       |
|    loss                  | 21.4         |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.00376     |
|    std                   | 1.01         |
|    value_loss            | 201          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.42211378] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 32            |
|    time_elapsed          | 525           |
|    total_timesteps       | 65536         |
| train/                   |               |
|    approx_kl             | 0.005486848   |
|    clip_fraction         | 0.0503        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0387        |
|    entropy_loss          | -2.81         |
|    explained_variance    | 0.0439        |
|    lagrangian_multiplier | 0.0586        |
|    learning_rate         | 0.0003        |
|    loss                  | 10.2          |
|    n_updates             | 310           |
|    policy_gradient_loss  | -0.00532      |
|    std                   | 0.985         |
|    value_loss            | 94.5          |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.8526955] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 32           |
|    time_elapsed          | 519          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.0072838273 |
|    clip_fraction         | 0.0595       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.765        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0206       |
|    lagrangian_multiplier | 0.0587       |
|    learning_rate         | 0.0003       |
|    loss                  | 108          |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00939     |
|    std                   | 0.987        |
|    value_loss            | 1.09e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.31504285] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 22            |
|    time_elapsed          | 393           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.007475345   |
|    clip_fraction         | 0.0717        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 209           |
|    entropy_loss          | -2.75         |
|    explained_variance    | -0.0615       |
|    lagrangian_multiplier | 0.0853        |
|    learning_rate         | 0.0003        |
|    loss                  | 84.4          |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.0115       |
|    std                   | 0.954         |
|    value_loss            | 1.02e+03      |
--------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.5072703] |
| time/                    |              |
|    fps                   | 103          |
|    iterations            | 24           |
|    time_elapsed          | 475          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.00546449   |
|    clip_fraction         | 0.0484       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0217       |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0494       |
|    lagrangian_multiplier | 0.0625       |
|    learning_rate         | 0.0003       |
|    loss                  | 20.3         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00514     |
|    std                   | 1.02         |
|    value_loss            | 232          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6329944] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 33           |
|    time_elapsed          | 541          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0061853994 |
|    clip_fraction         | 0.061        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.19         |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0239       |
|    lagrangian_multiplier | 0.0539       |
|    learning_rate         | 0.0003       |
|    loss                  | 14.8         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00688     |
|    std                   | 0.984        |
|    value_loss            | 131          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.7383792] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 33           |
|    time_elapsed          | 535          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0048724413 |
|    clip_fraction         | 0.0431       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0681       |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.016        |
|    lagrangian_multiplier | 0.0639       |
|    learning_rate         | 0.0003       |
|    loss                  | 28.2         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00637     |
|    std                   | 1            |
|    value_loss            | 315          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6792324] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 23           |
|    time_elapsed          | 411          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.0058397786 |
|    clip_fraction         | 0.0563       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 159          |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0499      |
|    lagrangian_multiplier | 0.0678       |
|    learning_rate         | 0.0003       |
|    loss                  | 59.8         |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.0111      |
|    std                   | 0.967        |
|    value_loss            | 535          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.86133343] |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 25            |
|    time_elapsed          | 491           |
|    total_timesteps       | 51200         |
| train/                   |               |
|    approx_kl             | 0.006966453   |
|    clip_fraction         | 0.0679        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0688        |
|    entropy_loss          | -2.89         |
|    explained_variance    | -0.0235       |
|    lagrangian_multiplier | 0.0505        |
|    learning_rate         | 0.0003        |
|    loss                  | 46.1          |
|    n_updates             | 240           |
|    policy_gradient_loss  | -0.00907      |
|    std                   | 1.03          |
|    value_loss            | 379           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.4998701] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 34           |
|    time_elapsed          | 558          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0030830996 |
|    clip_fraction         | 0.0351       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.248        |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0391       |
|    lagrangian_multiplier | 0.0571       |
|    learning_rate         | 0.0003       |
|    loss                  | 8.56         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00456     |
|    std                   | 0.973        |
|    value_loss            | 93.1         |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.2656865] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 34           |
|    time_elapsed          | 551          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0062060887 |
|    clip_fraction         | 0.0524       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.12         |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.00743      |
|    lagrangian_multiplier | 0.0663       |
|    learning_rate         | 0.0003       |
|    loss                  | 141          |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00581     |
|    std                   | 1            |
|    value_loss            | 1.32e+03     |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.92744637] |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 26            |
|    time_elapsed          | 508           |
|    total_timesteps       | 53248         |
| train/                   |               |
|    approx_kl             | 0.005432303   |
|    clip_fraction         | 0.0433        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0391        |
|    entropy_loss          | -2.9          |
|    explained_variance    | 0.00929       |
|    lagrangian_multiplier | 0.0583        |
|    learning_rate         | 0.0003        |
|    loss                  | 58.5          |
|    n_updates             | 250           |
|    policy_gradient_loss  | -0.00844      |
|    std                   | 1.03          |
|    value_loss            | 531           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.75248903] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 24            |
|    time_elapsed          | 429           |
|    total_timesteps       | 49152         |
| train/                   |               |
|    approx_kl             | 0.005709823   |
|    clip_fraction         | 0.0516        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 141           |
|    entropy_loss          | -2.75         |
|    explained_variance    | -0.0886       |
|    lagrangian_multiplier | 0.0674        |
|    learning_rate         | 0.0003        |
|    loss                  | 56.3          |
|    n_updates             | 230           |
|    policy_gradient_loss  | -0.00703      |
|    std                   | 0.947         |
|    value_loss            | 422           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.6106372] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 35           |
|    time_elapsed          | 567          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.006449422  |
|    clip_fraction         | 0.0692       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.16         |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0199       |
|    lagrangian_multiplier | 0.0522       |
|    learning_rate         | 0.0003       |
|    loss                  | 125          |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00908     |
|    std                   | 1            |
|    value_loss            | 914          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.38332638] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 35            |
|    time_elapsed          | 574           |
|    total_timesteps       | 71680         |
| train/                   |               |
|    approx_kl             | 0.006808252   |
|    clip_fraction         | 0.0539        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0114        |
|    entropy_loss          | -2.76         |
|    explained_variance    | 0.0103        |
|    lagrangian_multiplier | 0.0836        |
|    learning_rate         | 0.0003        |
|    loss                  | 7.01          |
|    n_updates             | 340           |
|    policy_gradient_loss  | -0.00679      |
|    std                   | 0.951         |
|    value_loss            | 89.5          |
--------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.36665377] |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 27            |
|    time_elapsed          | 524           |
|    total_timesteps       | 55296         |
| train/                   |               |
|    approx_kl             | 0.0049781906  |
|    clip_fraction         | 0.0366        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0167        |
|    entropy_loss          | -2.9          |
|    explained_variance    | 0.0129        |
|    lagrangian_multiplier | 0.0643        |
|    learning_rate         | 0.0003        |
|    loss                  | 90.1          |
|    n_updates             | 260           |
|    policy_gradient_loss  | -0.00422      |
|    std                   | 1.03          |
|    value_loss            | 845           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.0386139] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 25           |
|    time_elapsed          | 447          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.005815604  |
|    clip_fraction         | 0.0673       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 159          |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.0694      |
|    lagrangian_multiplier | 0.0599       |
|    learning_rate         | 0.0003       |
|    loss                  | 90.1         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00852     |
|    std                   | 0.95         |
|    value_loss            | 649          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5801482] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 36           |
|    time_elapsed          | 584          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.006574708  |
|    clip_fraction         | 0.0684       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 5.6          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0092       |
|    lagrangian_multiplier | 0.06         |
|    learning_rate         | 0.0003       |
|    loss                  | 62.6         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00734     |
|    std                   | 0.993        |
|    value_loss            | 693          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.45906195] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 36            |
|    time_elapsed          | 591           |
|    total_timesteps       | 73728         |
| train/                   |               |
|    approx_kl             | 0.00940451    |
|    clip_fraction         | 0.07          |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0184        |
|    entropy_loss          | -2.72         |
|    explained_variance    | 0.0307        |
|    lagrangian_multiplier | 0.0588        |
|    learning_rate         | 0.0003        |
|    loss                  | 7.46          |
|    n_updates             | 350           |
|    policy_gradient_loss  | -0.00748      |
|    std                   | 0.94          |
|    value_loss            | 68            |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.96917546] |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 28            |
|    time_elapsed          | 540           |
|    total_timesteps       | 57344         |
| train/                   |               |
|    approx_kl             | 0.0044960077  |
|    clip_fraction         | 0.0305        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0306        |
|    entropy_loss          | -2.89         |
|    explained_variance    | 0.0167        |
|    lagrangian_multiplier | 0.062         |
|    learning_rate         | 0.0003        |
|    loss                  | 37.9          |
|    n_updates             | 270           |
|    policy_gradient_loss  | -0.00273      |
|    std                   | 1.02          |
|    value_loss            | 411           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.95805746] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 26            |
|    time_elapsed          | 465           |
|    total_timesteps       | 53248         |
| train/                   |               |
|    approx_kl             | 0.0070750583  |
|    clip_fraction         | 0.077         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 203           |
|    entropy_loss          | -2.73         |
|    explained_variance    | -0.0683       |
|    lagrangian_multiplier | 0.0719        |
|    learning_rate         | 0.0003        |
|    loss                  | 68.2          |
|    n_updates             | 250           |
|    policy_gradient_loss  | -0.00957      |
|    std                   | 0.948         |
|    value_loss            | 544           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.1104295] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 37           |
|    time_elapsed          | 600          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.007563686  |
|    clip_fraction         | 0.0713       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 5.41         |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0204       |
|    lagrangian_multiplier | 0.0534       |
|    learning_rate         | 0.0003       |
|    loss                  | 84.4         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.0127      |
|    std                   | 0.991        |
|    value_loss            | 776          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.59325683] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 37            |
|    time_elapsed          | 608           |
|    total_timesteps       | 75776         |
| train/                   |               |
|    approx_kl             | 0.0055667087  |
|    clip_fraction         | 0.0378        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0177        |
|    entropy_loss          | -2.7          |
|    explained_variance    | 0.0536        |
|    lagrangian_multiplier | 0.0464        |
|    learning_rate         | 0.0003        |
|    loss                  | 4.92          |
|    n_updates             | 360           |
|    policy_gradient_loss  | -0.00312      |
|    std                   | 0.93          |
|    value_loss            | 44.1          |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.58733815] |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 29            |
|    time_elapsed          | 556           |
|    total_timesteps       | 59392         |
| train/                   |               |
|    approx_kl             | 0.005868014   |
|    clip_fraction         | 0.0547        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0784        |
|    entropy_loss          | -2.88         |
|    explained_variance    | 0.00644       |
|    lagrangian_multiplier | 0.054         |
|    learning_rate         | 0.0003        |
|    loss                  | 86            |
|    n_updates             | 280           |
|    policy_gradient_loss  | -0.00864      |
|    std                   | 1.02          |
|    value_loss            | 795           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.1810837] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 27           |
|    time_elapsed          | 483          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.007146163  |
|    clip_fraction         | 0.0806       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 132          |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.0666      |
|    lagrangian_multiplier | 0.0847       |
|    learning_rate         | 0.0003       |
|    loss                  | 74.3         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.0116      |
|    std                   | 0.951        |
|    value_loss            | 876          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.8466059] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 38           |
|    time_elapsed          | 616          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0056259697 |
|    clip_fraction         | 0.0567       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.42         |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.022       |
|    lagrangian_multiplier | 0.0677       |
|    learning_rate         | 0.0003       |
|    loss                  | 40.9         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00684     |
|    std                   | 0.983        |
|    value_loss            | 450          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.55560935] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 38            |
|    time_elapsed          | 624           |
|    total_timesteps       | 77824         |
| train/                   |               |
|    approx_kl             | 0.0061379904  |
|    clip_fraction         | 0.0564        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0188        |
|    entropy_loss          | -2.69         |
|    explained_variance    | -0.00402      |
|    lagrangian_multiplier | 0.0565        |
|    learning_rate         | 0.0003        |
|    loss                  | 6.99          |
|    n_updates             | 370           |
|    policy_gradient_loss  | -0.00428      |
|    std                   | 0.934         |
|    value_loss            | 67.8          |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.7800945] |
| time/                    |              |
|    fps                   | 107          |
|    iterations            | 30           |
|    time_elapsed          | 572          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0061277873 |
|    clip_fraction         | 0.0707       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.075        |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0019       |
|    lagrangian_multiplier | 0.0479       |
|    learning_rate         | 0.0003       |
|    loss                  | 171          |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.0123      |
|    std                   | 1.03         |
|    value_loss            | 1.28e+03     |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.513612]  |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 28           |
|    time_elapsed          | 501          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.0076353867 |
|    clip_fraction         | 0.0799       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 139          |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.0219      |
|    lagrangian_multiplier | 0.0625       |
|    learning_rate         | 0.0003       |
|    loss                  | 61.2         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.0119      |
|    std                   | 0.945        |
|    value_loss            | 470          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0663366] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 39           |
|    time_elapsed          | 633          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0048386957 |
|    clip_fraction         | 0.0479       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.91         |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.00537      |
|    lagrangian_multiplier | 0.0522       |
|    learning_rate         | 0.0003       |
|    loss                  | 71.5         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00863     |
|    std                   | 0.997        |
|    value_loss            | 501          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.41926345] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 39            |
|    time_elapsed          | 641           |
|    total_timesteps       | 79872         |
| train/                   |               |
|    approx_kl             | 0.0052950867  |
|    clip_fraction         | 0.0647        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0216        |
|    entropy_loss          | -2.7          |
|    explained_variance    | 0.04          |
|    lagrangian_multiplier | 0.0548        |
|    learning_rate         | 0.0003        |
|    loss                  | 6.18          |
|    n_updates             | 380           |
|    policy_gradient_loss  | -0.00516      |
|    std                   | 0.936         |
|    value_loss            | 56.7          |
--------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.8227304] |
| time/                    |              |
|    fps                   | 107          |
|    iterations            | 31           |
|    time_elapsed          | 589          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0067480505 |
|    clip_fraction         | 0.0721       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0517       |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0278       |
|    lagrangian_multiplier | 0.0553       |
|    learning_rate         | 0.0003       |
|    loss                  | 57.5         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00832     |
|    std                   | 1.02         |
|    value_loss            | 579          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4256519] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 29           |
|    time_elapsed          | 519          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.009775797  |
|    clip_fraction         | 0.105        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 152          |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.0598      |
|    lagrangian_multiplier | 0.0579       |
|    learning_rate         | 0.0003       |
|    loss                  | 92.8         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.0136      |
|    std                   | 0.95         |
|    value_loss            | 823          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.43251315] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 40            |
|    time_elapsed          | 649           |
|    total_timesteps       | 81920         |
| train/                   |               |
|    approx_kl             | 0.0047862376  |
|    clip_fraction         | 0.0566        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 3.02          |
|    entropy_loss          | -2.83         |
|    explained_variance    | 0.0501        |
|    lagrangian_multiplier | 0.0525        |
|    learning_rate         | 0.0003        |
|    loss                  | 31.8          |
|    n_updates             | 390           |
|    policy_gradient_loss  | -0.00874      |
|    std                   | 1             |
|    value_loss            | 341           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.38457248] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 40            |
|    time_elapsed          | 657           |
|    total_timesteps       | 81920         |
| train/                   |               |
|    approx_kl             | 0.006284791   |
|    clip_fraction         | 0.0462        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00352       |
|    entropy_loss          | -2.7          |
|    explained_variance    | 0.0209        |
|    lagrangian_multiplier | 0.0809        |
|    learning_rate         | 0.0003        |
|    loss                  | 5.82          |
|    n_updates             | 390           |
|    policy_gradient_loss  | -0.00298      |
|    std                   | 0.929         |
|    value_loss            | 66.5          |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.0374207] |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 32           |
|    time_elapsed          | 605          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.0069836886 |
|    clip_fraction         | 0.0718       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.153        |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0138       |
|    lagrangian_multiplier | 0.0452       |
|    learning_rate         | 0.0003       |
|    loss                  | 107          |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 1.02         |
|    value_loss            | 884          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2988825] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 41           |
|    time_elapsed          | 665          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0042695645 |
|    clip_fraction         | 0.0385       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.238        |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.009       |
|    lagrangian_multiplier | 0.0737       |
|    learning_rate         | 0.0003       |
|    loss                  | 10.6         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00589     |
|    std                   | 1.01         |
|    value_loss            | 143          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4227239] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 30           |
|    time_elapsed          | 537          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0073903585 |
|    clip_fraction         | 0.0905       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 212          |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0533      |
|    lagrangian_multiplier | 0.0606       |
|    learning_rate         | 0.0003       |
|    loss                  | 80.1         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.0113      |
|    std                   | 0.96         |
|    value_loss            | 650          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.45922086] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 41            |
|    time_elapsed          | 674           |
|    total_timesteps       | 83968         |
| train/                   |               |
|    approx_kl             | 0.0058091627  |
|    clip_fraction         | 0.0586        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00617       |
|    entropy_loss          | -2.68         |
|    explained_variance    | -0.123        |
|    lagrangian_multiplier | 0.0668        |
|    learning_rate         | 0.0003        |
|    loss                  | 6.06          |
|    n_updates             | 400           |
|    policy_gradient_loss  | -0.00527      |
|    std                   | 0.922         |
|    value_loss            | 66            |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.0728722] |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 33           |
|    time_elapsed          | 621          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.005705008  |
|    clip_fraction         | 0.0595       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.116        |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.00837     |
|    lagrangian_multiplier | 0.0409       |
|    learning_rate         | 0.0003       |
|    loss                  | 143          |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00834     |
|    std                   | 1.04         |
|    value_loss            | 1e+03        |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4963346] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 42           |
|    time_elapsed          | 681          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0049768253 |
|    clip_fraction         | 0.0496       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.125        |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0011      |
|    lagrangian_multiplier | 0.0452       |
|    learning_rate         | 0.0003       |
|    loss                  | 42           |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00549     |
|    std                   | 0.988        |
|    value_loss            | 308          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.4465538] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 42           |
|    time_elapsed          | 690          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0033785074 |
|    clip_fraction         | 0.0181       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0105       |
|    entropy_loss          | -2.68        |
|    explained_variance    | -0.0329      |
|    lagrangian_multiplier | 0.0665       |
|    learning_rate         | 0.0003       |
|    loss                  | 5.1          |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00086     |
|    std                   | 0.926        |
|    value_loss            | 56.6         |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1396453] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 31           |
|    time_elapsed          | 555          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.008094585  |
|    clip_fraction         | 0.0627       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 235          |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0569      |
|    lagrangian_multiplier | 0.0693       |
|    learning_rate         | 0.0003       |
|    loss                  | 124          |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 0.966        |
|    value_loss            | 1.11e+03     |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1928608] |
| time/                    |              |
|    fps                   | 109          |
|    iterations            | 34           |
|    time_elapsed          | 637          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.007816748  |
|    clip_fraction         | 0.0838       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.148        |
|    entropy_loss          | -2.92        |
|    explained_variance    | 0.0088       |
|    lagrangian_multiplier | 0.0561       |
|    learning_rate         | 0.0003       |
|    loss                  | 94.2         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.01        |
|    std                   | 1.04         |
|    value_loss            | 983          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.2801085] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 43           |
|    time_elapsed          | 698          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0073189884 |
|    clip_fraction         | 0.0793       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0622       |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.00446     |
|    lagrangian_multiplier | 0.058        |
|    learning_rate         | 0.0003       |
|    loss                  | 19.8         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00897     |
|    std                   | 0.967        |
|    value_loss            | 204          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.45076954] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 43            |
|    time_elapsed          | 707           |
|    total_timesteps       | 88064         |
| train/                   |               |
|    approx_kl             | 0.0050877524  |
|    clip_fraction         | 0.0385        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00734       |
|    entropy_loss          | -2.69         |
|    explained_variance    | 0.0997        |
|    lagrangian_multiplier | 0.0748        |
|    learning_rate         | 0.0003        |
|    loss                  | 3.97          |
|    n_updates             | 420           |
|    policy_gradient_loss  | -0.00124      |
|    std                   | 0.933         |
|    value_loss            | 44.9          |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.7387869] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 32           |
|    time_elapsed          | 573          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.006317663  |
|    clip_fraction         | 0.0565       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 210          |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.055       |
|    lagrangian_multiplier | 0.0698       |
|    learning_rate         | 0.0003       |
|    loss                  | 95.3         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00984     |
|    std                   | 0.959        |
|    value_loss            | 965          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.6126115] |
| time/                    |              |
|    fps                   | 109          |
|    iterations            | 35           |
|    time_elapsed          | 654          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0059895916 |
|    clip_fraction         | 0.0517       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0813       |
|    entropy_loss          | -2.93        |
|    explained_variance    | 0.0465       |
|    lagrangian_multiplier | 0.0657       |
|    learning_rate         | 0.0003       |
|    loss                  | 16.2         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00775     |
|    std                   | 1.05         |
|    value_loss            | 173          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.47134778] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 44            |
|    time_elapsed          | 714           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.007303184   |
|    clip_fraction         | 0.0611        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.49          |
|    entropy_loss          | -2.77         |
|    explained_variance    | 0.0224        |
|    lagrangian_multiplier | 0.0442        |
|    learning_rate         | 0.0003        |
|    loss                  | 82.8          |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.00772      |
|    std                   | 0.967         |
|    value_loss            | 611           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.42798617] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 44            |
|    time_elapsed          | 723           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.0041528232  |
|    clip_fraction         | 0.0411        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0114        |
|    entropy_loss          | -2.69         |
|    explained_variance    | -0.0396       |
|    lagrangian_multiplier | 0.0565        |
|    learning_rate         | 0.0003        |
|    loss                  | 2.95          |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.00316      |
|    std                   | 0.928         |
|    value_loss            | 31.3          |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.8650529] |
| time/                    |              |
|    fps                   | 110          |
|    iterations            | 36           |
|    time_elapsed          | 670          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.006605419  |
|    clip_fraction         | 0.0675       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.104        |
|    entropy_loss          | -2.94        |
|    explained_variance    | -0.00747     |
|    lagrangian_multiplier | 0.0475       |
|    learning_rate         | 0.0003       |
|    loss                  | 65.3         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 1.06         |
|    value_loss            | 602          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.1393256] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 33           |
|    time_elapsed          | 591          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.008097337  |
|    clip_fraction         | 0.0895       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 233          |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0814      |
|    lagrangian_multiplier | 0.0747       |
|    learning_rate         | 0.0003       |
|    loss                  | 92.7         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.012       |
|    std                   | 0.961        |
|    value_loss            | 867          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.49071833] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 45            |
|    time_elapsed          | 730           |
|    total_timesteps       | 92160         |
| train/                   |               |
|    approx_kl             | 0.006175272   |
|    clip_fraction         | 0.0467        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.32          |
|    entropy_loss          | -2.76         |
|    explained_variance    | 0.0224        |
|    lagrangian_multiplier | 0.0733        |
|    learning_rate         | 0.0003        |
|    loss                  | 10.5          |
|    n_updates             | 440           |
|    policy_gradient_loss  | -0.00807      |
|    std                   | 0.963         |
|    value_loss            | 120           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.50624096] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 45            |
|    time_elapsed          | 740           |
|    total_timesteps       | 92160         |
| train/                   |               |
|    approx_kl             | 0.0041599465  |
|    clip_fraction         | 0.0365        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00819       |
|    entropy_loss          | -2.68         |
|    explained_variance    | 0.146         |
|    lagrangian_multiplier | 0.0603        |
|    learning_rate         | 0.0003        |
|    loss                  | 6.44          |
|    n_updates             | 440           |
|    policy_gradient_loss  | -0.002        |
|    std                   | 0.925         |
|    value_loss            | 65.8          |
--------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.821412]  |
| time/                    |              |
|    fps                   | 110          |
|    iterations            | 37           |
|    time_elapsed          | 686          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.0065965834 |
|    clip_fraction         | 0.0526       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0517       |
|    entropy_loss          | -2.98        |
|    explained_variance    | 0.0346       |
|    lagrangian_multiplier | 0.0589       |
|    learning_rate         | 0.0003       |
|    loss                  | 38.4         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.0156      |
|    std                   | 1.09         |
|    value_loss            | 471          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9109313] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 34           |
|    time_elapsed          | 608          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.007447944  |
|    clip_fraction         | 0.0717       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 206          |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0812      |
|    lagrangian_multiplier | 0.0703       |
|    learning_rate         | 0.0003       |
|    loss                  | 84           |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00977     |
|    std                   | 0.966        |
|    value_loss            | 780          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.44351745] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 46            |
|    time_elapsed          | 746           |
|    total_timesteps       | 94208         |
| train/                   |               |
|    approx_kl             | 0.007158819   |
|    clip_fraction         | 0.0676        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.635         |
|    entropy_loss          | -2.77         |
|    explained_variance    | 0.0172        |
|    lagrangian_multiplier | 0.048         |
|    learning_rate         | 0.0003        |
|    loss                  | 65.7          |
|    n_updates             | 450           |
|    policy_gradient_loss  | -0.00813      |
|    std                   | 0.977         |
|    value_loss            | 655           |
--------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.36668375] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 46            |
|    time_elapsed          | 757           |
|    total_timesteps       | 94208         |
| train/                   |               |
|    approx_kl             | 0.008165518   |
|    clip_fraction         | 0.0609        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0114        |
|    entropy_loss          | -2.67         |
|    explained_variance    | -0.00827      |
|    lagrangian_multiplier | 0.05          |
|    learning_rate         | 0.0003        |
|    loss                  | 7.75          |
|    n_updates             | 450           |
|    policy_gradient_loss  | -0.00345      |
|    std                   | 0.918         |
|    value_loss            | 63.7          |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2914696] |
| time/                    |              |
|    fps                   | 110          |
|    iterations            | 38           |
|    time_elapsed          | 702          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0058942195 |
|    clip_fraction         | 0.064        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0492       |
|    entropy_loss          | -3.01        |
|    explained_variance    | -0.0493      |
|    lagrangian_multiplier | 0.0526       |
|    learning_rate         | 0.0003       |
|    loss                  | 18.9         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00698     |
|    std                   | 1.09         |
|    value_loss            | 167          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7786379] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 35           |
|    time_elapsed          | 626          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0067727985 |
|    clip_fraction         | 0.0724       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 188          |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0503      |
|    lagrangian_multiplier | 0.0758       |
|    learning_rate         | 0.0003       |
|    loss                  | 131          |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00992     |
|    std                   | 0.968        |
|    value_loss            | 1.2e+03      |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.2510637] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 47           |
|    time_elapsed          | 763          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.006792312  |
|    clip_fraction         | 0.0625       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0836       |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0224       |
|    lagrangian_multiplier | 0.0752       |
|    learning_rate         | 0.0003       |
|    loss                  | 12.6         |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.0122      |
|    std                   | 0.986        |
|    value_loss            | 127          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.42891473] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 47            |
|    time_elapsed          | 773           |
|    total_timesteps       | 96256         |
| train/                   |               |
|    approx_kl             | 0.007495026   |
|    clip_fraction         | 0.0577        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0177        |
|    entropy_loss          | -2.64         |
|    explained_variance    | -0.0115       |
|    lagrangian_multiplier | 0.0485        |
|    learning_rate         | 0.0003        |
|    loss                  | 4.99          |
|    n_updates             | 460           |
|    policy_gradient_loss  | -0.00463      |
|    std                   | 0.896         |
|    value_loss            | 42.8          |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.43808332] |
| time/                    |               |
|    fps                   | 111           |
|    iterations            | 39            |
|    time_elapsed          | 718           |
|    total_timesteps       | 79872         |
| train/                   |               |
|    approx_kl             | 0.004537645   |
|    clip_fraction         | 0.048         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0471        |
|    entropy_loss          | -3            |
|    explained_variance    | -0.0318       |
|    lagrangian_multiplier | 0.0522        |
|    learning_rate         | 0.0003        |
|    loss                  | 30.1          |
|    n_updates             | 380           |
|    policy_gradient_loss  | -0.00461      |
|    std                   | 1.08          |
|    value_loss            | 241           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.5944698] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 36           |
|    time_elapsed          | 644          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.008029338  |
|    clip_fraction         | 0.0996       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 221          |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0726      |
|    lagrangian_multiplier | 0.074        |
|    learning_rate         | 0.0003       |
|    loss                  | 88.3         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.015       |
|    std                   | 0.977        |
|    value_loss            | 796          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.6033798] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 48           |
|    time_elapsed          | 779          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.007712174  |
|    clip_fraction         | 0.0861       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0443       |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.00222     |
|    lagrangian_multiplier | 0.0557       |
|    learning_rate         | 0.0003       |
|    loss                  | 30.8         |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00797     |
|    std                   | 0.99         |
|    value_loss            | 261          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.49638107] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 48            |
|    time_elapsed          | 789           |
|    total_timesteps       | 98304         |
| train/                   |               |
|    approx_kl             | 0.0048510563  |
|    clip_fraction         | 0.0392        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0032        |
|    entropy_loss          | -2.61         |
|    explained_variance    | 0.0285        |
|    lagrangian_multiplier | 0.0938        |
|    learning_rate         | 0.0003        |
|    loss                  | 2.94          |
|    n_updates             | 470           |
|    policy_gradient_loss  | -0.00419      |
|    std                   | 0.893         |
|    value_loss            | 43.5          |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.0838411] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 40           |
|    time_elapsed          | 735          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.0026802118 |
|    clip_fraction         | 0.024        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0435       |
|    entropy_loss          | -2.99        |
|    explained_variance    | -0.028       |
|    lagrangian_multiplier | 0.0524       |
|    learning_rate         | 0.0003       |
|    loss                  | 25.3         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00245     |
|    std                   | 1.08         |
|    value_loss            | 203          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2770427] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 37           |
|    time_elapsed          | 662          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.007818386  |
|    clip_fraction         | 0.0691       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 221          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0424      |
|    lagrangian_multiplier | 0.0728       |
|    learning_rate         | 0.0003       |
|    loss                  | 124          |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00951     |
|    std                   | 0.971        |
|    value_loss            | 1.2e+03      |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.50463307] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 49            |
|    time_elapsed          | 795           |
|    total_timesteps       | 100352        |
| train/                   |               |
|    approx_kl             | 0.006197436   |
|    clip_fraction         | 0.0434        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0181        |
|    entropy_loss          | -2.83         |
|    explained_variance    | 0.0638        |
|    lagrangian_multiplier | 0.0782        |
|    learning_rate         | 0.0003        |
|    loss                  | 5.98          |
|    n_updates             | 480           |
|    policy_gradient_loss  | -0.00597      |
|    std                   | 1.01          |
|    value_loss            | 62.8          |
--------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
--------------------------------------------
| reward                   | [-0.50373393] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 49            |
|    time_elapsed          | 806           |
|    total_timesteps       | 100352        |
| train/                   |               |
|    approx_kl             | 0.0057049245  |
|    clip_fraction         | 0.0569        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00465       |
|    entropy_loss          | -2.61         |
|    explained_variance    | -0.05         |
|    lagrangian_multiplier | 0.0847        |
|    learning_rate         | 0.0003        |
|    loss                  | 5.04          |
|    n_updates             | 480           |
|    policy_gradient_loss  | -0.0063       |
|    std                   | 0.891         |
|    value_loss            | 62            |
--------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                      reward ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñÇ‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÉ‚ñà‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñá‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:             train/approx_kl ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÅ‚ñÉ‚ñÜ‚ñÑ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñÜ
wandb:         train/clip_fraction ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñà‚ñÑ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:          train/entropy_loss ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñà‚ñà‚ñÜ‚ñÖ‚ñÑ
wandb:    train/explained_variance ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñà‚ñÖ‚ñá‚ñà‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÜ
wandb: train/lagrangian_multiplier ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÅ‚ñá‚ñá‚ñÉ‚ñà
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñá‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÇ‚ñÜ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñà‚ñÜ‚ñÉ‚ñÖ‚ñÉ‚ñá‚ñà‚ñá‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÑ‚ñÜ
wandb:                   train/std ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÜ
wandb:            train/value_loss ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÖ‚ñÇ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                      reward -0.50463
wandb:             train/approx_kl 0.0062
wandb:         train/clip_fraction 0.04341
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 0.01806
wandb:          train/entropy_loss -2.83333
wandb:    train/explained_variance 0.06382
wandb: train/lagrangian_multiplier 0.0782
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 5.97733
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00597
wandb:                   train/std 1.00845
wandb:            train/value_loss 62.78816
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/m6hu2kdv
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_195806-m6hu2kdv/logs
-------------------------------------------
| reward                   | [-1.7416126] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 41           |
|    time_elapsed          | 751          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.005530387  |
|    clip_fraction         | 0.0471       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0418       |
|    entropy_loss          | -3.01        |
|    explained_variance    | 0.0337       |
|    lagrangian_multiplier | 0.0565       |
|    learning_rate         | 0.0003       |
|    loss                  | 34.8         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00606     |
|    std                   | 1.1          |
|    value_loss            | 321          |
-------------------------------------------
srun: error: gail.ist.berkeley.edu: task 0: Exited with exit code 1
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñÜ‚ñá‚ñÇ‚ñÅ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá
wandb:             train/approx_kl ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÜ‚ñÉ‚ñÑ
wandb:         train/clip_fraction ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñà‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÖ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñà‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñá‚ñÜ‚ñÉ‚ñà‚ñà‚ñÇ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñá‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:          train/entropy_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà
wandb:    train/explained_variance ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñÅ‚ñà‚ñÇ‚ñÑ‚ñÅ
wandb: train/lagrangian_multiplier ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÇ‚ñà‚ñá
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÑ
wandb:                   train/std ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ
wandb:            train/value_loss ‚ñÜ‚ñÖ‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                      reward -0.50373
wandb:             train/approx_kl 0.0057
wandb:         train/clip_fraction 0.05688
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 0.00465
wandb:          train/entropy_loss -2.60751
wandb:    train/explained_variance -0.05002
wandb: train/lagrangian_multiplier 0.08471
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 5.04365
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.0063
wandb:                   train/std 0.89097
wandb:            train/value_loss 62.03217
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/4p6nmkvx
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_195759-4p6nmkvx/logs
srun: error: dqn.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-1.2354256] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 38           |
|    time_elapsed          | 680          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0065607904 |
|    clip_fraction         | 0.0649       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 187          |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0301      |
|    lagrangian_multiplier | 0.0635       |
|    learning_rate         | 0.0003       |
|    loss                  | 125          |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00861     |
|    std                   | 0.981        |
|    value_loss            | 1.3e+03      |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114548.8
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
-------------------------------------------
| reward                   | [-1.1195749] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 42           |
|    time_elapsed          | 767          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0069356416 |
|    clip_fraction         | 0.0563       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0314       |
|    entropy_loss          | -3.02        |
|    explained_variance    | 0.00907      |
|    lagrangian_multiplier | 0.0529       |
|    learning_rate         | 0.0003       |
|    loss                  | 34.3         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00965     |
|    std                   | 1.1          |
|    value_loss            | 310          |
-------------------------------------------
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_201144-2vlyyznb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/2vlyyznb
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114548.9
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.6501006] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 39           |
|    time_elapsed          | 698          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.008322811  |
|    clip_fraction         | 0.0761       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 163          |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0514      |
|    lagrangian_multiplier | 0.0626       |
|    learning_rate         | 0.0003       |
|    loss                  | 60.1         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.012       |
|    std                   | 0.958        |
|    value_loss            | 449          |
-------------------------------------------
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_201157-noxwuniy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2/runs/noxwuniy
Using cpu device
--------------------------------------
| reward             | [-0.34340116] |
| time/              |               |
|    fps             | 134           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
--------------------------------------------
| reward                   | [-0.90519834] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 43            |
|    time_elapsed          | 784           |
|    total_timesteps       | 88064         |
| train/                   |               |
|    approx_kl             | 0.004615814   |
|    clip_fraction         | 0.045         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0378        |
|    entropy_loss          | -3.04         |
|    explained_variance    | 0.00946       |
|    lagrangian_multiplier | 0.0463        |
|    learning_rate         | 0.0003        |
|    loss                  | 53.2          |
|    n_updates             | 420           |
|    policy_gradient_loss  | -0.0053       |
|    std                   | 1.11          |
|    value_loss            | 459           |
--------------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.6121668] |
| time/              |              |
|    fps             | 134          |
|    iterations      | 1            |
|    time_elapsed    | 15           |
|    total_timesteps | 2048         |
-------------------------------------
-------------------------------------------
| reward                   | [-2.1783588] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 40           |
|    time_elapsed          | 716          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.010255484  |
|    clip_fraction         | 0.0983       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 185          |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0593      |
|    lagrangian_multiplier | 0.0635       |
|    learning_rate         | 0.0003       |
|    loss                  | 65.5         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.0129      |
|    std                   | 0.955        |
|    value_loss            | 590          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.56079423] |
| time/                    |               |
|    fps                   | 131           |
|    iterations            | 2             |
|    time_elapsed          | 31            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.005656678   |
|    clip_fraction         | 0.045         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 110           |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.0313        |
|    lagrangian_multiplier | 0.0673        |
|    learning_rate         | 0.0003        |
|    loss                  | 45.9          |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.0058       |
|    std                   | 1             |
|    value_loss            | 380           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.0924199] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 44           |
|    time_elapsed          | 800          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.0061197    |
|    clip_fraction         | 0.0607       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0828       |
|    entropy_loss          | -3.06        |
|    explained_variance    | 0.0471       |
|    lagrangian_multiplier | 0.0446       |
|    learning_rate         | 0.0003       |
|    loss                  | 42.1         |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.00821     |
|    std                   | 1.12         |
|    value_loss            | 272          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.54225683] |
| time/                    |               |
|    fps                   | 130           |
|    iterations            | 2             |
|    time_elapsed          | 31            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.0064656814  |
|    clip_fraction         | 0.0472        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 119           |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.00787       |
|    lagrangian_multiplier | 0.0606        |
|    learning_rate         | 0.0003        |
|    loss                  | 76.1          |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00524      |
|    std                   | 1             |
|    value_loss            | 777           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.7751594] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 41           |
|    time_elapsed          | 734          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0084804185 |
|    clip_fraction         | 0.0723       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 178          |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0758      |
|    lagrangian_multiplier | 0.0631       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.6         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.0108      |
|    std                   | 0.958        |
|    value_loss            | 434          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9670849] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 3            |
|    time_elapsed          | 47           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0064568236 |
|    clip_fraction         | 0.0541       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 125          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0244      |
|    lagrangian_multiplier | 0.0676       |
|    learning_rate         | 0.0003       |
|    loss                  | 74           |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00741     |
|    std                   | 1            |
|    value_loss            | 643          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.808639]  |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 45           |
|    time_elapsed          | 816          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.0048343483 |
|    clip_fraction         | 0.0334       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0272       |
|    entropy_loss          | -3.06        |
|    explained_variance    | 0.0611       |
|    lagrangian_multiplier | 0.0614       |
|    learning_rate         | 0.0003       |
|    loss                  | 26.4         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00539     |
|    std                   | 1.12         |
|    value_loss            | 268          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.6428197] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 3            |
|    time_elapsed          | 47           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.004735221  |
|    clip_fraction         | 0.0387       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 102          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.031       |
|    lagrangian_multiplier | 0.0577       |
|    learning_rate         | 0.0003       |
|    loss                  | 85.1         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00571     |
|    std                   | 1            |
|    value_loss            | 644          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9508236] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 4            |
|    time_elapsed          | 63           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.0047278833 |
|    clip_fraction         | 0.0376       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 55.9         |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0205       |
|    lagrangian_multiplier | 0.0664       |
|    learning_rate         | 0.0003       |
|    loss                  | 75.7         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.0053      |
|    std                   | 0.986        |
|    value_loss            | 706          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7250762] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 42           |
|    time_elapsed          | 752          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0077112606 |
|    clip_fraction         | 0.0699       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 218          |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0332      |
|    lagrangian_multiplier | 0.0648       |
|    learning_rate         | 0.0003       |
|    loss                  | 142          |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.0133      |
|    std                   | 0.963        |
|    value_loss            | 1.09e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5500817] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 46           |
|    time_elapsed          | 832          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.005850181  |
|    clip_fraction         | 0.06         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0928       |
|    entropy_loss          | -3.06        |
|    explained_variance    | 0.00742      |
|    lagrangian_multiplier | 0.0467       |
|    learning_rate         | 0.0003       |
|    loss                  | 61.4         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.00744     |
|    std                   | 1.12         |
|    value_loss            | 451          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.61258954] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 4             |
|    time_elapsed          | 63            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.003997284   |
|    clip_fraction         | 0.0239        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 130           |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.0258        |
|    lagrangian_multiplier | 0.0551        |
|    learning_rate         | 0.0003        |
|    loss                  | 57            |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00433      |
|    std                   | 1             |
|    value_loss            | 415           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.63607633] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 5             |
|    time_elapsed          | 79            |
|    total_timesteps       | 10240         |
| train/                   |               |
|    approx_kl             | 0.0061776936  |
|    clip_fraction         | 0.0504        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 12.2          |
|    entropy_loss          | -2.82         |
|    explained_variance    | -0.0722       |
|    lagrangian_multiplier | 0.0828        |
|    learning_rate         | 0.0003        |
|    loss                  | 47.9          |
|    n_updates             | 40            |
|    policy_gradient_loss  | -0.00599      |
|    std                   | 1             |
|    value_loss            | 516           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.800961]  |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 47           |
|    time_elapsed          | 849          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.0070471875 |
|    clip_fraction         | 0.0562       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0517       |
|    entropy_loss          | -3.06        |
|    explained_variance    | 0.0546       |
|    lagrangian_multiplier | 0.0548       |
|    learning_rate         | 0.0003       |
|    loss                  | 25.1         |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.00735     |
|    std                   | 1.12         |
|    value_loss            | 235          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2389144] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 43           |
|    time_elapsed          | 770          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0070651234 |
|    clip_fraction         | 0.0616       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 106          |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0729      |
|    lagrangian_multiplier | 0.0653       |
|    learning_rate         | 0.0003       |
|    loss                  | 37.8         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.0119      |
|    std                   | 0.969        |
|    value_loss            | 343          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.21680565] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 5             |
|    time_elapsed          | 79            |
|    total_timesteps       | 10240         |
| train/                   |               |
|    approx_kl             | 0.0053224904  |
|    clip_fraction         | 0.0475        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 145           |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.00965       |
|    lagrangian_multiplier | 0.0553        |
|    learning_rate         | 0.0003        |
|    loss                  | 50.5          |
|    n_updates             | 40            |
|    policy_gradient_loss  | -0.00552      |
|    std                   | 1.01          |
|    value_loss            | 347           |
--------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.96815634] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 6             |
|    time_elapsed          | 96            |
|    total_timesteps       | 12288         |
| train/                   |               |
|    approx_kl             | 0.007808913   |
|    clip_fraction         | 0.0725        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 120           |
|    entropy_loss          | -2.86         |
|    explained_variance    | 0.0354        |
|    lagrangian_multiplier | 0.0682        |
|    learning_rate         | 0.0003        |
|    loss                  | 70            |
|    n_updates             | 50            |
|    policy_gradient_loss  | -0.0114       |
|    std                   | 1.02          |
|    value_loss            | 630           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.46452728] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 48            |
|    time_elapsed          | 865           |
|    total_timesteps       | 98304         |
| train/                   |               |
|    approx_kl             | 0.0049755103  |
|    clip_fraction         | 0.0429        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0263        |
|    entropy_loss          | -3.07         |
|    explained_variance    | 0.0328        |
|    lagrangian_multiplier | 0.068         |
|    learning_rate         | 0.0003        |
|    loss                  | 22.6          |
|    n_updates             | 470           |
|    policy_gradient_loss  | -0.00515      |
|    std                   | 1.13          |
|    value_loss            | 237           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.5294269] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 44           |
|    time_elapsed          | 788          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.008062438  |
|    clip_fraction         | 0.075        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 188          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.039       |
|    lagrangian_multiplier | 0.0685       |
|    learning_rate         | 0.0003       |
|    loss                  | 44.1         |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.00945     |
|    std                   | 0.972        |
|    value_loss            | 277          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6151899] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 6            |
|    time_elapsed          | 95           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0039913664 |
|    clip_fraction         | 0.0368       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 88.6         |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0163      |
|    lagrangian_multiplier | 0.0552       |
|    learning_rate         | 0.0003       |
|    loss                  | 48.4         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00589     |
|    std                   | 1.01         |
|    value_loss            | 437          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1371397] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 7            |
|    time_elapsed          | 112          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.006398855  |
|    clip_fraction         | 0.0491       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 78.7         |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0066       |
|    lagrangian_multiplier | 0.0585       |
|    learning_rate         | 0.0003       |
|    loss                  | 49.6         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00619     |
|    std                   | 1.03         |
|    value_loss            | 396          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8629084] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 49           |
|    time_elapsed          | 881          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.0036383145 |
|    clip_fraction         | 0.0276       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0288       |
|    entropy_loss          | -3.08        |
|    explained_variance    | -0.0163      |
|    lagrangian_multiplier | 0.0576       |
|    learning_rate         | 0.0003       |
|    loss                  | 17.6         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.00386     |
|    std                   | 1.13         |
|    value_loss            | 159          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
-------------------------------------------
| reward                   | [-0.5005897] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 45           |
|    time_elapsed          | 806          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.006368635  |
|    clip_fraction         | 0.0617       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 160          |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0188      |
|    lagrangian_multiplier | 0.0629       |
|    learning_rate         | 0.0003       |
|    loss                  | 41.7         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00807     |
|    std                   | 0.974        |
|    value_loss            | 279          |
-------------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñÜ‚ñÇ‚ñÅ‚ñÇ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÖ
wandb:             train/approx_kl ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÇ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÅ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñÇ
wandb:         train/clip_fraction ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÇ‚ñá‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÜ‚ñá‚ñá‚ñÖ‚ñà‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÅ‚ñÖ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÅ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñá‚ñá‚ñÜ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñà‚ñÜ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÇ
wandb:          train/entropy_loss ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    train/explained_variance ‚ñÑ‚ñÖ‚ñÅ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà‚ñá‚ñÑ
wandb: train/lagrangian_multiplier ‚ñá‚ñÉ‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñá‚ñÑ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÜ‚ñÑ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÜ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÖ‚ñá‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñá‚ñà‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÅ‚ñÜ‚ñá‚ñà‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñá
wandb:                   train/std ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà
wandb:            train/value_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñÜ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                      reward -1.86291
wandb:             train/approx_kl 0.00364
wandb:         train/clip_fraction 0.02759
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 0.02883
wandb:          train/entropy_loss -3.07556
wandb:    train/explained_variance -0.01634
wandb: train/lagrangian_multiplier 0.05757
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 17.57015
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00386
wandb:                   train/std 1.12823
wandb:            train/value_loss 159.3291
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16/runs/31jz1zo6
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231227_035855-31jz1zo6/logs
srun: error: airl.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-1.6055695] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 7            |
|    time_elapsed          | 111          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.006052586  |
|    clip_fraction         | 0.0415       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 133          |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0264      |
|    lagrangian_multiplier | 0.0557       |
|    learning_rate         | 0.0003       |
|    loss                  | 61.5         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00773     |
|    std                   | 1.01         |
|    value_loss            | 446          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6242776] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 8            |
|    time_elapsed          | 129          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0049401885 |
|    clip_fraction         | 0.0367       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 81.4         |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0436       |
|    lagrangian_multiplier | 0.0804       |
|    learning_rate         | 0.0003       |
|    loss                  | 41.1         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00419     |
|    std                   | 1.03         |
|    value_loss            | 426          |
-------------------------------------------
------------------------------------------
| reward                   | [-0.880632] |
| time/                    |             |
|    fps                   | 114         |
|    iterations            | 46          |
|    time_elapsed          | 824         |
|    total_timesteps       | 94208       |
| train/                   |             |
|    approx_kl             | 0.006730789 |
|    clip_fraction         | 0.0754      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 138         |
|    entropy_loss          | -2.79       |
|    explained_variance    | -0.0831     |
|    lagrangian_multiplier | 0.061       |
|    learning_rate         | 0.0003      |
|    loss                  | 33.3        |
|    n_updates             | 450         |
|    policy_gradient_loss  | -0.00814    |
|    std                   | 0.973       |
|    value_loss            | 215         |
------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114548.10
-------------------------------------------
| reward                   | [-0.8390911] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 8            |
|    time_elapsed          | 128          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.004427695  |
|    clip_fraction         | 0.051        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 226          |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.029       |
|    lagrangian_multiplier | 0.0534       |
|    learning_rate         | 0.0003       |
|    loss                  | 137          |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00786     |
|    std                   | 1            |
|    value_loss            | 875          |
-------------------------------------------
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231227_041406-2o9isek1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/2o9isek1
-------------------------------------------
| reward                   | [-1.8941698] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 9            |
|    time_elapsed          | 145          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0031901884 |
|    clip_fraction         | 0.0212       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 30           |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0692       |
|    lagrangian_multiplier | 0.0614       |
|    learning_rate         | 0.0003       |
|    loss                  | 80.2         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00293     |
|    std                   | 1.03         |
|    value_loss            | 856          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9093733] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 47           |
|    time_elapsed          | 842          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.008485665  |
|    clip_fraction         | 0.0777       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 123          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.119       |
|    lagrangian_multiplier | 0.0646       |
|    learning_rate         | 0.0003       |
|    loss                  | 41.2         |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.00866     |
|    std                   | 0.973        |
|    value_loss            | 289          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2204349] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 9            |
|    time_elapsed          | 144          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.006079588  |
|    clip_fraction         | 0.0533       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 227          |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0237      |
|    lagrangian_multiplier | 0.0669       |
|    learning_rate         | 0.0003       |
|    loss                  | 125          |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00888     |
|    std                   | 1.01         |
|    value_loss            | 1.12e+03     |
-------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.53731525] |
| time/              |               |
|    fps             | 136           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.95323765] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 10            |
|    time_elapsed          | 161           |
|    total_timesteps       | 20480         |
| train/                   |               |
|    approx_kl             | 0.0060165557  |
|    clip_fraction         | 0.0567        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 86.5          |
|    entropy_loss          | -2.89         |
|    explained_variance    | 0.044         |
|    lagrangian_multiplier | 0.0735        |
|    learning_rate         | 0.0003        |
|    loss                  | 54.2          |
|    n_updates             | 90            |
|    policy_gradient_loss  | -0.00755      |
|    std                   | 1.03          |
|    value_loss            | 448           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2135328] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 48           |
|    time_elapsed          | 860          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.0075802933 |
|    clip_fraction         | 0.0701       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 120          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0909      |
|    lagrangian_multiplier | 0.0667       |
|    learning_rate         | 0.0003       |
|    loss                  | 35.7         |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00932     |
|    std                   | 0.969        |
|    value_loss            | 239          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7749743] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 10           |
|    time_elapsed          | 160          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0050717182 |
|    clip_fraction         | 0.0466       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 196          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0219      |
|    lagrangian_multiplier | 0.0561       |
|    learning_rate         | 0.0003       |
|    loss                  | 79.2         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.0067      |
|    std                   | 1.01         |
|    value_loss            | 607          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1369573] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 11           |
|    time_elapsed          | 178          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.00491907   |
|    clip_fraction         | 0.032        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 51.2         |
|    entropy_loss          | -2.9         |
|    explained_variance    | -0.00296     |
|    lagrangian_multiplier | 0.0917       |
|    learning_rate         | 0.0003       |
|    loss                  | 45.6         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00672     |
|    std                   | 1.03         |
|    value_loss            | 558          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5299551] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 49           |
|    time_elapsed          | 878          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.008497214  |
|    clip_fraction         | 0.0879       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 113          |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0478      |
|    lagrangian_multiplier | 0.0615       |
|    learning_rate         | 0.0003       |
|    loss                  | 37.3         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.0112      |
|    std                   | 0.965        |
|    value_loss            | 227          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5260808] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 11           |
|    time_elapsed          | 176          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.005262494  |
|    clip_fraction         | 0.0429       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 244          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0226      |
|    lagrangian_multiplier | 0.05         |
|    learning_rate         | 0.0003       |
|    loss                  | 94.3         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00652     |
|    std                   | 1.01         |
|    value_loss            | 557          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÅ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÅ‚ñÜ‚ñÑ‚ñÉ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñà
wandb:             train/approx_kl ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñÖ‚ñÇ‚ñÇ‚ñÖ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÜ
wandb:         train/clip_fraction ‚ñÇ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÖ‚ñÇ‚ñá‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÉ‚ñá‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñá
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ
wandb:          train/entropy_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ
wandb:    train/explained_variance ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÅ‚ñÇ‚ñÑ
wandb: train/lagrangian_multiplier ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÖ‚ñà‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÉ‚ñÅ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñá‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÉ‚ñÑ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñá‚ñà‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñá‚ñÉ‚ñà‚ñÖ‚ñà‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ
wandb:                   train/std ‚ñà‚ñà‚ñá‚ñà‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ
wandb:            train/value_loss ‚ñÉ‚ñÅ‚ñÇ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñà‚ñÇ‚ñÉ‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                      reward -0.52996
wandb:             train/approx_kl 0.0085
wandb:         train/clip_fraction 0.08794
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 112.50347
wandb:          train/entropy_loss -2.77
wandb:    train/explained_variance -0.04781
wandb: train/lagrangian_multiplier 0.06154
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 37.30386
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.01121
wandb:                   train/std 0.96462
wandb:            train/value_loss 227.25221
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/gwna5x1f
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_200016-gwna5x1f/logs
-------------------------------------------
| reward                   | [-1.39973]   |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 12           |
|    time_elapsed          | 194          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0045987708 |
|    clip_fraction         | 0.0395       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0802       |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0596       |
|    lagrangian_multiplier | 0.0797       |
|    learning_rate         | 0.0003       |
|    loss                  | 34.4         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00439     |
|    std                   | 1.02         |
|    value_loss            | 407          |
-------------------------------------------
srun: error: ddpg.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-1.2149994] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 12           |
|    time_elapsed          | 193          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0057425434 |
|    clip_fraction         | 0.0613       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 255          |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.0268      |
|    lagrangian_multiplier | 0.0563       |
|    learning_rate         | 0.0003       |
|    loss                  | 91.5         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00978     |
|    std                   | 1.03         |
|    value_loss            | 582          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114548.11
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
-------------------------------------------
| reward                   | [-1.3919309] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 13           |
|    time_elapsed          | 210          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0047115018 |
|    clip_fraction         | 0.0416       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.012        |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0748       |
|    lagrangian_multiplier | 0.0706       |
|    learning_rate         | 0.0003       |
|    loss                  | 47           |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00473     |
|    std                   | 1.01         |
|    value_loss            | 497          |
-------------------------------------------
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_201515-44wz606g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2/runs/44wz606g
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.254751]  |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 13           |
|    time_elapsed          | 209          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0068671163 |
|    clip_fraction         | 0.0588       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 224          |
|    entropy_loss          | -2.9         |
|    explained_variance    | 8.54e-05     |
|    lagrangian_multiplier | 0.0672       |
|    learning_rate         | 0.0003       |
|    loss                  | 59.6         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00945     |
|    std                   | 1.03         |
|    value_loss            | 380          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7690477] |
| time/                    |              |
|    fps                   | 51           |
|    iterations            | 2            |
|    time_elapsed          | 80           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0069466303 |
|    clip_fraction         | 0.0526       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0515       |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0292       |
|    lagrangian_multiplier | 0.0524       |
|    learning_rate         | 0.0003       |
|    loss                  | 26.3         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00643     |
|    std                   | 1.01         |
|    value_loss            | 261          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7279547] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 14           |
|    time_elapsed          | 226          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.007957466  |
|    clip_fraction         | 0.09         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.45         |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.074        |
|    lagrangian_multiplier | 0.0754       |
|    learning_rate         | 0.0003       |
|    loss                  | 57.9         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00748     |
|    std                   | 1.01         |
|    value_loss            | 650          |
-------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.38707218] |
| time/              |               |
|    fps             | 120           |
|    iterations      | 1             |
|    time_elapsed    | 17            |
|    total_timesteps | 2048          |
--------------------------------------
-------------------------------------------
| reward                   | [-0.7364907] |
| time/                    |              |
|    fps                   | 63           |
|    iterations            | 3            |
|    time_elapsed          | 96           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0072212946 |
|    clip_fraction         | 0.0639       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.266        |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.0237      |
|    lagrangian_multiplier | 0.0505       |
|    learning_rate         | 0.0003       |
|    loss                  | 44.5         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00772     |
|    std                   | 1.02         |
|    value_loss            | 396          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9815219] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 14           |
|    time_elapsed          | 225          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0072380137 |
|    clip_fraction         | 0.0623       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 226          |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0182       |
|    lagrangian_multiplier | 0.0614       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.4         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.0104      |
|    std                   | 1.02         |
|    value_loss            | 484          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7802953] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 15           |
|    time_elapsed          | 242          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.005248489  |
|    clip_fraction         | 0.0619       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 70.1         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0356       |
|    lagrangian_multiplier | 0.0726       |
|    learning_rate         | 0.0003       |
|    loss                  | 49.1         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00718     |
|    std                   | 1.01         |
|    value_loss            | 578          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8334727] |
| time/                    |              |
|    fps                   | 116          |
|    iterations            | 2            |
|    time_elapsed          | 35           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0053368313 |
|    clip_fraction         | 0.0327       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 104          |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0476       |
|    lagrangian_multiplier | 0.0644       |
|    learning_rate         | 0.0003       |
|    loss                  | 43.2         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00338     |
|    std                   | 1.01         |
|    value_loss            | 390          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7439307] |
| time/                    |              |
|    fps                   | 72           |
|    iterations            | 4            |
|    time_elapsed          | 112          |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.0060258703 |
|    clip_fraction         | 0.0466       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0612       |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.013       |
|    lagrangian_multiplier | 0.0507       |
|    learning_rate         | 0.0003       |
|    loss                  | 40.4         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00512     |
|    std                   | 1.02         |
|    value_loss            | 362          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6832906] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 15           |
|    time_elapsed          | 241          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.008393048  |
|    clip_fraction         | 0.0855       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 238          |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0355       |
|    lagrangian_multiplier | 0.0568       |
|    learning_rate         | 0.0003       |
|    loss                  | 92.3         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.0114      |
|    std                   | 1.03         |
|    value_loss            | 595          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4840884] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 16           |
|    time_elapsed          | 258          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.004575793  |
|    clip_fraction         | 0.036        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 56.8         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0452       |
|    lagrangian_multiplier | 0.0825       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.9         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00447     |
|    std                   | 1            |
|    value_loss            | 527          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7295915] |
| time/                    |              |
|    fps                   | 115          |
|    iterations            | 3            |
|    time_elapsed          | 52           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0049824715 |
|    clip_fraction         | 0.0267       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 109          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.00374     |
|    lagrangian_multiplier | 0.0665       |
|    learning_rate         | 0.0003       |
|    loss                  | 39.9         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00404     |
|    std                   | 1.01         |
|    value_loss            | 336          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1598697] |
| time/                    |              |
|    fps                   | 79           |
|    iterations            | 5            |
|    time_elapsed          | 128          |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.005832514  |
|    clip_fraction         | 0.0399       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0456       |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0372       |
|    lagrangian_multiplier | 0.0627       |
|    learning_rate         | 0.0003       |
|    loss                  | 33.3         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00595     |
|    std                   | 1.03         |
|    value_loss            | 316          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.446724]  |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 16           |
|    time_elapsed          | 258          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0076847486 |
|    clip_fraction         | 0.0798       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 217          |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0149       |
|    lagrangian_multiplier | 0.053        |
|    learning_rate         | 0.0003       |
|    loss                  | 86.5         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.0115      |
|    std                   | 1.02         |
|    value_loss            | 502          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9055808] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 17           |
|    time_elapsed          | 275          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0035855463 |
|    clip_fraction         | 0.0343       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 41.7         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0342       |
|    lagrangian_multiplier | 0.0982       |
|    learning_rate         | 0.0003       |
|    loss                  | 37           |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00623     |
|    std                   | 1.01         |
|    value_loss            | 459          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.37542614] |
| time/                    |               |
|    fps                   | 115           |
|    iterations            | 4             |
|    time_elapsed          | 70            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.004092478   |
|    clip_fraction         | 0.0274        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 188           |
|    entropy_loss          | -2.86         |
|    explained_variance    | 0.018         |
|    lagrangian_multiplier | 0.0603        |
|    learning_rate         | 0.0003        |
|    loss                  | 104           |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00696      |
|    std                   | 1.02          |
|    value_loss            | 781           |
--------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1639388] |
| time/                    |              |
|    fps                   | 84           |
|    iterations            | 6            |
|    time_elapsed          | 144          |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.006120584  |
|    clip_fraction         | 0.0665       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.26         |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.0373       |
|    lagrangian_multiplier | 0.0524       |
|    learning_rate         | 0.0003       |
|    loss                  | 95.9         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.0115      |
|    std                   | 1.04         |
|    value_loss            | 583          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.539187]  |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 17           |
|    time_elapsed          | 274          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0048976885 |
|    clip_fraction         | 0.0376       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 202          |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.0309      |
|    lagrangian_multiplier | 0.0582       |
|    learning_rate         | 0.0003       |
|    loss                  | 67.4         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00845     |
|    std                   | 1.03         |
|    value_loss            | 418          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7182773] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 18           |
|    time_elapsed          | 291          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0052346922 |
|    clip_fraction         | 0.03         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 24.3         |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.045        |
|    lagrangian_multiplier | 0.103        |
|    learning_rate         | 0.0003       |
|    loss                  | 37.9         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00498     |
|    std                   | 1.01         |
|    value_loss            | 550          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7648034] |
| time/                    |              |
|    fps                   | 115          |
|    iterations            | 5            |
|    time_elapsed          | 88           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0061077885 |
|    clip_fraction         | 0.0625       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 103          |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.0157      |
|    lagrangian_multiplier | 0.0688       |
|    learning_rate         | 0.0003       |
|    loss                  | 44.1         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00803     |
|    std                   | 1.01         |
|    value_loss            | 371          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.0703337] |
| time/                    |              |
|    fps                   | 89           |
|    iterations            | 7            |
|    time_elapsed          | 160          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.005947048  |
|    clip_fraction         | 0.0582       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 8.14         |
|    entropy_loss          | -2.92        |
|    explained_variance    | 0.0443       |
|    lagrangian_multiplier | 0.0538       |
|    learning_rate         | 0.0003       |
|    loss                  | 193          |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00903     |
|    std                   | 1.04         |
|    value_loss            | 1.64e+03     |
-------------------------------------------
------------------------------------------
| reward                   | [-1.233212] |
| time/                    |             |
|    fps                   | 126         |
|    iterations            | 18          |
|    time_elapsed          | 290         |
|    total_timesteps       | 36864       |
| train/                   |             |
|    approx_kl             | 0.005638268 |
|    clip_fraction         | 0.0525      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 256         |
|    entropy_loss          | -2.9        |
|    explained_variance    | -0.0298     |
|    lagrangian_multiplier | 0.0487      |
|    learning_rate         | 0.0003      |
|    loss                  | 86.1        |
|    n_updates             | 170         |
|    policy_gradient_loss  | -0.00763    |
|    std                   | 1.03        |
|    value_loss            | 408         |
------------------------------------------
-------------------------------------------
| reward                   | [-2.1824348] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 19           |
|    time_elapsed          | 307          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.006121901  |
|    clip_fraction         | 0.0437       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 74.7         |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0596       |
|    lagrangian_multiplier | 0.0835       |
|    learning_rate         | 0.0003       |
|    loss                  | 48.9         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00656     |
|    std                   | 0.997        |
|    value_loss            | 588          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2037592] |
| time/                    |              |
|    fps                   | 115          |
|    iterations            | 6            |
|    time_elapsed          | 106          |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.00740705   |
|    clip_fraction         | 0.0522       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 75.1         |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0628      |
|    lagrangian_multiplier | 0.0712       |
|    learning_rate         | 0.0003       |
|    loss                  | 29.2         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00731     |
|    std                   | 1            |
|    value_loss            | 225          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9783441] |
| time/                    |              |
|    fps                   | 92           |
|    iterations            | 8            |
|    time_elapsed          | 176          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0064343205 |
|    clip_fraction         | 0.0578       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.284        |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.0494       |
|    lagrangian_multiplier | 0.0678       |
|    learning_rate         | 0.0003       |
|    loss                  | 89           |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00754     |
|    std                   | 1.03         |
|    value_loss            | 995          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.85696614] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 19            |
|    time_elapsed          | 306           |
|    total_timesteps       | 38912         |
| train/                   |               |
|    approx_kl             | 0.0044596223  |
|    clip_fraction         | 0.0555        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 249           |
|    entropy_loss          | -2.9          |
|    explained_variance    | -0.03         |
|    lagrangian_multiplier | 0.0534        |
|    learning_rate         | 0.0003        |
|    loss                  | 92.9          |
|    n_updates             | 180           |
|    policy_gradient_loss  | -0.00733      |
|    std                   | 1.03          |
|    value_loss            | 609           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.3305695] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 20           |
|    time_elapsed          | 323          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0051709665 |
|    clip_fraction         | 0.052        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 34.8         |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.031        |
|    lagrangian_multiplier | 0.0778       |
|    learning_rate         | 0.0003       |
|    loss                  | 48.2         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00669     |
|    std                   | 0.992        |
|    value_loss            | 588          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.1697843] |
| time/                    |              |
|    fps                   | 95           |
|    iterations            | 9            |
|    time_elapsed          | 193          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0055555585 |
|    clip_fraction         | 0.055        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.59         |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0598       |
|    lagrangian_multiplier | 0.0613       |
|    learning_rate         | 0.0003       |
|    loss                  | 113          |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00654     |
|    std                   | 1.03         |
|    value_loss            | 1.05e+03     |
-------------------------------------------
------------------------------------------
| reward                   | [-1.347261] |
| time/                    |             |
|    fps                   | 115         |
|    iterations            | 7           |
|    time_elapsed          | 124         |
|    total_timesteps       | 14336       |
| train/                   |             |
|    approx_kl             | 0.005744483 |
|    clip_fraction         | 0.0533      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 155         |
|    entropy_loss          | -2.84       |
|    explained_variance    | 0.0541      |
|    lagrangian_multiplier | 0.0482      |
|    learning_rate         | 0.0003      |
|    loss                  | 70.8        |
|    n_updates             | 60          |
|    policy_gradient_loss  | -0.00861    |
|    std                   | 1           |
|    value_loss            | 400         |
------------------------------------------
-------------------------------------------
| reward                   | [-2.290341]  |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 20           |
|    time_elapsed          | 322          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0048410604 |
|    clip_fraction         | 0.0602       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 245          |
|    entropy_loss          | -2.91        |
|    explained_variance    | -0.0064      |
|    lagrangian_multiplier | 0.0617       |
|    learning_rate         | 0.0003       |
|    loss                  | 53.4         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00476     |
|    std                   | 1.04         |
|    value_loss            | 281          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.3728678] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 21           |
|    time_elapsed          | 340          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0032734121 |
|    clip_fraction         | 0.0276       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 32.6         |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0325       |
|    lagrangian_multiplier | 0.0861       |
|    learning_rate         | 0.0003       |
|    loss                  | 66           |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00211     |
|    std                   | 0.995        |
|    value_loss            | 804          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.9917581] |
| time/                    |              |
|    fps                   | 97           |
|    iterations            | 10           |
|    time_elapsed          | 209          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.006377264  |
|    clip_fraction         | 0.0525       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 14.1         |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0362       |
|    lagrangian_multiplier | 0.0572       |
|    learning_rate         | 0.0003       |
|    loss                  | 145          |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00882     |
|    std                   | 1.03         |
|    value_loss            | 1.42e+03     |
-------------------------------------------
------------------------------------------
| reward                   | [-2.233202] |
| time/                    |             |
|    fps                   | 126         |
|    iterations            | 21          |
|    time_elapsed          | 338         |
|    total_timesteps       | 43008       |
| train/                   |             |
|    approx_kl             | 0.007053418 |
|    clip_fraction         | 0.0625      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 239         |
|    entropy_loss          | -2.91       |
|    explained_variance    | -0.0409     |
|    lagrangian_multiplier | 0.0659      |
|    learning_rate         | 0.0003      |
|    loss                  | 92.3        |
|    n_updates             | 200         |
|    policy_gradient_loss  | -0.00898    |
|    std                   | 1.03        |
|    value_loss            | 638         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.3526987] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 8            |
|    time_elapsed          | 142          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.00584626   |
|    clip_fraction         | 0.0458       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 182          |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0547       |
|    lagrangian_multiplier | 0.0621       |
|    learning_rate         | 0.0003       |
|    loss                  | 49.8         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00712     |
|    std                   | 1            |
|    value_loss            | 401          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.41993335] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 22            |
|    time_elapsed          | 356           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.0058434717  |
|    clip_fraction         | 0.062         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 74.2          |
|    entropy_loss          | -2.81         |
|    explained_variance    | 0.035         |
|    lagrangian_multiplier | 0.0827        |
|    learning_rate         | 0.0003        |
|    loss                  | 69.7          |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.00635      |
|    std                   | 0.983         |
|    value_loss            | 801           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.32947862] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 22            |
|    time_elapsed          | 355           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.007324024   |
|    clip_fraction         | 0.0751        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 257           |
|    entropy_loss          | -2.92         |
|    explained_variance    | -0.0177       |
|    lagrangian_multiplier | 0.0524        |
|    learning_rate         | 0.0003        |
|    loss                  | 111           |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.0123       |
|    std                   | 1.05          |
|    value_loss            | 686           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.9795607] |
| time/                    |              |
|    fps                   | 99           |
|    iterations            | 11           |
|    time_elapsed          | 225          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.006257627  |
|    clip_fraction         | 0.0594       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.85         |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0205       |
|    lagrangian_multiplier | 0.0429       |
|    learning_rate         | 0.0003       |
|    loss                  | 120          |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00848     |
|    std                   | 1.03         |
|    value_loss            | 1.04e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.6548555] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 9            |
|    time_elapsed          | 160          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.006957359  |
|    clip_fraction         | 0.0517       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 228          |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.00534      |
|    lagrangian_multiplier | 0.0474       |
|    learning_rate         | 0.0003       |
|    loss                  | 210          |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00834     |
|    std                   | 0.985        |
|    value_loss            | 1.26e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.43630418] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 23            |
|    time_elapsed          | 372           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.004644111   |
|    clip_fraction         | 0.0445        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 2.12          |
|    entropy_loss          | -2.81         |
|    explained_variance    | 0.0474        |
|    lagrangian_multiplier | 0.0858        |
|    learning_rate         | 0.0003        |
|    loss                  | 55.2          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00456      |
|    std                   | 0.989         |
|    value_loss            | 712           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.54454744] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 23            |
|    time_elapsed          | 371           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.007235312   |
|    clip_fraction         | 0.0674        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 267           |
|    entropy_loss          | -2.92         |
|    explained_variance    | 0.00981       |
|    lagrangian_multiplier | 0.0508        |
|    learning_rate         | 0.0003        |
|    loss                  | 65.5          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00759      |
|    std                   | 1.04          |
|    value_loss            | 285           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.8737164] |
| time/                    |              |
|    fps                   | 101          |
|    iterations            | 12           |
|    time_elapsed          | 241          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0075416826 |
|    clip_fraction         | 0.0771       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.65         |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.00997      |
|    lagrangian_multiplier | 0.0618       |
|    learning_rate         | 0.0003       |
|    loss                  | 78.7         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.0113      |
|    std                   | 1.03         |
|    value_loss            | 785          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6994532] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 24           |
|    time_elapsed          | 389          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.007088545  |
|    clip_fraction         | 0.0504       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 10.8         |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0304       |
|    lagrangian_multiplier | 0.0787       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.4         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00436     |
|    std                   | 0.984        |
|    value_loss            | 616          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.8439727] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 10           |
|    time_elapsed          | 178          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.007676758  |
|    clip_fraction         | 0.0666       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 196          |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0028      |
|    lagrangian_multiplier | 0.0492       |
|    learning_rate         | 0.0003       |
|    loss                  | 160          |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00989     |
|    std                   | 0.981        |
|    value_loss            | 1.17e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.007337]  |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 24           |
|    time_elapsed          | 387          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.0054688393 |
|    clip_fraction         | 0.0399       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 212          |
|    entropy_loss          | -2.91        |
|    explained_variance    | -0.0258      |
|    lagrangian_multiplier | 0.0598       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.5         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00656     |
|    std                   | 1.03         |
|    value_loss            | 372          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4817309] |
| time/                    |              |
|    fps                   | 103          |
|    iterations            | 13           |
|    time_elapsed          | 258          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0065842373 |
|    clip_fraction         | 0.0463       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.35         |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.000834     |
|    lagrangian_multiplier | 0.0538       |
|    learning_rate         | 0.0003       |
|    loss                  | 86.4         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00831     |
|    std                   | 1.03         |
|    value_loss            | 797          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8091536] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 25           |
|    time_elapsed          | 405          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.006103818  |
|    clip_fraction         | 0.0673       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.6          |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0886       |
|    lagrangian_multiplier | 0.0856       |
|    learning_rate         | 0.0003       |
|    loss                  | 35.9         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00593     |
|    std                   | 0.977        |
|    value_loss            | 502          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.4298043] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 11           |
|    time_elapsed          | 196          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0058026128 |
|    clip_fraction         | 0.0346       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 226          |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.00725      |
|    lagrangian_multiplier | 0.0487       |
|    learning_rate         | 0.0003       |
|    loss                  | 257          |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00426     |
|    std                   | 0.982        |
|    value_loss            | 1.94e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4569167] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 25           |
|    time_elapsed          | 403          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.0071688015 |
|    clip_fraction         | 0.0688       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 249          |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.0113       |
|    lagrangian_multiplier | 0.057        |
|    learning_rate         | 0.0003       |
|    loss                  | 168          |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 1.04         |
|    value_loss            | 1.34e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0721748] |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 14           |
|    time_elapsed          | 274          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.006420652  |
|    clip_fraction         | 0.0683       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 7.2          |
|    entropy_loss          | -2.9         |
|    explained_variance    | -0.00632     |
|    lagrangian_multiplier | 0.0558       |
|    learning_rate         | 0.0003       |
|    loss                  | 63.5         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00922     |
|    std                   | 1.03         |
|    value_loss            | 556          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1026195] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 26           |
|    time_elapsed          | 421          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.005298386  |
|    clip_fraction         | 0.0554       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 54.3         |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.07         |
|    lagrangian_multiplier | 0.0942       |
|    learning_rate         | 0.0003       |
|    loss                  | 45.6         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00734     |
|    std                   | 0.988        |
|    value_loss            | 597          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-3.1917126] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 12           |
|    time_elapsed          | 214          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0064039    |
|    clip_fraction         | 0.0553       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 201          |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.000339     |
|    lagrangian_multiplier | 0.0442       |
|    learning_rate         | 0.0003       |
|    loss                  | 209          |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00625     |
|    std                   | 0.982        |
|    value_loss            | 1.42e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.79854167] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 26            |
|    time_elapsed          | 419           |
|    total_timesteps       | 53248         |
| train/                   |               |
|    approx_kl             | 0.007142367   |
|    clip_fraction         | 0.0809        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 225           |
|    entropy_loss          | -2.92         |
|    explained_variance    | -0.00782      |
|    lagrangian_multiplier | 0.057         |
|    learning_rate         | 0.0003        |
|    loss                  | 91.9          |
|    n_updates             | 250           |
|    policy_gradient_loss  | -0.0114       |
|    std                   | 1.05          |
|    value_loss            | 676           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3439876] |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 15           |
|    time_elapsed          | 290          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0074905413 |
|    clip_fraction         | 0.0779       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 2.09         |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.0201      |
|    lagrangian_multiplier | 0.0638       |
|    learning_rate         | 0.0003       |
|    loss                  | 41.6         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.0106      |
|    std                   | 1.03         |
|    value_loss            | 414          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3954843] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 27           |
|    time_elapsed          | 437          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.0069156475 |
|    clip_fraction         | 0.0667       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 26.8         |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0732       |
|    lagrangian_multiplier | 0.0817       |
|    learning_rate         | 0.0003       |
|    loss                  | 56.6         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00979     |
|    std                   | 1.01         |
|    value_loss            | 657          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.8611557] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 13           |
|    time_elapsed          | 232          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.005745642  |
|    clip_fraction         | 0.0578       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 210          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0122       |
|    lagrangian_multiplier | 0.0473       |
|    learning_rate         | 0.0003       |
|    loss                  | 217          |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00739     |
|    std                   | 0.969        |
|    value_loss            | 1.5e+03      |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3234329] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 27           |
|    time_elapsed          | 436          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.0063848975 |
|    clip_fraction         | 0.0629       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 214          |
|    entropy_loss          | -2.93        |
|    explained_variance    | -0.0255      |
|    lagrangian_multiplier | 0.0557       |
|    learning_rate         | 0.0003       |
|    loss                  | 152          |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.0096      |
|    std                   | 1.05         |
|    value_loss            | 1.08e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.521777]  |
| time/                    |              |
|    fps                   | 106          |
|    iterations            | 16           |
|    time_elapsed          | 306          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0060336106 |
|    clip_fraction         | 0.0538       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.46         |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0535       |
|    lagrangian_multiplier | 0.0589       |
|    learning_rate         | 0.0003       |
|    loss                  | 101          |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00807     |
|    std                   | 1.03         |
|    value_loss            | 936          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2361594] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 28           |
|    time_elapsed          | 453          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.005235821  |
|    clip_fraction         | 0.0391       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 73.4         |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0626       |
|    lagrangian_multiplier | 0.0839       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.5         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00596     |
|    std                   | 0.995        |
|    value_loss            | 645          |
-------------------------------------------
------------------------------------------
| reward                   | [-2.579721] |
| time/                    |             |
|    fps                   | 114         |
|    iterations            | 14          |
|    time_elapsed          | 250         |
|    total_timesteps       | 28672       |
| train/                   |             |
|    approx_kl             | 0.008940633 |
|    clip_fraction         | 0.0843      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 249         |
|    entropy_loss          | -2.76       |
|    explained_variance    | -0.0275     |
|    lagrangian_multiplier | 0.0549      |
|    learning_rate         | 0.0003      |
|    loss                  | 288         |
|    n_updates             | 130         |
|    policy_gradient_loss  | -0.0102     |
|    std                   | 0.963       |
|    value_loss            | 2.43e+03    |
------------------------------------------
-------------------------------------------
| reward                   | [-1.4553632] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 28           |
|    time_elapsed          | 452          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.007170761  |
|    clip_fraction         | 0.0546       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 249          |
|    entropy_loss          | -2.93        |
|    explained_variance    | -0.022       |
|    lagrangian_multiplier | 0.0588       |
|    learning_rate         | 0.0003       |
|    loss                  | 110          |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00773     |
|    std                   | 1.05         |
|    value_loss            | 908          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3725936] |
| time/                    |              |
|    fps                   | 107          |
|    iterations            | 17           |
|    time_elapsed          | 322          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0051542744 |
|    clip_fraction         | 0.0494       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 8.32         |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.0333       |
|    lagrangian_multiplier | 0.0553       |
|    learning_rate         | 0.0003       |
|    loss                  | 87.4         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00878     |
|    std                   | 1.03         |
|    value_loss            | 873          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4566144] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 29           |
|    time_elapsed          | 469          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.007224201  |
|    clip_fraction         | 0.053        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 89.4         |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.078        |
|    lagrangian_multiplier | 0.088        |
|    learning_rate         | 0.0003       |
|    loss                  | 62.9         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00572     |
|    std                   | 0.988        |
|    value_loss            | 676          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.7176279] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 15           |
|    time_elapsed          | 268          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0066245627 |
|    clip_fraction         | 0.0625       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 218          |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0113      |
|    lagrangian_multiplier | 0.0476       |
|    learning_rate         | 0.0003       |
|    loss                  | 231          |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00795     |
|    std                   | 0.953        |
|    value_loss            | 1.39e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1751119] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 29           |
|    time_elapsed          | 468          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.0060969526 |
|    clip_fraction         | 0.0806       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 237          |
|    entropy_loss          | -2.94        |
|    explained_variance    | -0.0212      |
|    lagrangian_multiplier | 0.0626       |
|    learning_rate         | 0.0003       |
|    loss                  | 137          |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.0111      |
|    std                   | 1.06         |
|    value_loss            | 998          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0332947] |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 18           |
|    time_elapsed          | 339          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.005834804  |
|    clip_fraction         | 0.0442       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.123        |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0485       |
|    lagrangian_multiplier | 0.06         |
|    learning_rate         | 0.0003       |
|    loss                  | 52.4         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00757     |
|    std                   | 1.02         |
|    value_loss            | 469          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.369731] |
| time/                    |             |
|    fps                   | 126         |
|    iterations            | 30          |
|    time_elapsed          | 485         |
|    total_timesteps       | 61440       |
| train/                   |             |
|    approx_kl             | 0.004271683 |
|    clip_fraction         | 0.0322      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.434       |
|    entropy_loss          | -2.81       |
|    explained_variance    | 0.0586      |
|    lagrangian_multiplier | 0.0926      |
|    learning_rate         | 0.0003      |
|    loss                  | 45.3        |
|    n_updates             | 290         |
|    policy_gradient_loss  | -0.00333    |
|    std                   | 0.986       |
|    value_loss            | 632         |
------------------------------------------
-------------------------------------------
| reward                   | [-2.2613888] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 16           |
|    time_elapsed          | 286          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0065596653 |
|    clip_fraction         | 0.0507       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 218          |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.0193      |
|    lagrangian_multiplier | 0.0466       |
|    learning_rate         | 0.0003       |
|    loss                  | 137          |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00746     |
|    std                   | 0.943        |
|    value_loss            | 824          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9524904] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 30           |
|    time_elapsed          | 484          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.009146103  |
|    clip_fraction         | 0.0815       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 229          |
|    entropy_loss          | -2.95        |
|    explained_variance    | 0.00654      |
|    lagrangian_multiplier | 0.0539       |
|    learning_rate         | 0.0003       |
|    loss                  | 75.7         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.0124      |
|    std                   | 1.05         |
|    value_loss            | 434          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7573713] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 31           |
|    time_elapsed          | 502          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0076239137 |
|    clip_fraction         | 0.0572       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0246       |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0518       |
|    lagrangian_multiplier | 0.0928       |
|    learning_rate         | 0.0003       |
|    loss                  | 59.7         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00459     |
|    std                   | 0.991        |
|    value_loss            | 863          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2981739] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 31           |
|    time_elapsed          | 500          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.007284701  |
|    clip_fraction         | 0.0664       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 252          |
|    entropy_loss          | -2.93        |
|    explained_variance    | -0.0291      |
|    lagrangian_multiplier | 0.052        |
|    learning_rate         | 0.0003       |
|    loss                  | 124          |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00996     |
|    std                   | 1.05         |
|    value_loss            | 849          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2370355] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 17           |
|    time_elapsed          | 303          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0061314697 |
|    clip_fraction         | 0.0473       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 192          |
|    entropy_loss          | -2.71        |
|    explained_variance    | -0.00559     |
|    lagrangian_multiplier | 0.0444       |
|    learning_rate         | 0.0003       |
|    loss                  | 129          |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00872     |
|    std                   | 0.937        |
|    value_loss            | 749          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7157567] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 32           |
|    time_elapsed          | 518          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.0051549654 |
|    clip_fraction         | 0.035        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0892       |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0701       |
|    lagrangian_multiplier | 0.0981       |
|    learning_rate         | 0.0003       |
|    loss                  | 42.7         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00286     |
|    std                   | 0.979        |
|    value_loss            | 631          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.5818007] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 32           |
|    time_elapsed          | 517          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.0075278473 |
|    clip_fraction         | 0.0657       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 254          |
|    entropy_loss          | -2.92        |
|    explained_variance    | -0.0332      |
|    lagrangian_multiplier | 0.0626       |
|    learning_rate         | 0.0003       |
|    loss                  | 132          |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.0103      |
|    std                   | 1.04         |
|    value_loss            | 851          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.73300916] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 18            |
|    time_elapsed          | 321           |
|    total_timesteps       | 36864         |
| train/                   |               |
|    approx_kl             | 0.005460631   |
|    clip_fraction         | 0.0627        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 148           |
|    entropy_loss          | -2.7          |
|    explained_variance    | 0.00915       |
|    lagrangian_multiplier | 0.049         |
|    learning_rate         | 0.0003        |
|    loss                  | 85            |
|    n_updates             | 170           |
|    policy_gradient_loss  | -0.00921      |
|    std                   | 0.939         |
|    value_loss            | 588           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.6187199] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 33           |
|    time_elapsed          | 534          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0051851915 |
|    clip_fraction         | 0.051        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.025        |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.066        |
|    lagrangian_multiplier | 0.0982       |
|    learning_rate         | 0.0003       |
|    loss                  | 56.3         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00446     |
|    std                   | 0.972        |
|    value_loss            | 772          |
-------------------------------------------
------------------------------------------
| reward                   | [-2.049519] |
| time/                    |             |
|    fps                   | 98          |
|    iterations            | 19          |
|    time_elapsed          | 394         |
|    total_timesteps       | 38912       |
| train/                   |             |
|    approx_kl             | 0.006126407 |
|    clip_fraction         | 0.0436      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.11        |
|    entropy_loss          | -2.87       |
|    explained_variance    | 0.0327      |
|    lagrangian_multiplier | 0.0607      |
|    learning_rate         | 0.0003      |
|    loss                  | 86          |
|    n_updates             | 180         |
|    policy_gradient_loss  | -0.00689    |
|    std                   | 1.02        |
|    value_loss            | 888         |
------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.621932]  |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 33           |
|    time_elapsed          | 533          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0069746748 |
|    clip_fraction         | 0.0468       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 188          |
|    entropy_loss          | -2.92        |
|    explained_variance    | -0.037       |
|    lagrangian_multiplier | 0.0659       |
|    learning_rate         | 0.0003       |
|    loss                  | 61.5         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00827     |
|    std                   | 1.04         |
|    value_loss            | 416          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6679944] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 34           |
|    time_elapsed          | 550          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.007963944  |
|    clip_fraction         | 0.0728       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0275       |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0675       |
|    lagrangian_multiplier | 0.102        |
|    learning_rate         | 0.0003       |
|    loss                  | 48           |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00869     |
|    std                   | 0.977        |
|    value_loss            | 710          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3722297] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 19           |
|    time_elapsed          | 339          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0060799075 |
|    clip_fraction         | 0.0521       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 139          |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.00797     |
|    lagrangian_multiplier | 0.0531       |
|    learning_rate         | 0.0003       |
|    loss                  | 74           |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00758     |
|    std                   | 0.955        |
|    value_loss            | 446          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9582789] |
| time/                    |              |
|    fps                   | 99           |
|    iterations            | 20           |
|    time_elapsed          | 410          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.004893312  |
|    clip_fraction         | 0.0511       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.278        |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0516       |
|    lagrangian_multiplier | 0.0575       |
|    learning_rate         | 0.0003       |
|    loss                  | 90.1         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00835     |
|    std                   | 1.01         |
|    value_loss            | 778          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8025183] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 34           |
|    time_elapsed          | 549          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.006465468  |
|    clip_fraction         | 0.0586       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 249          |
|    entropy_loss          | -2.92        |
|    explained_variance    | -0.0291      |
|    lagrangian_multiplier | 0.059        |
|    learning_rate         | 0.0003       |
|    loss                  | 98.5         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.0103      |
|    std                   | 1.04         |
|    value_loss            | 686          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.5384399] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 35           |
|    time_elapsed          | 567          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0058890157 |
|    clip_fraction         | 0.0424       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0255       |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.086        |
|    lagrangian_multiplier | 0.093        |
|    learning_rate         | 0.0003       |
|    loss                  | 42.8         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00531     |
|    std                   | 0.976        |
|    value_loss            | 578          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9370918] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 20           |
|    time_elapsed          | 357          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0060690357 |
|    clip_fraction         | 0.0629       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 102          |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.0201       |
|    lagrangian_multiplier | 0.0632       |
|    learning_rate         | 0.0003       |
|    loss                  | 25.5         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00754     |
|    std                   | 0.95         |
|    value_loss            | 194          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0169513] |
| time/                    |              |
|    fps                   | 100          |
|    iterations            | 21           |
|    time_elapsed          | 427          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.005100577  |
|    clip_fraction         | 0.0371       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0981       |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.042        |
|    lagrangian_multiplier | 0.0687       |
|    learning_rate         | 0.0003       |
|    loss                  | 55.8         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00684     |
|    std                   | 1.01         |
|    value_loss            | 610          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2367961] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 35           |
|    time_elapsed          | 566          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0050671836 |
|    clip_fraction         | 0.0604       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 269          |
|    entropy_loss          | -2.92        |
|    explained_variance    | -0.0196      |
|    lagrangian_multiplier | 0.0609       |
|    learning_rate         | 0.0003       |
|    loss                  | 70.2         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00689     |
|    std                   | 1.04         |
|    value_loss            | 376          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6410578] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 36           |
|    time_elapsed          | 583          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0040985625 |
|    clip_fraction         | 0.0556       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.61         |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0702       |
|    lagrangian_multiplier | 0.102        |
|    learning_rate         | 0.0003       |
|    loss                  | 43.1         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00602     |
|    std                   | 0.965        |
|    value_loss            | 562          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.4659594] |
| time/                    |              |
|    fps                   | 101          |
|    iterations            | 22           |
|    time_elapsed          | 443          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.0058834264 |
|    clip_fraction         | 0.0586       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0924       |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0556       |
|    lagrangian_multiplier | 0.0719       |
|    learning_rate         | 0.0003       |
|    loss                  | 77.3         |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00713     |
|    std                   | 0.996        |
|    value_loss            | 805          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4423815] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 21           |
|    time_elapsed          | 375          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0055603534 |
|    clip_fraction         | 0.0493       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 123          |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.034        |
|    lagrangian_multiplier | 0.0607       |
|    learning_rate         | 0.0003       |
|    loss                  | 60.7         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00652     |
|    std                   | 0.955        |
|    value_loss            | 462          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.90476745] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 36            |
|    time_elapsed          | 582           |
|    total_timesteps       | 73728         |
| train/                   |               |
|    approx_kl             | 0.0059560584  |
|    clip_fraction         | 0.0752        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 262           |
|    entropy_loss          | -2.92         |
|    explained_variance    | 0.0243        |
|    lagrangian_multiplier | 0.0551        |
|    learning_rate         | 0.0003        |
|    loss                  | 82.6          |
|    n_updates             | 350           |
|    policy_gradient_loss  | -0.00815      |
|    std                   | 1.04          |
|    value_loss            | 458           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.5190027] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 37           |
|    time_elapsed          | 599          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.004536644  |
|    clip_fraction         | 0.0577       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0176       |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0693       |
|    lagrangian_multiplier | 0.0946       |
|    learning_rate         | 0.0003       |
|    loss                  | 42.4         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00526     |
|    std                   | 0.969        |
|    value_loss            | 563          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.76715976] |
| time/                    |               |
|    fps                   | 102           |
|    iterations            | 23            |
|    time_elapsed          | 459           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.0062151216  |
|    clip_fraction         | 0.0615        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.084         |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.0456        |
|    lagrangian_multiplier | 0.0692        |
|    learning_rate         | 0.0003        |
|    loss                  | 99            |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00827      |
|    std                   | 0.99          |
|    value_loss            | 978           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.5130229] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 22           |
|    time_elapsed          | 393          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.0049832547 |
|    clip_fraction         | 0.0586       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 171          |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.0263       |
|    lagrangian_multiplier | 0.0497       |
|    learning_rate         | 0.0003       |
|    loss                  | 156          |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00937     |
|    std                   | 0.958        |
|    value_loss            | 883          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7780572] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 37           |
|    time_elapsed          | 598          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.008342311  |
|    clip_fraction         | 0.101        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 257          |
|    entropy_loss          | -2.93        |
|    explained_variance    | 0.00516      |
|    lagrangian_multiplier | 0.0616       |
|    learning_rate         | 0.0003       |
|    loss                  | 65.5         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.0166      |
|    std                   | 1.05         |
|    value_loss            | 397          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5383813] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 38           |
|    time_elapsed          | 616          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0065203244 |
|    clip_fraction         | 0.0689       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.923        |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0622       |
|    lagrangian_multiplier | 0.0905       |
|    learning_rate         | 0.0003       |
|    loss                  | 36.2         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00714     |
|    std                   | 0.964        |
|    value_loss            | 495          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0671302] |
| time/                    |              |
|    fps                   | 103          |
|    iterations            | 24           |
|    time_elapsed          | 475          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.0048091086 |
|    clip_fraction         | 0.0588       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.68         |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0426       |
|    lagrangian_multiplier | 0.0592       |
|    learning_rate         | 0.0003       |
|    loss                  | 97.7         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00661     |
|    std                   | 0.996        |
|    value_loss            | 949          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.61244154] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 23            |
|    time_elapsed          | 411           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.005990001   |
|    clip_fraction         | 0.0646        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 154           |
|    entropy_loss          | -2.74         |
|    explained_variance    | 0.041         |
|    lagrangian_multiplier | 0.0509        |
|    learning_rate         | 0.0003        |
|    loss                  | 121           |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00683      |
|    std                   | 0.957         |
|    value_loss            | 830           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3627409] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 38           |
|    time_elapsed          | 614          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0061709383 |
|    clip_fraction         | 0.0623       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 256          |
|    entropy_loss          | -2.93        |
|    explained_variance    | -0.00338     |
|    lagrangian_multiplier | 0.0548       |
|    learning_rate         | 0.0003       |
|    loss                  | 93.7         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00977     |
|    std                   | 1.04         |
|    value_loss            | 557          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8969499] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 39           |
|    time_elapsed          | 632          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.007354678  |
|    clip_fraction         | 0.0881       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 23           |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.0641       |
|    lagrangian_multiplier | 0.0752       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.2         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00837     |
|    std                   | 0.961        |
|    value_loss            | 544          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.70932704] |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 25            |
|    time_elapsed          | 492           |
|    total_timesteps       | 51200         |
| train/                   |               |
|    approx_kl             | 0.005830553   |
|    clip_fraction         | 0.0537        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 4.85          |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.0283        |
|    lagrangian_multiplier | 0.0582        |
|    learning_rate         | 0.0003        |
|    loss                  | 71.3          |
|    n_updates             | 240           |
|    policy_gradient_loss  | -0.00685      |
|    std                   | 0.987         |
|    value_loss            | 704           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.1345842] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 24           |
|    time_elapsed          | 429          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.0057526557 |
|    clip_fraction         | 0.0529       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 146          |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.0827       |
|    lagrangian_multiplier | 0.0522       |
|    learning_rate         | 0.0003       |
|    loss                  | 91.6         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00735     |
|    std                   | 0.956        |
|    value_loss            | 615          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.409128]  |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 39           |
|    time_elapsed          | 631          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0051402086 |
|    clip_fraction         | 0.0492       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 195          |
|    entropy_loss          | -2.93        |
|    explained_variance    | -0.0176      |
|    lagrangian_multiplier | 0.0552       |
|    learning_rate         | 0.0003       |
|    loss                  | 79.3         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00986     |
|    std                   | 1.05         |
|    value_loss            | 536          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7424417] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 40           |
|    time_elapsed          | 648          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.005515051  |
|    clip_fraction         | 0.0527       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 7.16         |
|    entropy_loss          | -2.75        |
|    explained_variance    | 0.0691       |
|    lagrangian_multiplier | 0.0881       |
|    learning_rate         | 0.0003       |
|    loss                  | 44.4         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00376     |
|    std                   | 0.952        |
|    value_loss            | 618          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2485029] |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 26           |
|    time_elapsed          | 508          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0043375324 |
|    clip_fraction         | 0.0384       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.04         |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0166       |
|    lagrangian_multiplier | 0.0623       |
|    learning_rate         | 0.0003       |
|    loss                  | 92.8         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00585     |
|    std                   | 0.989        |
|    value_loss            | 905          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.84547675] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 25            |
|    time_elapsed          | 447           |
|    total_timesteps       | 51200         |
| train/                   |               |
|    approx_kl             | 0.006097176   |
|    clip_fraction         | 0.0444        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 133           |
|    entropy_loss          | -2.73         |
|    explained_variance    | 0.0134        |
|    lagrangian_multiplier | 0.0515        |
|    learning_rate         | 0.0003        |
|    loss                  | 91.1          |
|    n_updates             | 240           |
|    policy_gradient_loss  | -0.00554      |
|    std                   | 0.95          |
|    value_loss            | 749           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2899965] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 40           |
|    time_elapsed          | 647          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.007253034  |
|    clip_fraction         | 0.0803       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 243          |
|    entropy_loss          | -2.92        |
|    explained_variance    | -0.00709     |
|    lagrangian_multiplier | 0.0496       |
|    learning_rate         | 0.0003       |
|    loss                  | 88.5         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.0126      |
|    std                   | 1.04         |
|    value_loss            | 525          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0735593] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 41           |
|    time_elapsed          | 664          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0039771018 |
|    clip_fraction         | 0.0497       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 7.99         |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.0646       |
|    lagrangian_multiplier | 0.0884       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.5         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00507     |
|    std                   | 0.972        |
|    value_loss            | 604          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1472814] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 26           |
|    time_elapsed          | 465          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0054420102 |
|    clip_fraction         | 0.0549       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 127          |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.0338       |
|    lagrangian_multiplier | 0.0593       |
|    learning_rate         | 0.0003       |
|    loss                  | 85.7         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.0082      |
|    std                   | 0.957        |
|    value_loss            | 780          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.859161] |
| time/                    |             |
|    fps                   | 126         |
|    iterations            | 41          |
|    time_elapsed          | 663         |
|    total_timesteps       | 83968       |
| train/                   |             |
|    approx_kl             | 0.007449129 |
|    clip_fraction         | 0.0616      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 265         |
|    entropy_loss          | -2.91       |
|    explained_variance    | 0.0128      |
|    lagrangian_multiplier | 0.062       |
|    learning_rate         | 0.0003      |
|    loss                  | 63          |
|    n_updates             | 400         |
|    policy_gradient_loss  | -0.00936    |
|    std                   | 1.04        |
|    value_loss            | 394         |
------------------------------------------
------------------------------------------
| reward                   | [-2.201142] |
| time/                    |             |
|    fps                   | 126         |
|    iterations            | 42          |
|    time_elapsed          | 680         |
|    total_timesteps       | 86016       |
| train/                   |             |
|    approx_kl             | 0.00654885  |
|    clip_fraction         | 0.0621      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0276      |
|    entropy_loss          | -2.78       |
|    explained_variance    | 0.0934      |
|    lagrangian_multiplier | 0.0976      |
|    learning_rate         | 0.0003      |
|    loss                  | 50.5        |
|    n_updates             | 410         |
|    policy_gradient_loss  | -0.00325    |
|    std                   | 0.973       |
|    value_loss            | 693         |
------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.75679636] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 42            |
|    time_elapsed          | 680           |
|    total_timesteps       | 86016         |
| train/                   |               |
|    approx_kl             | 0.008100651   |
|    clip_fraction         | 0.076         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 260           |
|    entropy_loss          | -2.93         |
|    explained_variance    | -0.0257       |
|    lagrangian_multiplier | 0.0576        |
|    learning_rate         | 0.0003        |
|    loss                  | 74.5          |
|    n_updates             | 410           |
|    policy_gradient_loss  | -0.012        |
|    std                   | 1.05          |
|    value_loss            | 448           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.9692719] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 27           |
|    time_elapsed          | 483          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.00507404   |
|    clip_fraction         | 0.0342       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 54.6         |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.00284      |
|    lagrangian_multiplier | 0.079        |
|    learning_rate         | 0.0003       |
|    loss                  | 27.2         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00407     |
|    std                   | 0.946        |
|    value_loss            | 277          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2630615] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 43           |
|    time_elapsed          | 697          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.005062778  |
|    clip_fraction         | 0.0537       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.026        |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0752       |
|    lagrangian_multiplier | 0.0989       |
|    learning_rate         | 0.0003       |
|    loss                  | 57.7         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00469     |
|    std                   | 0.962        |
|    value_loss            | 810          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7721919] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 43           |
|    time_elapsed          | 696          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0067852293 |
|    clip_fraction         | 0.0687       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 259          |
|    entropy_loss          | -2.93        |
|    explained_variance    | 0.0289       |
|    lagrangian_multiplier | 0.0497       |
|    learning_rate         | 0.0003       |
|    loss                  | 163          |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.0113      |
|    std                   | 1.04         |
|    value_loss            | 1.1e+03      |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7168553] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 28           |
|    time_elapsed          | 501          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.005924362  |
|    clip_fraction         | 0.0469       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 23.8         |
|    entropy_loss          | -2.69        |
|    explained_variance    | -0.0105      |
|    lagrangian_multiplier | 0.0798       |
|    learning_rate         | 0.0003       |
|    loss                  | 18.5         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00543     |
|    std                   | 0.928        |
|    value_loss            | 182          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.28897908] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 44            |
|    time_elapsed          | 713           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.0026153703  |
|    clip_fraction         | 0.0284        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 1.77          |
|    entropy_loss          | -2.75         |
|    explained_variance    | 0.0558        |
|    lagrangian_multiplier | 0.0861        |
|    learning_rate         | 0.0003        |
|    loss                  | 52.7          |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.00186      |
|    std                   | 0.955         |
|    value_loss            | 709           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.18315637] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 44            |
|    time_elapsed          | 712           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.0055628363  |
|    clip_fraction         | 0.0635        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 254           |
|    entropy_loss          | -2.92         |
|    explained_variance    | 0.0226        |
|    lagrangian_multiplier | 0.0528        |
|    learning_rate         | 0.0003        |
|    loss                  | 65            |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.00806      |
|    std                   | 1.04          |
|    value_loss            | 354           |
--------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.5784894] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 45           |
|    time_elapsed          | 729          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.006518842  |
|    clip_fraction         | 0.0667       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 19.3         |
|    entropy_loss          | -2.75        |
|    explained_variance    | 0.042        |
|    lagrangian_multiplier | 0.0812       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.3         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00739     |
|    std                   | 0.961        |
|    value_loss            | 710          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9275925] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 29           |
|    time_elapsed          | 519          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.006566898  |
|    clip_fraction         | 0.0719       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 13           |
|    entropy_loss          | -2.68        |
|    explained_variance    | -0.00334     |
|    lagrangian_multiplier | 0.0748       |
|    learning_rate         | 0.0003       |
|    loss                  | 21.8         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00683     |
|    std                   | 0.931        |
|    value_loss            | 226          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.295225] |
| time/                    |             |
|    fps                   | 93          |
|    iterations            | 27          |
|    time_elapsed          | 592         |
|    total_timesteps       | 55296       |
| train/                   |             |
|    approx_kl             | 0.006145738 |
|    clip_fraction         | 0.0592      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.108       |
|    entropy_loss          | -2.82       |
|    explained_variance    | 0.0239      |
|    lagrangian_multiplier | 0.0661      |
|    learning_rate         | 0.0003      |
|    loss                  | 75.6        |
|    n_updates             | 260         |
|    policy_gradient_loss  | -0.00732    |
|    std                   | 0.992       |
|    value_loss            | 717         |
------------------------------------------
-------------------------------------------
| reward                   | [-0.8437503] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 45           |
|    time_elapsed          | 728          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.004152194  |
|    clip_fraction         | 0.0499       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 251          |
|    entropy_loss          | -2.92        |
|    explained_variance    | -0.0293      |
|    lagrangian_multiplier | 0.0562       |
|    learning_rate         | 0.0003       |
|    loss                  | 81.1         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00894     |
|    std                   | 1.04         |
|    value_loss            | 511          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0546666] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 46           |
|    time_elapsed          | 745          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.005026864  |
|    clip_fraction         | 0.0482       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 71.6         |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.0423       |
|    lagrangian_multiplier | 0.0784       |
|    learning_rate         | 0.0003       |
|    loss                  | 64.1         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.00533     |
|    std                   | 0.969        |
|    value_loss            | 753          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.34291476] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 30            |
|    time_elapsed          | 537           |
|    total_timesteps       | 61440         |
| train/                   |               |
|    approx_kl             | 0.007841295   |
|    clip_fraction         | 0.0784        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 165           |
|    entropy_loss          | -2.7          |
|    explained_variance    | 0.00691       |
|    lagrangian_multiplier | 0.0529        |
|    learning_rate         | 0.0003        |
|    loss                  | 154           |
|    n_updates             | 290           |
|    policy_gradient_loss  | -0.0122       |
|    std                   | 0.947         |
|    value_loss            | 976           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.290959]  |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 46           |
|    time_elapsed          | 744          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0076178107 |
|    clip_fraction         | 0.0704       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 260          |
|    entropy_loss          | -2.92        |
|    explained_variance    | -0.00814     |
|    lagrangian_multiplier | 0.0532       |
|    learning_rate         | 0.0003       |
|    loss                  | 79           |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.0117      |
|    std                   | 1.04         |
|    value_loss            | 469          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9716513] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 47           |
|    time_elapsed          | 762          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.004823354  |
|    clip_fraction         | 0.0339       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.123        |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0796       |
|    lagrangian_multiplier | 0.105        |
|    learning_rate         | 0.0003       |
|    loss                  | 59.5         |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.00288     |
|    std                   | 0.971        |
|    value_loss            | 836          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.660004]  |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 31           |
|    time_elapsed          | 555          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0058307224 |
|    clip_fraction         | 0.0513       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 88.6         |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.0132       |
|    lagrangian_multiplier | 0.0775       |
|    learning_rate         | 0.0003       |
|    loss                  | 75.2         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00921     |
|    std                   | 0.958        |
|    value_loss            | 673          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2729827] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 47           |
|    time_elapsed          | 760          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.007401931  |
|    clip_fraction         | 0.0572       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 265          |
|    entropy_loss          | -2.92        |
|    explained_variance    | -0.0158      |
|    lagrangian_multiplier | 0.0533       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.2         |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.00724     |
|    std                   | 1.04         |
|    value_loss            | 250          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0426275] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 48           |
|    time_elapsed          | 778          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.005327679  |
|    clip_fraction         | 0.0457       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 27.1         |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0448       |
|    lagrangian_multiplier | 0.0862       |
|    learning_rate         | 0.0003       |
|    loss                  | 78.5         |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00736     |
|    std                   | 0.972        |
|    value_loss            | 960          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1928252] |
| time/                    |              |
|    fps                   | 89           |
|    iterations            | 28           |
|    time_elapsed          | 640          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.0051993923 |
|    clip_fraction         | 0.0568       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.679        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0137       |
|    lagrangian_multiplier | 0.0613       |
|    learning_rate         | 0.0003       |
|    loss                  | 87.9         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00765     |
|    std                   | 1            |
|    value_loss            | 756          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5783441] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 32           |
|    time_elapsed          | 573          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.00560446   |
|    clip_fraction         | 0.0431       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.529        |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.000265    |
|    lagrangian_multiplier | 0.0796       |
|    learning_rate         | 0.0003       |
|    loss                  | 8.97         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00361     |
|    std                   | 0.951        |
|    value_loss            | 107          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4654588] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 48           |
|    time_elapsed          | 777          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.008007002  |
|    clip_fraction         | 0.0776       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 254          |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.022        |
|    lagrangian_multiplier | 0.0472       |
|    learning_rate         | 0.0003       |
|    loss                  | 170          |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00918     |
|    std                   | 1.03         |
|    value_loss            | 982          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1481112] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 49           |
|    time_elapsed          | 794          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.006376677  |
|    clip_fraction         | 0.0568       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 49           |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0495       |
|    lagrangian_multiplier | 0.0939       |
|    learning_rate         | 0.0003       |
|    loss                  | 61.1         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 0.979        |
|    value_loss            | 818          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ
wandb:             train/approx_kl ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÑ‚ñÉ‚ñà‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñÅ‚ñÜ‚ñÑ‚ñÖ‚ñÜ
wandb:         train/clip_fraction ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÅ‚ñÖ‚ñÇ‚ñÉ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÜ‚ñÇ‚ñÉ‚ñÖ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñá‚ñà‚ñÑ‚ñÇ‚ñà‚ñÜ‚ñÉ‚ñÜ‚ñÑ‚ñÅ‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÑ
wandb:          train/entropy_loss ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÜ
wandb:    train/explained_variance ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ
wandb: train/lagrangian_multiplier ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñá‚ñá‚ñÖ‚ñÑ‚ñà‚ñÖ‚ñÜ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÉ‚ñá‚ñá‚ñÉ‚ñÜ‚ñÇ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñÖ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÅ‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñá‚ñá‚ñÜ‚ñà‚ñÑ‚ñá‚ñÑ‚ñÇ
wandb:                   train/std ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ
wandb:            train/value_loss ‚ñÅ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñá‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñà‚ñÜ
wandb: 
wandb: Run summary:
wandb:                      reward -1.14811
wandb:             train/approx_kl 0.00638
wandb:         train/clip_fraction 0.05679
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 49.03025
wandb:          train/entropy_loss -2.78502
wandb:    train/explained_variance 0.04954
wandb: train/lagrangian_multiplier 0.09391
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 61.07221
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.01007
wandb:                   train/std 0.97866
wandb:            train/value_loss 818.16633
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/2vlyyznb
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_201144-2vlyyznb/logs
------------------------------------------
| reward                   | [-1.839866] |
| time/                    |             |
|    fps                   | 90          |
|    iterations            | 29          |
|    time_elapsed          | 657         |
|    total_timesteps       | 59392       |
| train/                   |             |
|    approx_kl             | 0.005580659 |
|    clip_fraction         | 0.0491      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.833       |
|    entropy_loss          | -2.84       |
|    explained_variance    | 0.0554      |
|    lagrangian_multiplier | 0.0597      |
|    learning_rate         | 0.0003      |
|    loss                  | 65.4        |
|    n_updates             | 280         |
|    policy_gradient_loss  | -0.00873    |
|    std                   | 1           |
|    value_loss            | 763         |
------------------------------------------
srun: error: dqn.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-2.0454655] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 33           |
|    time_elapsed          | 591          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0063833212 |
|    clip_fraction         | 0.0402       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 14.5         |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.00879      |
|    lagrangian_multiplier | 0.0686       |
|    learning_rate         | 0.0003       |
|    loss                  | 14.2         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00296     |
|    std                   | 0.945        |
|    value_loss            | 146          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.97236556] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 49            |
|    time_elapsed          | 793           |
|    total_timesteps       | 100352        |
| train/                   |               |
|    approx_kl             | 0.006052728   |
|    clip_fraction         | 0.0538        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 255           |
|    entropy_loss          | -2.89         |
|    explained_variance    | 0.0266        |
|    lagrangian_multiplier | 0.0497        |
|    learning_rate         | 0.0003        |
|    loss                  | 111           |
|    n_updates             | 480           |
|    policy_gradient_loss  | -0.00999      |
|    std                   | 1.02          |
|    value_loss            | 838           |
--------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                      reward ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÑ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÅ‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñá
wandb:             train/approx_kl ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñÅ‚ñÜ‚ñÜ‚ñÑ
wandb:         train/clip_fraction ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñÜ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÑ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñà‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñÖ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà
wandb:          train/entropy_loss ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ
wandb:    train/explained_variance ‚ñÖ‚ñÇ‚ñá‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÜ‚ñà‚ñÜ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñá‚ñá‚ñÇ‚ñÉ‚ñá‚ñá
wandb: train/lagrangian_multiplier ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñà‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñà‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÜ‚ñà‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÇ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÇ‚ñà‚ñÑ‚ñá‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñà‚ñÖ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÅ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ
wandb:                   train/std ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÑ
wandb:            train/value_loss ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÅ‚ñà‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÖ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÜ‚ñÇ‚ñÉ‚ñÅ‚ñÜ‚ñÖ
wandb: 
wandb: Run summary:
wandb:                      reward -0.97237
wandb:             train/approx_kl 0.00605
wandb:         train/clip_fraction 0.05376
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 254.5659
wandb:          train/entropy_loss -2.88553
wandb:    train/explained_variance 0.02662
wandb: train/lagrangian_multiplier 0.04974
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 110.74275
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00999
wandb:                   train/std 1.02444
wandb:            train/value_loss 838.17528
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2/runs/noxwuniy
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_201157-noxwuniy/logs
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114548.12
srun: error: gail.ist.berkeley.edu: task 0: Exited with exit code 1
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_202521-lt22mw1s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16/runs/lt22mw1s
-------------------------------------------
| reward                   | [-1.3693047] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 34           |
|    time_elapsed          | 609          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0054460885 |
|    clip_fraction         | 0.0586       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 61.3         |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.0423       |
|    lagrangian_multiplier | 0.0729       |
|    learning_rate         | 0.0003       |
|    loss                  | 22.8         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.0107      |
|    std                   | 0.958        |
|    value_loss            | 219          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7901019] |
| time/                    |              |
|    fps                   | 90           |
|    iterations            | 30           |
|    time_elapsed          | 678          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0075276587 |
|    clip_fraction         | 0.0785       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.22         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.00839     |
|    lagrangian_multiplier | 0.0621       |
|    learning_rate         | 0.0003       |
|    loss                  | 38.8         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.0104      |
|    std                   | 0.998        |
|    value_loss            | 375          |
-------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.59647775] |
| time/              |               |
|    fps             | 132           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
-------------------------------------------
| reward                   | [-0.6614354] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 35           |
|    time_elapsed          | 627          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0072636344 |
|    clip_fraction         | 0.0851       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 186          |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.0306       |
|    lagrangian_multiplier | 0.0415       |
|    learning_rate         | 0.0003       |
|    loss                  | 141          |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00802     |
|    std                   | 0.969        |
|    value_loss            | 889          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.68999]  |
| time/                    |             |
|    fps                   | 90          |
|    iterations            | 31          |
|    time_elapsed          | 698         |
|    total_timesteps       | 63488       |
| train/                   |             |
|    approx_kl             | 0.006061188 |
|    clip_fraction         | 0.0541      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.249       |
|    entropy_loss          | -2.84       |
|    explained_variance    | 0.0476      |
|    lagrangian_multiplier | 0.0628      |
|    learning_rate         | 0.0003      |
|    loss                  | 52.8        |
|    n_updates             | 300         |
|    policy_gradient_loss  | -0.00779    |
|    std                   | 1           |
|    value_loss            | 597         |
------------------------------------------
-------------------------------------------
| reward                   | [-0.8140809] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 2            |
|    time_elapsed          | 31           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0053686257 |
|    clip_fraction         | 0.04         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.136        |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0216      |
|    lagrangian_multiplier | 0.0734       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.1         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00628     |
|    std                   | 1            |
|    value_loss            | 684          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114548.13
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_202601-qzefy8k7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/qzefy8k7
-------------------------------------------
| reward                   | [-1.0247011] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 36           |
|    time_elapsed          | 645          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.006111814  |
|    clip_fraction         | 0.066        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 88.3         |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.0408       |
|    lagrangian_multiplier | 0.0543       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.7         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00845     |
|    std                   | 0.967        |
|    value_loss            | 438          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9373931] |
| time/                    |              |
|    fps                   | 91           |
|    iterations            | 32           |
|    time_elapsed          | 714          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.0053624073 |
|    clip_fraction         | 0.0326       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.401        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0443       |
|    lagrangian_multiplier | 0.0611       |
|    learning_rate         | 0.0003       |
|    loss                  | 80.2         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00455     |
|    std                   | 0.994        |
|    value_loss            | 852          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7350772] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 3            |
|    time_elapsed          | 48           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.004675942  |
|    clip_fraction         | 0.0449       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.425        |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.00369     |
|    lagrangian_multiplier | 0.0711       |
|    learning_rate         | 0.0003       |
|    loss                  | 27.3         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00549     |
|    std                   | 0.997        |
|    value_loss            | 333          |
-------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.58440197] |
| time/              |               |
|    fps             | 133           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
-------------------------------------------
| reward                   | [-1.2699895] |
| time/                    |              |
|    fps                   | 92           |
|    iterations            | 33           |
|    time_elapsed          | 731          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.006070247  |
|    clip_fraction         | 0.0522       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.9          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.00981      |
|    lagrangian_multiplier | 0.0734       |
|    learning_rate         | 0.0003       |
|    loss                  | 73.5         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00704     |
|    std                   | 1            |
|    value_loss            | 818          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.65979415] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 37            |
|    time_elapsed          | 663           |
|    total_timesteps       | 75776         |
| train/                   |               |
|    approx_kl             | 0.005626899   |
|    clip_fraction         | 0.0578        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 32.3          |
|    entropy_loss          | -2.76         |
|    explained_variance    | -0.0133       |
|    lagrangian_multiplier | 0.0629        |
|    learning_rate         | 0.0003        |
|    loss                  | 29.4          |
|    n_updates             | 360           |
|    policy_gradient_loss  | -0.00768      |
|    std                   | 0.975         |
|    value_loss            | 269           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.5719537] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 4            |
|    time_elapsed          | 64           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.0061566974 |
|    clip_fraction         | 0.0461       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.29         |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0346       |
|    lagrangian_multiplier | 0.0682       |
|    learning_rate         | 0.0003       |
|    loss                  | 66.2         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.0072      |
|    std                   | 0.997        |
|    value_loss            | 749          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.92917013] |
| time/                    |               |
|    fps                   | 129           |
|    iterations            | 2             |
|    time_elapsed          | 31            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.005948405   |
|    clip_fraction         | 0.0485        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 4.42          |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.0185        |
|    lagrangian_multiplier | 0.0674        |
|    learning_rate         | 0.0003        |
|    loss                  | 46            |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00454      |
|    std                   | 1             |
|    value_loss            | 436           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.6002587] |
| time/                    |              |
|    fps                   | 93           |
|    iterations            | 34           |
|    time_elapsed          | 747          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.006382731  |
|    clip_fraction         | 0.053        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0392       |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0681       |
|    lagrangian_multiplier | 0.0731       |
|    learning_rate         | 0.0003       |
|    loss                  | 84.5         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00703     |
|    std                   | 0.982        |
|    value_loss            | 852          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6222612] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 38           |
|    time_elapsed          | 681          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.004698876  |
|    clip_fraction         | 0.0468       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 98.7         |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0337       |
|    lagrangian_multiplier | 0.0568       |
|    learning_rate         | 0.0003       |
|    loss                  | 66.2         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00683     |
|    std                   | 0.984        |
|    value_loss            | 444          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.4253997] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 5            |
|    time_elapsed          | 81           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0057237614 |
|    clip_fraction         | 0.0361       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0896       |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.089        |
|    lagrangian_multiplier | 0.0639       |
|    learning_rate         | 0.0003       |
|    loss                  | 32.4         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00441     |
|    std                   | 0.989        |
|    value_loss            | 392          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.88244617] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 3             |
|    time_elapsed          | 47            |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.0054621706  |
|    clip_fraction         | 0.0576        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 53.9          |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.0189        |
|    lagrangian_multiplier | 0.0519        |
|    learning_rate         | 0.0003        |
|    loss                  | 60.6          |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00616      |
|    std                   | 1.01          |
|    value_loss            | 471           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.8649963] |
| time/                    |              |
|    fps                   | 93           |
|    iterations            | 35           |
|    time_elapsed          | 763          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0068287635 |
|    clip_fraction         | 0.0632       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.36         |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0438       |
|    lagrangian_multiplier | 0.0578       |
|    learning_rate         | 0.0003       |
|    loss                  | 63.6         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00718     |
|    std                   | 0.985        |
|    value_loss            | 581          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.4732618] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 39           |
|    time_elapsed          | 699          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0048390008 |
|    clip_fraction         | 0.0408       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 12.5         |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0357      |
|    lagrangian_multiplier | 0.0811       |
|    learning_rate         | 0.0003       |
|    loss                  | 9.93         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00482     |
|    std                   | 0.968        |
|    value_loss            | 116          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0896797] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 6            |
|    time_elapsed          | 97           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0052075814 |
|    clip_fraction         | 0.0455       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.166        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0356       |
|    lagrangian_multiplier | 0.0621       |
|    learning_rate         | 0.0003       |
|    loss                  | 39.9         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.0065      |
|    std                   | 1            |
|    value_loss            | 297          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.5409386] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 4            |
|    time_elapsed          | 63           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.005522712  |
|    clip_fraction         | 0.0347       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.12         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0264      |
|    lagrangian_multiplier | 0.0837       |
|    learning_rate         | 0.0003       |
|    loss                  | 49.5         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00642     |
|    std                   | 0.995        |
|    value_loss            | 590          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5718893] |
| time/                    |              |
|    fps                   | 94           |
|    iterations            | 36           |
|    time_elapsed          | 779          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0064294827 |
|    clip_fraction         | 0.0571       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.25         |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0156       |
|    lagrangian_multiplier | 0.065        |
|    learning_rate         | 0.0003       |
|    loss                  | 49.9         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00926     |
|    std                   | 0.961        |
|    value_loss            | 507          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6720295] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 40           |
|    time_elapsed          | 717          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.0077154064 |
|    clip_fraction         | 0.0913       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 117          |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0291       |
|    lagrangian_multiplier | 0.0513       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.7         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.0119      |
|    std                   | 0.983        |
|    value_loss            | 642          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9159984] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 7            |
|    time_elapsed          | 114          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.005432551  |
|    clip_fraction         | 0.04         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0908       |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.00182      |
|    lagrangian_multiplier | 0.0651       |
|    learning_rate         | 0.0003       |
|    loss                  | 34.7         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00586     |
|    std                   | 0.991        |
|    value_loss            | 337          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.38948327] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 5             |
|    time_elapsed          | 80            |
|    total_timesteps       | 10240         |
| train/                   |               |
|    approx_kl             | 0.0056588612  |
|    clip_fraction         | 0.0611        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 193           |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.0502        |
|    lagrangian_multiplier | 0.0537        |
|    learning_rate         | 0.0003        |
|    loss                  | 98.3          |
|    n_updates             | 40            |
|    policy_gradient_loss  | -0.00546      |
|    std                   | 1.01          |
|    value_loss            | 679           |
--------------------------------------------
------------------------------------------
| reward                   | [-1.433339] |
| time/                    |             |
|    fps                   | 95          |
|    iterations            | 37          |
|    time_elapsed          | 796         |
|    total_timesteps       | 75776       |
| train/                   |             |
|    approx_kl             | 0.006681009 |
|    clip_fraction         | 0.055       |
|    clip_range            | 0.2         |
|    cost_value_loss       | 1.11        |
|    entropy_loss          | -2.76       |
|    explained_variance    | 0.0397      |
|    lagrangian_multiplier | 0.0647      |
|    learning_rate         | 0.0003      |
|    loss                  | 77.5        |
|    n_updates             | 360         |
|    policy_gradient_loss  | -0.00601    |
|    std                   | 0.961       |
|    value_loss            | 773         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.38453123] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 41            |
|    time_elapsed          | 735           |
|    total_timesteps       | 83968         |
| train/                   |               |
|    approx_kl             | 0.006939104   |
|    clip_fraction         | 0.0634        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 203           |
|    entropy_loss          | -2.8          |
|    explained_variance    | 0.0383        |
|    lagrangian_multiplier | 0.0431        |
|    learning_rate         | 0.0003        |
|    loss                  | 111           |
|    n_updates             | 400           |
|    policy_gradient_loss  | -0.00951      |
|    std                   | 0.987         |
|    value_loss            | 881           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.74857384] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 8             |
|    time_elapsed          | 130           |
|    total_timesteps       | 16384         |
| train/                   |               |
|    approx_kl             | 0.0056568813  |
|    clip_fraction         | 0.0431        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.22          |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.0136       |
|    lagrangian_multiplier | 0.0591        |
|    learning_rate         | 0.0003        |
|    loss                  | 46            |
|    n_updates             | 70            |
|    policy_gradient_loss  | -0.00612      |
|    std                   | 0.996         |
|    value_loss            | 419           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.7303214] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 6            |
|    time_elapsed          | 96           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0052125156 |
|    clip_fraction         | 0.0468       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 41.1         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0282       |
|    lagrangian_multiplier | 0.0684       |
|    learning_rate         | 0.0003       |
|    loss                  | 30           |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00702     |
|    std                   | 1            |
|    value_loss            | 342          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8932902] |
| time/                    |              |
|    fps                   | 95           |
|    iterations            | 38           |
|    time_elapsed          | 812          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0065967655 |
|    clip_fraction         | 0.0503       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0513       |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.068        |
|    lagrangian_multiplier | 0.084        |
|    learning_rate         | 0.0003       |
|    loss                  | 45.9         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00744     |
|    std                   | 0.963        |
|    value_loss            | 620          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8634784] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 42           |
|    time_elapsed          | 753          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.006332858  |
|    clip_fraction         | 0.0527       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 21           |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0265       |
|    lagrangian_multiplier | 0.0703       |
|    learning_rate         | 0.0003       |
|    loss                  | 18.7         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.0106      |
|    std                   | 0.985        |
|    value_loss            | 173          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2531977] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 9            |
|    time_elapsed          | 146          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.004925316  |
|    clip_fraction         | 0.0321       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.422        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.031        |
|    lagrangian_multiplier | 0.0591       |
|    learning_rate         | 0.0003       |
|    loss                  | 34           |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00527     |
|    std                   | 0.993        |
|    value_loss            | 312          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9965538] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 7            |
|    time_elapsed          | 112          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.0043521505 |
|    clip_fraction         | 0.0469       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 2.74         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0534       |
|    lagrangian_multiplier | 0.0799       |
|    learning_rate         | 0.0003       |
|    loss                  | 36.4         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00436     |
|    std                   | 1.01         |
|    value_loss            | 451          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4437648] |
| time/                    |              |
|    fps                   | 96           |
|    iterations            | 39           |
|    time_elapsed          | 828          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0060916115 |
|    clip_fraction         | 0.0732       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.081        |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.0376       |
|    lagrangian_multiplier | 0.0737       |
|    learning_rate         | 0.0003       |
|    loss                  | 79.7         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00893     |
|    std                   | 0.96         |
|    value_loss            | 903          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3751364] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 10           |
|    time_elapsed          | 163          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.006565555  |
|    clip_fraction         | 0.0495       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.276        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0378       |
|    lagrangian_multiplier | 0.0478       |
|    learning_rate         | 0.0003       |
|    loss                  | 79.9         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00893     |
|    std                   | 0.985        |
|    value_loss            | 645          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6490079] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 43           |
|    time_elapsed          | 771          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.005255185  |
|    clip_fraction         | 0.0532       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.52         |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0142       |
|    lagrangian_multiplier | 0.0813       |
|    learning_rate         | 0.0003       |
|    loss                  | 8.88         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00584     |
|    std                   | 0.976        |
|    value_loss            | 105          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.94497555] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 8             |
|    time_elapsed          | 128           |
|    total_timesteps       | 16384         |
| train/                   |               |
|    approx_kl             | 0.005954638   |
|    clip_fraction         | 0.0657        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 183           |
|    entropy_loss          | -2.86         |
|    explained_variance    | 0.017         |
|    lagrangian_multiplier | 0.065         |
|    learning_rate         | 0.0003        |
|    loss                  | 91.8          |
|    n_updates             | 70            |
|    policy_gradient_loss  | -0.00701      |
|    std                   | 1.02          |
|    value_loss            | 728           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2452726] |
| time/                    |              |
|    fps                   | 96           |
|    iterations            | 40           |
|    time_elapsed          | 844          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.0083757695 |
|    clip_fraction         | 0.0844       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.57         |
|    entropy_loss          | -2.75        |
|    explained_variance    | 0.0248       |
|    lagrangian_multiplier | 0.0615       |
|    learning_rate         | 0.0003       |
|    loss                  | 66.4         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.014       |
|    std                   | 0.958        |
|    value_loss            | 655          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.53776264] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 11            |
|    time_elapsed          | 179           |
|    total_timesteps       | 22528         |
| train/                   |               |
|    approx_kl             | 0.004674784   |
|    clip_fraction         | 0.0425        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.234         |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.0398        |
|    lagrangian_multiplier | 0.0475        |
|    learning_rate         | 0.0003        |
|    loss                  | 125           |
|    n_updates             | 100           |
|    policy_gradient_loss  | -0.00541      |
|    std                   | 0.994         |
|    value_loss            | 943           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.30402952] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 44            |
|    time_elapsed          | 789           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.0069114156  |
|    clip_fraction         | 0.0636        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 106           |
|    entropy_loss          | -2.78         |
|    explained_variance    | 0.0428        |
|    lagrangian_multiplier | 0.0545        |
|    learning_rate         | 0.0003        |
|    loss                  | 60.4          |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.0128       |
|    std                   | 0.975         |
|    value_loss            | 318           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.1913283] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 9            |
|    time_elapsed          | 144          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0060073803 |
|    clip_fraction         | 0.053        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 127          |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0619       |
|    lagrangian_multiplier | 0.0571       |
|    learning_rate         | 0.0003       |
|    loss                  | 76.6         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00612     |
|    std                   | 1.02         |
|    value_loss            | 494          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.719365]  |
| time/                    |              |
|    fps                   | 97           |
|    iterations            | 41           |
|    time_elapsed          | 861          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0056398734 |
|    clip_fraction         | 0.063        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.27         |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.0316       |
|    lagrangian_multiplier | 0.0577       |
|    learning_rate         | 0.0003       |
|    loss                  | 101          |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00859     |
|    std                   | 0.972        |
|    value_loss            | 845          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5180354] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 12           |
|    time_elapsed          | 196          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0059110904 |
|    clip_fraction         | 0.0417       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.334        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0741       |
|    lagrangian_multiplier | 0.047        |
|    learning_rate         | 0.0003       |
|    loss                  | 64.2         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00586     |
|    std                   | 0.997        |
|    value_loss            | 585          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1047888] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 10           |
|    time_elapsed          | 161          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0064848633 |
|    clip_fraction         | 0.0683       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.28         |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0242       |
|    lagrangian_multiplier | 0.0695       |
|    learning_rate         | 0.0003       |
|    loss                  | 32.9         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00572     |
|    std                   | 1.01         |
|    value_loss            | 356          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.610313]  |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 45           |
|    time_elapsed          | 807          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.0072323116 |
|    clip_fraction         | 0.0839       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 224          |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0236       |
|    lagrangian_multiplier | 0.0452       |
|    learning_rate         | 0.0003       |
|    loss                  | 187          |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00955     |
|    std                   | 0.965        |
|    value_loss            | 1.37e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9943035] |
| time/                    |              |
|    fps                   | 98           |
|    iterations            | 42           |
|    time_elapsed          | 877          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0049157944 |
|    clip_fraction         | 0.0463       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0377       |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0464       |
|    lagrangian_multiplier | 0.0714       |
|    learning_rate         | 0.0003       |
|    loss                  | 55.9         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00726     |
|    std                   | 0.964        |
|    value_loss            | 609          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.73319644] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 13            |
|    time_elapsed          | 212           |
|    total_timesteps       | 26624         |
| train/                   |               |
|    approx_kl             | 0.005973582   |
|    clip_fraction         | 0.0557        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.147         |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.0509        |
|    lagrangian_multiplier | 0.0386        |
|    learning_rate         | 0.0003        |
|    loss                  | 99.4          |
|    n_updates             | 120           |
|    policy_gradient_loss  | -0.00644      |
|    std                   | 1.01          |
|    value_loss            | 769           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.5778818] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 11           |
|    time_elapsed          | 177          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0050817095 |
|    clip_fraction         | 0.0465       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 62.1         |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0745       |
|    lagrangian_multiplier | 0.0696       |
|    learning_rate         | 0.0003       |
|    loss                  | 53.1         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00677     |
|    std                   | 1.02         |
|    value_loss            | 525          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9078409] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 46           |
|    time_elapsed          | 824          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.007250381  |
|    clip_fraction         | 0.0684       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 121          |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0395       |
|    lagrangian_multiplier | 0.055        |
|    learning_rate         | 0.0003       |
|    loss                  | 85.4         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.0116      |
|    std                   | 0.975        |
|    value_loss            | 535          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5289744] |
| time/                    |              |
|    fps                   | 98           |
|    iterations            | 43           |
|    time_elapsed          | 893          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0063942475 |
|    clip_fraction         | 0.0479       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.304        |
|    entropy_loss          | -2.75        |
|    explained_variance    | 0.0392       |
|    lagrangian_multiplier | 0.0685       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.4         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00692     |
|    std                   | 0.953        |
|    value_loss            | 529          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.0555319] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 14           |
|    time_elapsed          | 229          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.005086637  |
|    clip_fraction         | 0.0463       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.112        |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0266       |
|    lagrangian_multiplier | 0.0742       |
|    learning_rate         | 0.0003       |
|    loss                  | 19.6         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.0071      |
|    std                   | 1.01         |
|    value_loss            | 243          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2280474] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 12           |
|    time_elapsed          | 194          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0067835944 |
|    clip_fraction         | 0.084        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 66.2         |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0557       |
|    lagrangian_multiplier | 0.0716       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.6         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00962     |
|    std                   | 1.03         |
|    value_loss            | 518          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6888405] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 47           |
|    time_elapsed          | 842          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.009913405  |
|    clip_fraction         | 0.104        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 214          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0189       |
|    lagrangian_multiplier | 0.0409       |
|    learning_rate         | 0.0003       |
|    loss                  | 227          |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.0135      |
|    std                   | 0.974        |
|    value_loss            | 1.52e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9915458] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 15           |
|    time_elapsed          | 245          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0039261384 |
|    clip_fraction         | 0.0325       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0124       |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.053        |
|    lagrangian_multiplier | 0.0844       |
|    learning_rate         | 0.0003       |
|    loss                  | 13.8         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.0032      |
|    std                   | 1.01         |
|    value_loss            | 157          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3412501] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 13           |
|    time_elapsed          | 210          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0051908856 |
|    clip_fraction         | 0.0701       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.58         |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.0483       |
|    lagrangian_multiplier | 0.0723       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.5         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00671     |
|    std                   | 1.03         |
|    value_loss            | 579          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3687032] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 48           |
|    time_elapsed          | 860          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.0059756157 |
|    clip_fraction         | 0.0509       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 106          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0474       |
|    lagrangian_multiplier | 0.0601       |
|    learning_rate         | 0.0003       |
|    loss                  | 63.2         |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00796     |
|    std                   | 0.98         |
|    value_loss            | 667          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.95228165] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 16            |
|    time_elapsed          | 261           |
|    total_timesteps       | 32768         |
| train/                   |               |
|    approx_kl             | 0.0065893237  |
|    clip_fraction         | 0.0668        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0662        |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.0396        |
|    lagrangian_multiplier | 0.0536        |
|    learning_rate         | 0.0003        |
|    loss                  | 41.8          |
|    n_updates             | 150           |
|    policy_gradient_loss  | -0.00616      |
|    std                   | 1             |
|    value_loss            | 353           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.3491709] |
| time/                    |              |
|    fps                   | 96           |
|    iterations            | 44           |
|    time_elapsed          | 937          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.007277686  |
|    clip_fraction         | 0.0695       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.901        |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0452       |
|    lagrangian_multiplier | 0.0634       |
|    learning_rate         | 0.0003       |
|    loss                  | 48.4         |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.0116      |
|    std                   | 0.942        |
|    value_loss            | 539          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6862218] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 14           |
|    time_elapsed          | 226          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.005687798  |
|    clip_fraction         | 0.043        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 104          |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.0332       |
|    lagrangian_multiplier | 0.066        |
|    learning_rate         | 0.0003       |
|    loss                  | 86.8         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00829     |
|    std                   | 1.03         |
|    value_loss            | 762          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1454694] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 49           |
|    time_elapsed          | 878          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.004488183  |
|    clip_fraction         | 0.0213       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 15.5         |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0831      |
|    lagrangian_multiplier | 0.0907       |
|    learning_rate         | 0.0003       |
|    loss                  | 13.1         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.00332     |
|    std                   | 0.96         |
|    value_loss            | 161          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÇ‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñÑ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÑ‚ñá‚ñà‚ñÖ‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ
wandb:             train/approx_kl ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñà‚ñÉ‚ñÅ
wandb:         train/clip_fraction ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñÑ‚ñÅ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÜ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÅ‚ñÑ‚ñá‚ñá‚ñÑ‚ñÅ
wandb:          train/entropy_loss ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb:    train/explained_variance ‚ñà‚ñÖ‚ñÜ‚ñÑ‚ñÇ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÖ‚ñá‚ñÉ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñà‚ñÅ
wandb: train/lagrangian_multiplier ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñá‚ñÇ‚ñÖ‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñà
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñà‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÜ‚ñÇ‚ñÅ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñá‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÇ‚ñÑ‚ñà‚ñà‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÇ‚ñÉ‚ñÜ‚ñÅ‚ñÑ‚ñÅ‚ñÖ‚ñà
wandb:                   train/std ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ
wandb:            train/value_loss ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÉ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                      reward -1.14547
wandb:             train/approx_kl 0.00449
wandb:         train/clip_fraction 0.02129
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 15.54168
wandb:          train/entropy_loss -2.76306
wandb:    train/explained_variance -0.08306
wandb: train/lagrangian_multiplier 0.09067
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 13.10512
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00332
wandb:                   train/std 0.95981
wandb:            train/value_loss 161.30286
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2/runs/44wz606g
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_201515-44wz606g/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/usr/lib/python3.9/threading.py", line 954, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.9/threading.py", line 892, in run
    self._target(*self._args, **self._kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 259, in check_network_status
    self._loop_check_status(
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 215, in _loop_check_status
    local_handle = request()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface.py", line 795, in deliver_network_status
    return self._deliver_network_status(status)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py", line 601, in _deliver_network_status
    return self._deliver_record(record)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py", line 560, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
-------------------------------------------
| reward                   | [-1.3536577] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 17           |
|    time_elapsed          | 278          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.006795706  |
|    clip_fraction         | 0.0518       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.272        |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0619       |
|    lagrangian_multiplier | 0.0472       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.6         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00787     |
|    std                   | 1            |
|    value_loss            | 467          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.33103567] |
| time/                    |               |
|    fps                   | 96            |
|    iterations            | 45            |
|    time_elapsed          | 954           |
|    total_timesteps       | 92160         |
| train/                   |               |
|    approx_kl             | 0.0072279563  |
|    clip_fraction         | 0.082         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.249         |
|    entropy_loss          | -2.72         |
|    explained_variance    | 0.013         |
|    lagrangian_multiplier | 0.0603        |
|    learning_rate         | 0.0003        |
|    loss                  | 55.2          |
|    n_updates             | 440           |
|    policy_gradient_loss  | -0.00906      |
|    std                   | 0.952         |
|    value_loss            | 538           |
--------------------------------------------
srun: error: ddpg.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-1.9381864] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 15           |
|    time_elapsed          | 242          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0063600414 |
|    clip_fraction         | 0.0375       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 150          |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.0513       |
|    lagrangian_multiplier | 0.0562       |
|    learning_rate         | 0.0003       |
|    loss                  | 101          |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00638     |
|    std                   | 1.03         |
|    value_loss            | 743          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1430993] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 18           |
|    time_elapsed          | 294          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.005792359  |
|    clip_fraction         | 0.0462       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.55         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0214      |
|    lagrangian_multiplier | 0.0629       |
|    learning_rate         | 0.0003       |
|    loss                  | 28.2         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00752     |
|    std                   | 1            |
|    value_loss            | 305          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8780928] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 16           |
|    time_elapsed          | 259          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.004839962  |
|    clip_fraction         | 0.0416       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 68.9         |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0719       |
|    lagrangian_multiplier | 0.0735       |
|    learning_rate         | 0.0003       |
|    loss                  | 49.6         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00749     |
|    std                   | 1.02         |
|    value_loss            | 518          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.83660066] |
| time/                    |               |
|    fps                   | 96            |
|    iterations            | 46            |
|    time_elapsed          | 978           |
|    total_timesteps       | 94208         |
| train/                   |               |
|    approx_kl             | 0.004883323   |
|    clip_fraction         | 0.0495        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.274         |
|    entropy_loss          | -2.73         |
|    explained_variance    | 0.049         |
|    lagrangian_multiplier | 0.0756        |
|    learning_rate         | 0.0003        |
|    loss                  | 65            |
|    n_updates             | 450           |
|    policy_gradient_loss  | -0.00528      |
|    std                   | 0.951         |
|    value_loss            | 681           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.7513562] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 19           |
|    time_elapsed          | 311          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0060744463 |
|    clip_fraction         | 0.0668       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.542        |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0119       |
|    lagrangian_multiplier | 0.0529       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.6         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00907     |
|    std                   | 0.999        |
|    value_loss            | 362          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1577852] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 17           |
|    time_elapsed          | 275          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0059362417 |
|    clip_fraction         | 0.0484       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 65           |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0826       |
|    lagrangian_multiplier | 0.0675       |
|    learning_rate         | 0.0003       |
|    loss                  | 55.1         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00748     |
|    std                   | 1.02         |
|    value_loss            | 465          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0631187] |
| time/                    |              |
|    fps                   | 96           |
|    iterations            | 47           |
|    time_elapsed          | 994          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.008870121  |
|    clip_fraction         | 0.0952       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.71         |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0224       |
|    lagrangian_multiplier | 0.0644       |
|    learning_rate         | 0.0003       |
|    loss                  | 79.4         |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.0113      |
|    std                   | 0.944        |
|    value_loss            | 760          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114548.14
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_203047-9gehkoio
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16/runs/9gehkoio
------------------------------------------
| reward                   | [-2.068458] |
| time/                    |             |
|    fps                   | 124         |
|    iterations            | 20          |
|    time_elapsed          | 327         |
|    total_timesteps       | 40960       |
| train/                   |             |
|    approx_kl             | 0.005632281 |
|    clip_fraction         | 0.0445      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.139       |
|    entropy_loss          | -2.84       |
|    explained_variance    | -0.0363     |
|    lagrangian_multiplier | 0.0873      |
|    learning_rate         | 0.0003      |
|    loss                  | 30.9        |
|    n_updates             | 190         |
|    policy_gradient_loss  | -0.00588    |
|    std                   | 1.01        |
|    value_loss            | 347         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.237048]  |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 18           |
|    time_elapsed          | 291          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0067476146 |
|    clip_fraction         | 0.0551       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 46           |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0496       |
|    lagrangian_multiplier | 0.0706       |
|    learning_rate         | 0.0003       |
|    loss                  | 40.1         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00824     |
|    std                   | 1.01         |
|    value_loss            | 450          |
-------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.49573934] |
| time/              |               |
|    fps             | 118           |
|    iterations      | 1             |
|    time_elapsed    | 17            |
|    total_timesteps | 2048          |
--------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.8959302] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 21           |
|    time_elapsed          | 344          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.007346948  |
|    clip_fraction         | 0.0679       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.377        |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.00974      |
|    lagrangian_multiplier | 0.0574       |
|    learning_rate         | 0.0003       |
|    loss                  | 93.6         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00923     |
|    std                   | 1.01         |
|    value_loss            | 838          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9322146] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 19           |
|    time_elapsed          | 307          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0047977436 |
|    clip_fraction         | 0.0236       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 89.5         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0702       |
|    lagrangian_multiplier | 0.0691       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.3         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00372     |
|    std                   | 1            |
|    value_loss            | 493          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0219319] |
| time/                    |              |
|    fps                   | 95           |
|    iterations            | 48           |
|    time_elapsed          | 1029         |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.005240812  |
|    clip_fraction         | 0.0625       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.717        |
|    entropy_loss          | -2.71        |
|    explained_variance    | 0.0442       |
|    lagrangian_multiplier | 0.0779       |
|    learning_rate         | 0.0003       |
|    loss                  | 103          |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.0046      |
|    std                   | 0.943        |
|    value_loss            | 1.3e+03      |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.39906287] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 22            |
|    time_elapsed          | 360           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.006226718   |
|    clip_fraction         | 0.0509        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.436         |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.0302       |
|    lagrangian_multiplier | 0.0659        |
|    learning_rate         | 0.0003        |
|    loss                  | 23.5          |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.00795      |
|    std                   | 1.01          |
|    value_loss            | 265           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.7894357] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 2            |
|    time_elapsed          | 35           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0056534507 |
|    clip_fraction         | 0.041        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.025        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0189       |
|    lagrangian_multiplier | 0.0599       |
|    learning_rate         | 0.0003       |
|    loss                  | 41.9         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00601     |
|    std                   | 0.992        |
|    value_loss            | 413          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3015555] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 20           |
|    time_elapsed          | 324          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.006427473  |
|    clip_fraction         | 0.0719       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 145          |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0403       |
|    lagrangian_multiplier | 0.0547       |
|    learning_rate         | 0.0003       |
|    loss                  | 106          |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00749     |
|    std                   | 1.01         |
|    value_loss            | 701          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5490032] |
| time/                    |              |
|    fps                   | 95           |
|    iterations            | 49           |
|    time_elapsed          | 1045         |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.0091143595 |
|    clip_fraction         | 0.08         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.86         |
|    entropy_loss          | -2.71        |
|    explained_variance    | 0.0567       |
|    lagrangian_multiplier | 0.0736       |
|    learning_rate         | 0.0003       |
|    loss                  | 70.5         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.00922     |
|    std                   | 0.945        |
|    value_loss            | 1.03e+03     |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                      reward ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñà‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñà‚ñÜ‚ñÖ‚ñÑ
wandb:             train/approx_kl ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñÇ‚ñà
wandb:         train/clip_fraction ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñà‚ñÑ‚ñÜ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ
wandb:          train/entropy_loss ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:    train/explained_variance ‚ñÖ‚ñÅ‚ñÇ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÜ‚ñÜ‚ñÑ‚ñà‚ñÑ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñá
wandb: train/lagrangian_multiplier ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÜ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÑ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñá‚ñÜ‚ñà‚ñá‚ñÉ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñÅ‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñÉ‚ñà‚ñÖ
wandb:                   train/std ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:            train/value_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñÜ
wandb: 
wandb: Run summary:
wandb:                      reward -1.549
wandb:             train/approx_kl 0.00911
wandb:         train/clip_fraction 0.07998
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 3.85777
wandb:          train/entropy_loss -2.70866
wandb:    train/explained_variance 0.0567
wandb: train/lagrangian_multiplier 0.07357
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 70.51915
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00922
wandb:                   train/std 0.94465
wandb:            train/value_loss 1034.68768
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/2o9isek1
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231227_041406-2o9isek1/logs
--------------------------------------------
| reward                   | [-0.68494487] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 23            |
|    time_elapsed          | 377           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.0068394886  |
|    clip_fraction         | 0.0581        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.301         |
|    entropy_loss          | -2.87         |
|    explained_variance    | -0.012        |
|    lagrangian_multiplier | 0.0617        |
|    learning_rate         | 0.0003        |
|    loss                  | 59.6          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00897      |
|    std                   | 1.02          |
|    value_loss            | 723           |
--------------------------------------------
srun: error: airl.ist.berkeley.edu: task 0: Exited with exit code 1
------------------------------------------
| reward                   | [-0.928511] |
| time/                    |             |
|    fps                   | 114         |
|    iterations            | 3           |
|    time_elapsed          | 53          |
|    total_timesteps       | 6144        |
| train/                   |             |
|    approx_kl             | 0.004755778 |
|    clip_fraction         | 0.0334      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0693      |
|    entropy_loss          | -2.81       |
|    explained_variance    | 0.0559      |
|    lagrangian_multiplier | 0.0358      |
|    learning_rate         | 0.0003      |
|    loss                  | 50.5        |
|    n_updates             | 20          |
|    policy_gradient_loss  | -0.00518    |
|    std                   | 0.984       |
|    value_loss            | 349         |
------------------------------------------
------------------------------------------
| reward                   | [-1.209916] |
| time/                    |             |
|    fps                   | 126         |
|    iterations            | 21          |
|    time_elapsed          | 340         |
|    total_timesteps       | 43008       |
| train/                   |             |
|    approx_kl             | 0.00431218  |
|    clip_fraction         | 0.0305      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 9.6         |
|    entropy_loss          | -2.85       |
|    explained_variance    | 0.0836      |
|    lagrangian_multiplier | 0.0867      |
|    learning_rate         | 0.0003      |
|    loss                  | 28.5        |
|    n_updates             | 200         |
|    policy_gradient_loss  | -0.00527    |
|    std                   | 1.01        |
|    value_loss            | 356         |
------------------------------------------
-------------------------------------------
| reward                   | [-0.8318833] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 24           |
|    time_elapsed          | 393          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.00662053   |
|    clip_fraction         | 0.0549       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.153        |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.00358     |
|    lagrangian_multiplier | 0.0661       |
|    learning_rate         | 0.0003       |
|    loss                  | 43.8         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.0107      |
|    std                   | 1.03         |
|    value_loss            | 418          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8767061] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 22           |
|    time_elapsed          | 356          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.007670691  |
|    clip_fraction         | 0.0676       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0116       |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.106        |
|    lagrangian_multiplier | 0.0823       |
|    learning_rate         | 0.0003       |
|    loss                  | 23.5         |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00683     |
|    std                   | 0.989        |
|    value_loss            | 315          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.41200143] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 4             |
|    time_elapsed          | 71            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.0053811288  |
|    clip_fraction         | 0.0501        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0915        |
|    entropy_loss          | -2.81         |
|    explained_variance    | 0.0235        |
|    lagrangian_multiplier | 0.0363        |
|    learning_rate         | 0.0003        |
|    loss                  | 81.4          |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00435      |
|    std                   | 0.983         |
|    value_loss            | 618           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.48407692] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 25            |
|    time_elapsed          | 409           |
|    total_timesteps       | 51200         |
| train/                   |               |
|    approx_kl             | 0.006349631   |
|    clip_fraction         | 0.0598        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.165         |
|    entropy_loss          | -2.89         |
|    explained_variance    | 0.0405        |
|    lagrangian_multiplier | 0.0531        |
|    learning_rate         | 0.0003        |
|    loss                  | 58.6          |
|    n_updates             | 240           |
|    policy_gradient_loss  | -0.00657      |
|    std                   | 1.02          |
|    value_loss            | 513           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.5763568] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 23           |
|    time_elapsed          | 373          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.0047821486 |
|    clip_fraction         | 0.0435       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 24           |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0749       |
|    lagrangian_multiplier | 0.0738       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.1         |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.00581     |
|    std                   | 0.984        |
|    value_loss            | 596          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.76146394] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 5             |
|    time_elapsed          | 90            |
|    total_timesteps       | 10240         |
| train/                   |               |
|    approx_kl             | 0.0064740432  |
|    clip_fraction         | 0.068         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0668        |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.0266       |
|    lagrangian_multiplier | 0.0485        |
|    learning_rate         | 0.0003        |
|    loss                  | 87.7          |
|    n_updates             | 40            |
|    policy_gradient_loss  | -0.0107       |
|    std                   | 1.01          |
|    value_loss            | 816           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.82326776] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 26            |
|    time_elapsed          | 426           |
|    total_timesteps       | 53248         |
| train/                   |               |
|    approx_kl             | 0.0044754082  |
|    clip_fraction         | 0.041         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.109         |
|    entropy_loss          | -2.89         |
|    explained_variance    | -0.0585       |
|    lagrangian_multiplier | 0.0493        |
|    learning_rate         | 0.0003        |
|    loss                  | 44            |
|    n_updates             | 250           |
|    policy_gradient_loss  | -0.00442      |
|    std                   | 1.02          |
|    value_loss            | 345           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.2505114] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 24           |
|    time_elapsed          | 389          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.004838436  |
|    clip_fraction         | 0.0576       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 8.88         |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0711       |
|    lagrangian_multiplier | 0.0701       |
|    learning_rate         | 0.0003       |
|    loss                  | 59.7         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00506     |
|    std                   | 0.979        |
|    value_loss            | 630          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0817153] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 6            |
|    time_elapsed          | 108          |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0046082637 |
|    clip_fraction         | 0.0377       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0333       |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0182       |
|    lagrangian_multiplier | 0.0467       |
|    learning_rate         | 0.0003       |
|    loss                  | 45.4         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00515     |
|    std                   | 1            |
|    value_loss            | 323          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.52667135] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 27            |
|    time_elapsed          | 442           |
|    total_timesteps       | 55296         |
| train/                   |               |
|    approx_kl             | 0.0072506554  |
|    clip_fraction         | 0.0681        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.128         |
|    entropy_loss          | -2.88         |
|    explained_variance    | -0.0101       |
|    lagrangian_multiplier | 0.0522        |
|    learning_rate         | 0.0003        |
|    loss                  | 20.9          |
|    n_updates             | 260           |
|    policy_gradient_loss  | -0.00818      |
|    std                   | 1.02          |
|    value_loss            | 171           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.7893507] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 25           |
|    time_elapsed          | 405          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.004624727  |
|    clip_fraction         | 0.04         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.436        |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0492       |
|    lagrangian_multiplier | 0.0763       |
|    learning_rate         | 0.0003       |
|    loss                  | 49.2         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00397     |
|    std                   | 0.978        |
|    value_loss            | 624          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.53862584] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 7             |
|    time_elapsed          | 126           |
|    total_timesteps       | 14336         |
| train/                   |               |
|    approx_kl             | 0.0047630854  |
|    clip_fraction         | 0.0414        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.181         |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.015        |
|    lagrangian_multiplier | 0.0496        |
|    learning_rate         | 0.0003        |
|    loss                  | 67.2          |
|    n_updates             | 60            |
|    policy_gradient_loss  | -0.00788      |
|    std                   | 1.01          |
|    value_loss            | 471           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.64016664] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 28            |
|    time_elapsed          | 459           |
|    total_timesteps       | 57344         |
| train/                   |               |
|    approx_kl             | 0.005242496   |
|    clip_fraction         | 0.0382        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.262         |
|    entropy_loss          | -2.87         |
|    explained_variance    | -0.0137       |
|    lagrangian_multiplier | 0.0875        |
|    learning_rate         | 0.0003        |
|    loss                  | 26.3          |
|    n_updates             | 270           |
|    policy_gradient_loss  | -0.00595      |
|    std                   | 1.01          |
|    value_loss            | 340           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.0112334] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 26           |
|    time_elapsed          | 422          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0067889052 |
|    clip_fraction         | 0.0592       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0221       |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0658       |
|    lagrangian_multiplier | 0.0822       |
|    learning_rate         | 0.0003       |
|    loss                  | 35.1         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00379     |
|    std                   | 0.984        |
|    value_loss            | 487          |
-------------------------------------------
srun: Job 114548 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114548.15
------------------------------------------
| reward                   | [-1.049815] |
| time/                    |             |
|    fps                   | 113         |
|    iterations            | 8           |
|    time_elapsed          | 144         |
|    total_timesteps       | 16384       |
| train/                   |             |
|    approx_kl             | 0.005438998 |
|    clip_fraction         | 0.0378      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.147       |
|    entropy_loss          | -2.84       |
|    explained_variance    | 0.012       |
|    lagrangian_multiplier | 0.0559      |
|    learning_rate         | 0.0003      |
|    loss                  | 136         |
|    n_updates             | 70          |
|    policy_gradient_loss  | -0.007      |
|    std                   | 1           |
|    value_loss            | 1.06e+03    |
------------------------------------------
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231227_043313-pbzne7n4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/pbzne7n4
-------------------------------------------
| reward                   | [-0.5482574] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 29           |
|    time_elapsed          | 475          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.0060517224 |
|    clip_fraction         | 0.0726       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0886       |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.0241      |
|    lagrangian_multiplier | 0.054        |
|    learning_rate         | 0.0003       |
|    loss                  | 24.9         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00865     |
|    std                   | 1.01         |
|    value_loss            | 218          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.79680884] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 27            |
|    time_elapsed          | 438           |
|    total_timesteps       | 55296         |
| train/                   |               |
|    approx_kl             | 0.0062964614  |
|    clip_fraction         | 0.0722        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 19.2          |
|    entropy_loss          | -2.81         |
|    explained_variance    | 0.0711        |
|    lagrangian_multiplier | 0.0663        |
|    learning_rate         | 0.0003        |
|    loss                  | 46.2          |
|    n_updates             | 260           |
|    policy_gradient_loss  | -0.0079       |
|    std                   | 0.995         |
|    value_loss            | 435           |
--------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.61960673] |
| time/              |               |
|    fps             | 134           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
-------------------------------------------
| reward                   | [-1.5026686] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 9            |
|    time_elapsed          | 163          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.006040814  |
|    clip_fraction         | 0.0522       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0244       |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.00441     |
|    lagrangian_multiplier | 0.0572       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.1         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00775     |
|    std                   | 0.989        |
|    value_loss            | 512          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.92122746] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 30            |
|    time_elapsed          | 492           |
|    total_timesteps       | 61440         |
| train/                   |               |
|    approx_kl             | 0.009201182   |
|    clip_fraction         | 0.0734        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0257        |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.248        |
|    lagrangian_multiplier | 0.0511        |
|    learning_rate         | 0.0003        |
|    loss                  | 15.9          |
|    n_updates             | 290           |
|    policy_gradient_loss  | -0.00566      |
|    std                   | 1             |
|    value_loss            | 129           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.0846534] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 28           |
|    time_elapsed          | 454          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.004618707  |
|    clip_fraction         | 0.0406       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.02         |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0795       |
|    lagrangian_multiplier | 0.0767       |
|    learning_rate         | 0.0003       |
|    loss                  | 37.7         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00555     |
|    std                   | 0.991        |
|    value_loss            | 437          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.913651]  |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 10           |
|    time_elapsed          | 181          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0056598987 |
|    clip_fraction         | 0.0502       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0713       |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0216       |
|    lagrangian_multiplier | 0.0501       |
|    learning_rate         | 0.0003       |
|    loss                  | 113          |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00836     |
|    std                   | 1.01         |
|    value_loss            | 763          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5608475] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 31           |
|    time_elapsed          | 508          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.007924045  |
|    clip_fraction         | 0.0841       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0314       |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.162       |
|    lagrangian_multiplier | 0.0483       |
|    learning_rate         | 0.0003       |
|    loss                  | 15.5         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00721     |
|    std                   | 1            |
|    value_loss            | 135          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.133535] |
| time/                    |             |
|    fps                   | 126         |
|    iterations            | 29          |
|    time_elapsed          | 470         |
|    total_timesteps       | 59392       |
| train/                   |             |
|    approx_kl             | 0.006198544 |
|    clip_fraction         | 0.0591      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 140         |
|    entropy_loss          | -2.8        |
|    explained_variance    | 0.0384      |
|    lagrangian_multiplier | 0.0672      |
|    learning_rate         | 0.0003      |
|    loss                  | 82.2        |
|    n_updates             | 280         |
|    policy_gradient_loss  | -0.00629    |
|    std                   | 0.982       |
|    value_loss            | 968         |
------------------------------------------
-------------------------------------------
| reward                   | [-0.7479686] |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 2            |
|    time_elapsed          | 39           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.006506319  |
|    clip_fraction         | 0.0514       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.973        |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0078      |
|    lagrangian_multiplier | 0.0518       |
|    learning_rate         | 0.0003       |
|    loss                  | 33.5         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00773     |
|    std                   | 0.994        |
|    value_loss            | 302          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.74470955] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 32            |
|    time_elapsed          | 524           |
|    total_timesteps       | 65536         |
| train/                   |               |
|    approx_kl             | 0.006518106   |
|    clip_fraction         | 0.061         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0672        |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.0354       |
|    lagrangian_multiplier | 0.0761        |
|    learning_rate         | 0.0003        |
|    loss                  | 12            |
|    n_updates             | 310           |
|    policy_gradient_loss  | -0.0058       |
|    std                   | 1             |
|    value_loss            | 151           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.3070133] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 11           |
|    time_elapsed          | 199          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.00763434   |
|    clip_fraction         | 0.0782       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0909       |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.00056      |
|    lagrangian_multiplier | 0.0489       |
|    learning_rate         | 0.0003       |
|    loss                  | 118          |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00883     |
|    std                   | 0.985        |
|    value_loss            | 1.05e+03     |
-------------------------------------------
------------------------------------------
| reward                   | [-1.307985] |
| time/                    |             |
|    fps                   | 126         |
|    iterations            | 30          |
|    time_elapsed          | 486         |
|    total_timesteps       | 61440       |
| train/                   |             |
|    approx_kl             | 0.007434626 |
|    clip_fraction         | 0.0714      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 211         |
|    entropy_loss          | -2.79       |
|    explained_variance    | 0.0449      |
|    lagrangian_multiplier | 0.0567      |
|    learning_rate         | 0.0003      |
|    loss                  | 108         |
|    n_updates             | 290         |
|    policy_gradient_loss  | -0.00942    |
|    std                   | 0.974       |
|    value_loss            | 751         |
------------------------------------------
-------------------------------------------
| reward                   | [-0.6721878] |
| time/                    |              |
|    fps                   | 110          |
|    iterations            | 3            |
|    time_elapsed          | 55           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0047059    |
|    clip_fraction         | 0.033        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0548       |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0261       |
|    lagrangian_multiplier | 0.0672       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.4         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00419     |
|    std                   | 1.01         |
|    value_loss            | 613          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.44498655] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 33            |
|    time_elapsed          | 541           |
|    total_timesteps       | 67584         |
| train/                   |               |
|    approx_kl             | 0.006091942   |
|    clip_fraction         | 0.0447        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0819        |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.0354       |
|    lagrangian_multiplier | 0.0652        |
|    learning_rate         | 0.0003        |
|    loss                  | 21.8          |
|    n_updates             | 320           |
|    policy_gradient_loss  | -0.00576      |
|    std                   | 0.998         |
|    value_loss            | 205           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.1214284] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 31           |
|    time_elapsed          | 503          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0063932547 |
|    clip_fraction         | 0.0483       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 208          |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0369       |
|    lagrangian_multiplier | 0.0614       |
|    learning_rate         | 0.0003       |
|    loss                  | 102          |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00756     |
|    std                   | 0.97         |
|    value_loss            | 755          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6509881] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 12           |
|    time_elapsed          | 218          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.004087489  |
|    clip_fraction         | 0.0512       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.088        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.00656      |
|    lagrangian_multiplier | 0.0407       |
|    learning_rate         | 0.0003       |
|    loss                  | 108          |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.007       |
|    std                   | 0.986        |
|    value_loss            | 818          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6290502] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 4            |
|    time_elapsed          | 72           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.004735035  |
|    clip_fraction         | 0.0477       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.133        |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0212       |
|    lagrangian_multiplier | 0.0507       |
|    learning_rate         | 0.0003       |
|    loss                  | 45.7         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00562     |
|    std                   | 1.01         |
|    value_loss            | 347          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.39208925] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 34            |
|    time_elapsed          | 557           |
|    total_timesteps       | 69632         |
| train/                   |               |
|    approx_kl             | 0.00514185    |
|    clip_fraction         | 0.0349        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.036         |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.0487       |
|    lagrangian_multiplier | 0.058         |
|    learning_rate         | 0.0003        |
|    loss                  | 16.6          |
|    n_updates             | 330           |
|    policy_gradient_loss  | -0.00292      |
|    std                   | 0.999         |
|    value_loss            | 138           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.5226953] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 32           |
|    time_elapsed          | 519          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.006158946  |
|    clip_fraction         | 0.0472       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 162          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0391       |
|    lagrangian_multiplier | 0.0621       |
|    learning_rate         | 0.0003       |
|    loss                  | 105          |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00587     |
|    std                   | 0.978        |
|    value_loss            | 854          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.539873] |
| time/                    |             |
|    fps                   | 115         |
|    iterations            | 5           |
|    time_elapsed          | 88          |
|    total_timesteps       | 10240       |
| train/                   |             |
|    approx_kl             | 0.00557384  |
|    clip_fraction         | 0.049       |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.938       |
|    entropy_loss          | -2.87       |
|    explained_variance    | -0.0308     |
|    lagrangian_multiplier | 0.0606      |
|    learning_rate         | 0.0003      |
|    loss                  | 39          |
|    n_updates             | 40          |
|    policy_gradient_loss  | -0.006      |
|    std                   | 1.02        |
|    value_loss            | 469         |
------------------------------------------
-------------------------------------------
| reward                   | [-2.1109958] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 13           |
|    time_elapsed          | 236          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0059486832 |
|    clip_fraction         | 0.0524       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0782       |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0143       |
|    lagrangian_multiplier | 0.0474       |
|    learning_rate         | 0.0003       |
|    loss                  | 158          |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00754     |
|    std                   | 0.986        |
|    value_loss            | 1.23e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0355818] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 35           |
|    time_elapsed          | 574          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.00625161   |
|    clip_fraction         | 0.0392       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0144       |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0325       |
|    lagrangian_multiplier | 0.102        |
|    learning_rate         | 0.0003       |
|    loss                  | 3.52         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00325     |
|    std                   | 0.996        |
|    value_loss            | 48.3         |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6820898] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 33           |
|    time_elapsed          | 536          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0050997105 |
|    clip_fraction         | 0.0588       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 169          |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0458       |
|    lagrangian_multiplier | 0.0537       |
|    learning_rate         | 0.0003       |
|    loss                  | 85.3         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00611     |
|    std                   | 0.979        |
|    value_loss            | 562          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0763272] |
| time/                    |              |
|    fps                   | 117          |
|    iterations            | 6            |
|    time_elapsed          | 104          |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.003649696  |
|    clip_fraction         | 0.0252       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.108        |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.00325      |
|    lagrangian_multiplier | 0.0632       |
|    learning_rate         | 0.0003       |
|    loss                  | 24.9         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00495     |
|    std                   | 1.02         |
|    value_loss            | 264          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.4569304] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 14           |
|    time_elapsed          | 254          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0061920118 |
|    clip_fraction         | 0.0484       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.12         |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0071       |
|    lagrangian_multiplier | 0.0403       |
|    learning_rate         | 0.0003       |
|    loss                  | 147          |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00745     |
|    std                   | 0.979        |
|    value_loss            | 1.02e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.4335556] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 36           |
|    time_elapsed          | 590          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.007950256  |
|    clip_fraction         | 0.0733       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.126        |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0857      |
|    lagrangian_multiplier | 0.0565       |
|    learning_rate         | 0.0003       |
|    loss                  | 19.1         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0066      |
|    std                   | 0.981        |
|    value_loss            | 158          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4758463] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 34           |
|    time_elapsed          | 552          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0065907706 |
|    clip_fraction         | 0.0604       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 172          |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0457       |
|    lagrangian_multiplier | 0.0678       |
|    learning_rate         | 0.0003       |
|    loss                  | 112          |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00791     |
|    std                   | 0.982        |
|    value_loss            | 826          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8744567] |
| time/                    |              |
|    fps                   | 118          |
|    iterations            | 7            |
|    time_elapsed          | 121          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.005104252  |
|    clip_fraction         | 0.0414       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 8.64         |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.033        |
|    lagrangian_multiplier | 0.0536       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.9         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00816     |
|    std                   | 1.02         |
|    value_loss            | 488          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.309691]  |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 15           |
|    time_elapsed          | 273          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0054666987 |
|    clip_fraction         | 0.0377       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.117        |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.00561      |
|    lagrangian_multiplier | 0.0467       |
|    learning_rate         | 0.0003       |
|    loss                  | 179          |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00658     |
|    std                   | 0.978        |
|    value_loss            | 1.39e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5496093] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 37           |
|    time_elapsed          | 607          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.00647162   |
|    clip_fraction         | 0.0625       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0459       |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0145      |
|    lagrangian_multiplier | 0.0503       |
|    learning_rate         | 0.0003       |
|    loss                  | 25.5         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00485     |
|    std                   | 0.97         |
|    value_loss            | 199          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5303704] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 35           |
|    time_elapsed          | 568          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0077042086 |
|    clip_fraction         | 0.0601       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 160          |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0491       |
|    lagrangian_multiplier | 0.0674       |
|    learning_rate         | 0.0003       |
|    loss                  | 69.1         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00794     |
|    std                   | 0.971        |
|    value_loss            | 697          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5519323] |
| time/                    |              |
|    fps                   | 118          |
|    iterations            | 8            |
|    time_elapsed          | 137          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0038056518 |
|    clip_fraction         | 0.0306       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.262        |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.00411      |
|    lagrangian_multiplier | 0.0749       |
|    learning_rate         | 0.0003       |
|    loss                  | 21.6         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00714     |
|    std                   | 1.02         |
|    value_loss            | 264          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.5189114] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 16           |
|    time_elapsed          | 291          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0065897694 |
|    clip_fraction         | 0.045        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0624       |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.014        |
|    lagrangian_multiplier | 0.0381       |
|    learning_rate         | 0.0003       |
|    loss                  | 156          |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00642     |
|    std                   | 0.978        |
|    value_loss            | 1.34e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.054684]  |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 38           |
|    time_elapsed          | 623          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0046478743 |
|    clip_fraction         | 0.0243       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0107       |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0299       |
|    lagrangian_multiplier | 0.0947       |
|    learning_rate         | 0.0003       |
|    loss                  | 5.5          |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00388     |
|    std                   | 0.965        |
|    value_loss            | 77.4         |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2857816] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 36           |
|    time_elapsed          | 585          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0072203325 |
|    clip_fraction         | 0.0567       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 143          |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0538       |
|    lagrangian_multiplier | 0.0641       |
|    learning_rate         | 0.0003       |
|    loss                  | 64.1         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00685     |
|    std                   | 0.966        |
|    value_loss            | 547          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.93434423] |
| time/                    |               |
|    fps                   | 119           |
|    iterations            | 9             |
|    time_elapsed          | 154           |
|    total_timesteps       | 18432         |
| train/                   |               |
|    approx_kl             | 0.0065924604  |
|    clip_fraction         | 0.0539        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.207         |
|    entropy_loss          | -2.87         |
|    explained_variance    | -0.0055       |
|    lagrangian_multiplier | 0.063         |
|    learning_rate         | 0.0003        |
|    loss                  | 58.2          |
|    n_updates             | 80            |
|    policy_gradient_loss  | -0.00842      |
|    std                   | 1.01          |
|    value_loss            | 520           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.7111566] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 17           |
|    time_elapsed          | 309          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0067536845 |
|    clip_fraction         | 0.0478       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0989       |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.00951      |
|    lagrangian_multiplier | 0.0488       |
|    learning_rate         | 0.0003       |
|    loss                  | 206          |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00573     |
|    std                   | 0.972        |
|    value_loss            | 1.77e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6909798] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 37           |
|    time_elapsed          | 601          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.006225652  |
|    clip_fraction         | 0.0654       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 168          |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0467       |
|    lagrangian_multiplier | 0.0553       |
|    learning_rate         | 0.0003       |
|    loss                  | 85.4         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00882     |
|    std                   | 0.973        |
|    value_loss            | 641          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5305576] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 39           |
|    time_elapsed          | 640          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.006461215  |
|    clip_fraction         | 0.054        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0649       |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.062        |
|    lagrangian_multiplier | 0.0378       |
|    learning_rate         | 0.0003       |
|    loss                  | 92.9         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00641     |
|    std                   | 0.973        |
|    value_loss            | 698          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2445483] |
| time/                    |              |
|    fps                   | 120          |
|    iterations            | 10           |
|    time_elapsed          | 170          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.006009145  |
|    clip_fraction         | 0.0495       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0911       |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0141      |
|    lagrangian_multiplier | 0.0653       |
|    learning_rate         | 0.0003       |
|    loss                  | 44.6         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00642     |
|    std                   | 0.999        |
|    value_loss            | 444          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.981434] |
| time/                    |             |
|    fps                   | 112         |
|    iterations            | 18          |
|    time_elapsed          | 328         |
|    total_timesteps       | 36864       |
| train/                   |             |
|    approx_kl             | 0.004576618 |
|    clip_fraction         | 0.0508      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0596      |
|    entropy_loss          | -2.78       |
|    explained_variance    | 0.0236      |
|    lagrangian_multiplier | 0.0349      |
|    learning_rate         | 0.0003      |
|    loss                  | 160         |
|    n_updates             | 170         |
|    policy_gradient_loss  | -0.00454    |
|    std                   | 0.967       |
|    value_loss            | 1.02e+03    |
------------------------------------------
-------------------------------------------
| reward                   | [-1.8871374] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 38           |
|    time_elapsed          | 617          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.008234153  |
|    clip_fraction         | 0.0828       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 206          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0433       |
|    lagrangian_multiplier | 0.061        |
|    learning_rate         | 0.0003       |
|    loss                  | 89.5         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00822     |
|    std                   | 0.97         |
|    value_loss            | 675          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9844055] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 40           |
|    time_elapsed          | 657          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.0067305258 |
|    clip_fraction         | 0.0478       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.059        |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0381       |
|    lagrangian_multiplier | 0.0605       |
|    learning_rate         | 0.0003       |
|    loss                  | 32.4         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00784     |
|    std                   | 0.981        |
|    value_loss            | 334          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0242635] |
| time/                    |              |
|    fps                   | 120          |
|    iterations            | 11           |
|    time_elapsed          | 186          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0039690435 |
|    clip_fraction         | 0.0267       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0972       |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0388      |
|    lagrangian_multiplier | 0.0722       |
|    learning_rate         | 0.0003       |
|    loss                  | 40.3         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00298     |
|    std                   | 0.994        |
|    value_loss            | 383          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7706535] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 19           |
|    time_elapsed          | 346          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0055985716 |
|    clip_fraction         | 0.0522       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0788       |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.00639      |
|    lagrangian_multiplier | 0.0503       |
|    learning_rate         | 0.0003       |
|    loss                  | 106          |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00937     |
|    std                   | 0.974        |
|    value_loss            | 800          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7624258] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 39           |
|    time_elapsed          | 634          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0068982197 |
|    clip_fraction         | 0.0614       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 206          |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0402       |
|    lagrangian_multiplier | 0.058        |
|    learning_rate         | 0.0003       |
|    loss                  | 89.3         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00699     |
|    std                   | 0.969        |
|    value_loss            | 671          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3057365] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 41           |
|    time_elapsed          | 673          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.004629863  |
|    clip_fraction         | 0.0513       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0555       |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0873       |
|    lagrangian_multiplier | 0.0362       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.8         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00312     |
|    std                   | 0.976        |
|    value_loss            | 361          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8404555] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 40           |
|    time_elapsed          | 650          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.0067612496 |
|    clip_fraction         | 0.0622       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 204          |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0343       |
|    lagrangian_multiplier | 0.0617       |
|    learning_rate         | 0.0003       |
|    loss                  | 73           |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00718     |
|    std                   | 0.972        |
|    value_loss            | 539          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7000768] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 42           |
|    time_elapsed          | 690          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0036455947 |
|    clip_fraction         | 0.0479       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0545       |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0978       |
|    lagrangian_multiplier | 0.0392       |
|    learning_rate         | 0.0003       |
|    loss                  | 33.2         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00767     |
|    std                   | 0.982        |
|    value_loss            | 238          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2729175] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 20           |
|    time_elapsed          | 364          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.004838207  |
|    clip_fraction         | 0.0424       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0575       |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0262       |
|    lagrangian_multiplier | 0.0441       |
|    learning_rate         | 0.0003       |
|    loss                  | 60.1         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00426     |
|    std                   | 0.975        |
|    value_loss            | 473          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1677647] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 12           |
|    time_elapsed          | 218          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.006939684  |
|    clip_fraction         | 0.063        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.03         |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0139      |
|    lagrangian_multiplier | 0.0582       |
|    learning_rate         | 0.0003       |
|    loss                  | 33           |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00968     |
|    std                   | 0.997        |
|    value_loss            | 356          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8828406] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 41           |
|    time_elapsed          | 666          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.007051141  |
|    clip_fraction         | 0.0867       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 175          |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0401       |
|    lagrangian_multiplier | 0.0603       |
|    learning_rate         | 0.0003       |
|    loss                  | 77.7         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00821     |
|    std                   | 0.964        |
|    value_loss            | 634          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.47749257] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 43            |
|    time_elapsed          | 706           |
|    total_timesteps       | 88064         |
| train/                   |               |
|    approx_kl             | 0.0033462315  |
|    clip_fraction         | 0.0199        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0186        |
|    entropy_loss          | -2.81         |
|    explained_variance    | 0.0356        |
|    lagrangian_multiplier | 0.0992        |
|    learning_rate         | 0.0003        |
|    loss                  | 5.24          |
|    n_updates             | 420           |
|    policy_gradient_loss  | -0.00412      |
|    std                   | 0.989         |
|    value_loss            | 80.7          |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.87980396] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 13            |
|    time_elapsed          | 235           |
|    total_timesteps       | 26624         |
| train/                   |               |
|    approx_kl             | 0.0064720567  |
|    clip_fraction         | 0.0642        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 1.38          |
|    entropy_loss          | -2.83         |
|    explained_variance    | 0.00623       |
|    lagrangian_multiplier | 0.0691        |
|    learning_rate         | 0.0003        |
|    loss                  | 34.4          |
|    n_updates             | 120           |
|    policy_gradient_loss  | -0.00738      |
|    std                   | 0.997         |
|    value_loss            | 367           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.8341616] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 21           |
|    time_elapsed          | 383          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.007379829  |
|    clip_fraction         | 0.0735       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.173        |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0205       |
|    lagrangian_multiplier | 0.0512       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.9         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00798     |
|    std                   | 0.981        |
|    value_loss            | 368          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5351214] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 42           |
|    time_elapsed          | 682          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0090598    |
|    clip_fraction         | 0.107        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 187          |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0444       |
|    lagrangian_multiplier | 0.0607       |
|    learning_rate         | 0.0003       |
|    loss                  | 78.6         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.0137      |
|    std                   | 0.987        |
|    value_loss            | 643          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.53222376] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 44            |
|    time_elapsed          | 722           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.005672355   |
|    clip_fraction         | 0.0516        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0819        |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.0162        |
|    lagrangian_multiplier | 0.0535        |
|    learning_rate         | 0.0003        |
|    loss                  | 8.95          |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.0102       |
|    std                   | 0.993         |
|    value_loss            | 92.4          |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.331583]  |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 14           |
|    time_elapsed          | 251          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0053782305 |
|    clip_fraction         | 0.0627       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.22         |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.00562      |
|    lagrangian_multiplier | 0.0667       |
|    learning_rate         | 0.0003       |
|    loss                  | 40.8         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00869     |
|    std                   | 1            |
|    value_loss            | 461          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8869591] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 22           |
|    time_elapsed          | 401          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.0071607456 |
|    clip_fraction         | 0.0659       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.091        |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0197       |
|    lagrangian_multiplier | 0.0471       |
|    learning_rate         | 0.0003       |
|    loss                  | 53.5         |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00953     |
|    std                   | 0.982        |
|    value_loss            | 445          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.943703] |
| time/                    |             |
|    fps                   | 125         |
|    iterations            | 43          |
|    time_elapsed          | 699         |
|    total_timesteps       | 88064       |
| train/                   |             |
|    approx_kl             | 0.006761211 |
|    clip_fraction         | 0.0891      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 154         |
|    entropy_loss          | -2.81       |
|    explained_variance    | 0.0413      |
|    lagrangian_multiplier | 0.0626      |
|    learning_rate         | 0.0003      |
|    loss                  | 57.2        |
|    n_updates             | 420         |
|    policy_gradient_loss  | -0.0124     |
|    std                   | 0.985       |
|    value_loss            | 466         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.58409745] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 45            |
|    time_elapsed          | 739           |
|    total_timesteps       | 92160         |
| train/                   |               |
|    approx_kl             | 0.002655486   |
|    clip_fraction         | 0.0122        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0106        |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.016         |
|    lagrangian_multiplier | 0.1           |
|    learning_rate         | 0.0003        |
|    loss                  | 5.82          |
|    n_updates             | 440           |
|    policy_gradient_loss  | -0.00111      |
|    std                   | 0.99          |
|    value_loss            | 79.8          |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.5856483] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 15           |
|    time_elapsed          | 267          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.006358029  |
|    clip_fraction         | 0.0598       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.297        |
|    entropy_loss          | -2.85        |
|    explained_variance    | 2.06e-05     |
|    lagrangian_multiplier | 0.0616       |
|    learning_rate         | 0.0003       |
|    loss                  | 87.2         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00823     |
|    std                   | 1.01         |
|    value_loss            | 1.1e+03      |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.55368185] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 23            |
|    time_elapsed          | 419           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.0071953526  |
|    clip_fraction         | 0.0654        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.159         |
|    entropy_loss          | -2.79         |
|    explained_variance    | 0.0112        |
|    lagrangian_multiplier | 0.0507        |
|    learning_rate         | 0.0003        |
|    loss                  | 94.2          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00937      |
|    std                   | 0.97          |
|    value_loss            | 798           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.533793]  |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 44           |
|    time_elapsed          | 715          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.0068493653 |
|    clip_fraction         | 0.0611       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 179          |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0526       |
|    lagrangian_multiplier | 0.0568       |
|    learning_rate         | 0.0003       |
|    loss                  | 82           |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.00722     |
|    std                   | 0.999        |
|    value_loss            | 590          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.4970792] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 46           |
|    time_elapsed          | 755          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.008327753  |
|    clip_fraction         | 0.0827       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.103        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0378       |
|    lagrangian_multiplier | 0.0528       |
|    learning_rate         | 0.0003       |
|    loss                  | 20.3         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.012       |
|    std                   | 0.992        |
|    value_loss            | 201          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2961726] |
| time/                    |              |
|    fps                   | 115          |
|    iterations            | 16           |
|    time_elapsed          | 284          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.006841014  |
|    clip_fraction         | 0.0785       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 2.78         |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0238       |
|    lagrangian_multiplier | 0.0489       |
|    learning_rate         | 0.0003       |
|    loss                  | 97.4         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.0106      |
|    std                   | 1.01         |
|    value_loss            | 852          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.43165714] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 24            |
|    time_elapsed          | 437           |
|    total_timesteps       | 49152         |
| train/                   |               |
|    approx_kl             | 0.0063610547  |
|    clip_fraction         | 0.0604        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.158         |
|    entropy_loss          | -2.78         |
|    explained_variance    | 0.00378       |
|    lagrangian_multiplier | 0.0371        |
|    learning_rate         | 0.0003        |
|    loss                  | 81.7          |
|    n_updates             | 230           |
|    policy_gradient_loss  | -0.00819      |
|    std                   | 0.977         |
|    value_loss            | 610           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3212091] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 45           |
|    time_elapsed          | 731          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.007437345  |
|    clip_fraction         | 0.0695       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 196          |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0429       |
|    lagrangian_multiplier | 0.0507       |
|    learning_rate         | 0.0003       |
|    loss                  | 86.4         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00921     |
|    std                   | 1            |
|    value_loss            | 546          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.28078553] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 47            |
|    time_elapsed          | 772           |
|    total_timesteps       | 96256         |
| train/                   |               |
|    approx_kl             | 0.0050119776  |
|    clip_fraction         | 0.0469        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0149        |
|    entropy_loss          | -2.81         |
|    explained_variance    | -0.167        |
|    lagrangian_multiplier | 0.0557        |
|    learning_rate         | 0.0003        |
|    loss                  | 13.5          |
|    n_updates             | 460           |
|    policy_gradient_loss  | -0.00286      |
|    std                   | 0.986         |
|    value_loss            | 121           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.2316384] |
| time/                    |              |
|    fps                   | 115          |
|    iterations            | 17           |
|    time_elapsed          | 300          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.007737375  |
|    clip_fraction         | 0.065        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 7.65         |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.017        |
|    lagrangian_multiplier | 0.0523       |
|    learning_rate         | 0.0003       |
|    loss                  | 91.3         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00958     |
|    std                   | 0.996        |
|    value_loss            | 716          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.59344316] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 25            |
|    time_elapsed          | 455           |
|    total_timesteps       | 51200         |
| train/                   |               |
|    approx_kl             | 0.005968718   |
|    clip_fraction         | 0.0551        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.162         |
|    entropy_loss          | -2.78         |
|    explained_variance    | 0.0391        |
|    lagrangian_multiplier | 0.0449        |
|    learning_rate         | 0.0003        |
|    loss                  | 65.5          |
|    n_updates             | 240           |
|    policy_gradient_loss  | -0.0078       |
|    std                   | 0.966         |
|    value_loss            | 480           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.4289545] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 46           |
|    time_elapsed          | 748          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0077007078 |
|    clip_fraction         | 0.073        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 183          |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0447       |
|    lagrangian_multiplier | 0.0563       |
|    learning_rate         | 0.0003       |
|    loss                  | 82.8         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.00903     |
|    std                   | 1            |
|    value_loss            | 561          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.68686163] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 48            |
|    time_elapsed          | 788           |
|    total_timesteps       | 98304         |
| train/                   |               |
|    approx_kl             | 0.0056463624  |
|    clip_fraction         | 0.0688        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0208        |
|    entropy_loss          | -2.8          |
|    explained_variance    | -0.235        |
|    lagrangian_multiplier | 0.0582        |
|    learning_rate         | 0.0003        |
|    loss                  | 8.45          |
|    n_updates             | 470           |
|    policy_gradient_loss  | -0.00697      |
|    std                   | 0.985         |
|    value_loss            | 75.7          |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.3059404] |
| time/                    |              |
|    fps                   | 116          |
|    iterations            | 18           |
|    time_elapsed          | 317          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.008911157  |
|    clip_fraction         | 0.0851       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.96         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0171      |
|    lagrangian_multiplier | 0.0554       |
|    learning_rate         | 0.0003       |
|    loss                  | 68.7         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.0113      |
|    std                   | 1            |
|    value_loss            | 582          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6786603] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 26           |
|    time_elapsed          | 474          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0058734636 |
|    clip_fraction         | 0.0643       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.197        |
|    entropy_loss          | -2.75        |
|    explained_variance    | 0.0118       |
|    lagrangian_multiplier | 0.047        |
|    learning_rate         | 0.0003       |
|    loss                  | 33           |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00945     |
|    std                   | 0.95         |
|    value_loss            | 328          |
-------------------------------------------
------------------------------------------
| reward                   | [-0.694868] |
| time/                    |             |
|    fps                   | 125         |
|    iterations            | 47          |
|    time_elapsed          | 764         |
|    total_timesteps       | 96256       |
| train/                   |             |
|    approx_kl             | 0.008734539 |
|    clip_fraction         | 0.0754      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 182         |
|    entropy_loss          | -2.83       |
|    explained_variance    | 0.06        |
|    lagrangian_multiplier | 0.0552      |
|    learning_rate         | 0.0003      |
|    loss                  | 78.5        |
|    n_updates             | 460         |
|    policy_gradient_loss  | -0.00921    |
|    std                   | 0.992       |
|    value_loss            | 538         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.43369168] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 49            |
|    time_elapsed          | 805           |
|    total_timesteps       | 100352        |
| train/                   |               |
|    approx_kl             | 0.0039693285  |
|    clip_fraction         | 0.0408        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0181        |
|    entropy_loss          | -2.81         |
|    explained_variance    | -0.149        |
|    lagrangian_multiplier | 0.0503        |
|    learning_rate         | 0.0003        |
|    loss                  | 12.4          |
|    n_updates             | 480           |
|    policy_gradient_loss  | -0.00103      |
|    std                   | 0.988         |
|    value_loss            | 99.7          |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.2396057] |
| time/                    |              |
|    fps                   | 116          |
|    iterations            | 19           |
|    time_elapsed          | 333          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.00669363   |
|    clip_fraction         | 0.0663       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.734        |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0198      |
|    lagrangian_multiplier | 0.0563       |
|    learning_rate         | 0.0003       |
|    loss                  | 68.4         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.0088      |
|    std                   | 0.999        |
|    value_loss            | 601          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñà‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñà‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñà
wandb:             train/approx_kl ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÇ
wandb:         train/clip_fraction ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñá‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñá‚ñÜ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÅ‚ñÑ‚ñá‚ñÑ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÉ‚ñÜ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñÉ‚ñÜ‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:          train/entropy_loss ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ
wandb:    train/explained_variance ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÉ‚ñÅ‚ñÉ
wandb: train/lagrangian_multiplier ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñá‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñá‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñá‚ñÅ‚ñÑ‚ñÅ‚ñà‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÇ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÜ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÅ‚ñà‚ñá‚ñÉ‚ñà
wandb:                   train/std ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ
wandb:            train/value_loss ‚ñÜ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñÉ‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                      reward -0.43369
wandb:             train/approx_kl 0.00397
wandb:         train/clip_fraction 0.04077
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 0.0181
wandb:          train/entropy_loss -2.8073
wandb:    train/explained_variance -0.14913
wandb: train/lagrangian_multiplier 0.05027
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 12.44969
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00103
wandb:                   train/std 0.98834
wandb:            train/value_loss 99.68361
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16/runs/lt22mw1s
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_202521-lt22mw1s/logs
srun: error: dqn.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-0.4755511] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 27           |
|    time_elapsed          | 492          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.00667186   |
|    clip_fraction         | 0.0715       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.162        |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0159       |
|    lagrangian_multiplier | 0.0496       |
|    learning_rate         | 0.0003       |
|    loss                  | 34.1         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00843     |
|    std                   | 0.94         |
|    value_loss            | 342          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.83816165] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 48            |
|    time_elapsed          | 780           |
|    total_timesteps       | 98304         |
| train/                   |               |
|    approx_kl             | 0.008038653   |
|    clip_fraction         | 0.0797        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 196           |
|    entropy_loss          | -2.81         |
|    explained_variance    | 0.0505        |
|    lagrangian_multiplier | 0.0551        |
|    learning_rate         | 0.0003        |
|    loss                  | 78.9          |
|    n_updates             | 470           |
|    policy_gradient_loss  | -0.00903      |
|    std                   | 0.986         |
|    value_loss            | 479           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.7965958] |
| time/                    |              |
|    fps                   | 117          |
|    iterations            | 20           |
|    time_elapsed          | 349          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0067146653 |
|    clip_fraction         | 0.05         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 5.82         |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0113      |
|    lagrangian_multiplier | 0.0545       |
|    learning_rate         | 0.0003       |
|    loss                  | 76           |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.0082      |
|    std                   | 0.987        |
|    value_loss            | 619          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0501765] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 49           |
|    time_elapsed          | 796          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.008622304  |
|    clip_fraction         | 0.0948       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 199          |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0637       |
|    lagrangian_multiplier | 0.058        |
|    learning_rate         | 0.0003       |
|    loss                  | 64.2         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 0.983        |
|    value_loss            | 415          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1926304] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 28           |
|    time_elapsed          | 510          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.008759937  |
|    clip_fraction         | 0.0708       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.181        |
|    entropy_loss          | -2.71        |
|    explained_variance    | 0.0181       |
|    lagrangian_multiplier | 0.045        |
|    learning_rate         | 0.0003       |
|    loss                  | 54           |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.0103      |
|    std                   | 0.938        |
|    value_loss            | 421          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
-------------------------------------------
| reward                   | [-1.0357958] |
| time/                    |              |
|    fps                   | 117          |
|    iterations            | 21           |
|    time_elapsed          | 366          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0068938085 |
|    clip_fraction         | 0.0795       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 13.2         |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0097       |
|    lagrangian_multiplier | 0.051        |
|    learning_rate         | 0.0003       |
|    loss                  | 72.4         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.0111      |
|    std                   | 0.982        |
|    value_loss            | 609          |
-------------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñá‚ñÖ‚ñá‚ñá‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÜ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÖ
wandb:             train/approx_kl ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÜ‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÅ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñá
wandb:         train/clip_fraction ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñà‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñÉ‚ñÅ‚ñá‚ñÇ‚ñá‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñà
wandb:          train/entropy_loss ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ
wandb:    train/explained_variance ‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ
wandb: train/lagrangian_multiplier ‚ñÑ‚ñÅ‚ñá‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÇ‚ñà‚ñá‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÉ‚ñÑ‚ñÉ‚ñá‚ñÇ‚ñÜ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñÉ‚ñÑ‚ñÉ‚ñà‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñà‚ñá‚ñà‚ñÜ‚ñà‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÅ‚ñÇ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ
wandb:                   train/std ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ
wandb:            train/value_loss ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÅ‚ñÜ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñá‚ñá‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÇ‚ñÅ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñá‚ñá‚ñà‚ñÑ‚ñà‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ
wandb: 
wandb: Run summary:
wandb:                      reward -1.05018
wandb:             train/approx_kl 0.00862
wandb:         train/clip_fraction 0.09478
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 198.6931
wandb:          train/entropy_loss -2.80336
wandb:    train/explained_variance 0.06374
wandb: train/lagrangian_multiplier 0.05805
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 64.15354
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.01096
wandb:                   train/std 0.98256
wandb:            train/value_loss 414.9437
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/qzefy8k7
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_202601-qzefy8k7/logs
srun: error: gail.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-1.5165418] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 29           |
|    time_elapsed          | 528          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.005668377  |
|    clip_fraction         | 0.0531       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.128        |
|    entropy_loss          | -2.7         |
|    explained_variance    | -0.00123     |
|    lagrangian_multiplier | 0.0524       |
|    learning_rate         | 0.0003       |
|    loss                  | 93.3         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00685     |
|    std                   | 0.926        |
|    value_loss            | 612          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.51390064] |
| time/                    |               |
|    fps                   | 117           |
|    iterations            | 22            |
|    time_elapsed          | 383           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.0043376675  |
|    clip_fraction         | 0.0432        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 6.13          |
|    entropy_loss          | -2.8          |
|    explained_variance    | -0.00688      |
|    lagrangian_multiplier | 0.056         |
|    learning_rate         | 0.0003        |
|    loss                  | 38.1          |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.00688      |
|    std                   | 0.979         |
|    value_loss            | 347           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.63852674] |
| time/                    |               |
|    fps                   | 117           |
|    iterations            | 23            |
|    time_elapsed          | 399           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.006051317   |
|    clip_fraction         | 0.0581        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 1.17          |
|    entropy_loss          | -2.8          |
|    explained_variance    | -0.0221       |
|    lagrangian_multiplier | 0.0565        |
|    learning_rate         | 0.0003        |
|    loss                  | 40.9          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.0108       |
|    std                   | 0.979         |
|    value_loss            | 427           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.51377475] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 30            |
|    time_elapsed          | 547           |
|    total_timesteps       | 61440         |
| train/                   |               |
|    approx_kl             | 0.006315912   |
|    clip_fraction         | 0.0492        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.116         |
|    entropy_loss          | -2.68         |
|    explained_variance    | -0.00226      |
|    lagrangian_multiplier | 0.067         |
|    learning_rate         | 0.0003        |
|    loss                  | 110           |
|    n_updates             | 290           |
|    policy_gradient_loss  | -0.00882      |
|    std                   | 0.922         |
|    value_loss            | 991           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.5183409] |
| time/                    |              |
|    fps                   | 118          |
|    iterations            | 24           |
|    time_elapsed          | 416          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.005379313  |
|    clip_fraction         | 0.0417       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 7.06         |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0248       |
|    lagrangian_multiplier | 0.0694       |
|    learning_rate         | 0.0003       |
|    loss                  | 80           |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00804     |
|    std                   | 0.98         |
|    value_loss            | 941          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4283769] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 31           |
|    time_elapsed          | 565          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0058685765 |
|    clip_fraction         | 0.0549       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.127        |
|    entropy_loss          | -2.67        |
|    explained_variance    | 0.0159       |
|    lagrangian_multiplier | 0.0635       |
|    learning_rate         | 0.0003       |
|    loss                  | 34.2         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00821     |
|    std                   | 0.918        |
|    value_loss            | 335          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7240463] |
| time/                    |              |
|    fps                   | 118          |
|    iterations            | 25           |
|    time_elapsed          | 432          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.006423141  |
|    clip_fraction         | 0.063        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 22.6         |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0133       |
|    lagrangian_multiplier | 0.0482       |
|    learning_rate         | 0.0003       |
|    loss                  | 101          |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00955     |
|    std                   | 0.975        |
|    value_loss            | 784          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6425498] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 32           |
|    time_elapsed          | 583          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.007860976  |
|    clip_fraction         | 0.0645       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0793       |
|    entropy_loss          | -2.67        |
|    explained_variance    | -0.000914    |
|    lagrangian_multiplier | 0.0543       |
|    learning_rate         | 0.0003       |
|    loss                  | 127          |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 0.922        |
|    value_loss            | 974          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.90011805] |
| time/                    |               |
|    fps                   | 118           |
|    iterations            | 26            |
|    time_elapsed          | 449           |
|    total_timesteps       | 53248         |
| train/                   |               |
|    approx_kl             | 0.0073085907  |
|    clip_fraction         | 0.0597        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 12.9          |
|    entropy_loss          | -2.79         |
|    explained_variance    | 0.0149        |
|    lagrangian_multiplier | 0.0511        |
|    learning_rate         | 0.0003        |
|    loss                  | 41.6          |
|    n_updates             | 250           |
|    policy_gradient_loss  | -0.00687      |
|    std                   | 0.978         |
|    value_loss            | 283           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.8816989] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 33           |
|    time_elapsed          | 602          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0057857186 |
|    clip_fraction         | 0.052        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.099        |
|    entropy_loss          | -2.67        |
|    explained_variance    | -0.00537     |
|    lagrangian_multiplier | 0.0629       |
|    learning_rate         | 0.0003       |
|    loss                  | 49.1         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.0081      |
|    std                   | 0.918        |
|    value_loss            | 499          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.61269397] |
| time/                    |               |
|    fps                   | 118           |
|    iterations            | 27            |
|    time_elapsed          | 465           |
|    total_timesteps       | 55296         |
| train/                   |               |
|    approx_kl             | 0.0063888286  |
|    clip_fraction         | 0.0688        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 2.01          |
|    entropy_loss          | -2.79         |
|    explained_variance    | 0.0228        |
|    lagrangian_multiplier | 0.0609        |
|    learning_rate         | 0.0003        |
|    loss                  | 60.2          |
|    n_updates             | 260           |
|    policy_gradient_loss  | -0.0116       |
|    std                   | 0.977         |
|    value_loss            | 531           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.7607907] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 34           |
|    time_elapsed          | 620          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0052978653 |
|    clip_fraction         | 0.0411       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.121        |
|    entropy_loss          | -2.68        |
|    explained_variance    | -0.00582     |
|    lagrangian_multiplier | 0.0517       |
|    learning_rate         | 0.0003       |
|    loss                  | 31.4         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.0077      |
|    std                   | 0.926        |
|    value_loss            | 336          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.060463]  |
| time/                    |              |
|    fps                   | 118          |
|    iterations            | 28           |
|    time_elapsed          | 482          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.0053671463 |
|    clip_fraction         | 0.0428       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 17.7         |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0124       |
|    lagrangian_multiplier | 0.0604       |
|    learning_rate         | 0.0003       |
|    loss                  | 31.8         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00856     |
|    std                   | 0.99         |
|    value_loss            | 295          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9720359] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 35           |
|    time_elapsed          | 638          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0058612796 |
|    clip_fraction         | 0.049        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.103        |
|    entropy_loss          | -2.69        |
|    explained_variance    | 0.00574      |
|    lagrangian_multiplier | 0.0515       |
|    learning_rate         | 0.0003       |
|    loss                  | 42.5         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.928        |
|    value_loss            | 397          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0425775] |
| time/                    |              |
|    fps                   | 119          |
|    iterations            | 29           |
|    time_elapsed          | 498          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.0057404083 |
|    clip_fraction         | 0.0474       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.168        |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.013       |
|    lagrangian_multiplier | 0.0669       |
|    learning_rate         | 0.0003       |
|    loss                  | 98.2         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00656     |
|    std                   | 0.986        |
|    value_loss            | 940          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.42853028] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 36            |
|    time_elapsed          | 657           |
|    total_timesteps       | 73728         |
| train/                   |               |
|    approx_kl             | 0.007530445   |
|    clip_fraction         | 0.0959        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.161         |
|    entropy_loss          | -2.69         |
|    explained_variance    | 0.0054        |
|    lagrangian_multiplier | 0.0537        |
|    learning_rate         | 0.0003        |
|    loss                  | 47            |
|    n_updates             | 350           |
|    policy_gradient_loss  | -0.0126       |
|    std                   | 0.93          |
|    value_loss            | 476           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.9837791] |
| time/                    |              |
|    fps                   | 119          |
|    iterations            | 30           |
|    time_elapsed          | 514          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0052012103 |
|    clip_fraction         | 0.0471       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 2.28         |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.039       |
|    lagrangian_multiplier | 0.0687       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.6         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00845     |
|    std                   | 1            |
|    value_loss            | 501          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8420618] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 37           |
|    time_elapsed          | 675          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.00469481   |
|    clip_fraction         | 0.0531       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0248       |
|    entropy_loss          | -2.69        |
|    explained_variance    | 0.0384       |
|    lagrangian_multiplier | 0.0488       |
|    learning_rate         | 0.0003       |
|    loss                  | 16.7         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00751     |
|    std                   | 0.928        |
|    value_loss            | 171          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.041956] |
| time/                    |             |
|    fps                   | 119         |
|    iterations            | 31          |
|    time_elapsed          | 531         |
|    total_timesteps       | 63488       |
| train/                   |             |
|    approx_kl             | 0.006279647 |
|    clip_fraction         | 0.0586      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 1.04        |
|    entropy_loss          | -2.87       |
|    explained_variance    | 0.0222      |
|    lagrangian_multiplier | 0.0545      |
|    learning_rate         | 0.0003      |
|    loss                  | 49.9        |
|    n_updates             | 300         |
|    policy_gradient_loss  | -0.0102     |
|    std                   | 1.03        |
|    value_loss            | 490         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.0629762] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 38           |
|    time_elapsed          | 693          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0077926954 |
|    clip_fraction         | 0.082        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0939       |
|    entropy_loss          | -2.69        |
|    explained_variance    | -0.0238      |
|    lagrangian_multiplier | 0.062        |
|    learning_rate         | 0.0003       |
|    loss                  | 50.6         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.012       |
|    std                   | 0.924        |
|    value_loss            | 525          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0319483] |
| time/                    |              |
|    fps                   | 119          |
|    iterations            | 32           |
|    time_elapsed          | 547          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.0060252184 |
|    clip_fraction         | 0.0473       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0253       |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.0383      |
|    lagrangian_multiplier | 0.0823       |
|    learning_rate         | 0.0003       |
|    loss                  | 23.7         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00455     |
|    std                   | 1.02         |
|    value_loss            | 304          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2459719] |
| time/                    |              |
|    fps                   | 119          |
|    iterations            | 33           |
|    time_elapsed          | 564          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.007014807  |
|    clip_fraction         | 0.0627       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.46         |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.00168      |
|    lagrangian_multiplier | 0.0663       |
|    learning_rate         | 0.0003       |
|    loss                  | 22.5         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00964     |
|    std                   | 1.02         |
|    value_loss            | 272          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6695063] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 39           |
|    time_elapsed          | 711          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0051393807 |
|    clip_fraction         | 0.0468       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.105        |
|    entropy_loss          | -2.68        |
|    explained_variance    | -2.38e-05    |
|    lagrangian_multiplier | 0.0576       |
|    learning_rate         | 0.0003       |
|    loss                  | 63.6         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00788     |
|    std                   | 0.924        |
|    value_loss            | 535          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1246775] |
| time/                    |              |
|    fps                   | 119          |
|    iterations            | 34           |
|    time_elapsed          | 580          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0055417824 |
|    clip_fraction         | 0.0475       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.143        |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0181       |
|    lagrangian_multiplier | 0.0565       |
|    learning_rate         | 0.0003       |
|    loss                  | 37.2         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00615     |
|    std                   | 1.01         |
|    value_loss            | 353          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.58662206] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 40            |
|    time_elapsed          | 730           |
|    total_timesteps       | 81920         |
| train/                   |               |
|    approx_kl             | 0.008261717   |
|    clip_fraction         | 0.0744        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.155         |
|    entropy_loss          | -2.69         |
|    explained_variance    | 0.00535       |
|    lagrangian_multiplier | 0.0483        |
|    learning_rate         | 0.0003        |
|    loss                  | 42.9          |
|    n_updates             | 390           |
|    policy_gradient_loss  | -0.0111       |
|    std                   | 0.931         |
|    value_loss            | 427           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.0993996] |
| time/                    |              |
|    fps                   | 120          |
|    iterations            | 35           |
|    time_elapsed          | 597          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.006010309  |
|    clip_fraction         | 0.058        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.171        |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0117      |
|    lagrangian_multiplier | 0.0655       |
|    learning_rate         | 0.0003       |
|    loss                  | 28.5         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00892     |
|    std                   | 1.01         |
|    value_loss            | 322          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.4363098] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 41           |
|    time_elapsed          | 748          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0067318995 |
|    clip_fraction         | 0.0698       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.137        |
|    entropy_loss          | -2.7         |
|    explained_variance    | 0.0151       |
|    lagrangian_multiplier | 0.0593       |
|    learning_rate         | 0.0003       |
|    loss                  | 41.4         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00752     |
|    std                   | 0.937        |
|    value_loss            | 431          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4640379] |
| time/                    |              |
|    fps                   | 120          |
|    iterations            | 36           |
|    time_elapsed          | 613          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0055154082 |
|    clip_fraction         | 0.0484       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 7.03         |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0217       |
|    lagrangian_multiplier | 0.0583       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.7         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00862     |
|    std                   | 1            |
|    value_loss            | 484          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6600115] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 42           |
|    time_elapsed          | 766          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0071949735 |
|    clip_fraction         | 0.0694       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.118        |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0288       |
|    lagrangian_multiplier | 0.0438       |
|    learning_rate         | 0.0003       |
|    loss                  | 51.4         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00718     |
|    std                   | 0.942        |
|    value_loss            | 364          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9424684] |
| time/                    |              |
|    fps                   | 120          |
|    iterations            | 37           |
|    time_elapsed          | 630          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.005282688  |
|    clip_fraction         | 0.0545       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.204        |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.000975    |
|    lagrangian_multiplier | 0.0514       |
|    learning_rate         | 0.0003       |
|    loss                  | 39.2         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00726     |
|    std                   | 1            |
|    value_loss            | 347          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.32175666] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 43            |
|    time_elapsed          | 784           |
|    total_timesteps       | 88064         |
| train/                   |               |
|    approx_kl             | 0.0068915267  |
|    clip_fraction         | 0.0602        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.179         |
|    entropy_loss          | -2.72         |
|    explained_variance    | 0.0148        |
|    lagrangian_multiplier | 0.0559        |
|    learning_rate         | 0.0003        |
|    loss                  | 62.8          |
|    n_updates             | 420           |
|    policy_gradient_loss  | -0.00736      |
|    std                   | 0.944         |
|    value_loss            | 595           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.9060946] |
| time/                    |              |
|    fps                   | 120          |
|    iterations            | 38           |
|    time_elapsed          | 646          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.006472484  |
|    clip_fraction         | 0.0655       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.77         |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0202       |
|    lagrangian_multiplier | 0.0559       |
|    learning_rate         | 0.0003       |
|    loss                  | 77.7         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.0072      |
|    std                   | 1            |
|    value_loss            | 599          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.47169226] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 44            |
|    time_elapsed          | 803           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.008305138   |
|    clip_fraction         | 0.0642        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.196         |
|    entropy_loss          | -2.72         |
|    explained_variance    | 0.000288      |
|    lagrangian_multiplier | 0.0483        |
|    learning_rate         | 0.0003        |
|    loss                  | 114           |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.0122       |
|    std                   | 0.942         |
|    value_loss            | 981           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.5248842] |
| time/                    |              |
|    fps                   | 120          |
|    iterations            | 39           |
|    time_elapsed          | 663          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.006629972  |
|    clip_fraction         | 0.0612       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 21.6         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0113       |
|    lagrangian_multiplier | 0.0522       |
|    learning_rate         | 0.0003       |
|    loss                  | 73.8         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00889     |
|    std                   | 1.01         |
|    value_loss            | 552          |
-------------------------------------------
------------------------------------------
| reward                   | [-0.667739] |
| time/                    |             |
|    fps                   | 112         |
|    iterations            | 45          |
|    time_elapsed          | 821         |
|    total_timesteps       | 92160       |
| train/                   |             |
|    approx_kl             | 0.00752196  |
|    clip_fraction         | 0.0745      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.129       |
|    entropy_loss          | -2.71       |
|    explained_variance    | 0.0359      |
|    lagrangian_multiplier | 0.0574      |
|    learning_rate         | 0.0003      |
|    loss                  | 55.3        |
|    n_updates             | 440         |
|    policy_gradient_loss  | -0.00936    |
|    std                   | 0.937       |
|    value_loss            | 482         |
------------------------------------------
-------------------------------------------
| reward                   | [-2.5420947] |
| time/                    |              |
|    fps                   | 120          |
|    iterations            | 40           |
|    time_elapsed          | 679          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.0072572445 |
|    clip_fraction         | 0.068        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.69         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0131       |
|    lagrangian_multiplier | 0.0508       |
|    learning_rate         | 0.0003       |
|    loss                  | 74.2         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.0091      |
|    std                   | 1.01         |
|    value_loss            | 583          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.087671] |
| time/                    |             |
|    fps                   | 112         |
|    iterations            | 46          |
|    time_elapsed          | 839         |
|    total_timesteps       | 94208       |
| train/                   |             |
|    approx_kl             | 0.006539672 |
|    clip_fraction         | 0.0593      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.204       |
|    entropy_loss          | -2.7        |
|    explained_variance    | 0.0219      |
|    lagrangian_multiplier | 0.0551      |
|    learning_rate         | 0.0003      |
|    loss                  | 70.1        |
|    n_updates             | 450         |
|    policy_gradient_loss  | -0.00871    |
|    std                   | 0.931       |
|    value_loss            | 608         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.8461462] |
| time/                    |              |
|    fps                   | 120          |
|    iterations            | 41           |
|    time_elapsed          | 696          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0050227484 |
|    clip_fraction         | 0.0445       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 5.66         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.00693      |
|    lagrangian_multiplier | 0.062        |
|    learning_rate         | 0.0003       |
|    loss                  | 53.1         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.0109      |
|    std                   | 1.01         |
|    value_loss            | 544          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2864023] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 47           |
|    time_elapsed          | 857          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.0085955495 |
|    clip_fraction         | 0.101        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.15         |
|    entropy_loss          | -2.69        |
|    explained_variance    | 0.00933      |
|    lagrangian_multiplier | 0.0484       |
|    learning_rate         | 0.0003       |
|    loss                  | 106          |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.0152      |
|    std                   | 0.927        |
|    value_loss            | 734          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8400821] |
| time/                    |              |
|    fps                   | 120          |
|    iterations            | 42           |
|    time_elapsed          | 712          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.006949928  |
|    clip_fraction         | 0.0714       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 16.5         |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0168      |
|    lagrangian_multiplier | 0.0594       |
|    learning_rate         | 0.0003       |
|    loss                  | 49.5         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.0127      |
|    std                   | 1.01         |
|    value_loss            | 380          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7447889] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 48           |
|    time_elapsed          | 875          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.006570749  |
|    clip_fraction         | 0.0681       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.138        |
|    entropy_loss          | -2.7         |
|    explained_variance    | -0.00712     |
|    lagrangian_multiplier | 0.0569       |
|    learning_rate         | 0.0003       |
|    loss                  | 25.3         |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00932     |
|    std                   | 0.936        |
|    value_loss            | 225          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9579421] |
| time/                    |              |
|    fps                   | 120          |
|    iterations            | 43           |
|    time_elapsed          | 729          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0057842396 |
|    clip_fraction         | 0.0545       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.116        |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0686      |
|    lagrangian_multiplier | 0.0681       |
|    learning_rate         | 0.0003       |
|    loss                  | 35.5         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00689     |
|    std                   | 0.992        |
|    value_loss            | 370          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5000497] |
| time/                    |              |
|    fps                   | 120          |
|    iterations            | 44           |
|    time_elapsed          | 745          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.0048444853 |
|    clip_fraction         | 0.0461       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.339        |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.00278     |
|    lagrangian_multiplier | 0.0571       |
|    learning_rate         | 0.0003       |
|    loss                  | 37.4         |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.00801     |
|    std                   | 0.989        |
|    value_loss            | 386          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.77966815] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 49            |
|    time_elapsed          | 893           |
|    total_timesteps       | 100352        |
| train/                   |               |
|    approx_kl             | 0.0060741     |
|    clip_fraction         | 0.0685        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.111         |
|    entropy_loss          | -2.71         |
|    explained_variance    | -0.0125       |
|    lagrangian_multiplier | 0.0533        |
|    learning_rate         | 0.0003        |
|    loss                  | 78.4          |
|    n_updates             | 480           |
|    policy_gradient_loss  | -0.00797      |
|    std                   | 0.936         |
|    value_loss            | 534           |
--------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñà‚ñá‚ñà‚ñá‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñà‚ñà‚ñà‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñá‚ñÖ‚ñá‚ñÑ‚ñá‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñá
wandb:             train/approx_kl ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÉ‚ñÜ‚ñÇ‚ñá‚ñÉ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñà‚ñÖ‚ñÑ
wandb:         train/clip_fraction ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñá‚ñÉ‚ñÜ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÖ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÜ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñá‚ñÑ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÖ
wandb:          train/entropy_loss ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá
wandb:    train/explained_variance ‚ñÖ‚ñà‚ñÖ‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÇ
wandb: train/lagrangian_multiplier ‚ñÜ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñà‚ñá‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÖ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÅ‚ñÉ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñá‚ñá‚ñà‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñÉ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÜ
wandb:                   train/std ‚ñá‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:            train/value_loss ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÅ‚ñÉ
wandb: 
wandb: Run summary:
wandb:                      reward -0.77967
wandb:             train/approx_kl 0.00607
wandb:         train/clip_fraction 0.06846
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 0.1114
wandb:          train/entropy_loss -2.70675
wandb:    train/explained_variance -0.01252
wandb: train/lagrangian_multiplier 0.05331
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 78.36489
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00797
wandb:                   train/std 0.93564
wandb:            train/value_loss 533.64457
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16/runs/9gehkoio
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_203047-9gehkoio/logs
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/usr/lib/python3.9/threading.py", line 954, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.9/threading.py", line 892, in run
    self._target(*self._args, **self._kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 277, in check_stop_status
    self._loop_check_status(
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 215, in _loop_check_status
    local_handle = request()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface.py", line 787, in deliver_stop_status
    return self._deliver_stop_status(status)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py", line 585, in _deliver_stop_status
    return self._deliver_record(record)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py", line 560, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
srun: error: ddpg.ist.berkeley.edu: task 0: Exited with exit code 1
--------------------------------------------
| reward                   | [-0.39917415] |
| time/                    |               |
|    fps                   | 120           |
|    iterations            | 45            |
|    time_elapsed          | 762           |
|    total_timesteps       | 92160         |
| train/                   |               |
|    approx_kl             | 0.006390177   |
|    clip_fraction         | 0.0518        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 15.8          |
|    entropy_loss          | -2.82         |
|    explained_variance    | -0.00239      |
|    lagrangian_multiplier | 0.0595        |
|    learning_rate         | 0.0003        |
|    loss                  | 69.8          |
|    n_updates             | 440           |
|    policy_gradient_loss  | -0.00916      |
|    std                   | 0.995         |
|    value_loss            | 590           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.1195911] |
| time/                    |              |
|    fps                   | 121          |
|    iterations            | 46           |
|    time_elapsed          | 778          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0042946236 |
|    clip_fraction         | 0.0367       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.1          |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0137       |
|    lagrangian_multiplier | 0.0558       |
|    learning_rate         | 0.0003       |
|    loss                  | 41.2         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.00522     |
|    std                   | 0.988        |
|    value_loss            | 390          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2360529] |
| time/                    |              |
|    fps                   | 121          |
|    iterations            | 47           |
|    time_elapsed          | 794          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.0073612574 |
|    clip_fraction         | 0.0752       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.56         |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0164       |
|    lagrangian_multiplier | 0.044        |
|    learning_rate         | 0.0003       |
|    loss                  | 84.7         |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.00946     |
|    std                   | 0.98         |
|    value_loss            | 569          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0400999] |
| time/                    |              |
|    fps                   | 121          |
|    iterations            | 48           |
|    time_elapsed          | 811          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.007001846  |
|    clip_fraction         | 0.0585       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.58         |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0133       |
|    lagrangian_multiplier | 0.0503       |
|    learning_rate         | 0.0003       |
|    loss                  | 71.4         |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00965     |
|    std                   | 0.98         |
|    value_loss            | 579          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3634105] |
| time/                    |              |
|    fps                   | 121          |
|    iterations            | 49           |
|    time_elapsed          | 827          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.009335771  |
|    clip_fraction         | 0.0833       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 8.41         |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0126       |
|    lagrangian_multiplier | 0.0552       |
|    learning_rate         | 0.0003       |
|    loss                  | 54.2         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.012       |
|    std                   | 0.96         |
|    value_loss            | 477          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: check_build_path() missing 1 required positional argument: 'path'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñà‚ñá‚ñá‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÅ‚ñÉ‚ñÜ‚ñÇ‚ñÑ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñà‚ñà‚ñá‚ñÖ‚ñÖ
wandb:             train/approx_kl ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÖ‚ñÑ‚ñÅ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñÖ‚ñà
wandb:         train/clip_fraction ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÅ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñà‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñà
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÅ‚ñà‚ñÖ‚ñÇ‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñà‚ñÉ‚ñÜ‚ñÅ‚ñÅ‚ñÜ‚ñÉ‚ñÇ‚ñÑ
wandb:          train/entropy_loss ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà
wandb:    train/explained_variance ‚ñÖ‚ñà‚ñà‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñá‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñá‚ñá‚ñà‚ñá‚ñÉ‚ñà‚ñÉ‚ñÜ‚ñá‚ñà‚ñÜ‚ñà‚ñá‚ñá‚ñÖ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá
wandb: train/lagrangian_multiplier ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñà‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÉ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñá‚ñà‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÇ‚ñÉ‚ñà‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñá‚ñÖ‚ñÑ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñá‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÅ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ
wandb:                   train/std ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÅ
wandb:            train/value_loss ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÖ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÉ
wandb: 
wandb: Run summary:
wandb:                      reward -1.36341
wandb:             train/approx_kl 0.00934
wandb:         train/clip_fraction 0.08325
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 8.40815
wandb:          train/entropy_loss -2.76709
wandb:    train/explained_variance 0.0126
wandb: train/lagrangian_multiplier 0.0552
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 54.16135
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.012
wandb:                   train/std 0.95972
wandb:            train/value_loss 476.64877
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/pbzne7n4
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231227_043313-pbzne7n4/logs
srun: error: airl.ist.berkeley.edu: task 0: Exited with exit code 1
