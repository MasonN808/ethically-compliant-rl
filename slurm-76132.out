wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231018_034255-m26jkgli
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(1.0)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty/runs/m26jkgli
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231017_204256-kyqec2a7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(0.1)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty/runs/kyqec2a7
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231017_204256-wl446ngy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(0.05)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty/runs/wl446ngy
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231017_204256-bsl47cfa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(3.0)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty/runs/bsl47cfa
Using cpu device
--------------------------------------
| reward             | [-0.31189573] |
| time/              |               |
|    fps             | 184           |
|    iterations      | 1             |
|    time_elapsed    | 11            |
|    total_timesteps | 2048          |
--------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.42961523] |
| time/              |               |
|    fps             | 164           |
|    iterations      | 1             |
|    time_elapsed    | 12            |
|    total_timesteps | 2048          |
--------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.28516948] |
| time/              |               |
|    fps             | 164           |
|    iterations      | 1             |
|    time_elapsed    | 12            |
|    total_timesteps | 2048          |
--------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.47174126] |
| time/              |               |
|    fps             | 163           |
|    iterations      | 1             |
|    time_elapsed    | 12            |
|    total_timesteps | 2048          |
--------------------------------------
------------------------------------------
| reward                  | [-0.5471012] |
| time/                   |              |
|    fps                  | 177          |
|    iterations           | 2            |
|    time_elapsed         | 23           |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.056712978  |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.82        |
|    explained_variance   | 0.0108       |
|    learning_rate        | 0.0003       |
|    loss                 | 60.6         |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0346      |
|    std                  | 0.985        |
|    value_loss           | 226          |
------------------------------------------
------------------------------------------
| reward                  | [-0.6072777] |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 2            |
|    time_elapsed         | 25           |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.06890589   |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | -0.00126     |
|    learning_rate        | 0.0003       |
|    loss                 | 33.7         |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0145      |
|    std                  | 1            |
|    value_loss           | 173          |
------------------------------------------
-----------------------------------------
| reward                  | [-0.428773] |
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 2           |
|    time_elapsed         | 26          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.11767039  |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.83       |
|    explained_variance   | -0.0413     |
|    learning_rate        | 0.0003      |
|    loss                 | 23.6        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0152     |
|    std                  | 0.995       |
|    value_loss           | 146         |
-----------------------------------------
-------------------------------------------
| reward                  | [-0.59723806] |
| time/                   |               |
|    fps                  | 157           |
|    iterations           | 2             |
|    time_elapsed         | 25            |
|    total_timesteps      | 4096          |
| train/                  |               |
|    approx_kl            | 0.06118238    |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.83         |
|    explained_variance   | -0.00745      |
|    learning_rate        | 0.0003        |
|    loss                 | 44            |
|    n_updates            | 10            |
|    policy_gradient_loss | -0.0718       |
|    std                  | 0.992         |
|    value_loss           | 166           |
-------------------------------------------
-------------------------------------------
| reward                  | [-0.98772997] |
| time/                   |               |
|    fps                  | 175           |
|    iterations           | 3             |
|    time_elapsed         | 35            |
|    total_timesteps      | 6144          |
| train/                  |               |
|    approx_kl            | 0.04305496    |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.81         |
|    explained_variance   | -0.612        |
|    learning_rate        | 0.0003        |
|    loss                 | 242           |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.0222       |
|    std                  | 0.984         |
|    value_loss           | 735           |
-------------------------------------------
-------------------------------------------
| reward                  | [-0.45332113] |
| time/                   |               |
|    fps                  | 155           |
|    iterations           | 3             |
|    time_elapsed         | 39            |
|    total_timesteps      | 6144          |
| train/                  |               |
|    approx_kl            | 0.96932256    |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.86         |
|    explained_variance   | 0.421         |
|    learning_rate        | 0.0003        |
|    loss                 | 12.3          |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.0704       |
|    std                  | 1.03          |
|    value_loss           | 67.5          |
-------------------------------------------
------------------------------------------
| reward                  | [-0.5839562] |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 3            |
|    time_elapsed         | 39           |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 0.33485198   |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.82        |
|    explained_variance   | 0.114        |
|    learning_rate        | 0.0003       |
|    loss                 | 24.1         |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.249       |
|    std                  | 0.987        |
|    value_loss           | 130          |
------------------------------------------
-------------------------------------------
| reward                  | [-0.40660268] |
| time/                   |               |
|    fps                  | 152           |
|    iterations           | 3             |
|    time_elapsed         | 40            |
|    total_timesteps      | 6144          |
| train/                  |               |
|    approx_kl            | 0.25940156    |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.82         |
|    explained_variance   | 0.482         |
|    learning_rate        | 0.0003        |
|    loss                 | 16.3          |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.0311       |
|    std                  | 0.974         |
|    value_loss           | 69.4          |
-------------------------------------------
-----------------------------------------
| reward                  | [-2.425843] |
| time/                   |             |
|    fps                  | 174         |
|    iterations           | 4           |
|    time_elapsed         | 47          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.4175508   |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.81       |
|    explained_variance   | -0.297      |
|    learning_rate        | 0.0003      |
|    loss                 | 264         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.127      |
|    std                  | 0.988       |
|    value_loss           | 788         |
-----------------------------------------
------------------------------------------
| reward                  | [-0.8384946] |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 4            |
|    time_elapsed         | 52           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.24727508   |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.88        |
|    explained_variance   | -0.265       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.95         |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.018       |
|    std                  | 1.01         |
|    value_loss           | 40.1         |
------------------------------------------
------------------------------------------
| reward                  | [-2.2624257] |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 4            |
|    time_elapsed         | 52           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 7.735033     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.8         |
|    explained_variance   | 0.628        |
|    learning_rate        | 0.0003       |
|    loss                 | -7.03        |
|    n_updates            | 30           |
|    policy_gradient_loss | -8           |
|    std                  | 0.972        |
|    value_loss           | 40.7         |
------------------------------------------
------------------------------------------
| reward                  | [-1.0912696] |
| time/                   |              |
|    fps                  | 151          |
|    iterations           | 4            |
|    time_elapsed         | 54           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.38368654   |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.74        |
|    explained_variance   | 0.809        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.21         |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.0508      |
|    std                  | 0.923        |
|    value_loss           | 8.89         |
------------------------------------------
------------------------------------------
| reward                  | [-2.8814557] |
| time/                   |              |
|    fps                  | 173          |
|    iterations           | 5            |
|    time_elapsed         | 59           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.03178834   |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.82        |
|    explained_variance   | 0.0123       |
|    learning_rate        | 0.0003       |
|    loss                 | 576          |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.0185      |
|    std                  | 0.991        |
|    value_loss           | 1.24e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-0.8060302] |
| time/                   |              |
|    fps                  | 153          |
|    iterations           | 5            |
|    time_elapsed         | 66           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.036036327  |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.86        |
|    explained_variance   | -0.102       |
|    learning_rate        | 0.0003       |
|    loss                 | 10.5         |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00763     |
|    std                  | 1.01         |
|    value_loss           | 59.7         |
------------------------------------------
------------------------------------------
| reward                  | [-2.6558292] |
| time/                   |              |
|    fps                  | 153          |
|    iterations           | 5            |
|    time_elapsed         | 66           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.0057667727 |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.78        |
|    explained_variance   | 0.149        |
|    learning_rate        | 0.0003       |
|    loss                 | 520          |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.015       |
|    std                  | 0.97         |
|    value_loss           | 1.28e+03     |
------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/save_util.py:283: UserWarning: Path 'PPO_penalty/models/0.05' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
------------------------------------------
| reward                  | [-1.7196944] |
| time/                   |              |
|    fps                  | 151          |
|    iterations           | 5            |
|    time_elapsed         | 67           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.08887073   |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.67        |
|    explained_variance   | 0.499        |
|    learning_rate        | 0.0003       |
|    loss                 | 51.3         |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.0168      |
|    std                  | 0.922        |
|    value_loss           | 153          |
------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/save_util.py:283: UserWarning: Path 'PPO_penalty/models/0.1' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
------------------------------------
| reward             | [-3.510666] |
| time/              |             |
|    fps             | 182         |
|    iterations      | 1           |
|    time_elapsed    | 11          |
|    total_timesteps | 12288       |
------------------------------------
--------------------------------------
| reward             | [-0.89853483] |
| time/              |               |
|    fps             | 164           |
|    iterations      | 1             |
|    time_elapsed    | 12            |
|    total_timesteps | 12288         |
--------------------------------------
-------------------------------------
| reward             | [-3.0186913] |
| time/              |              |
|    fps             | 163          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 12288        |
-------------------------------------
-------------------------------------
| reward             | [-1.9719744] |
| time/              |              |
|    fps             | 162          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 12288        |
-------------------------------------
-----------------------------------------
| reward                  | [-3.817822] |
| time/                   |             |
|    fps                  | 175         |
|    iterations           | 2           |
|    time_elapsed         | 23          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.4046716   |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.81       |
|    explained_variance   | 0.00745     |
|    learning_rate        | 0.0003      |
|    loss                 | 739         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.11       |
|    std                  | 0.985       |
|    value_loss           | 1.62e+03    |
-----------------------------------------
------------------------------------------
| reward                  | [-2.6629877] |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 2            |
|    time_elapsed         | 26           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.03115979   |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.85        |
|    explained_variance   | 0.328        |
|    learning_rate        | 0.0003       |
|    loss                 | 160          |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.0115      |
|    std                  | 1            |
|    value_loss           | 358          |
------------------------------------------
------------------------------------------
| reward                  | [-3.3394814] |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 2            |
|    time_elapsed         | 26           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.11390058   |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.78        |
|    explained_variance   | 0.029        |
|    learning_rate        | 0.0003       |
|    loss                 | 530          |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.138       |
|    std                  | 0.97         |
|    value_loss           | 1.33e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-1.4364003] |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 2            |
|    time_elapsed         | 26           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.15664643   |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.67        |
|    explained_variance   | 0.00401      |
|    learning_rate        | 0.0003       |
|    loss                 | 193          |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.0252      |
|    std                  | 0.922        |
|    value_loss           | 477          |
------------------------------------------
------------------------------------------
| reward                  | [-3.4040403] |
| time/                   |              |
|    fps                  | 173          |
|    iterations           | 3            |
|    time_elapsed         | 35           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 0.5837747    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.81        |
|    explained_variance   | 0.00578      |
|    learning_rate        | 0.0003       |
|    loss                 | 824          |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.198       |
|    std                  | 0.989        |
|    value_loss           | 1.75e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-2.1544604] |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 3            |
|    time_elapsed         | 39           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 0.07240224   |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.85        |
|    explained_variance   | 0.265        |
|    learning_rate        | 0.0003       |
|    loss                 | 93.9         |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00916     |
|    std                  | 1            |
|    value_loss           | 303          |
------------------------------------------
-------------------------------------------
| reward                  | [-0.81772774] |
| time/                   |               |
|    fps                  | 172           |
|    iterations           | 4             |
|    time_elapsed         | 47            |
|    total_timesteps      | 18432         |
| train/                  |               |
|    approx_kl            | 0.43020242    |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.82         |
|    explained_variance   | 0.0108        |
|    learning_rate        | 0.0003        |
|    loss                 | 534           |
|    n_updates            | 80            |
|    policy_gradient_loss | -0.128        |
|    std                  | 0.989         |
|    value_loss           | 1.2e+03       |
-------------------------------------------
------------------------------------------
| reward                  | [-3.8454335] |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 3            |
|    time_elapsed         | 39           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 0.21456552   |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.78        |
|    explained_variance   | 0.00873      |
|    learning_rate        | 0.0003       |
|    loss                 | 639          |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.248       |
|    std                  | 0.97         |
|    value_loss           | 1.43e+03     |
------------------------------------------
-----------------------------------------
| reward                  | [-2.462941] |
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 3           |
|    time_elapsed         | 39          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.09084687  |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.67       |
|    explained_variance   | 0.544       |
|    learning_rate        | 0.0003      |
|    loss                 | 117         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0143     |
|    std                  | 0.921       |
|    value_loss           | 280         |
-----------------------------------------
srun: Job 76132 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 76132 step creation temporarily disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                  | [-4.3481913] |
| time/                   |              |
|    fps                  | 171          |
|    iterations           | 5            |
|    time_elapsed         | 59           |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.7680691    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.79        |
|    explained_variance   | 0.0504       |
|    learning_rate        | 0.0003       |
|    loss                 | 176          |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.275       |
|    std                  | 0.964        |
|    value_loss           | 594          |
------------------------------------------
srun: Job 76132 step creation temporarily disabled, retrying (Requested nodes are busy)
slurmstepd: error: *** STEP 76132.0 ON airl.ist.berkeley.edu CANCELLED AT 2023-10-18T03:44:57 ***
slurmstepd: error: *** STEP 76132.2 ON dqn.ist.berkeley.edu CANCELLED AT 2023-10-17T20:44:57 ***
slurmstepd: error: *** STEP 76132.3 ON gail.ist.berkeley.edu CANCELLED AT 2023-10-17T20:44:57 ***
slurmstepd: error: *** STEP 76132.1 ON ddpg.ist.berkeley.edu CANCELLED AT 2023-10-17T20:44:57 ***
slurmstepd: error: *** JOB 76132 ON airl.ist.berkeley.edu CANCELLED AT 2023-10-18T03:44:57 ***
