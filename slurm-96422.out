usage: train_ppo_penalty.py [-h] [--beta BETA]
train_ppo_penalty.py: error: argument --beta: invalid float value: 'dynamic'
srun: error: dqn.ist.berkeley.edu: task 0: Exited with exit code 2
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231117_081919-cpet2hoy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(40.0)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty-Report5
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty-Report5/runs/cpet2hoy
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231117_081919-wl73bzu6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(0.0)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty-Report5
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty-Report5/runs/wl73bzu6
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231117_161919-6i7cst9r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(15.0)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty-Report5
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty-Report5/runs/6i7cst9r
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231117_081919-lofx57n3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(150.0)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty-Report5
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty-Report5/runs/lofx57n3
Using cpu device
--------------------------------------
| reward             | [-0.42131245] |
| time/              |               |
|    fps             | 185           |
|    iterations      | 1             |
|    time_elapsed    | 11            |
|    total_timesteps | 2048          |
--------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.52883506] |
| time/              |               |
|    fps             | 185           |
|    iterations      | 1             |
|    time_elapsed    | 11            |
|    total_timesteps | 2048          |
--------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Using cpu device
--------------------------------------
| reward             | [-0.36390653] |
| time/              |               |
|    fps             | 161           |
|    iterations      | 1             |
|    time_elapsed    | 12            |
|    total_timesteps | 2048          |
--------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.7614589] |
| time/              |              |
|    fps             | 162          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 2048         |
-------------------------------------
----------------------------------------------
| reward                      | [-1.1007127] |
| time/                       |              |
|    fps                      | 178          |
|    iterations               | 2            |
|    time_elapsed             | 23           |
|    total_timesteps          | 4096         |
| train/                      |              |
|    approx_kl                | 4.5624266    |
|    approx_ln(kl)            | 1.5178547    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.04         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.96         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 7.09         |
|    n_updates                | 10           |
|    policy_gradient_loss     | -0.289       |
|    std                      | 0.868        |
|    value_loss               | 76           |
----------------------------------------------
----------------------------------------------
| reward                      | [-1.8393699] |
| time/                       |              |
|    fps                      | 176          |
|    iterations               | 2            |
|    time_elapsed             | 23           |
|    total_timesteps          | 4096         |
| train/                      |              |
|    approx_kl                | 3.7723145    |
|    approx_ln(kl)            | 1.3276888    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | -0.048       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.08         |
|    ln(policy_gradient_loss) | 3.29         |
|    loss                     | 59.1         |
|    n_updates                | 10           |
|    policy_gradient_loss     | 26.9         |
|    std                      | 0.938        |
|    value_loss               | 47.9         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
---------------------------------------------
| reward                      | [-1.303985] |
| time/                       |             |
|    fps                      | 156         |
|    iterations               | 2           |
|    time_elapsed             | 26          |
|    total_timesteps          | 4096        |
| train/                      |             |
|    approx_kl                | 4.973378    |
|    approx_ln(kl)            | 1.6040993   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.81       |
|    explained_variance       | 0.000651    |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 5.09        |
|    ln(policy_gradient_loss) | 4.14        |
|    loss                     | 163         |
|    n_updates                | 10          |
|    policy_gradient_loss     | 62.7        |
|    std                      | 0.95        |
|    value_loss               | 52.1        |
---------------------------------------------
----------------------------------------------
| reward                      | [-0.6649231] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 2            |
|    time_elapsed             | 26           |
|    total_timesteps          | 4096         |
| train/                      |              |
|    approx_kl                | 5.2177076    |
|    approx_ln(kl)            | 1.6520581    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.89        |
|    explained_variance       | -0.0348      |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 6.59         |
|    ln(policy_gradient_loss) | 5.68         |
|    loss                     | 725          |
|    n_updates                | 10           |
|    policy_gradient_loss     | 293          |
|    std                      | 0.999        |
|    value_loss               | 99.8         |
----------------------------------------------
----------------------------------------------
| reward                      | [-1.1517944] |
| time/                       |              |
|    fps                      | 174          |
|    iterations               | 3            |
|    time_elapsed             | 35           |
|    total_timesteps          | 6144         |
| train/                      |              |
|    approx_kl                | 3.8454022    |
|    approx_ln(kl)            | 1.3468782    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.53        |
|    explained_variance       | -0.0327      |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.97         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 7.17         |
|    n_updates                | 20           |
|    policy_gradient_loss     | -0.159       |
|    std                      | 0.84         |
|    value_loss               | 87.4         |
----------------------------------------------
----------------------------------------------
| reward                      | [-0.5010519] |
| time/                       |              |
|    fps                      | 174          |
|    iterations               | 3            |
|    time_elapsed             | 35           |
|    total_timesteps          | 6144         |
| train/                      |              |
|    approx_kl                | 1.6136438    |
|    approx_ln(kl)            | 0.47849482   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.69        |
|    explained_variance       | -0.456       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.31         |
|    ln(policy_gradient_loss) | 2.33         |
|    loss                     | 74.5         |
|    n_updates                | 20           |
|    policy_gradient_loss     | 10.3         |
|    std                      | 0.927        |
|    value_loss               | 181          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
----------------------------------------------
| reward                      | [-1.4662046] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 3            |
|    time_elapsed             | 39           |
|    total_timesteps          | 6144         |
| train/                      |              |
|    approx_kl                | 6.0188694    |
|    approx_ln(kl)            | 1.7948995    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.73        |
|    explained_variance       | -0.509       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 6.23         |
|    ln(policy_gradient_loss) | 4.4          |
|    loss                     | 508          |
|    n_updates                | 20           |
|    policy_gradient_loss     | 81.4         |
|    std                      | 0.945        |
|    value_loss               | 644          |
----------------------------------------------
----------------------------------------------
| reward                      | [-2.1576195] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 3            |
|    time_elapsed             | 39           |
|    total_timesteps          | 6144         |
| train/                      |              |
|    approx_kl                | 0.18671803   |
|    approx_ln(kl)            | -1.6781557   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.474        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.01         |
|    ln(policy_gradient_loss) | 2.67         |
|    loss                     | 150          |
|    n_updates                | 20           |
|    policy_gradient_loss     | 14.5         |
|    std                      | 0.99         |
|    value_loss               | 547          |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.51419646] |
| time/                       |               |
|    fps                      | 173           |
|    iterations               | 4             |
|    time_elapsed             | 47            |
|    total_timesteps          | 8192          |
| train/                      |               |
|    approx_kl                | 7.6577177     |
|    approx_ln(kl)            | 2.035714      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.46         |
|    explained_variance       | -1.28         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.139         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.15          |
|    n_updates                | 30            |
|    policy_gradient_loss     | -0.324        |
|    std                      | 0.797         |
|    value_loss               | 20.7          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
----------------------------------------------
| reward                      | [-0.6016992] |
| time/                       |              |
|    fps                      | 172          |
|    iterations               | 4            |
|    time_elapsed             | 47           |
|    total_timesteps          | 8192         |
| train/                      |              |
|    approx_kl                | 2.635767     |
|    approx_ln(kl)            | 0.9691742    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.69        |
|    explained_variance       | 0.623        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.48         |
|    ln(policy_gradient_loss) | 2.7          |
|    loss                     | 32.3         |
|    n_updates                | 30           |
|    policy_gradient_loss     | 14.9         |
|    std                      | 0.917        |
|    value_loss               | 167          |
----------------------------------------------
slurmstepd: error: *** STEP 96422.1 ON ddpg.ist.berkeley.edu CANCELLED AT 2023-11-17T08:20:11 ***
slurmstepd: error: *** STEP 96422.4 ON gan.ist.berkeley.edu CANCELLED AT 2023-11-17T08:20:11 ***
slurmstepd: error: *** STEP 96422.3 ON gail.ist.berkeley.edu CANCELLED AT 2023-11-17T08:20:12 ***
slurmstepd: error: *** STEP 96422.0 ON airl.ist.berkeley.edu CANCELLED AT 2023-11-17T16:20:11 ***
slurmstepd: error: *** JOB 96422 ON airl.ist.berkeley.edu CANCELLED AT 2023-11-17T16:20:11 ***
