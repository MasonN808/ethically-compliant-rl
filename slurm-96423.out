wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231117_082035-c3jzbnms
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(80.0)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty-Report5
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty-Report5/runs/c3jzbnms
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231117_082035-e1sir4ny
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231117_162035-a64vb243
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(15.0)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty-Report5
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty-Report5/runs/e1sir4ny
wandb: Syncing run ppo-KLpenalty-beta(1.0)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty-Report5
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty-Report5/runs/a64vb243
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231117_082036-4kcapje6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(5000.0)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty-Report5
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty-Report5/runs/4kcapje6
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231117_082036-gbti24u7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(0.05)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty-Report5
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty-Report5/runs/gbti24u7
Using cpu device
--------------------------------------
| reward             | [-0.59013647] |
| time/              |               |
|    fps             | 187           |
|    iterations      | 1             |
|    time_elapsed    | 10            |
|    total_timesteps | 2048          |
--------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.4726198] |
| time/              |              |
|    fps             | 186          |
|    iterations      | 1            |
|    time_elapsed    | 10           |
|    total_timesteps | 2048         |
-------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.5598391] |
| time/              |              |
|    fps             | 183          |
|    iterations      | 1            |
|    time_elapsed    | 11           |
|    total_timesteps | 2048         |
-------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Using cpu device
-------------------------------------
| reward             | [-0.4185096] |
| time/              |              |
|    fps             | 164          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 2048         |
-------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.4705042] |
| time/              |              |
|    fps             | 162          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 2048         |
-------------------------------------
----------------------------------------------
| reward                      | [-1.3160241] |
| time/                       |              |
|    fps                      | 178          |
|    iterations               | 2            |
|    time_elapsed             | 23           |
|    total_timesteps          | 4096         |
| train/                      |              |
|    approx_kl                | 2.1271143    |
|    approx_ln(kl)            | 0.7547663    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.78        |
|    explained_variance       | 0.00322      |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.18         |
|    ln(policy_gradient_loss) | 3.82         |
|    loss                     | 178          |
|    n_updates                | 10           |
|    policy_gradient_loss     | 45.8         |
|    std                      | 0.932        |
|    value_loss               | 66.9         |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.81912565] |
| time/                       |               |
|    fps                      | 179           |
|    iterations               | 2             |
|    time_elapsed             | 22            |
|    total_timesteps          | 4096          |
| train/                      |               |
|    approx_kl                | 4.9251523     |
|    approx_ln(kl)            | 1.5943552     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | -0.0298       |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.61          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 5.01          |
|    n_updates                | 10            |
|    policy_gradient_loss     | -0.103        |
|    std                      | 0.952         |
|    value_loss               | 74.2          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
----------------------------------------------
| reward                      | [-1.3997289] |
| time/                       |              |
|    fps                      | 159          |
|    iterations               | 2            |
|    time_elapsed             | 25           |
|    total_timesteps          | 4096         |
| train/                      |              |
|    approx_kl                | 3.707985     |
|    approx_ln(kl)            | 1.3104886    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.75        |
|    explained_variance       | 0.0624       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.37         |
|    ln(policy_gradient_loss) | 2.76         |
|    loss                     | 79.4         |
|    n_updates                | 10           |
|    policy_gradient_loss     | 15.9         |
|    std                      | 0.869        |
|    value_loss               | 113          |
----------------------------------------------
---------------------------------------------
| reward                      | [-1.516849] |
| time/                       |             |
|    fps                      | 157         |
|    iterations               | 2           |
|    time_elapsed             | 26          |
|    total_timesteps          | 4096        |
| train/                      |             |
|    approx_kl                | 3.2137928   |
|    approx_ln(kl)            | 1.1674519   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.77       |
|    explained_variance       | -0.0149     |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 9.83        |
|    ln(policy_gradient_loss) | 8.19        |
|    loss                     | 1.85e+04    |
|    n_updates                | 10          |
|    policy_gradient_loss     | 3.62e+03    |
|    std                      | 0.914       |
|    value_loss               | 102         |
---------------------------------------------
--------------------------------------------
| reward                      | [-2.29083] |
| time/                       |            |
|    fps                      | 177        |
|    iterations               | 3          |
|    time_elapsed             | 34         |
|    total_timesteps          | 6144       |
| train/                      |            |
|    approx_kl                | 3.7772315  |
|    approx_ln(kl)            | 1.3289913  |
|    clip_range               | 0.2        |
|    entropy_loss             | -2.74      |
|    explained_variance       | 0.528      |
|    learning_rate            | 0.001      |
|    ln(loss)                 | 1.93       |
|    ln(policy_gradient_loss) | nan        |
|    loss                     | 6.91       |
|    n_updates                | 20         |
|    policy_gradient_loss     | -0.0741    |
|    std                      | 0.949      |
|    value_loss               | 135        |
--------------------------------------------
----------------------------------------------
| reward                      | [-2.6548479] |
| time/                       |              |
|    fps                      | 175          |
|    iterations               | 3            |
|    time_elapsed             | 35           |
|    total_timesteps          | 6144         |
| train/                      |              |
|    approx_kl                | 1.7722753    |
|    approx_ln(kl)            | 0.5722642    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.69        |
|    explained_variance       | 0.254        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.78         |
|    ln(policy_gradient_loss) | 3.46         |
|    loss                     | 323          |
|    n_updates                | 20           |
|    policy_gradient_loss     | 31.7         |
|    std                      | 0.932        |
|    value_loss               | 712          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
-----------------------------------------------
| reward                      | [-0.98679954] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 3             |
|    time_elapsed             | 38            |
|    total_timesteps          | 6144          |
| train/                      |               |
|    approx_kl                | 4.169531      |
|    approx_ln(kl)            | 1.4278035     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.54         |
|    explained_variance       | 0.246         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 4.43          |
|    ln(policy_gradient_loss) | 2.51          |
|    loss                     | 84.1          |
|    n_updates                | 20            |
|    policy_gradient_loss     | 12.4          |
|    std                      | 0.853         |
|    value_loss               | 253           |
-----------------------------------------------
----------------------------------------------
| reward                      | [-1.2324823] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 3            |
|    time_elapsed             | 39           |
|    total_timesteps          | 6144         |
| train/                      |              |
|    approx_kl                | 11.499058    |
|    approx_ln(kl)            | 2.442265     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.6         |
|    explained_variance       | 0.593        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 10.6         |
|    ln(policy_gradient_loss) | 10           |
|    loss                     | 4.13e+04     |
|    n_updates                | 20           |
|    policy_gradient_loss     | 2.28e+04     |
|    std                      | 0.835        |
|    value_loss               | 61.2         |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.36578116] |
| time/                       |               |
|    fps                      | 99            |
|    iterations               | 2             |
|    time_elapsed             | 41            |
|    total_timesteps          | 4096          |
| train/                      |               |
|    approx_kl                | 3.4486141     |
|    approx_ln(kl)            | 1.2379725     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.0213        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.87          |
|    ln(policy_gradient_loss) | 0.155         |
|    loss                     | 6.51          |
|    n_updates                | 10            |
|    policy_gradient_loss     | 1.17          |
|    std                      | 0.905         |
|    value_loss               | 76            |
-----------------------------------------------
----------------------------------------------
| reward                      | [-3.2558935] |
| time/                       |              |
|    fps                      | 174          |
|    iterations               | 4            |
|    time_elapsed             | 46           |
|    total_timesteps          | 8192         |
| train/                      |              |
|    approx_kl                | 3.3167315    |
|    approx_ln(kl)            | 1.1989799    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.69        |
|    explained_variance       | 0.613        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.99         |
|    ln(policy_gradient_loss) | 4.53         |
|    loss                     | 398          |
|    n_updates                | 30           |
|    policy_gradient_loss     | 93.2         |
|    std                      | 0.925        |
|    value_loss               | 699          |
----------------------------------------------
----------------------------------------------
| reward                      | [-2.2760983] |
| time/                       |              |
|    fps                      | 174          |
|    iterations               | 4            |
|    time_elapsed             | 46           |
|    total_timesteps          | 8192         |
| train/                      |              |
|    approx_kl                | 0.4985286    |
|    approx_ln(kl)            | -0.69609433  |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.73        |
|    explained_variance       | 0.645        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.09         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 162          |
|    n_updates                | 30           |
|    policy_gradient_loss     | -0.02        |
|    std                      | 0.949        |
|    value_loss               | 456          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
---------------------------------------------
| reward                      | [-1.695687] |
| time/                       |             |
|    fps                      | 155         |
|    iterations               | 4           |
|    time_elapsed             | 52          |
|    total_timesteps          | 8192        |
| train/                      |             |
|    approx_kl                | 6.545887    |
|    approx_ln(kl)            | 1.8788369   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.46       |
|    explained_variance       | 0.522       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 4.23        |
|    ln(policy_gradient_loss) | 4.02        |
|    loss                     | 68.9        |
|    n_updates                | 30          |
|    policy_gradient_loss     | 55.9        |
|    std                      | 0.775       |
|    value_loss               | 51.1        |
---------------------------------------------
----------------------------------------------
| reward                      | [-0.5557167] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 4            |
|    time_elapsed             | 52           |
|    total_timesteps          | 8192         |
| train/                      |              |
|    approx_kl                | 8.478245     |
|    approx_ln(kl)            | 2.1375034    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.46        |
|    explained_variance       | 0.781        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 10.5         |
|    ln(policy_gradient_loss) | 9.74         |
|    loss                     | 3.5e+04      |
|    n_updates                | 30           |
|    policy_gradient_loss     | 1.7e+04      |
|    std                      | 0.808        |
|    value_loss               | 20.8         |
----------------------------------------------
----------------------------------------------
| reward                      | [-0.6183813] |
| time/                       |              |
|    fps                      | 114          |
|    iterations               | 3            |
|    time_elapsed             | 53           |
|    total_timesteps          | 6144         |
| train/                      |              |
|    approx_kl                | 7.232506     |
|    approx_ln(kl)            | 1.9785856    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.58        |
|    explained_variance       | -3.19        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.41         |
|    ln(policy_gradient_loss) | 1.28         |
|    loss                     | 11.2         |
|    n_updates                | 20           |
|    policy_gradient_loss     | 3.59         |
|    std                      | 0.825        |
|    value_loss               | 80.5         |
----------------------------------------------
----------------------------------------------
| reward                      | [-3.1968524] |
| time/                       |              |
|    fps                      | 173          |
|    iterations               | 5            |
|    time_elapsed             | 59           |
|    total_timesteps          | 10240        |
| train/                      |              |
|    approx_kl                | 0.49163476   |
|    approx_ln(kl)            | -0.71001923  |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.68        |
|    explained_variance       | 0.294        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.35         |
|    ln(policy_gradient_loss) | 2.93         |
|    loss                     | 210          |
|    n_updates                | 40           |
|    policy_gradient_loss     | 18.7         |
|    std                      | 0.921        |
|    value_loss               | 761          |
----------------------------------------------
----------------------------------------------
| reward                      | [-3.1922078] |
| time/                       |              |
|    fps                      | 173          |
|    iterations               | 5            |
|    time_elapsed             | 59           |
|    total_timesteps          | 10240        |
| train/                      |              |
|    approx_kl                | 3.6162214    |
|    approx_ln(kl)            | 1.2854297    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.73        |
|    explained_variance       | 0.677        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.13         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 170          |
|    n_updates                | 40           |
|    policy_gradient_loss     | -0.0703      |
|    std                      | 0.942        |
|    value_loss               | 782          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
-----------------------------------------------
| reward                      | [-0.83356166] |
| time/                       |               |
|    fps                      | 124           |
|    iterations               | 4             |
|    time_elapsed             | 65            |
|    total_timesteps          | 8192          |
| train/                      |               |
|    approx_kl                | 7.828047      |
|    approx_ln(kl)            | 2.057713      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.44         |
|    explained_variance       | 0.552         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 3.82          |
|    ln(policy_gradient_loss) | 1.45          |
|    loss                     | 45.8          |
|    n_updates                | 30            |
|    policy_gradient_loss     | 4.25          |
|    std                      | 0.816         |
|    value_loss               | 265           |
-----------------------------------------------
----------------------------------------------
| reward                      | [-2.5547621] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 5            |
|    time_elapsed             | 66           |
|    total_timesteps          | 10240        |
| train/                      |              |
|    approx_kl                | 0.47375172   |
|    approx_ln(kl)            | -0.74707186  |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.31        |
|    explained_variance       | 0.857        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.1          |
|    ln(policy_gradient_loss) | 1.15         |
|    loss                     | 164          |
|    n_updates                | 40           |
|    policy_gradient_loss     | 3.15         |
|    std                      | 0.771        |
|    value_loss               | 528          |
----------------------------------------------
----------------------------------------------
| reward                      | [-3.8189077] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 5            |
|    time_elapsed             | 66           |
|    total_timesteps          | 10240        |
| train/                      |              |
|    approx_kl                | 6.5285482    |
|    approx_ln(kl)            | 1.8761846    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.38        |
|    explained_variance       | 0.978        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 10.1         |
|    ln(policy_gradient_loss) | 9.85         |
|    loss                     | 2.38e+04     |
|    n_updates                | 40           |
|    policy_gradient_loss     | 1.89e+04     |
|    std                      | 0.776        |
|    value_loss               | 11.4         |
----------------------------------------------
----------------------------------------------
| reward                      | [-2.6171494] |
| time/                       |              |
|    fps                      | 172          |
|    iterations               | 6            |
|    time_elapsed             | 71           |
|    total_timesteps          | 12288        |
| train/                      |              |
|    approx_kl                | 0.48310637   |
|    approx_ln(kl)            | -0.72751844  |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.67        |
|    explained_variance       | 0.239        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.29         |
|    ln(policy_gradient_loss) | 2.53         |
|    loss                     | 199          |
|    n_updates                | 50           |
|    policy_gradient_loss     | 12.5         |
|    std                      | 0.918        |
|    value_loss               | 729          |
----------------------------------------------
----------------------------------------------
| reward                      | [-3.6277473] |
| time/                       |              |
|    fps                      | 172          |
|    iterations               | 6            |
|    time_elapsed             | 71           |
|    total_timesteps          | 12288        |
| train/                      |              |
|    approx_kl                | 0.18299103   |
|    approx_ln(kl)            | -1.6983181   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.72        |
|    explained_variance       | 0.705        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.69         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 297          |
|    n_updates                | 50           |
|    policy_gradient_loss     | -0.0161      |
|    std                      | 0.941        |
|    value_loss               | 773          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
----------------------------------------------
| reward                      | [-3.0691319] |
| time/                       |              |
|    fps                      | 131          |
|    iterations               | 5            |
|    time_elapsed             | 77           |
|    total_timesteps          | 10240        |
| train/                      |              |
|    approx_kl                | 5.837684     |
|    approx_ln(kl)            | 1.7643342    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.43        |
|    explained_variance       | 0.735        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.97         |
|    ln(policy_gradient_loss) | 1.67         |
|    loss                     | 145          |
|    n_updates                | 40           |
|    policy_gradient_loss     | 5.34         |
|    std                      | 0.815        |
|    value_loss               | 691          |
----------------------------------------------
----------------------------------------------
| reward                      | [-3.3214777] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 6            |
|    time_elapsed             | 79           |
|    total_timesteps          | 12288        |
| train/                      |              |
|    approx_kl                | 3.6743665    |
|    approx_ln(kl)            | 1.3013808    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.33        |
|    explained_variance       | 0.663        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 10.2         |
|    ln(policy_gradient_loss) | 9.03         |
|    loss                     | 2.68e+04     |
|    n_updates                | 50           |
|    policy_gradient_loss     | 8.37e+03     |
|    std                      | 0.776        |
|    value_loss               | 681          |
----------------------------------------------
----------------------------------------------
| reward                      | [-3.1323326] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 6            |
|    time_elapsed             | 80           |
|    total_timesteps          | 12288        |
| train/                      |              |
|    approx_kl                | 1.6924056    |
|    approx_ln(kl)            | 0.52615094   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.31        |
|    explained_variance       | 0.624        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.5          |
|    ln(policy_gradient_loss) | 1.75         |
|    loss                     | 90.4         |
|    n_updates                | 50           |
|    policy_gradient_loss     | 5.78         |
|    std                      | 0.768        |
|    value_loss               | 266          |
----------------------------------------------
----------------------------------------------
| reward                      | [-4.6248374] |
| time/                       |              |
|    fps                      | 172          |
|    iterations               | 7            |
|    time_elapsed             | 83           |
|    total_timesteps          | 14336        |
| train/                      |              |
|    approx_kl                | 3.5143037    |
|    approx_ln(kl)            | 1.2568414    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.64        |
|    explained_variance       | 0.586        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.95         |
|    ln(policy_gradient_loss) | 4.81         |
|    loss                     | 385          |
|    n_updates                | 60           |
|    policy_gradient_loss     | 122          |
|    std                      | 0.89         |
|    value_loss               | 278          |
----------------------------------------------
---------------------------------------------
| reward                      | [-4.092288] |
| time/                       |             |
|    fps                      | 171         |
|    iterations               | 7           |
|    time_elapsed             | 83          |
|    total_timesteps          | 14336       |
| train/                      |             |
|    approx_kl                | 0.28499523  |
|    approx_ln(kl)            | -1.2552829  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.71       |
|    explained_variance       | 0.643       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 5.16        |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 173         |
|    n_updates                | 60          |
|    policy_gradient_loss     | -0.0222     |
|    std                      | 0.938       |
|    value_loss               | 357         |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
----------------------------------------------
| reward                      | [-0.4201372] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 7            |
|    time_elapsed             | 93           |
|    total_timesteps          | 14336        |
| train/                      |              |
|    approx_kl                | 1.9119594    |
|    approx_ln(kl)            | 0.64812857   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.32        |
|    explained_variance       | 0.873        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 9.4          |
|    ln(policy_gradient_loss) | 8.58         |
|    loss                     | 1.21e+04     |
|    n_updates                | 60           |
|    policy_gradient_loss     | 5.32e+03     |
|    std                      | 0.77         |
|    value_loss               | 210          |
----------------------------------------------
----------------------------------------------
| reward                      | [-3.3165333] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 7            |
|    time_elapsed             | 93           |
|    total_timesteps          | 14336        |
| train/                      |              |
|    approx_kl                | 0.14986214   |
|    approx_ln(kl)            | -1.8980395   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.3         |
|    explained_variance       | 0.783        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.17         |
|    ln(policy_gradient_loss) | 0.176        |
|    loss                     | 177          |
|    n_updates                | 60           |
|    policy_gradient_loss     | 1.19         |
|    std                      | 0.767        |
|    value_loss               | 465          |
----------------------------------------------
----------------------------------------------
| reward                      | [-4.5343776] |
| time/                       |              |
|    fps                      | 172          |
|    iterations               | 8            |
|    time_elapsed             | 95           |
|    total_timesteps          | 16384        |
| train/                      |              |
|    approx_kl                | 0.094559014  |
|    approx_ln(kl)            | -2.3585312   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.59        |
|    explained_variance       | 0.501        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.3          |
|    ln(policy_gradient_loss) | 0.963        |
|    loss                     | 200          |
|    n_updates                | 70           |
|    policy_gradient_loss     | 2.62         |
|    std                      | 0.881        |
|    value_loss               | 885          |
----------------------------------------------
----------------------------------------------
| reward                      | [-2.7656016] |
| time/                       |              |
|    fps                      | 171          |
|    iterations               | 8            |
|    time_elapsed             | 95           |
|    total_timesteps          | 16384        |
| train/                      |              |
|    approx_kl                | 1.4380286    |
|    approx_ln(kl)            | 0.36327317   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | -0.536       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.07         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 7.9          |
|    n_updates                | 70           |
|    policy_gradient_loss     | -0.153       |
|    std                      | 0.924        |
|    value_loss               | 133          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
----------------------------------------------
| reward                      | [-2.2100453] |
| time/                       |              |
|    fps                      | 172          |
|    iterations               | 9            |
|    time_elapsed             | 107          |
|    total_timesteps          | 18432        |
| train/                      |              |
|    approx_kl                | 3.8349128    |
|    approx_ln(kl)            | 1.3441467    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.58        |
|    explained_variance       | 0.454        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 6.17         |
|    ln(policy_gradient_loss) | 4.81         |
|    loss                     | 478          |
|    n_updates                | 80           |
|    policy_gradient_loss     | 122          |
|    std                      | 0.877        |
|    value_loss               | 1.02e+03     |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.58150315] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 8             |
|    time_elapsed             | 106           |
|    total_timesteps          | 16384         |
| train/                      |               |
|    approx_kl                | 7.659828      |
|    approx_ln(kl)            | 2.0359895     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.31         |
|    explained_variance       | 0.912         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 10.9          |
|    ln(policy_gradient_loss) | 9.94          |
|    loss                     | 5.3e+04       |
|    n_updates                | 70            |
|    policy_gradient_loss     | 2.07e+04      |
|    std                      | 0.767         |
|    value_loss               | 123           |
-----------------------------------------------
----------------------------------------------
| reward                      | [-3.5275884] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 8            |
|    time_elapsed             | 107          |
|    total_timesteps          | 16384        |
| train/                      |              |
|    approx_kl                | 0.03453021   |
|    approx_ln(kl)            | -3.3659205   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.3         |
|    explained_variance       | 0.75         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.97         |
|    ln(policy_gradient_loss) | -1.64        |
|    loss                     | 144          |
|    n_updates                | 70           |
|    policy_gradient_loss     | 0.194        |
|    std                      | 0.766        |
|    value_loss               | 404          |
----------------------------------------------
----------------------------------------------
| reward                      | [-2.1548128] |
| time/                       |              |
|    fps                      | 171          |
|    iterations               | 9            |
|    time_elapsed             | 107          |
|    total_timesteps          | 18432        |
| train/                      |              |
|    approx_kl                | 5.8371444    |
|    approx_ln(kl)            | 1.7642417    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.63        |
|    explained_variance       | -0.0391      |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.7          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 14.9         |
|    n_updates                | 80           |
|    policy_gradient_loss     | -0.125       |
|    std                      | 0.837        |
|    value_loss               | 156          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
----------------------------------------------
| reward                      | [-3.2362745] |
| time/                       |              |
|    fps                      | 110          |
|    iterations               | 6            |
|    time_elapsed             | 110          |
|    total_timesteps          | 12288        |
| train/                      |              |
|    approx_kl                | 2.5373828    |
|    approx_ln(kl)            | 0.93113315   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.43        |
|    explained_variance       | 0.552        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.31         |
|    ln(policy_gradient_loss) | -0.215       |
|    loss                     | 10.1         |
|    n_updates                | 50           |
|    policy_gradient_loss     | 0.806        |
|    std                      | 0.815        |
|    value_loss               | 167          |
----------------------------------------------
srun: Job 96423 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 96423 step creation temporarily disabled, retrying (Requested nodes are busy)
----------------------------------------------
| reward                      | [-1.1219654] |
| time/                       |              |
|    fps                      | 172          |
|    iterations               | 10           |
|    time_elapsed             | 118          |
|    total_timesteps          | 20480        |
| train/                      |              |
|    approx_kl                | 8.154354     |
|    approx_ln(kl)            | 2.098552     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.56        |
|    explained_variance       | 0.554        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 6.38         |
|    ln(policy_gradient_loss) | 5.62         |
|    loss                     | 588          |
|    n_updates                | 90           |
|    policy_gradient_loss     | 276          |
|    std                      | 0.833        |
|    value_loss               | 119          |
----------------------------------------------
srun: Job 96423 step creation temporarily disabled, retrying (Requested nodes are busy)
----------------------------------------------
| reward                      | [-3.1962256] |
| time/                       |              |
|    fps                      | 171          |
|    iterations               | 10           |
|    time_elapsed             | 119          |
|    total_timesteps          | 20480        |
| train/                      |              |
|    approx_kl                | 0.30887356   |
|    approx_ln(kl)            | -1.1748233   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.47        |
|    explained_variance       | 0.849        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.57         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 35.6         |
|    n_updates                | 90           |
|    policy_gradient_loss     | -0.0171      |
|    std                      | 0.829        |
|    value_loss               | 125          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
---------------------------------------------
| reward                      | [-0.555736] |
| time/                       |             |
|    fps                      | 153         |
|    iterations               | 9           |
|    time_elapsed             | 120         |
|    total_timesteps          | 18432       |
| train/                      |             |
|    approx_kl                | 5.66536     |
|    approx_ln(kl)            | 1.7343705   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.27       |
|    explained_variance       | 0.875       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 9.62        |
|    ln(policy_gradient_loss) | 9.6         |
|    loss                     | 1.5e+04     |
|    n_updates                | 80          |
|    policy_gradient_loss     | 1.48e+04    |
|    std                      | 0.737       |
|    value_loss               | 2.65        |
---------------------------------------------
srun: Job 96423 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 96423 step creation temporarily disabled, retrying (Requested nodes are busy)
----------------------------------------------
| reward                      | [-4.2235184] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 9            |
|    time_elapsed             | 121          |
|    total_timesteps          | 18432        |
| train/                      |              |
|    approx_kl                | 0.81859934   |
|    approx_ln(kl)            | -0.20016052  |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.3         |
|    explained_variance       | 0.747        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.63         |
|    ln(policy_gradient_loss) | 1.37         |
|    loss                     | 103          |
|    n_updates                | 80           |
|    policy_gradient_loss     | 3.92         |
|    std                      | 0.764        |
|    value_loss               | 356          |
----------------------------------------------
srun: Job 96423 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 96423 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 96423 step creation temporarily disabled, retrying (Requested nodes are busy)
----------------------------------------------
| reward                      | [-3.7112188] |
| time/                       |              |
|    fps                      | 116          |
|    iterations               | 7            |
|    time_elapsed             | 123          |
|    total_timesteps          | 14336        |
| train/                      |              |
|    approx_kl                | 4.1438704    |
|    approx_ln(kl)            | 1.4216303    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.43        |
|    explained_variance       | 0.589        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.2          |
|    ln(policy_gradient_loss) | 0.889        |
|    loss                     | 66.8         |
|    n_updates                | 60           |
|    policy_gradient_loss     | 2.43         |
|    std                      | 0.814        |
|    value_loss               | 313          |
----------------------------------------------
srun: Job 96423 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 96423 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 96423 step creation temporarily disabled, retrying (Requested nodes are busy)
----------------------------------------------
| reward                      | [-0.6695124] |
| time/                       |              |
|    fps                      | 172          |
|    iterations               | 11           |
|    time_elapsed             | 130          |
|    total_timesteps          | 22528        |
| train/                      |              |
|    approx_kl                | 5.087246     |
|    approx_ln(kl)            | 1.6267366    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.4         |
|    explained_variance       | 0.427        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.92         |
|    ln(policy_gradient_loss) | 5.56         |
|    loss                     | 371          |
|    n_updates                | 100          |
|    policy_gradient_loss     | 260          |
|    std                      | 0.749        |
|    value_loss               | 15.5         |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.32898968] |
| time/                       |               |
|    fps                      | 171           |
|    iterations               | 11            |
|    time_elapsed             | 131           |
|    total_timesteps          | 22528         |
| train/                      |               |
|    approx_kl                | 6.176963      |
|    approx_ln(kl)            | 1.8208268     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.46         |
|    explained_variance       | 0.914         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 3.43          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 30.9          |
|    n_updates                | 100           |
|    policy_gradient_loss     | -0.0757       |
|    std                      | 0.82          |
|    value_loss               | 157           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
-----------------------------------------------
| reward                      | [-0.74072707] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 10            |
|    time_elapsed             | 133           |
|    total_timesteps          | 20480         |
| train/                      |               |
|    approx_kl                | 16.358538     |
|    approx_ln(kl)            | 2.79475       |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.22         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 11.3          |
|    ln(policy_gradient_loss) | 11.2          |
|    loss                     | 8.41e+04      |
|    n_updates                | 90            |
|    policy_gradient_loss     | 7.18e+04      |
|    std                      | 0.721         |
|    value_loss               | 0.88          |
-----------------------------------------------
---------------------------------------------
| reward                      | [-4.680442] |
| time/                       |             |
|    fps                      | 151         |
|    iterations               | 10          |
|    time_elapsed             | 134         |
|    total_timesteps          | 20480       |
| train/                      |             |
|    approx_kl                | 0.9133169   |
|    approx_ln(kl)            | -0.09067236 |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.29       |
|    explained_variance       | 0.837       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 5.17        |
|    ln(policy_gradient_loss) | 1.51        |
|    loss                     | 176         |
|    n_updates                | 90          |
|    policy_gradient_loss     | 4.52        |
|    std                      | 0.76        |
|    value_loss               | 474         |
---------------------------------------------
----------------------------------------------
| reward                      | [-2.5263946] |
| time/                       |              |
|    fps                      | 120          |
|    iterations               | 8            |
|    time_elapsed             | 135          |
|    total_timesteps          | 16384        |
| train/                      |              |
|    approx_kl                | 6.663397     |
|    approx_ln(kl)            | 1.8966293    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.42        |
|    explained_variance       | 0.0455       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.24         |
|    ln(policy_gradient_loss) | 1.36         |
|    loss                     | 69.5         |
|    n_updates                | 70           |
|    policy_gradient_loss     | 3.89         |
|    std                      | 0.799        |
|    value_loss               | 372          |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.69321793] |
| time/                       |               |
|    fps                      | 171           |
|    iterations               | 12            |
|    time_elapsed             | 142           |
|    total_timesteps          | 24576         |
| train/                      |               |
|    approx_kl                | 12.062516     |
|    approx_ln(kl)            | 2.4901028     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.2          |
|    explained_variance       | 0.71          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 6.98          |
|    ln(policy_gradient_loss) | 6.69          |
|    loss                     | 1.08e+03      |
|    n_updates                | 110           |
|    policy_gradient_loss     | 807           |
|    std                      | 0.714         |
|    value_loss               | 8.33          |
-----------------------------------------------
----------------------------------------------
| reward                      | [-1.4119856] |
| time/                       |              |
|    fps                      | 171          |
|    iterations               | 12           |
|    time_elapsed             | 143          |
|    total_timesteps          | 24576        |
| train/                      |              |
|    approx_kl                | 2.9645164    |
|    approx_ln(kl)            | 1.0867139    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.42        |
|    explained_variance       | 0.947        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.978        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 2.66         |
|    n_updates                | 110          |
|    policy_gradient_loss     | -0.077       |
|    std                      | 0.789        |
|    value_loss               | 22.4         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
----------------------------------------------
| reward                      | [-0.4220888] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 11           |
|    time_elapsed             | 146          |
|    total_timesteps          | 22528        |
| train/                      |              |
|    approx_kl                | 18.571451    |
|    approx_ln(kl)            | 2.9216256    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.12        |
|    explained_variance       | 0.967        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 11.5         |
|    ln(policy_gradient_loss) | 10.5         |
|    loss                     | 1.04e+05     |
|    n_updates                | 100          |
|    policy_gradient_loss     | 3.73e+04     |
|    std                      | 0.678        |
|    value_loss               | 1.47         |
----------------------------------------------
---------------------------------------------
| reward                      | [-4.117287] |
| time/                       |             |
|    fps                      | 124         |
|    iterations               | 9           |
|    time_elapsed             | 147         |
|    total_timesteps          | 18432       |
| train/                      |             |
|    approx_kl                | 8.7225685   |
|    approx_ln(kl)            | 2.1659138   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.38       |
|    explained_variance       | 0.773       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 3.47        |
|    ln(policy_gradient_loss) | 1.31        |
|    loss                     | 32.2        |
|    n_updates                | 80          |
|    policy_gradient_loss     | 3.7         |
|    std                      | 0.786       |
|    value_loss               | 248         |
---------------------------------------------
-----------------------------------------------
| reward                      | [-0.66641796] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 11            |
|    time_elapsed             | 148           |
|    total_timesteps          | 22528         |
| train/                      |               |
|    approx_kl                | 0.32957464    |
|    approx_ln(kl)            | -1.1099524    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.28         |
|    explained_variance       | 0.799         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 4.1           |
|    ln(policy_gradient_loss) | 0.522         |
|    loss                     | 60.6          |
|    n_updates                | 100           |
|    policy_gradient_loss     | 1.68          |
|    std                      | 0.758         |
|    value_loss               | 239           |
-----------------------------------------------
-----------------------------------------------
| reward                      | [-0.88277566] |
| time/                       |               |
|    fps                      | 171           |
|    iterations               | 13            |
|    time_elapsed             | 154           |
|    total_timesteps          | 26624         |
| train/                      |               |
|    approx_kl                | 5.4510775     |
|    approx_ln(kl)            | 1.6958133     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.13         |
|    explained_variance       | 0.739         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 6.47          |
|    ln(policy_gradient_loss) | 5.81          |
|    loss                     | 647           |
|    n_updates                | 120           |
|    policy_gradient_loss     | 333           |
|    std                      | 0.676         |
|    value_loss               | 8.13          |
-----------------------------------------------
----------------------------------------------
| reward                      | [-1.6922477] |
| time/                       |              |
|    fps                      | 171          |
|    iterations               | 13           |
|    time_elapsed             | 155          |
|    total_timesteps          | 26624        |
| train/                      |              |
|    approx_kl                | 0.38388738   |
|    approx_ln(kl)            | -0.95740604  |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.36        |
|    explained_variance       | 0.898        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.46         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 86.5         |
|    n_updates                | 120          |
|    policy_gradient_loss     | -0.0432      |
|    std                      | 0.784        |
|    value_loss               | 405          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
---------------------------------------------
| reward                      | [-4.592863] |
| time/                       |             |
|    fps                      | 128         |
|    iterations               | 10          |
|    time_elapsed             | 159         |
|    total_timesteps          | 20480       |
| train/                      |             |
|    approx_kl                | 32.651905   |
|    approx_ln(kl)            | 3.4859033   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.33       |
|    explained_variance       | 0.854       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 4.43        |
|    ln(policy_gradient_loss) | 2.61        |
|    loss                     | 84.3        |
|    n_updates                | 90          |
|    policy_gradient_loss     | 13.6        |
|    std                      | 0.754       |
|    value_loss               | 458         |
---------------------------------------------
-----------------------------------------------
| reward                      | [-0.44576448] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 12            |
|    time_elapsed             | 160           |
|    total_timesteps          | 24576         |
| train/                      |               |
|    approx_kl                | 8.839691      |
|    approx_ln(kl)            | 2.179252      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2            |
|    explained_variance       | 0.977         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 10.7          |
|    ln(policy_gradient_loss) | 10.1          |
|    loss                     | 4.23e+04      |
|    n_updates                | 110           |
|    policy_gradient_loss     | 2.42e+04      |
|    std                      | 0.628         |
|    value_loss               | 0.212         |
-----------------------------------------------
----------------------------------------------
| reward                      | [-1.4589056] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 12           |
|    time_elapsed             | 162          |
|    total_timesteps          | 24576        |
| train/                      |              |
|    approx_kl                | 2.304051     |
|    approx_ln(kl)            | 0.8346689    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.28        |
|    explained_variance       | 0.672        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.78         |
|    ln(policy_gradient_loss) | 1.92         |
|    loss                     | 323          |
|    n_updates                | 110          |
|    policy_gradient_loss     | 6.85         |
|    std                      | 0.758        |
|    value_loss               | 341          |
----------------------------------------------
----------------------------------------------
| reward                      | [-1.3710653] |
| time/                       |              |
|    fps                      | 171          |
|    iterations               | 14           |
|    time_elapsed             | 166          |
|    total_timesteps          | 28672        |
| train/                      |              |
|    approx_kl                | 13.245908    |
|    approx_ln(kl)            | 2.5836887    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2           |
|    explained_variance       | 0.954        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 6.85         |
|    ln(policy_gradient_loss) | 6.68         |
|    loss                     | 942          |
|    n_updates                | 130          |
|    policy_gradient_loss     | 794          |
|    std                      | 0.627        |
|    value_loss               | 2.82         |
----------------------------------------------
----------------------------------------------
| reward                      | [-2.4236674] |
| time/                       |              |
|    fps                      | 171          |
|    iterations               | 14           |
|    time_elapsed             | 167          |
|    total_timesteps          | 28672        |
| train/                      |              |
|    approx_kl                | 0.6834738    |
|    approx_ln(kl)            | -0.38056692  |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.35        |
|    explained_variance       | 0.813        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.24         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 69.7         |
|    n_updates                | 130          |
|    policy_gradient_loss     | -0.0374      |
|    std                      | 0.785        |
|    value_loss               | 243          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
----------------------------------------------
| reward                      | [-0.5798461] |
| time/                       |              |
|    fps                      | 130          |
|    iterations               | 11           |
|    time_elapsed             | 172          |
|    total_timesteps          | 22528        |
| train/                      |              |
|    approx_kl                | 22.754436    |
|    approx_ln(kl)            | 3.1247602    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.27        |
|    explained_variance       | 0.857        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.26         |
|    ln(policy_gradient_loss) | 2.15         |
|    loss                     | 70.6         |
|    n_updates                | 100          |
|    policy_gradient_loss     | 8.62         |
|    std                      | 0.757        |
|    value_loss               | 422          |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.34703362] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 13            |
|    time_elapsed             | 173           |
|    total_timesteps          | 26624         |
| train/                      |               |
|    approx_kl                | 16.099422     |
|    approx_ln(kl)            | 2.7787833     |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.87         |
|    explained_variance       | 0.969         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 11.2          |
|    ln(policy_gradient_loss) | 11.2          |
|    loss                     | 7.4e+04       |
|    n_updates                | 120           |
|    policy_gradient_loss     | 7.43e+04      |
|    std                      | 0.617         |
|    value_loss               | 0.401         |
-----------------------------------------------
----------------------------------------------
| reward                      | [-1.2166212] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 13           |
|    time_elapsed             | 176          |
|    total_timesteps          | 26624        |
| train/                      |              |
|    approx_kl                | 16.286625    |
|    approx_ln(kl)            | 2.7903442    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.24        |
|    explained_variance       | 0.678        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.71         |
|    ln(policy_gradient_loss) | 5            |
|    loss                     | 302          |
|    n_updates                | 120          |
|    policy_gradient_loss     | 148          |
|    std                      | 0.714        |
|    value_loss               | 464          |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.55341727] |
| time/                       |               |
|    fps                      | 171           |
|    iterations               | 15            |
|    time_elapsed             | 178           |
|    total_timesteps          | 30720         |
| train/                      |               |
|    approx_kl                | 17.162174     |
|    approx_ln(kl)            | 2.8427079     |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.89         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 7.19          |
|    ln(policy_gradient_loss) | 6.73          |
|    loss                     | 1.33e+03      |
|    n_updates                | 140           |
|    policy_gradient_loss     | 839           |
|    std                      | 0.6           |
|    value_loss               | 1.35          |
-----------------------------------------------
----------------------------------------------
| reward                      | [-2.9903052] |
| time/                       |              |
|    fps                      | 171          |
|    iterations               | 15           |
|    time_elapsed             | 179          |
|    total_timesteps          | 30720        |
| train/                      |              |
|    approx_kl                | 0.29405743   |
|    approx_ln(kl)            | -1.2239802   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.34        |
|    explained_variance       | 0.762        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.47         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 87.4         |
|    n_updates                | 140          |
|    policy_gradient_loss     | -0.0273      |
|    std                      | 0.773        |
|    value_loss               | 359          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
----------------------------------------------
| reward                      | [-1.4587524] |
| time/                       |              |
|    fps                      | 133          |
|    iterations               | 12           |
|    time_elapsed             | 184          |
|    total_timesteps          | 24576        |
| train/                      |              |
|    approx_kl                | 4.359727     |
|    approx_ln(kl)            | 1.4724094    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.28        |
|    explained_variance       | 0.91         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.98         |
|    ln(policy_gradient_loss) | 1.25         |
|    loss                     | 53.7         |
|    n_updates                | 110          |
|    policy_gradient_loss     | 3.5          |
|    std                      | 0.76         |
|    value_loss               | 343          |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.34494117] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 14            |
|    time_elapsed             | 186           |
|    total_timesteps          | 28672         |
| train/                      |               |
|    approx_kl                | 26.060455     |
|    approx_ln(kl)            | 3.2604191     |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.78         |
|    explained_variance       | 0.944         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 11.8          |
|    ln(policy_gradient_loss) | 11.4          |
|    loss                     | 1.29e+05      |
|    n_updates                | 130           |
|    policy_gradient_loss     | 9.05e+04      |
|    std                      | 0.567         |
|    value_loss               | 0.562         |
-----------------------------------------------
---------------------------------------------
| reward                      | [-2.008963] |
| time/                       |             |
|    fps                      | 150         |
|    iterations               | 14          |
|    time_elapsed             | 190         |
|    total_timesteps          | 28672       |
| train/                      |             |
|    approx_kl                | 6.00382     |
|    approx_ln(kl)            | 1.7923958   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.16       |
|    explained_variance       | 0.76        |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 4.9         |
|    ln(policy_gradient_loss) | 4.09        |
|    loss                     | 135         |
|    n_updates                | 130         |
|    policy_gradient_loss     | 60          |
|    std                      | 0.717       |
|    value_loss               | 52.1        |
---------------------------------------------
----------------------------------------------
| reward                      | [-0.6018542] |
| time/                       |              |
|    fps                      | 171          |
|    iterations               | 16           |
|    time_elapsed             | 190          |
|    total_timesteps          | 32768        |
| train/                      |              |
|    approx_kl                | 6.7448444    |
|    approx_ln(kl)            | 1.9087784    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.74        |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 6.37         |
|    ln(policy_gradient_loss) | 5.84         |
|    loss                     | 582          |
|    n_updates                | 150          |
|    policy_gradient_loss     | 343          |
|    std                      | 0.544        |
|    value_loss               | 2.51         |
----------------------------------------------
----------------------------------------------
| reward                      | [-2.8136556] |
| time/                       |              |
|    fps                      | 171          |
|    iterations               | 16           |
|    time_elapsed             | 191          |
|    total_timesteps          | 32768        |
| train/                      |              |
|    approx_kl                | 0.6780199    |
|    approx_ln(kl)            | -0.38857868  |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.31        |
|    explained_variance       | 0.856        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.43         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 84           |
|    n_updates                | 150          |
|    policy_gradient_loss     | -0.0617      |
|    std                      | 0.758        |
|    value_loss               | 321          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
----------------------------------------------
| reward                      | [-1.9466828] |
| time/                       |              |
|    fps                      | 135          |
|    iterations               | 13           |
|    time_elapsed             | 196          |
|    total_timesteps          | 26624        |
| train/                      |              |
|    approx_kl                | 16.99836     |
|    approx_ln(kl)            | 2.8331168    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.28        |
|    explained_variance       | 0.945        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.71         |
|    ln(policy_gradient_loss) | 2.62         |
|    loss                     | 40.7         |
|    n_updates                | 120          |
|    policy_gradient_loss     | 13.7         |
|    std                      | 0.748        |
|    value_loss               | 292          |
----------------------------------------------
----------------------------------------------
| reward                      | [-0.5122935] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 15           |
|    time_elapsed             | 200          |
|    total_timesteps          | 30720        |
| train/                      |              |
|    approx_kl                | 31.999607    |
|    approx_ln(kl)            | 3.4657235    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.66        |
|    explained_variance       | 0.891        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 11.9         |
|    ln(policy_gradient_loss) | 11.8         |
|    loss                     | 1.53e+05     |
|    n_updates                | 140          |
|    policy_gradient_loss     | 1.27e+05     |
|    std                      | 0.541        |
|    value_loss               | 0.511        |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.67712843] |
| time/                       |               |
|    fps                      | 171           |
|    iterations               | 17            |
|    time_elapsed             | 202           |
|    total_timesteps          | 34816         |
| train/                      |               |
|    approx_kl                | 8.947841      |
|    approx_ln(kl)            | 2.1914122     |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.53         |
|    explained_variance       | 0.972         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 6.36          |
|    ln(policy_gradient_loss) | 6.2           |
|    loss                     | 581           |
|    n_updates                | 160           |
|    policy_gradient_loss     | 492           |
|    std                      | 0.502         |
|    value_loss               | 1.83          |
-----------------------------------------------
-----------------------------------------------
| reward                      | [-0.78487206] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 15            |
|    time_elapsed             | 204           |
|    total_timesteps          | 30720         |
| train/                      |               |
|    approx_kl                | 2.0719595     |
|    approx_ln(kl)            | 0.72849476    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.16         |
|    explained_variance       | 0.815         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 3.48          |
|    ln(policy_gradient_loss) | 2.5           |
|    loss                     | 32.6          |
|    n_updates                | 140           |
|    policy_gradient_loss     | 12.2          |
|    std                      | 0.718         |
|    value_loss               | 40.7          |
-----------------------------------------------
---------------------------------------------
| reward                      | [-2.320124] |
| time/                       |             |
|    fps                      | 170         |
|    iterations               | 17          |
|    time_elapsed             | 203         |
|    total_timesteps          | 34816       |
| train/                      |             |
|    approx_kl                | 4.0545197   |
|    approx_ln(kl)            | 1.3998322   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.24       |
|    explained_variance       | 0.899       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 3.39        |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 29.6        |
|    n_updates                | 160         |
|    policy_gradient_loss     | -0.108      |
|    std                      | 0.716       |
|    value_loss               | 159         |
---------------------------------------------
----------------------------------------------
| reward                      | [-2.7557771] |
| time/                       |              |
|    fps                      | 137          |
|    iterations               | 14           |
|    time_elapsed             | 208          |
|    total_timesteps          | 28672        |
| train/                      |              |
|    approx_kl                | 45.110607    |
|    approx_ln(kl)            | 3.8091173    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.24        |
|    explained_variance       | 0.96         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.98         |
|    ln(policy_gradient_loss) | 3.27         |
|    loss                     | 53.6         |
|    n_updates                | 130          |
|    policy_gradient_loss     | 26.2         |
|    std                      | 0.725        |
|    value_loss               | 205          |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.38131663] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 16            |
|    time_elapsed             | 213           |
|    total_timesteps          | 32768         |
| train/                      |               |
|    approx_kl                | 50.879787     |
|    approx_ln(kl)            | 3.9294658     |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.52         |
|    explained_variance       | 0.474         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 12.5          |
|    ln(policy_gradient_loss) | 12.2          |
|    loss                     | 2.81e+05      |
|    n_updates                | 150           |
|    policy_gradient_loss     | 1.96e+05      |
|    std                      | 0.495         |
|    value_loss               | 0.25          |
-----------------------------------------------
-----------------------------------------------
| reward                      | [-0.60403186] |
| time/                       |               |
|    fps                      | 171           |
|    iterations               | 18            |
|    time_elapsed             | 214           |
|    total_timesteps          | 36864         |
| train/                      |               |
|    approx_kl                | 6.0153637     |
|    approx_ln(kl)            | 1.7943168     |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.42         |
|    explained_variance       | 0.976         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 6.25          |
|    ln(policy_gradient_loss) | 5.72          |
|    loss                     | 520           |
|    n_updates                | 170           |
|    policy_gradient_loss     | 304           |
|    std                      | 0.476         |
|    value_loss               | 2.5           |
-----------------------------------------------
----------------------------------------------
| reward                      | [-2.7898338] |
| time/                       |              |
|    fps                      | 170          |
|    iterations               | 18           |
|    time_elapsed             | 215          |
|    total_timesteps          | 36864        |
| train/                      |              |
|    approx_kl                | 6.5066276    |
|    approx_ln(kl)            | 1.8728213    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.17        |
|    explained_variance       | 0.977        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.12         |
|    ln(policy_gradient_loss) | -2.98        |
|    loss                     | 3.07         |
|    n_updates                | 170          |
|    policy_gradient_loss     | 0.0506       |
|    std                      | 0.696        |
|    value_loss               | 23.8         |
----------------------------------------------
----------------------------------------------
| reward                      | [-2.4527533] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 16           |
|    time_elapsed             | 217          |
|    total_timesteps          | 32768        |
| train/                      |              |
|    approx_kl                | 7.1838894    |
|    approx_ln(kl)            | 1.971841     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.16        |
|    explained_variance       | 0.784        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.89         |
|    ln(policy_gradient_loss) | 4.11         |
|    loss                     | 133          |
|    n_updates                | 150          |
|    policy_gradient_loss     | 61.1         |
|    std                      | 0.713        |
|    value_loss               | 94.8         |
----------------------------------------------
----------------------------------------------
| reward                      | [-2.8294907] |
| time/                       |              |
|    fps                      | 139          |
|    iterations               | 15           |
|    time_elapsed             | 220          |
|    total_timesteps          | 30720        |
| train/                      |              |
|    approx_kl                | 13.381692    |
|    approx_ln(kl)            | 2.5938876    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.14        |
|    explained_variance       | 0.968        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.05         |
|    ln(policy_gradient_loss) | 2.21         |
|    loss                     | 21           |
|    n_updates                | 140          |
|    policy_gradient_loss     | 9.09         |
|    std                      | 0.663        |
|    value_loss               | 29.9         |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.35690102] |
| time/                       |               |
|    fps                      | 171           |
|    iterations               | 19            |
|    time_elapsed             | 226           |
|    total_timesteps          | 38912         |
| train/                      |               |
|    approx_kl                | 12.773539     |
|    approx_ln(kl)            | 2.5473757     |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.3          |
|    explained_variance       | 0.971         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 7.21          |
|    ln(policy_gradient_loss) | 6.46          |
|    loss                     | 1.35e+03      |
|    n_updates                | 180           |
|    policy_gradient_loss     | 641           |
|    std                      | 0.455         |
|    value_loss               | 2.57          |
-----------------------------------------------
-----------------------------------------------
| reward                      | [-0.23871844] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 17            |
|    time_elapsed             | 227           |
|    total_timesteps          | 34816         |
| train/                      |               |
|    approx_kl                | 8.384416      |
|    approx_ln(kl)            | 2.1263747     |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.42         |
|    explained_variance       | 0.975         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 11.1          |
|    ln(policy_gradient_loss) | 10.6          |
|    loss                     | 6.82e+04      |
|    n_updates                | 160           |
|    policy_gradient_loss     | 4.1e+04       |
|    std                      | 0.483         |
|    value_loss               | 0.172         |
-----------------------------------------------
---------------------------------------------
| reward                      | [-3.010636] |
| time/                       |             |
|    fps                      | 170         |
|    iterations               | 19          |
|    time_elapsed             | 227         |
|    total_timesteps          | 38912       |
| train/                      |             |
|    approx_kl                | 4.2959013   |
|    approx_ln(kl)            | 1.4576614   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.1        |
|    explained_variance       | 0.831       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 0.343       |
|    ln(policy_gradient_loss) | -5.57       |
|    loss                     | 1.41        |
|    n_updates                | 180         |
|    policy_gradient_loss     | 0.00382     |
|    std                      | 0.699       |
|    value_loss               | 23.3        |
---------------------------------------------
----------------------------------------------
| reward                      | [-3.7039099] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 17           |
|    time_elapsed             | 231          |
|    total_timesteps          | 34816        |
| train/                      |              |
|    approx_kl                | 16.957767    |
|    approx_ln(kl)            | 2.830726     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.08        |
|    explained_variance       | 0.963        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.33         |
|    ln(policy_gradient_loss) | 4.88         |
|    loss                     | 205          |
|    n_updates                | 160          |
|    policy_gradient_loss     | 132          |
|    std                      | 0.656        |
|    value_loss               | 25           |
----------------------------------------------
----------------------------------------------
| reward                      | [-1.3561629] |
| time/                       |              |
|    fps                      | 140          |
|    iterations               | 16           |
|    time_elapsed             | 232          |
|    total_timesteps          | 32768        |
| train/                      |              |
|    approx_kl                | 8.960863     |
|    approx_ln(kl)            | 2.1928666    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2           |
|    explained_variance       | 0.979        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.75         |
|    ln(policy_gradient_loss) | 1.73         |
|    loss                     | 15.6         |
|    n_updates                | 150          |
|    policy_gradient_loss     | 5.66         |
|    std                      | 0.643        |
|    value_loss               | 22           |
----------------------------------------------
srun: Job 96423 step creation still disabled, retrying (Requested nodes are busy)
-----------------------------------------------
| reward                      | [-0.72349167] |
| time/                       |               |
|    fps                      | 171           |
|    iterations               | 20            |
|    time_elapsed             | 238           |
|    total_timesteps          | 40960         |
| train/                      |               |
|    approx_kl                | 5.889553      |
|    approx_ln(kl)            | 1.7731801     |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.22         |
|    explained_variance       | 0.718         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 5.8           |
|    ln(policy_gradient_loss) | 5.72          |
|    loss                     | 330           |
|    n_updates                | 190           |
|    policy_gradient_loss     | 304           |
|    std                      | 0.441         |
|    value_loss               | 19.8          |
-----------------------------------------------
srun: Job 96423 step creation still disabled, retrying (Requested nodes are busy)
----------------------------------------------
| reward                      | [-2.9703727] |
| time/                       |              |
|    fps                      | 170          |
|    iterations               | 20           |
|    time_elapsed             | 240          |
|    total_timesteps          | 40960        |
| train/                      |              |
|    approx_kl                | 8.034613     |
|    approx_ln(kl)            | 2.0837588    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.1         |
|    explained_variance       | 0.977        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.69         |
|    ln(policy_gradient_loss) | -2.32        |
|    loss                     | 5.4          |
|    n_updates                | 190          |
|    policy_gradient_loss     | 0.0983       |
|    std                      | 0.684        |
|    value_loss               | 20           |
----------------------------------------------
----------------------------------------------
| reward                      | [-0.3597041] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 18           |
|    time_elapsed             | 240          |
|    total_timesteps          | 36864        |
| train/                      |              |
|    approx_kl                | 24.594013    |
|    approx_ln(kl)            | 3.202503     |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.33        |
|    explained_variance       | 0.935        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 11.8         |
|    ln(policy_gradient_loss) | 11.3         |
|    loss                     | 1.37e+05     |
|    n_updates                | 170          |
|    policy_gradient_loss     | 8.32e+04     |
|    std                      | 0.458        |
|    value_loss               | 1.42         |
----------------------------------------------
srun: Job 96423 step creation still disabled, retrying (Requested nodes are busy)
-----------------------------------------------
| reward                      | [-0.37075916] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 18            |
|    time_elapsed             | 244           |
|    total_timesteps          | 36864         |
| train/                      |               |
|    approx_kl                | 5.8008556     |
|    approx_ln(kl)            | 1.7580054     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2            |
|    explained_variance       | 0.94          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 4.59          |
|    ln(policy_gradient_loss) | 4.34          |
|    loss                     | 98.1          |
|    n_updates                | 170           |
|    policy_gradient_loss     | 76.7          |
|    std                      | 0.664         |
|    value_loss               | 42.8          |
-----------------------------------------------
---------------------------------------------
| reward                      | [-2.320791] |
| time/                       |             |
|    fps                      | 142         |
|    iterations               | 17          |
|    time_elapsed             | 244         |
|    total_timesteps          | 34816       |
| train/                      |             |
|    approx_kl                | 8.117679    |
|    approx_ln(kl)            | 2.0940442   |
|    clip_range               | 0.2         |
|    entropy_loss             | -1.95       |
|    explained_variance       | 0.876       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 3.01        |
|    ln(policy_gradient_loss) | 1.76        |
|    loss                     | 20.3        |
|    n_updates                | 160         |
|    policy_gradient_loss     | 5.79        |
|    std                      | 0.641       |
|    value_loss               | 146         |
---------------------------------------------
srun: Job 96423 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 96423 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 96423 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 96423 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 96423 step creation still disabled, retrying (Requested nodes are busy)
---------------------------------------------
| reward                      | [-1.002451] |
| time/                       |             |
|    fps                      | 171         |
|    iterations               | 21          |
|    time_elapsed             | 250         |
|    total_timesteps          | 43008       |
| train/                      |             |
|    approx_kl                | 15.085259   |
|    approx_ln(kl)            | 2.7137182   |
|    clip_range               | 0.2         |
|    entropy_loss             | -1.13       |
|    explained_variance       | 0.914       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 7.27        |
|    ln(policy_gradient_loss) | 6.87        |
|    loss                     | 1.43e+03    |
|    n_updates                | 200         |
|    policy_gradient_loss     | 967         |
|    std                      | 0.418       |
|    value_loss               | 0.91        |
---------------------------------------------
srun: Job 96423 step creation still disabled, retrying (Requested nodes are busy)
-----------------------------------------------
| reward                      | [-0.34694397] |
| time/                       |               |
|    fps                      | 170           |
|    iterations               | 21            |
|    time_elapsed             | 252           |
|    total_timesteps          | 43008         |
| train/                      |               |
|    approx_kl                | 11.730324     |
|    approx_ln(kl)            | 2.4621773     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.05         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.08          |
|    ln(policy_gradient_loss) | -2.57         |
|    loss                     | 2.94          |
|    n_updates                | 200           |
|    policy_gradient_loss     | 0.0768        |
|    std                      | 0.664         |
|    value_loss               | 8.01          |
-----------------------------------------------
srun: Job 96423 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 96423 step creation still disabled, retrying (Requested nodes are busy)
-----------------------------------------------
| reward                      | [-0.51532376] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 19            |
|    time_elapsed             | 253           |
|    total_timesteps          | 38912         |
| train/                      |               |
|    approx_kl                | 14.105755     |
|    approx_ln(kl)            | 2.6465828     |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.23         |
|    explained_variance       | 0.906         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 11.1          |
|    ln(policy_gradient_loss) | 11            |
|    loss                     | 6.66e+04      |
|    n_updates                | 180           |
|    policy_gradient_loss     | 5.88e+04      |
|    std                      | 0.437         |
|    value_loss               | 0.18          |
-----------------------------------------------
----------------------------------------------
| reward                      | [-3.2913089] |
| time/                       |              |
|    fps                      | 143          |
|    iterations               | 18           |
|    time_elapsed             | 256          |
|    total_timesteps          | 36864        |
| train/                      |              |
|    approx_kl                | 2.639465     |
|    approx_ln(kl)            | 0.9705763    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.94        |
|    explained_variance       | 0.956        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.73         |
|    ln(policy_gradient_loss) | 0.338        |
|    loss                     | 5.64         |
|    n_updates                | 170          |
|    policy_gradient_loss     | 1.4          |
|    std                      | 0.638        |
|    value_loss               | 34.2         |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.54937035] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 19            |
|    time_elapsed             | 258           |
|    total_timesteps          | 38912         |
| train/                      |               |
|    approx_kl                | 7.6428194     |
|    approx_ln(kl)            | 2.0337665     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2            |
|    explained_variance       | 0.737         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 4.93          |
|    ln(policy_gradient_loss) | 3.74          |
|    loss                     | 138           |
|    n_updates                | 180           |
|    policy_gradient_loss     | 42.2          |
|    std                      | 0.665         |
|    value_loss               | 109           |
-----------------------------------------------
----------------------------------------------
| reward                      | [-0.3661987] |
| time/                       |              |
|    fps                      | 171          |
|    iterations               | 22           |
|    time_elapsed             | 262          |
|    total_timesteps          | 45056        |
| train/                      |              |
|    approx_kl                | 15.929699    |
|    approx_ln(kl)            | 2.7681851    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.03        |
|    explained_variance       | 0.982        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 7.29         |
|    ln(policy_gradient_loss) | 6.87         |
|    loss                     | 1.47e+03     |
|    n_updates                | 210          |
|    policy_gradient_loss     | 966          |
|    std                      | 0.401        |
|    value_loss               | 0.73         |
----------------------------------------------
----------------------------------------------
| reward                      | [-0.5924783] |
| time/                       |              |
|    fps                      | 170          |
|    iterations               | 22           |
|    time_elapsed             | 264          |
|    total_timesteps          | 45056        |
| train/                      |              |
|    approx_kl                | 10.249247    |
|    approx_ln(kl)            | 2.3272042    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2           |
|    explained_variance       | 0.99         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.82         |
|    ln(policy_gradient_loss) | -2.32        |
|    loss                     | 2.27         |
|    n_updates                | 210          |
|    policy_gradient_loss     | 0.098        |
|    std                      | 0.65         |
|    value_loss               | 20.3         |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.49388275] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 20            |
|    time_elapsed             | 267           |
|    total_timesteps          | 40960         |
| train/                      |               |
|    approx_kl                | 15.718561     |
|    approx_ln(kl)            | 2.7548423     |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.12         |
|    explained_variance       | 0.895         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 10.9          |
|    ln(policy_gradient_loss) | 11.1          |
|    loss                     | 5.6e+04       |
|    n_updates                | 190           |
|    policy_gradient_loss     | 6.66e+04      |
|    std                      | 0.414         |
|    value_loss               | 0.119         |
-----------------------------------------------
---------------------------------------------
| reward                      | [-3.862592] |
| time/                       |             |
|    fps                      | 144         |
|    iterations               | 19          |
|    time_elapsed             | 268         |
|    total_timesteps          | 38912       |
| train/                      |             |
|    approx_kl                | 10.538782   |
|    approx_ln(kl)            | 2.355062    |
|    clip_range               | 0.2         |
|    entropy_loss             | -1.94       |
|    explained_variance       | 0.905       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 4.54        |
|    ln(policy_gradient_loss) | 1.48        |
|    loss                     | 93.7        |
|    n_updates                | 180         |
|    policy_gradient_loss     | 4.4         |
|    std                      | 0.637       |
|    value_loss               | 140         |
---------------------------------------------
----------------------------------------------
| reward                      | [-2.8048718] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 20           |
|    time_elapsed             | 272          |
|    total_timesteps          | 40960        |
| train/                      |              |
|    approx_kl                | 20.656769    |
|    approx_ln(kl)            | 3.028043     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.07        |
|    explained_variance       | 0.967        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.4          |
|    ln(policy_gradient_loss) | 5.47         |
|    loss                     | 221          |
|    n_updates                | 190          |
|    policy_gradient_loss     | 238          |
|    std                      | 0.7          |
|    value_loss               | 2.99         |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.22730282] |
| time/                       |               |
|    fps                      | 171           |
|    iterations               | 23            |
|    time_elapsed             | 274           |
|    total_timesteps          | 47104         |
| train/                      |               |
|    approx_kl                | 15.730103     |
|    approx_ln(kl)            | 2.7555761     |
|    clip_range               | 0.2           |
|    entropy_loss             | -0.957        |
|    explained_variance       | 0.977         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 7.21          |
|    ln(policy_gradient_loss) | 6.97          |
|    loss                     | 1.36e+03      |
|    n_updates                | 220           |
|    policy_gradient_loss     | 1.06e+03      |
|    std                      | 0.39          |
|    value_loss               | 0.756         |
-----------------------------------------------
----------------------------------------------
| reward                      | [-0.4986057] |
| time/                       |              |
|    fps                      | 170          |
|    iterations               | 23           |
|    time_elapsed             | 276          |
|    total_timesteps          | 47104        |
| train/                      |              |
|    approx_kl                | 16.430246    |
|    approx_ln(kl)            | 2.799124     |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.92        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.32         |
|    ln(policy_gradient_loss) | -1.65        |
|    loss                     | 1.38         |
|    n_updates                | 220          |
|    policy_gradient_loss     | 0.193        |
|    std                      | 0.621        |
|    value_loss               | 1.67         |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.55288804] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 21            |
|    time_elapsed             | 280           |
|    total_timesteps          | 43008         |
| train/                      |               |
|    approx_kl                | 9.028229      |
|    approx_ln(kl)            | 2.2003562     |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.04         |
|    explained_variance       | 0.949         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 10.7          |
|    ln(policy_gradient_loss) | 10.3          |
|    loss                     | 4.28e+04      |
|    n_updates                | 200           |
|    policy_gradient_loss     | 2.89e+04      |
|    std                      | 0.396         |
|    value_loss               | 0.105         |
-----------------------------------------------
---------------------------------------------
| reward                      | [-3.552416] |
| time/                       |             |
|    fps                      | 145         |
|    iterations               | 20          |
|    time_elapsed             | 280         |
|    total_timesteps          | 40960       |
| train/                      |             |
|    approx_kl                | 0.80465496  |
|    approx_ln(kl)            | -0.21734172 |
|    clip_range               | 0.2         |
|    entropy_loss             | -1.93       |
|    explained_variance       | 0.928       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 3.35        |
|    ln(policy_gradient_loss) | 0.187       |
|    loss                     | 28.6        |
|    n_updates                | 190         |
|    policy_gradient_loss     | 1.21        |
|    std                      | 0.635       |
|    value_loss               | 136         |
---------------------------------------------
----------------------------------------------
| reward                      | [-1.7353287] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 21           |
|    time_elapsed             | 286          |
|    total_timesteps          | 43008        |
| train/                      |              |
|    approx_kl                | 4.7323008    |
|    approx_ln(kl)            | 1.5544115    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.13        |
|    explained_variance       | 0.91         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.35         |
|    ln(policy_gradient_loss) | 4.49         |
|    loss                     | 77.4         |
|    n_updates                | 200          |
|    policy_gradient_loss     | 88.7         |
|    std                      | 0.7          |
|    value_loss               | 26.7         |
----------------------------------------------
----------------------------------------------
| reward                      | [-0.6314634] |
| time/                       |              |
|    fps                      | 171          |
|    iterations               | 24           |
|    time_elapsed             | 287          |
|    total_timesteps          | 49152        |
| train/                      |              |
|    approx_kl                | 5.8257575    |
|    approx_ln(kl)            | 1.762289     |
|    clip_range               | 0.2          |
|    entropy_loss             | -0.881       |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 6.38         |
|    ln(policy_gradient_loss) | 5.76         |
|    loss                     | 590          |
|    n_updates                | 230          |
|    policy_gradient_loss     | 316          |
|    std                      | 0.366        |
|    value_loss               | 0.419        |
----------------------------------------------
----------------------------------------------
| reward                      | [-0.5758594] |
| time/                       |              |
|    fps                      | 170          |
|    iterations               | 24           |
|    time_elapsed             | 288          |
|    total_timesteps          | 49152        |
| train/                      |              |
|    approx_kl                | 11.795057    |
|    approx_ln(kl)            | 2.4676805    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.88        |
|    explained_variance       | 0.47         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.78         |
|    ln(policy_gradient_loss) | -0.964       |
|    loss                     | 2.18         |
|    n_updates                | 230          |
|    policy_gradient_loss     | 0.381        |
|    std                      | 0.612        |
|    value_loss               | 8.01         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
-----------------------------------------------
| reward                      | [-0.55638176] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 21            |
|    time_elapsed             | 293           |
|    total_timesteps          | 43008         |
| train/                      |               |
|    approx_kl                | 18.785301     |
|    approx_ln(kl)            | 2.9330747     |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.92         |
|    explained_variance       | 0.966         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 2.97          |
|    ln(policy_gradient_loss) | 2.82          |
|    loss                     | 19.5          |
|    n_updates                | 200           |
|    policy_gradient_loss     | 16.8          |
|    std                      | 0.622         |
|    value_loss               | 43.2          |
-----------------------------------------------
----------------------------------------------
| reward                      | [-0.4697485] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 22           |
|    time_elapsed             | 294          |
|    total_timesteps          | 45056        |
| train/                      |              |
|    approx_kl                | 24.292416    |
|    approx_ln(kl)            | 3.190164     |
|    clip_range               | 0.2          |
|    entropy_loss             | -0.939       |
|    explained_variance       | 0.871        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 11.5         |
|    ln(policy_gradient_loss) | 11.6         |
|    loss                     | 1e+05        |
|    n_updates                | 210          |
|    policy_gradient_loss     | 1.14e+05     |
|    std                      | 0.373        |
|    value_loss               | 0.144        |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.61629456] |
| time/                       |               |
|    fps                      | 171           |
|    iterations               | 25            |
|    time_elapsed             | 298           |
|    total_timesteps          | 51200         |
| train/                      |               |
|    approx_kl                | 15.810982     |
|    approx_ln(kl)            | 2.7607048     |
|    clip_range               | 0.2           |
|    entropy_loss             | -0.76         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 7.32          |
|    ln(policy_gradient_loss) | 6.6           |
|    loss                     | 1.51e+03      |
|    n_updates                | 240           |
|    policy_gradient_loss     | 738           |
|    std                      | 0.349         |
|    value_loss               | 0.532         |
-----------------------------------------------
---------------------------------------------
| reward                      | [-2.659359] |
| time/                       |             |
|    fps                      | 150         |
|    iterations               | 22          |
|    time_elapsed             | 300         |
|    total_timesteps          | 45056       |
| train/                      |             |
|    approx_kl                | 3.303982    |
|    approx_ln(kl)            | 1.1951284   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.1        |
|    explained_variance       | 0.899       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 3.75        |
|    ln(policy_gradient_loss) | 2.95        |
|    loss                     | 42.7        |
|    n_updates                | 210         |
|    policy_gradient_loss     | 19          |
|    std                      | 0.687       |
|    value_loss               | 13.9        |
---------------------------------------------
----------------------------------------------
| reward                      | [-1.0265074] |
| time/                       |              |
|    fps                      | 170          |
|    iterations               | 25           |
|    time_elapsed             | 300          |
|    total_timesteps          | 51200        |
| train/                      |              |
|    approx_kl                | 4.477354     |
|    approx_ln(kl)            | 1.4990323    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.79        |
|    explained_variance       | 0.0718       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.91        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.148        |
|    n_updates                | 240          |
|    policy_gradient_loss     | -0.102       |
|    std                      | 0.564        |
|    value_loss               | 12           |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
-----------------------------------------------
| reward                      | [-0.33603618] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 22            |
|    time_elapsed             | 305           |
|    total_timesteps          | 45056         |
| train/                      |               |
|    approx_kl                | 9.186129      |
|    approx_ln(kl)            | 2.2176945     |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.87         |
|    explained_variance       | 0.949         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 2.73          |
|    ln(policy_gradient_loss) | 1.52          |
|    loss                     | 15.3          |
|    n_updates                | 210           |
|    policy_gradient_loss     | 4.58          |
|    std                      | 0.601         |
|    value_loss               | 26.2          |
-----------------------------------------------
----------------------------------------------
| reward                      | [-0.4535343] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 23           |
|    time_elapsed             | 307          |
|    total_timesteps          | 47104        |
| train/                      |              |
|    approx_kl                | 12.212633    |
|    approx_ln(kl)            | 2.502471     |
|    clip_range               | 0.2          |
|    entropy_loss             | -0.85        |
|    explained_variance       | 0.871        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 11           |
|    ln(policy_gradient_loss) | 10.7         |
|    loss                     | 5.89e+04     |
|    n_updates                | 220          |
|    policy_gradient_loss     | 4.49e+04     |
|    std                      | 0.367        |
|    value_loss               | 0.323        |
----------------------------------------------
----------------------------------------------
| reward                      | [-0.7789509] |
| time/                       |              |
|    fps                      | 171          |
|    iterations               | 26           |
|    time_elapsed             | 310          |
|    total_timesteps          | 53248        |
| train/                      |              |
|    approx_kl                | 8.667383     |
|    approx_ln(kl)            | 2.1595669    |
|    clip_range               | 0.2          |
|    entropy_loss             | -0.687       |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 6.62         |
|    ln(policy_gradient_loss) | 6.14         |
|    loss                     | 753          |
|    n_updates                | 250          |
|    policy_gradient_loss     | 464          |
|    std                      | 0.337        |
|    value_loss               | 0.552        |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.39892527] |
| time/                       |               |
|    fps                      | 170           |
|    iterations               | 26            |
|    time_elapsed             | 312           |
|    total_timesteps          | 53248         |
| train/                      |               |
|    approx_kl                | 20.719429     |
|    approx_ln(kl)            | 3.031072      |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.69         |
|    explained_variance       | -2.43         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.258        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.773         |
|    n_updates                | 250           |
|    policy_gradient_loss     | -3.33         |
|    std                      | 0.57          |
|    value_loss               | 4.53          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
-----------------------------------------------
| reward                      | [-0.71672475] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 23            |
|    time_elapsed             | 313           |
|    total_timesteps          | 47104         |
| train/                      |               |
|    approx_kl                | 10.00577      |
|    approx_ln(kl)            | 2.3031619     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.01         |
|    explained_variance       | 0.867         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 4.88          |
|    ln(policy_gradient_loss) | 4.45          |
|    loss                     | 132           |
|    n_updates                | 220           |
|    policy_gradient_loss     | 85.7          |
|    std                      | 0.622         |
|    value_loss               | 5.38          |
-----------------------------------------------
-----------------------------------------------
| reward                      | [-0.53120166] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 23            |
|    time_elapsed             | 317           |
|    total_timesteps          | 47104         |
| train/                      |               |
|    approx_kl                | 12.246212     |
|    approx_ln(kl)            | 2.5052166     |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.82         |
|    explained_variance       | 0.966         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 2.96          |
|    ln(policy_gradient_loss) | 1.67          |
|    loss                     | 19.4          |
|    n_updates                | 220           |
|    policy_gradient_loss     | 5.32          |
|    std                      | 0.601         |
|    value_loss               | 37.4          |
-----------------------------------------------
----------------------------------------------
| reward                      | [-0.3838989] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 24           |
|    time_elapsed             | 321          |
|    total_timesteps          | 49152        |
| train/                      |              |
|    approx_kl                | 9.394636     |
|    approx_ln(kl)            | 2.240139     |
|    clip_range               | 0.2          |
|    entropy_loss             | -0.794       |
|    explained_variance       | 0.895        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 10.5         |
|    ln(policy_gradient_loss) | 10.5         |
|    loss                     | 3.71e+04     |
|    n_updates                | 230          |
|    policy_gradient_loss     | 3.79e+04     |
|    std                      | 0.351        |
|    value_loss               | 0.213        |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.76925784] |
| time/                       |               |
|    fps                      | 171           |
|    iterations               | 27            |
|    time_elapsed             | 322           |
|    total_timesteps          | 55296         |
| train/                      |               |
|    approx_kl                | 5.9915743     |
|    approx_ln(kl)            | 1.7903543     |
|    clip_range               | 0.2           |
|    entropy_loss             | -0.618        |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 6.43          |
|    ln(policy_gradient_loss) | 5.94          |
|    loss                     | 619           |
|    n_updates                | 260           |
|    policy_gradient_loss     | 380           |
|    std                      | 0.326         |
|    value_loss               | 0.918         |
-----------------------------------------------
----------------------------------------------
| reward                      | [-1.1224564] |
| time/                       |              |
|    fps                      | 170          |
|    iterations               | 27           |
|    time_elapsed             | 324          |
|    total_timesteps          | 55296        |
| train/                      |              |
|    approx_kl                | 3.3150053    |
|    approx_ln(kl)            | 1.1984591    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.7         |
|    explained_variance       | -0.276       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.08         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 2.94         |
|    n_updates                | 260          |
|    policy_gradient_loss     | -0.0318      |
|    std                      | 0.563        |
|    value_loss               | 17.5         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
---------------------------------------------
| reward                      | [-0.740447] |
| time/                       |             |
|    fps                      | 150         |
|    iterations               | 24          |
|    time_elapsed             | 327         |
|    total_timesteps          | 49152       |
| train/                      |             |
|    approx_kl                | 4.713516    |
|    approx_ln(kl)            | 1.5504342   |
|    clip_range               | 0.2         |
|    entropy_loss             | -1.86       |
|    explained_variance       | 0.14        |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 4.07        |
|    ln(policy_gradient_loss) | 3.91        |
|    loss                     | 58.5        |
|    n_updates                | 230         |
|    policy_gradient_loss     | 49.9        |
|    std                      | 0.593       |
|    value_loss               | 27.4        |
---------------------------------------------
----------------------------------------------
| reward                      | [-1.5831748] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 24           |
|    time_elapsed             | 329          |
|    total_timesteps          | 49152        |
| train/                      |              |
|    approx_kl                | 48.940712    |
|    approx_ln(kl)            | 3.8906097    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.81        |
|    explained_variance       | 0.979        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.78         |
|    ln(policy_gradient_loss) | 3.63         |
|    loss                     | 43.9         |
|    n_updates                | 230          |
|    policy_gradient_loss     | 37.6         |
|    std                      | 0.598        |
|    value_loss               | 7.04         |
----------------------------------------------
----------------------------------------------
| reward                      | [-0.7654667] |
| time/                       |              |
|    fps                      | 171          |
|    iterations               | 28           |
|    time_elapsed             | 335          |
|    total_timesteps          | 57344        |
| train/                      |              |
|    approx_kl                | 4.9953346    |
|    approx_ln(kl)            | 1.6085044    |
|    clip_range               | 0.2          |
|    entropy_loss             | -0.536       |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.85         |
|    ln(policy_gradient_loss) | 5.91         |
|    loss                     | 348          |
|    n_updates                | 270          |
|    policy_gradient_loss     | 368          |
|    std                      | 0.312        |
|    value_loss               | 0.355        |
----------------------------------------------
----------------------------------------------
| reward                      | [-0.5295523] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 25           |
|    time_elapsed             | 334          |
|    total_timesteps          | 51200        |
| train/                      |              |
|    approx_kl                | 25.656574    |
|    approx_ln(kl)            | 3.2447999    |
|    clip_range               | 0.2          |
|    entropy_loss             | -0.716       |
|    explained_variance       | 0.964        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 11.5         |
|    ln(policy_gradient_loss) | 11.5         |
|    loss                     | 9.76e+04     |
|    n_updates                | 240          |
|    policy_gradient_loss     | 1.02e+05     |
|    std                      | 0.336        |
|    value_loss               | 0.0771       |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.71905804] |
| time/                       |               |
|    fps                      | 170           |
|    iterations               | 28            |
|    time_elapsed             | 336           |
|    total_timesteps          | 57344         |
| train/                      |               |
|    approx_kl                | 2.9804196     |
|    approx_ln(kl)            | 1.0920641     |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.68         |
|    explained_variance       | -0.614        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.67          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 5.33          |
|    n_updates                | 270           |
|    policy_gradient_loss     | -0.0715       |
|    std                      | 0.562         |
|    value_loss               | 36.4          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
----------------------------------------------
| reward                      | [-0.6531251] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 25           |
|    time_elapsed             | 341          |
|    total_timesteps          | 51200        |
| train/                      |              |
|    approx_kl                | 20.063393    |
|    approx_ln(kl)            | 2.9988968    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.74        |
|    explained_variance       | 0.873        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.59         |
|    ln(policy_gradient_loss) | 5.37         |
|    loss                     | 269          |
|    n_updates                | 240          |
|    policy_gradient_loss     | 214          |
|    std                      | 0.554        |
|    value_loss               | 10           |
----------------------------------------------
----------------------------------------------
| reward                      | [-2.5409029] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 25           |
|    time_elapsed             | 341          |
|    total_timesteps          | 51200        |
| train/                      |              |
|    approx_kl                | 2.5781243    |
|    approx_ln(kl)            | 0.94706213   |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.81        |
|    explained_variance       | 0.789        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.14         |
|    ln(policy_gradient_loss) | -0.13        |
|    loss                     | 62.9         |
|    n_updates                | 240          |
|    policy_gradient_loss     | 0.878        |
|    std                      | 0.598        |
|    value_loss               | 188          |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.81166285] |
| time/                       |               |
|    fps                      | 170           |
|    iterations               | 29            |
|    time_elapsed             | 347           |
|    total_timesteps          | 59392         |
| train/                      |               |
|    approx_kl                | 10.32605      |
|    approx_ln(kl)            | 2.3346698     |
|    clip_range               | 0.2           |
|    entropy_loss             | -0.406        |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 6.07          |
|    ln(policy_gradient_loss) | 6.31          |
|    loss                     | 434           |
|    n_updates                | 280           |
|    policy_gradient_loss     | 550           |
|    std                      | 0.286         |
|    value_loss               | 0.191         |
-----------------------------------------------
-----------------------------------------------
| reward                      | [-0.49579105] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 26            |
|    time_elapsed             | 348           |
|    total_timesteps          | 53248         |
| train/                      |               |
|    approx_kl                | 16.828074     |
|    approx_ln(kl)            | 2.8230486     |
|    clip_range               | 0.2           |
|    entropy_loss             | -0.627        |
|    explained_variance       | 0.942         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 11.4          |
|    ln(policy_gradient_loss) | 11.1          |
|    loss                     | 9.06e+04      |
|    n_updates                | 250           |
|    policy_gradient_loss     | 6.86e+04      |
|    std                      | 0.321         |
|    value_loss               | 0.161         |
-----------------------------------------------
----------------------------------------------
| reward                      | [-0.6317516] |
| time/                       |              |
|    fps                      | 170          |
|    iterations               | 29           |
|    time_elapsed             | 348          |
|    total_timesteps          | 59392        |
| train/                      |              |
|    approx_kl                | 1.651242     |
|    approx_ln(kl)            | 0.5015278    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.69        |
|    explained_variance       | 0.637        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.27         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 9.68         |
|    n_updates                | 280          |
|    policy_gradient_loss     | -0.187       |
|    std                      | 0.563        |
|    value_loss               | 31.7         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
----------------------------------------------
| reward                      | [-2.4575646] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 26           |
|    time_elapsed             | 353          |
|    total_timesteps          | 53248        |
| train/                      |              |
|    approx_kl                | 54.369038    |
|    approx_ln(kl)            | 3.9957948    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.79        |
|    explained_variance       | 0.923        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.02         |
|    ln(policy_gradient_loss) | 3.59         |
|    loss                     | 55.6         |
|    n_updates                | 250          |
|    policy_gradient_loss     | 36.2         |
|    std                      | 0.579        |
|    value_loss               | 70.8         |
----------------------------------------------
----------------------------------------------
| reward                      | [-1.1288228] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 26           |
|    time_elapsed             | 355          |
|    total_timesteps          | 53248        |
| train/                      |              |
|    approx_kl                | 4.195821     |
|    approx_ln(kl)            | 1.434089     |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.6         |
|    explained_variance       | 0.961        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.41         |
|    ln(policy_gradient_loss) | 3.94         |
|    loss                     | 30.2         |
|    n_updates                | 250          |
|    policy_gradient_loss     | 51.3         |
|    std                      | 0.523        |
|    value_loss               | 4.02         |
----------------------------------------------
srun: Job 96423 step creation still disabled, retrying (Requested nodes are busy)
----------------------------------------------
| reward                      | [-0.6804234] |
| time/                       |              |
|    fps                      | 170          |
|    iterations               | 30           |
|    time_elapsed             | 359          |
|    total_timesteps          | 61440        |
| train/                      |              |
|    approx_kl                | 9.630572     |
|    approx_ln(kl)            | 2.2649426    |
|    clip_range               | 0.2          |
|    entropy_loss             | -0.256       |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 6.86         |
|    ln(policy_gradient_loss) | 6.6          |
|    loss                     | 958          |
|    n_updates                | 290          |
|    policy_gradient_loss     | 735          |
|    std                      | 0.271        |
|    value_loss               | 0.314        |
----------------------------------------------
----------------------------------------------
| reward                      | [-1.6717652] |
| time/                       |              |
|    fps                      | 170          |
|    iterations               | 30           |
|    time_elapsed             | 360          |
|    total_timesteps          | 61440        |
| train/                      |              |
|    approx_kl                | 3.6383886    |
|    approx_ln(kl)            | 1.2915409    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.68        |
|    explained_variance       | 0.553        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.71         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 15.1         |
|    n_updates                | 290          |
|    policy_gradient_loss     | -0.0348      |
|    std                      | 0.561        |
|    value_loss               | 57.7         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
-----------------------------------------------
| reward                      | [-0.49385312] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 27            |
|    time_elapsed             | 361           |
|    total_timesteps          | 55296         |
| train/                      |               |
|    approx_kl                | 21.247417     |
|    approx_ln(kl)            | 3.0562353     |
|    clip_range               | 0.2           |
|    entropy_loss             | -0.507        |
|    explained_variance       | 0.919         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 11.6          |
|    ln(policy_gradient_loss) | 11.5          |
|    loss                     | 1.06e+05      |
|    n_updates                | 260           |
|    policy_gradient_loss     | 9.96e+04      |
|    std                      | 0.3           |
|    value_loss               | 0.0663        |
-----------------------------------------------
srun: Job 96423 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 96423 step creation still disabled, retrying (Requested nodes are busy)
----------------------------------------------
| reward                      | [-2.9349291] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 27           |
|    time_elapsed             | 365          |
|    total_timesteps          | 55296        |
| train/                      |              |
|    approx_kl                | 27.053291    |
|    approx_ln(kl)            | 3.2978086    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.73        |
|    explained_variance       | 0.943        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.02         |
|    ln(policy_gradient_loss) | 2.82         |
|    loss                     | 55.8         |
|    n_updates                | 260          |
|    policy_gradient_loss     | 16.8         |
|    std                      | 0.573        |
|    value_loss               | 96.2         |
----------------------------------------------
slurmstepd: error: *** STEP 96423.1 ON ddpg.ist.berkeley.edu CANCELLED AT 2023-11-17T08:26:45 ***
slurmstepd: error: *** STEP 96423.4 ON gan.ist.berkeley.edu CANCELLED AT 2023-11-17T08:26:45 ***
slurmstepd: error: *** STEP 96423.2 ON dqn.ist.berkeley.edu CANCELLED AT 2023-11-17T08:26:45 ***
slurmstepd: error: *** STEP 96423.3 ON gail.ist.berkeley.edu CANCELLED AT 2023-11-17T08:26:45 ***
slurmstepd: error: *** JOB 96423 ON airl.ist.berkeley.edu CANCELLED AT 2023-11-17T16:26:45 ***
slurmstepd: error: *** STEP 96423.0 ON airl.ist.berkeley.edu CANCELLED AT 2023-11-17T16:26:45 ***
