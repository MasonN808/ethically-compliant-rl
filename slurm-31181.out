slurmstepd: error: execve(): /nas/ucb/mason/ethically-compliant-rl/CPO/train_cpo_2.py: Permission denied
srun: error: ddpg.ist.berkeley.edu: task 0: Exited with exit code 13
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20230728_195924-3a8f19c4-1399-462d-af88-31f9ed6e5f6d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cpo_gamma0.95_step_per_epoch20000-01a6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/fast-safe-rl
wandb: üöÄ View run at https://wandb.ai/ecrl/fast-safe-rl/runs/3a8f19c4-1399-462d-af88-31f9ed6e5f6d
[32;1mLogging data to logs/fast-safe-rl/parking-v0-cost-10/cpo_gamma0.95_step_per_epoch20000-01a6/progress.txt[0m
[36;1mSaving config:
[0m
{
    "action_bound_method":	"clip",
    "action_scaling":	true,
    "backtrack_coeff":	0.8,
    "batch_size":	99999,
    "buffer_size":	100000,
    "cost_limit":	10,
    "damping_coeff":	0.1,
    "deterministic_eval":	true,
    "device":	"cuda",
    "env_config_file":	"configs/ParkingEnv/env-kinematicsGoal.txt",
    "episode_per_collect":	20,
    "epoch":	100,
    "gae_lambda":	0.95,
    "gamma":	0.95,
    "group":	null,
    "hidden_sizes":	[
        128,
        128
    ],
    "l2_reg":	0.001,
    "last_layer_scale":	false,
    "logdir":	"logs",
    "lr":	0.001,
    "max_backtracks":	100,
    "max_batchsize":	99999,
    "name":	"cpo_gamma0.95_step_per_epoch20000-01a6",
    "norm_adv":	true,
    "optim_critic_iters":	10,
    "prefix":	"cpo",
    "project":	"fast-safe-rl",
    "render":	null,
    "render_mode":	null,
    "repeat_per_collect":	4,
    "resume":	false,
    "rew_norm":	false,
    "reward_threshold":	10000,
    "save_ckpt":	true,
    "save_interval":	4,
    "seed":	10,
    "step_per_epoch":	20000,
    "suffix":	"",
    "target_kl":	0.01,
    "task":	"parking-v0",
    "testing_num":	2,
    "thread":	320,
    "training_num":	20,
    "unbounded":	false,
    "use_default_cfg":	false,
    "verbose":	true,
    "worker":	"ShmemVectorEnv"
}
Observation Space: Dict('achieved_goal': Box(-inf, inf, (6,), float64), 'desired_goal': Box(-inf, inf, (6,), float64), 'observation': Box(-inf, inf, (6,), float64))
Action Space: Box(-1.0, 1.0, (2,), float32)
Render Mode: None
Epoch #1:   0%|          | 0/20000 [00:00<?, ?it/s]Epoch #1:   0%|          | 0/20000 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/CPO/train_cpo_1.py", line 206, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/CPO/train_cpo_1.py", line 172, in train
    agent.learn(
  File "/nas/ucb/mason/ethically-compliant-rl/FSRL/fsrl/agent/cpo_agent.py", line 232, in learn
    return super().learn(
  File "/nas/ucb/mason/ethically-compliant-rl/FSRL/fsrl/agent/base_agent.py", line 319, in learn
    for epoch, _epoch_stat, info in trainer:
  File "/nas/ucb/mason/ethically-compliant-rl/FSRL/fsrl/trainer/base_trainer.py", line 202, in __next__
    stats_train = self.train_step()
  File "/nas/ucb/mason/ethically-compliant-rl/FSRL/fsrl/trainer/base_trainer.py", line 285, in train_step
    stats_train = self.train_collector.collect(self.episode_per_collect)
  File "/nas/ucb/mason/ethically-compliant-rl/FSRL/fsrl/data/fast_collector.py", line 269, in collect
    result = self.policy(self.data, last_state)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/FSRL/fsrl/policy/base_policy.py", line 178, in forward
    logits, hidden = self.actor(batch.obs, state=state)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/tianshou/utils/net/common.py", line 365, in forward
    obs = torch.as_tensor(obs, dtype=torch.float32)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/tianshou/data/batch.py", line 686, in __len__
    raise TypeError(f"Object {obj} in {self} has no len()")
TypeError: Object 0.0 in Batch(
    observation: 0.0,
    desired_goal: 0.22,
    achieved_goal: 0.0,
) has no len()
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/CPO/train_cpo_1.py", line 206, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/CPO/train_cpo_1.py", line 172, in train
    agent.learn(
  File "/nas/ucb/mason/ethically-compliant-rl/FSRL/fsrl/agent/cpo_agent.py", line 232, in learn
    return super().learn(
  File "/nas/ucb/mason/ethically-compliant-rl/FSRL/fsrl/agent/base_agent.py", line 319, in learn
    for epoch, _epoch_stat, info in trainer:
  File "/nas/ucb/mason/ethically-compliant-rl/FSRL/fsrl/trainer/base_trainer.py", line 202, in __next__
    stats_train = self.train_step()
  File "/nas/ucb/mason/ethically-compliant-rl/FSRL/fsrl/trainer/base_trainer.py", line 285, in train_step
    stats_train = self.train_collector.collect(self.episode_per_collect)
  File "/nas/ucb/mason/ethically-compliant-rl/FSRL/fsrl/data/fast_collector.py", line 269, in collect
    result = self.policy(self.data, last_state)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/FSRL/fsrl/policy/base_policy.py", line 178, in forward
    logits, hidden = self.actor(batch.obs, state=state)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/tianshou/utils/net/common.py", line 365, in forward
    obs = torch.as_tensor(obs, dtype=torch.float32)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/tianshou/data/batch.py", line 686, in __len__
    raise TypeError(f"Object {obj} in {self} has no len()")
TypeError: Object 0.0 in Batch(
    observation: 0.0,
    desired_goal: 0.22,
    achieved_goal: 0.0,
) has no len()
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: üöÄ View run cpo_gamma0.95_step_per_epoch20000-01a6 at: https://wandb.ai/ecrl/fast-safe-rl/runs/3a8f19c4-1399-462d-af88-31f9ed6e5f6d
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230728_195924-3a8f19c4-1399-462d-af88-31f9ed6e5f6d/logs
srun: error: airl.ist.berkeley.edu: task 0: Exited with exit code 1
