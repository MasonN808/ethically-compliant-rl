wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231230_084355-05ce4c9y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-paper-60
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D100
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D100/runs/05ce4c9y
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231230_004355-k8xa7w0u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-morning-61
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D100
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D100/runs/k8xa7w0u
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231230_004355-lt2vx7i3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-wind-62
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D100
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D100/runs/lt2vx7i3
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231230_004355-9xuicc5m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-oath-63
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D100
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D100/runs/9xuicc5m
Using cpu device
-------------------------------------
| cost               | [0]          |
| is_success         | False        |
| reward             | [-0.4910298] |
| rollout/           |              |
|    ep_len_mean     | 1e+03        |
|    ep_rew_mean     | -1.1e+03     |
| time/              |              |
|    fps             | 121          |
|    iterations      | 1            |
|    time_elapsed    | 16           |
|    total_timesteps | 2048         |
-------------------------------------
Using cpu device
--------------------------------------
| cost               | [0]           |
| is_success         | False         |
| reward             | [-0.45871085] |
| rollout/           |               |
|    ep_len_mean     | 1e+03         |
|    ep_rew_mean     | -1.33e+03     |
| time/              |               |
|    fps             | 115           |
|    iterations      | 1             |
|    time_elapsed    | 17            |
|    total_timesteps | 2048          |
--------------------------------------
Using cpu device
-------------------------------------
| cost               | [0]          |
| is_success         | False        |
| reward             | [-0.6292938] |
| rollout/           |              |
|    ep_len_mean     | 1e+03        |
|    ep_rew_mean     | -1.24e+03    |
| time/              |              |
|    fps             | 113          |
|    iterations      | 1            |
|    time_elapsed    | 17           |
|    total_timesteps | 2048         |
-------------------------------------
Using cpu device
-------------------------------------
| cost               | [0]          |
| is_success         | False        |
| reward             | [-0.7711294] |
| rollout/           |              |
|    ep_len_mean     | 1e+03        |
|    ep_rew_mean     | -1.4e+03     |
| time/              |              |
|    fps             | 112          |
|    iterations      | 1            |
|    time_elapsed    | 18           |
|    total_timesteps | 2048         |
-------------------------------------
--------------------------------------------
| cost                     | [0]           |
| is_success               | False         |
| reward                   | [-0.44394618] |
| rollout/                 |               |
|    ep_len_mean           | 1e+03         |
|    ep_rew_mean           | -1.25e+03     |
| time/                    |               |
|    fps                   | 118           |
|    iterations            | 2             |
|    time_elapsed          | 34            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.0045216926  |
|    clip_fraction         | 0.0177        |
|    clip_range            | 0.2           |
|    cost_returns          | 0.0689        |
|    cost_value_loss       | 0.0556        |
|    cost_values           | 0.0888        |
|    entropy               | -2.84         |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.0614        |
|    lagrangian_multiplier | 0.0507        |
|    learning_rate         | 0.0003        |
|    loss                  | 7.07          |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00408      |
|    std                   | 1             |
|    value_loss            | 381           |
--------------------------------------------
-------------------------------------------
| cost                     | [0]          |
| is_success               | False        |
| reward                   | [-0.6955648] |
| rollout/                 |              |
|    ep_len_mean           | 1e+03        |
|    ep_rew_mean           | -1.35e+03    |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 2            |
|    time_elapsed          | 36           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.004598189  |
|    clip_fraction         | 0.0221       |
|    clip_range            | 0.2          |
|    cost_returns          | -0.0372      |
|    cost_value_loss       | 0.0252       |
|    cost_values           | -0.0464      |
|    entropy               | -2.83        |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.00928     |
|    lagrangian_multiplier | 0.0583       |
|    learning_rate         | 0.0003       |
|    loss                  | 9.08         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00388     |
|    std                   | 0.996        |
|    value_loss            | 551          |
-------------------------------------------
--------------------------------------------
| cost                     | [0]           |
| is_success               | False         |
| reward                   | [-0.68656796] |
| rollout/                 |               |
|    ep_len_mean           | 1e+03         |
|    ep_rew_mean           | -1.26e+03     |
| time/                    |               |
|    fps                   | 111           |
|    iterations            | 2             |
|    time_elapsed          | 36            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.0033301145  |
|    clip_fraction         | 0.00859       |
|    clip_range            | 0.2           |
|    cost_returns          | -0.0821       |
|    cost_value_loss       | 0.12          |
|    cost_values           | -0.102        |
|    entropy               | -2.84         |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.0393        |
|    lagrangian_multiplier | 0.0572        |
|    learning_rate         | 0.0003        |
|    loss                  | 7.2           |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00284      |
|    std                   | 1             |
|    value_loss            | 443           |
--------------------------------------------
-------------------------------------------
| cost                     | [0]          |
| is_success               | False        |
| reward                   | [-0.6712095] |
| rollout/                 |              |
|    ep_len_mean           | 1e+03        |
|    ep_rew_mean           | -1.51e+03    |
| time/                    |              |
|    fps                   | 110          |
|    iterations            | 2            |
|    time_elapsed          | 37           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0055175074 |
|    clip_fraction         | 0.0273       |
|    clip_range            | 0.2          |
|    cost_returns          | -0.14        |
|    cost_value_loss       | 0.213        |
|    cost_values           | -0.165       |
|    entropy               | -2.83        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0267       |
|    lagrangian_multiplier | 0.067        |
|    learning_rate         | 0.0003       |
|    loss                  | 8.95         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00509     |
|    std                   | 0.995        |
|    value_loss            | 600          |
-------------------------------------------
--------------------------------------------
| cost                     | [0]           |
| is_success               | False         |
| reward                   | [-0.53492045] |
| rollout/                 |               |
|    ep_len_mean           | 1e+03         |
|    ep_rew_mean           | -1.09e+03     |
| time/                    |               |
|    fps                   | 117           |
|    iterations            | 3             |
|    time_elapsed          | 52            |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.003644233   |
|    clip_fraction         | 0.0135        |
|    clip_range            | 0.2           |
|    cost_returns          | 0.186         |
|    cost_value_loss       | 0.0984        |
|    cost_values           | 0.22          |
|    entropy               | -2.83         |
|    entropy_loss          | -2.83         |
|    explained_variance    | 0.0543        |
|    lagrangian_multiplier | 0.0499        |
|    learning_rate         | 0.0003        |
|    loss                  | 11            |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00266      |
|    std                   | 0.997         |
|    value_loss            | 610           |
--------------------------------------------
-------------------------------------------
| cost                     | [0]          |
| is_success               | False        |
| reward                   | [-0.9425534] |
| rollout/                 |              |
|    ep_len_mean           | 1e+03        |
|    ep_rew_mean           | -1.18e+03    |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 3            |
|    time_elapsed          | 54           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.002863325  |
|    clip_fraction         | 0.00352      |
|    clip_range            | 0.2          |
|    cost_returns          | 0.247        |
|    cost_value_loss       | 0.113        |
|    cost_values           | 0.304        |
|    entropy               | -2.82        |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0719      |
|    lagrangian_multiplier | 0.0464       |
|    learning_rate         | 0.0003       |
|    loss                  | 11.2         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00251     |
|    std                   | 0.989        |
|    value_loss            | 555          |
-------------------------------------------
-------------------------------------------
| cost                     | [0]          |
| is_success               | False        |
| reward                   | [-0.8510435] |
| rollout/                 |              |
|    ep_len_mean           | 1e+03        |
|    ep_rew_mean           | -1.27e+03    |
| time/                    |              |
|    fps                   | 110          |
|    iterations            | 3            |
|    time_elapsed          | 55           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0040950067 |
|    clip_fraction         | 0.0152       |
|    clip_range            | 0.2          |
|    cost_returns          | -0.525       |
|    cost_value_loss       | 0.109        |
|    cost_values           | -0.623       |
|    entropy               | -2.84        |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.000337    |
|    lagrangian_multiplier | 0.0771       |
|    learning_rate         | 0.0003       |
|    loss                  | 7.35         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.0035      |
|    std                   | 1            |
|    value_loss            | 539          |
-------------------------------------------
--------------------------------------------
| cost                     | [0]           |
| is_success               | False         |
| reward                   | [-0.46586248] |
| rollout/                 |               |
|    ep_len_mean           | 1e+03         |
|    ep_rew_mean           | -1.52e+03     |
| time/                    |               |
|    fps                   | 108           |
|    iterations            | 3             |
|    time_elapsed          | 56            |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.0029966687  |
|    clip_fraction         | 0.00645       |
|    clip_range            | 0.2           |
|    cost_returns          | -0.181        |
|    cost_value_loss       | 0.141         |
|    cost_values           | -0.225        |
|    entropy               | -2.82         |
|    entropy_loss          | -2.83         |
|    explained_variance    | 0.0187        |
|    lagrangian_multiplier | 0.0721        |
|    learning_rate         | 0.0003        |
|    loss                  | 11.1          |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00238      |
|    std                   | 0.994         |
|    value_loss            | 841           |
--------------------------------------------
--------------------------------------------
| cost                     | [0]           |
| is_success               | False         |
| reward                   | [-0.82654005] |
| rollout/                 |               |
|    ep_len_mean           | 1e+03         |
|    ep_rew_mean           | -1.19e+03     |
| time/                    |               |
|    fps                   | 116           |
|    iterations            | 4             |
|    time_elapsed          | 70            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.0016211267  |
|    clip_fraction         | 0.000732      |
|    clip_range            | 0.2           |
|    cost_returns          | 0.0239        |
|    cost_value_loss       | 0.0386        |
|    cost_values           | 0.0352        |
|    entropy               | -2.84         |
|    entropy_loss          | -2.83         |
|    explained_variance    | 0.0704        |
|    lagrangian_multiplier | 0.0558        |
|    learning_rate         | 0.0003        |
|    loss                  | 3.41          |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00144      |
|    std                   | 1             |
|    value_loss            | 203           |
--------------------------------------------
-------------------------------------------
| cost                     | [0]          |
| is_success               | False        |
| reward                   | [-0.8710959] |
| rollout/                 |              |
|    ep_len_mean           | 1e+03        |
|    ep_rew_mean           | -1.17e+03    |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 4            |
|    time_elapsed          | 73           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.0045509916 |
|    clip_fraction         | 0.0151       |
|    clip_range            | 0.2          |
|    cost_returns          | 0.0792       |
|    cost_value_loss       | 0.0848       |
|    cost_values           | 0.107        |
|    entropy               | -2.81        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0231       |
|    lagrangian_multiplier | 0.0527       |
|    learning_rate         | 0.0003       |
|    loss                  | 4.01         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.0025      |
|    std                   | 0.984        |
|    value_loss            | 223          |
-------------------------------------------
-------------------------------------------
| cost                     | [0]          |
| is_success               | False        |
| reward                   | [-0.5118486] |
| rollout/                 |              |
|    ep_len_mean           | 1e+03        |
|    ep_rew_mean           | -1.34e+03    |
| time/                    |              |
|    fps                   | 110          |
|    iterations            | 4            |
|    time_elapsed          | 74           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.0052270037 |
|    clip_fraction         | 0.0198       |
|    clip_range            | 0.2          |
|    cost_returns          | 0.239        |
|    cost_value_loss       | 0.142        |
|    cost_values           | 0.276        |
|    entropy               | -2.86        |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0691      |
|    lagrangian_multiplier | 0.049        |
|    learning_rate         | 0.0003       |
|    loss                  | 9.49         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00384     |
|    std                   | 1.01         |
|    value_loss            | 512          |
-------------------------------------------
-------------------------------------------
| cost                     | [0]          |
| is_success               | False        |
| reward                   | [-0.7531153] |
| rollout/                 |              |
|    ep_len_mean           | 1e+03        |
|    ep_rew_mean           | -1.51e+03    |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 4            |
|    time_elapsed          | 75           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.004919985  |
|    clip_fraction         | 0.0184       |
|    clip_range            | 0.2          |
|    cost_returns          | -0.126       |
|    cost_value_loss       | 0.322        |
|    cost_values           | -0.145       |
|    entropy               | -2.83        |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.00527     |
|    lagrangian_multiplier | 0.0604       |
|    learning_rate         | 0.0003       |
|    loss                  | 11.2         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00304     |
|    std                   | 0.998        |
|    value_loss            | 715          |
-------------------------------------------
-------------------------------------------
| cost                     | [0]          |
| is_success               | False        |
| reward                   | [-1.0634891] |
| rollout/                 |              |
|    ep_len_mean           | 1e+03        |
|    ep_rew_mean           | -1.2e+03     |
| time/                    |              |
|    fps                   | 116          |
|    iterations            | 5            |
|    time_elapsed          | 87           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.004294698  |
|    clip_fraction         | 0.015        |
|    clip_range            | 0.2          |
|    cost_returns          | -0.217       |
|    cost_value_loss       | 0.254        |
|    cost_values           | -0.257       |
|    entropy               | -2.84        |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0667       |
|    lagrangian_multiplier | 0.065        |
|    learning_rate         | 0.0003       |
|    loss                  | 10.7         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.0035      |
|    std                   | 0.999        |
|    value_loss            | 755          |
-------------------------------------------
-------------------------------------------
| cost                     | [0]          |
| is_success               | False        |
| reward                   | [-1.2427324] |
| rollout/                 |              |
|    ep_len_mean           | 1e+03        |
|    ep_rew_mean           | -1.23e+03    |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 5            |
|    time_elapsed          | 91           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0044249226 |
|    clip_fraction         | 0.0132       |
|    clip_range            | 0.2          |
|    cost_returns          | -0.0924      |
|    cost_value_loss       | 0.067        |
|    cost_values           | -0.113       |
|    entropy               | -2.8         |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0256      |
|    lagrangian_multiplier | 0.0628       |
|    learning_rate         | 0.0003       |
|    loss                  | 5.98         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00293     |
|    std                   | 0.98         |
|    value_loss            | 375          |
-------------------------------------------
-------------------------------------------
| cost                     | [0]          |
| is_success               | False        |
| reward                   | [-0.3239913] |
| rollout/                 |              |
|    ep_len_mean           | 1e+03        |
|    ep_rew_mean           | -1.29e+03    |
| time/                    |              |
|    fps                   | 110          |
|    iterations            | 5            |
|    time_elapsed          | 92           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0041721836 |
|    clip_fraction         | 0.0129       |
|    clip_range            | 0.2          |
|    cost_returns          | -0.0611      |
|    cost_value_loss       | 0.163        |
|    cost_values           | -0.0807      |
|    entropy               | -2.85        |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0317       |
|    lagrangian_multiplier | 0.0562       |
|    learning_rate         | 0.0003       |
|    loss                  | 11.8         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.0026      |
|    std                   | 1.01         |
|    value_loss            | 723          |
-------------------------------------------
-------------------------------------------
| cost                     | [0]          |
| is_success               | False        |
| reward                   | [-1.1364663] |
| rollout/                 |              |
|    ep_len_mean           | 1e+03        |
|    ep_rew_mean           | -1.47e+03    |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 5            |
|    time_elapsed          | 94           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.006175459  |
|    clip_fraction         | 0.0306       |
|    clip_range            | 0.2          |
|    cost_returns          | -0.0252      |
|    cost_value_loss       | 0.271        |
|    cost_values           | -0.0185      |
|    entropy               | -2.84        |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0196       |
|    lagrangian_multiplier | 0.0579       |
|    learning_rate         | 0.0003       |
|    loss                  | 12.4         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00392     |
|    std                   | 1            |
|    value_loss            | 749          |
-------------------------------------------
-------------------------------------------
| cost                     | [0]          |
| is_success               | False        |
| reward                   | [-1.1175205] |
| rollout/                 |              |
|    ep_len_mean           | 1e+03        |
|    ep_rew_mean           | -1.18e+03    |
| time/                    |              |
|    fps                   | 116          |
|    iterations            | 6            |
|    time_elapsed          | 105          |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.005356132  |
|    clip_fraction         | 0.0201       |
|    clip_range            | 0.2          |
|    cost_returns          | 0.0486       |
|    cost_value_loss       | 0.299        |
|    cost_values           | 0.0469       |
|    entropy               | -2.84        |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0438      |
|    lagrangian_multiplier | 0.0517       |
|    learning_rate         | 0.0003       |
|    loss                  | 8.59         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00304     |
|    std                   | 1            |
|    value_loss            | 471          |
-------------------------------------------
--------------------------------------------
| cost                     | [0]           |
| is_success               | False         |
| reward                   | [-0.59466165] |
| rollout/                 |               |
|    ep_len_mean           | 1e+03         |
|    ep_rew_mean           | -1.28e+03     |
| time/                    |               |
|    fps                   | 111           |
|    iterations            | 6             |
|    time_elapsed          | 110           |
|    total_timesteps       | 12288         |
| train/                   |               |
|    approx_kl             | 0.005404681   |
|    clip_fraction         | 0.0295        |
|    clip_range            | 0.2           |
|    cost_returns          | -0.421        |
|    cost_value_loss       | 0.122         |
|    cost_values           | -0.502        |
|    entropy               | -2.79         |
|    entropy_loss          | -2.8          |
|    explained_variance    | -0.0195       |
|    lagrangian_multiplier | 0.0667        |
|    learning_rate         | 0.0003        |
|    loss                  | 9.76          |
|    n_updates             | 50            |
|    policy_gradient_loss  | -0.0042       |
|    std                   | 0.979         |
|    value_loss            | 680           |
--------------------------------------------
--------------------------------------------
| cost                     | [0]           |
| is_success               | False         |
| reward                   | [-0.57811874] |
| rollout/                 |               |
|    ep_len_mean           | 1e+03         |
|    ep_rew_mean           | -1.27e+03     |
| time/                    |               |
|    fps                   | 110           |
|    iterations            | 6             |
|    time_elapsed          | 111           |
|    total_timesteps       | 12288         |
| train/                   |               |
|    approx_kl             | 0.0028092058  |
|    clip_fraction         | 0.00854       |
|    clip_range            | 0.2           |
|    cost_returns          | 0.256         |
|    cost_value_loss       | 0.115         |
|    cost_values           | 0.305         |
|    entropy               | -2.84         |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.0477        |
|    lagrangian_multiplier | 0.0459        |
|    learning_rate         | 0.0003        |
|    loss                  | 7.39          |
|    n_updates             | 50            |
|    policy_gradient_loss  | -0.00228      |
|    std                   | 1             |
|    value_loss            | 385           |
--------------------------------------------
-------------------------------------------
| cost                     | [0]          |
| is_success               | False        |
| reward                   | [-1.1740443] |
| rollout/                 |              |
|    ep_len_mean           | 1e+03        |
|    ep_rew_mean           | -1.45e+03    |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 6            |
|    time_elapsed          | 113          |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.005430789  |
|    clip_fraction         | 0.0261       |
|    clip_range            | 0.2          |
|    cost_returns          | 0.0793       |
|    cost_value_loss       | 0.288        |
|    cost_values           | 0.102        |
|    entropy               | -2.84        |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0333       |
|    lagrangian_multiplier | 0.0593       |
|    learning_rate         | 0.0003       |
|    loss                  | 7.94         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00413     |
|    std                   | 1            |
|    value_loss            | 513          |
-------------------------------------------
slurmstepd: error: *** STEP 115200.1 ON ddpg.ist.berkeley.edu CANCELLED AT 2023-12-30T00:45:55 ***
slurmstepd: error: *** STEP 115200.3 ON gail.ist.berkeley.edu CANCELLED AT 2023-12-30T00:45:55 ***
slurmstepd: error: *** JOB 115200 ON airl.ist.berkeley.edu CANCELLED AT 2023-12-30T08:45:55 ***
slurmstepd: error: *** STEP 115200.2 ON dqn.ist.berkeley.edu CANCELLED AT 2023-12-30T00:45:55 ***
slurmstepd: error: *** STEP 115200.0 ON airl.ist.berkeley.edu CANCELLED AT 2023-12-30T08:45:55 ***
