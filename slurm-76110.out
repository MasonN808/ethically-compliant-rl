wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: ERROR api_key not configured (no-tty). call wandb.login(key=[your_api_key])
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPO_penalty/train_ppo_penalty.py", line 56, in <module>
    wandb.init(name=f"ppo-KLpenalty-beta({args.beta})-parking", project="PPO-Penalty", sync_tensorboard=True)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1164, in init
    raise e
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1141, in init
    wi.setup(kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/wandb_init.py", line 289, in setup
    wandb_login._login(
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/wandb_login.py", line 298, in _login
    wlogin.prompt_api_key()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/wandb_login.py", line 228, in prompt_api_key
    raise UsageError("api_key not configured (no-tty). call " + directive)
wandb.errors.UsageError: api_key not configured (no-tty). call wandb.login(key=[your_api_key])
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231017_230001-dg547qb4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(3.0)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty/runs/dg547qb4
srun: error: dqn.ist.berkeley.edu: task 0: Exited with exit code 1
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231017_160001-lwzn4llj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(10.0)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty/runs/lwzn4llj
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231017_160001-ftt552li
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(0.3)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty/runs/ftt552li
Using cpu device
-------------------------------------
| reward             | [-1.4640868] |
| time/              |              |
|    fps             | 185          |
|    iterations      | 1            |
|    time_elapsed    | 11           |
|    total_timesteps | 2048         |
-------------------------------------
Using cpu device
-------------------------------------
| reward             | [-1.4640868] |
| time/              |              |
|    fps             | 164          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 2048         |
-------------------------------------
Using cpu device
-------------------------------------
| reward             | [-1.4640868] |
| time/              |              |
|    fps             | 161          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 2048         |
-------------------------------------
-------------------------------------------
| reward                  | [-0.58534485] |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 2             |
|    time_elapsed         | 22            |
|    total_timesteps      | 4096          |
| train/                  |               |
|    approx_kl            | 0.15166444    |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.83         |
|    explained_variance   | 0.0414        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.818        |
|    n_updates            | 10            |
|    policy_gradient_loss | -0.182        |
|    std                  | 0.993         |
|    value_loss           | 298           |
-------------------------------------------
-------------------------------------------
| reward                  | [-0.58534485] |
| time/                   |               |
|    fps                  | 158           |
|    iterations           | 2             |
|    time_elapsed         | 25            |
|    total_timesteps      | 4096          |
| train/                  |               |
|    approx_kl            | 0.15166444    |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.83         |
|    explained_variance   | 0.0414        |
|    learning_rate        | 0.0003        |
|    loss                 | -2.13         |
|    n_updates            | 10            |
|    policy_gradient_loss | -0.541        |
|    std                  | 0.993         |
|    value_loss           | 298           |
-------------------------------------------
-------------------------------------------
| reward                  | [-0.58534485] |
| time/                   |               |
|    fps                  | 156           |
|    iterations           | 2             |
|    time_elapsed         | 26            |
|    total_timesteps      | 4096          |
| train/                  |               |
|    approx_kl            | 0.15166444    |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.83         |
|    explained_variance   | 0.0414        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.311        |
|    n_updates            | 10            |
|    policy_gradient_loss | -0.0434       |
|    std                  | 0.993         |
|    value_loss           | 298           |
-------------------------------------------
-------------------------------------------
| reward                  | [-0.78479373] |
| time/                   |               |
|    fps                  | 177           |
|    iterations           | 3             |
|    time_elapsed         | 34            |
|    total_timesteps      | 6144          |
| train/                  |               |
|    approx_kl            | 0.54889315    |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.83         |
|    explained_variance   | 0.0594        |
|    learning_rate        | 0.0003        |
|    loss                 | -1.78         |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.626        |
|    std                  | 0.996         |
|    value_loss           | 196           |
-------------------------------------------
-------------------------------------------
| reward                  | [-0.78479373] |
| time/                   |               |
|    fps                  | 156           |
|    iterations           | 3             |
|    time_elapsed         | 39            |
|    total_timesteps      | 6144          |
| train/                  |               |
|    approx_kl            | 0.54889315    |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.83         |
|    explained_variance   | 0.0594        |
|    learning_rate        | 0.0003        |
|    loss                 | -5.25         |
|    n_updates            | 20            |
|    policy_gradient_loss | -1.95         |
|    std                  | 0.996         |
|    value_loss           | 196           |
-------------------------------------------
-------------------------------------------
| reward                  | [-0.78479373] |
| time/                   |               |
|    fps                  | 156           |
|    iterations           | 3             |
|    time_elapsed         | 39            |
|    total_timesteps      | 6144          |
| train/                  |               |
|    approx_kl            | 0.54889315    |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.83         |
|    explained_variance   | 0.0594        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.444        |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.116        |
|    std                  | 0.996         |
|    value_loss           | 196           |
-------------------------------------------
------------------------------------------
| reward                  | [-0.5839704] |
| time/                   |              |
|    fps                  | 177          |
|    iterations           | 4            |
|    time_elapsed         | 46           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.5152277    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | 0.102        |
|    learning_rate        | 0.0003       |
|    loss                 | -1.66        |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.588       |
|    std                  | 1            |
|    value_loss           | 306          |
------------------------------------------
------------------------------------------
| reward                  | [-0.5839704] |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 4            |
|    time_elapsed         | 52           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.5152277    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | 0.102        |
|    learning_rate        | 0.0003       |
|    loss                 | -5.26        |
|    n_updates            | 30           |
|    policy_gradient_loss | -1.89        |
|    std                  | 1            |
|    value_loss           | 306          |
------------------------------------------
------------------------------------------
| reward                  | [-0.5839704] |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 4            |
|    time_elapsed         | 52           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.5152277    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | 0.102        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.275       |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.0853      |
|    std                  | 1            |
|    value_loss           | 306          |
------------------------------------------
------------------------------------------
| reward                  | [-1.3529372] |
| time/                   |              |
|    fps                  | 176          |
|    iterations           | 5            |
|    time_elapsed         | 57           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.26126283   |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | -0.0076      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.814       |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.426       |
|    std                  | 0.998        |
|    value_loss           | 900          |
------------------------------------------
------------------------------------------
| reward                  | [-1.3529372] |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 5            |
|    time_elapsed         | 65           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.26126283   |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | -0.0076      |
|    learning_rate        | 0.0003       |
|    loss                 | -2.5         |
|    n_updates            | 40           |
|    policy_gradient_loss | -1.35        |
|    std                  | 0.998        |
|    value_loss           | 900          |
------------------------------------------
------------------------------------------
| reward                  | [-1.3529372] |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 5            |
|    time_elapsed         | 66           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.26126283   |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | -0.0076      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.162       |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.0678      |
|    std                  | 0.998        |
|    value_loss           | 900          |
------------------------------------------
-------------------------------------
| reward             | [-2.4382849] |
| time/              |              |
|    fps             | 185          |
|    iterations      | 1            |
|    time_elapsed    | 11           |
|    total_timesteps | 12288        |
-------------------------------------
-------------------------------------
| reward             | [-2.4382849] |
| time/              |              |
|    fps             | 161          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 12288        |
-------------------------------------
-------------------------------------
| reward             | [-2.4382849] |
| time/              |              |
|    fps             | 162          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 12288        |
-------------------------------------
------------------------------------------
| reward                  | [-3.2339594] |
| time/                   |              |
|    fps                  | 177          |
|    iterations           | 2            |
|    time_elapsed         | 23           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 1.0407298    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | 0.0155       |
|    learning_rate        | 0.0003       |
|    loss                 | -3.56        |
|    n_updates            | 60           |
|    policy_gradient_loss | -1.09        |
|    std                  | 1            |
|    value_loss           | 2.75e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-3.2339594] |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 2            |
|    time_elapsed         | 26           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 1.0407298    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | 0.0155       |
|    learning_rate        | 0.0003       |
|    loss                 | -11.6        |
|    n_updates            | 60           |
|    policy_gradient_loss | -3.52        |
|    std                  | 1            |
|    value_loss           | 2.75e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-3.2339594] |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 2            |
|    time_elapsed         | 26           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 1.0407298    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | 0.0155       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.463       |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.157       |
|    std                  | 1            |
|    value_loss           | 2.75e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-3.1957934] |
| time/                   |              |
|    fps                  | 172          |
|    iterations           | 3            |
|    time_elapsed         | 35           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 2.9328527    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.83        |
|    explained_variance   | 0.00873      |
|    learning_rate        | 0.0003       |
|    loss                 | -10.2        |
|    n_updates            | 70           |
|    policy_gradient_loss | -3.61        |
|    std                  | 0.988        |
|    value_loss           | 2.57e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-4.1294026] |
| time/                   |              |
|    fps                  | 172          |
|    iterations           | 4            |
|    time_elapsed         | 47           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 4.496603     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.81        |
|    explained_variance   | 0.0195       |
|    learning_rate        | 0.0003       |
|    loss                 | -9.35        |
|    n_updates            | 80           |
|    policy_gradient_loss | -7.34        |
|    std                  | 0.981        |
|    value_loss           | 2.42e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-3.1957934] |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 3            |
|    time_elapsed         | 39           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 2.9328527    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.83        |
|    explained_variance   | 0.00873      |
|    learning_rate        | 0.0003       |
|    loss                 | -34.1        |
|    n_updates            | 70           |
|    policy_gradient_loss | -11.7        |
|    std                  | 0.988        |
|    value_loss           | 2.57e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-3.1957934] |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 3            |
|    time_elapsed         | 39           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 2.9328527    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.83        |
|    explained_variance   | 0.00873      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.978       |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.474       |
|    std                  | 0.988        |
|    value_loss           | 2.57e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-4.2938156] |
| time/                   |              |
|    fps                  | 173          |
|    iterations           | 5            |
|    time_elapsed         | 59           |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 4.079999     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.79        |
|    explained_variance   | 0.0147       |
|    learning_rate        | 0.0003       |
|    loss                 | -9.69        |
|    n_updates            | 90           |
|    policy_gradient_loss | -6.05        |
|    std                  | 0.967        |
|    value_loss           | 2.49e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-4.1294026] |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 4            |
|    time_elapsed         | 52           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 4.496603     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.81        |
|    explained_variance   | 0.0195       |
|    learning_rate        | 0.0003       |
|    loss                 | -30.6        |
|    n_updates            | 80           |
|    policy_gradient_loss | -24.1        |
|    std                  | 0.981        |
|    value_loss           | 2.42e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-4.1294026] |
| time/                   |              |
|    fps                  | 153          |
|    iterations           | 4            |
|    time_elapsed         | 53           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 4.496603     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.81        |
|    explained_variance   | 0.0195       |
|    learning_rate        | 0.0003       |
|    loss                 | -1.17        |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.887       |
|    std                  | 0.981        |
|    value_loss           | 2.42e+03     |
------------------------------------------
-------------------------------------
| reward             | [-4.2858114] |
| time/              |              |
|    fps             | 183          |
|    iterations      | 1            |
|    time_elapsed    | 11           |
|    total_timesteps | 22528        |
-------------------------------------
------------------------------------------
| reward                  | [-4.2938156] |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 5            |
|    time_elapsed         | 66           |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 4.079999     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.79        |
|    explained_variance   | 0.0147       |
|    learning_rate        | 0.0003       |
|    loss                 | -31.6        |
|    n_updates            | 90           |
|    policy_gradient_loss | -19.6        |
|    std                  | 0.967        |
|    value_loss           | 2.49e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-4.2938156] |
| time/                   |              |
|    fps                  | 152          |
|    iterations           | 5            |
|    time_elapsed         | 66           |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 4.079999     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.79        |
|    explained_variance   | 0.0147       |
|    learning_rate        | 0.0003       |
|    loss                 | -1.25        |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.813       |
|    std                  | 0.967        |
|    value_loss           | 2.49e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-5.0819564] |
| time/                   |              |
|    fps                  | 176          |
|    iterations           | 2            |
|    time_elapsed         | 23           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 5.6577096    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.75        |
|    explained_variance   | 0.00196      |
|    learning_rate        | 0.0003       |
|    loss                 | -14.5        |
|    n_updates            | 110          |
|    policy_gradient_loss | -8.61        |
|    std                  | 0.952        |
|    value_loss           | 2.28e+03     |
------------------------------------------
-------------------------------------
| reward             | [-4.2858114] |
| time/              |              |
|    fps             | 162          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 22528        |
-------------------------------------
-------------------------------------
| reward             | [-4.2858114] |
| time/              |              |
|    fps             | 160          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 22528        |
-------------------------------------
------------------------------------------
| reward                  | [-0.6590864] |
| time/                   |              |
|    fps                  | 174          |
|    iterations           | 3            |
|    time_elapsed         | 35           |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 7.4623985    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.73        |
|    explained_variance   | 0.0102       |
|    learning_rate        | 0.0003       |
|    loss                 | -28.4        |
|    n_updates            | 120          |
|    policy_gradient_loss | -12.3        |
|    std                  | 0.942        |
|    value_loss           | 3.04e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-5.0819564] |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 2            |
|    time_elapsed         | 25           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 5.6577096    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.75        |
|    explained_variance   | 0.00196      |
|    learning_rate        | 0.0003       |
|    loss                 | -47.6        |
|    n_updates            | 110          |
|    policy_gradient_loss | -28.3        |
|    std                  | 0.952        |
|    value_loss           | 2.28e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-5.0819564] |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 2            |
|    time_elapsed         | 26           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 5.6577096    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.75        |
|    explained_variance   | 0.00196      |
|    learning_rate        | 0.0003       |
|    loss                 | -1.77        |
|    n_updates            | 110          |
|    policy_gradient_loss | -1.01        |
|    std                  | 0.952        |
|    value_loss           | 2.28e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-1.4089038] |
| time/                   |              |
|    fps                  | 174          |
|    iterations           | 4            |
|    time_elapsed         | 46           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 6.0030413    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.69        |
|    explained_variance   | 0.00284      |
|    learning_rate        | 0.0003       |
|    loss                 | -26.3        |
|    n_updates            | 130          |
|    policy_gradient_loss | -10.6        |
|    std                  | 0.915        |
|    value_loss           | 1.78e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-0.6590864] |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 3            |
|    time_elapsed         | 39           |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 7.4623985    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.73        |
|    explained_variance   | 0.0102       |
|    learning_rate        | 0.0003       |
|    loss                 | -93.8        |
|    n_updates            | 120          |
|    policy_gradient_loss | -40.5        |
|    std                  | 0.942        |
|    value_loss           | 3.04e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-0.6590864] |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 3            |
|    time_elapsed         | 39           |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 7.4623985    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.73        |
|    explained_variance   | 0.0102       |
|    learning_rate        | 0.0003       |
|    loss                 | -3.22        |
|    n_updates            | 120          |
|    policy_gradient_loss | -1.45        |
|    std                  | 0.942        |
|    value_loss           | 3.04e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-2.1252704] |
| time/                   |              |
|    fps                  | 174          |
|    iterations           | 5            |
|    time_elapsed         | 58           |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 5.0570273    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.65        |
|    explained_variance   | 0.0441       |
|    learning_rate        | 0.0003       |
|    loss                 | -10          |
|    n_updates            | 140          |
|    policy_gradient_loss | -8.89        |
|    std                  | 0.904        |
|    value_loss           | 1.39e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-1.4089038] |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 4            |
|    time_elapsed         | 52           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 6.0030413    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.69        |
|    explained_variance   | 0.00284      |
|    learning_rate        | 0.0003       |
|    loss                 | -86.8        |
|    n_updates            | 130          |
|    policy_gradient_loss | -34.9        |
|    std                  | 0.915        |
|    value_loss           | 1.78e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-1.4089038] |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 4            |
|    time_elapsed         | 52           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 6.0030413    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.69        |
|    explained_variance   | 0.00284      |
|    learning_rate        | 0.0003       |
|    loss                 | -3.02        |
|    n_updates            | 130          |
|    policy_gradient_loss | -1.26        |
|    std                  | 0.915        |
|    value_loss           | 1.78e+03     |
------------------------------------------
-------------------------------------
| reward             | [-2.5978532] |
| time/              |              |
|    fps             | 182          |
|    iterations      | 1            |
|    time_elapsed    | 11           |
|    total_timesteps | 32768        |
-------------------------------------
------------------------------------------
| reward                  | [-2.1252704] |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 5            |
|    time_elapsed         | 65           |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 5.0570273    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.65        |
|    explained_variance   | 0.0441       |
|    learning_rate        | 0.0003       |
|    loss                 | -33          |
|    n_updates            | 140          |
|    policy_gradient_loss | -29.1        |
|    std                  | 0.904        |
|    value_loss           | 1.39e+03     |
------------------------------------------
-----------------------------------------
| reward                  | [-2.720141] |
| time/                   |             |
|    fps                  | 177         |
|    iterations           | 2           |
|    time_elapsed         | 23          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 3.4222      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.61       |
|    explained_variance   | 0.0461      |
|    learning_rate        | 0.0003      |
|    loss                 | -12.7       |
|    n_updates            | 160         |
|    policy_gradient_loss | -6.52       |
|    std                  | 0.889       |
|    value_loss           | 1.62e+03    |
-----------------------------------------
------------------------------------------
| reward                  | [-2.1252704] |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 5            |
|    time_elapsed         | 66           |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 5.0570273    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.65        |
|    explained_variance   | 0.0441       |
|    learning_rate        | 0.0003       |
|    loss                 | -1.18        |
|    n_updates            | 140          |
|    policy_gradient_loss | -1.08        |
|    std                  | 0.904        |
|    value_loss           | 1.39e+03     |
------------------------------------------
-----------------------------------------
| reward                  | [-2.984849] |
| time/                   |             |
|    fps                  | 175         |
|    iterations           | 3           |
|    time_elapsed         | 34          |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 3.8716764   |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.58       |
|    explained_variance   | 0.033       |
|    learning_rate        | 0.0003      |
|    loss                 | -11.3       |
|    n_updates            | 170         |
|    policy_gradient_loss | -4.59       |
|    std                  | 0.869       |
|    value_loss           | 1.8e+03     |
-----------------------------------------
-------------------------------------
| reward             | [-2.5978532] |
| time/              |              |
|    fps             | 163          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 32768        |
-------------------------------------
-------------------------------------
| reward             | [-2.5978532] |
| time/              |              |
|    fps             | 162          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 32768        |
-------------------------------------
------------------------------------------
| reward                  | [-3.1864355] |
| time/                   |              |
|    fps                  | 174          |
|    iterations           | 4            |
|    time_elapsed         | 46           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 4.870161     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.53        |
|    explained_variance   | 0.0407       |
|    learning_rate        | 0.0003       |
|    loss                 | -10.7        |
|    n_updates            | 180          |
|    policy_gradient_loss | -8.92        |
|    std                  | 0.854        |
|    value_loss           | 1.62e+03     |
------------------------------------------
-----------------------------------------
| reward                  | [-2.720141] |
| time/                   |             |
|    fps                  | 158         |
|    iterations           | 2           |
|    time_elapsed         | 25          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 3.4222      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.61       |
|    explained_variance   | 0.0461      |
|    learning_rate        | 0.0003      |
|    loss                 | -41.6       |
|    n_updates            | 160         |
|    policy_gradient_loss | -21.5       |
|    std                  | 0.889       |
|    value_loss           | 1.62e+03    |
-----------------------------------------
-----------------------------------------
| reward                  | [-2.720141] |
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 2           |
|    time_elapsed         | 25          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 3.4222      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.61       |
|    explained_variance   | 0.0461      |
|    learning_rate        | 0.0003      |
|    loss                 | -1.49       |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.738      |
|    std                  | 0.889       |
|    value_loss           | 1.62e+03    |
-----------------------------------------
------------------------------------------
| reward                  | [-3.2194228] |
| time/                   |              |
|    fps                  | 174          |
|    iterations           | 5            |
|    time_elapsed         | 58           |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 4.0592093    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.51        |
|    explained_variance   | 0.0383       |
|    learning_rate        | 0.0003       |
|    loss                 | -17          |
|    n_updates            | 190          |
|    policy_gradient_loss | -4.45        |
|    std                  | 0.851        |
|    value_loss           | 1.78e+03     |
------------------------------------------
-----------------------------------------
| reward                  | [-2.984849] |
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 3           |
|    time_elapsed         | 39          |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 3.8716764   |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.58       |
|    explained_variance   | 0.033       |
|    learning_rate        | 0.0003      |
|    loss                 | -37         |
|    n_updates            | 170         |
|    policy_gradient_loss | -14.9       |
|    std                  | 0.869       |
|    value_loss           | 1.8e+03     |
-----------------------------------------
-----------------------------------------
| reward                  | [-2.984849] |
| time/                   |             |
|    fps                  | 155         |
|    iterations           | 3           |
|    time_elapsed         | 39          |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 3.8716764   |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.58       |
|    explained_variance   | 0.033       |
|    learning_rate        | 0.0003      |
|    loss                 | -1.36       |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.62       |
|    std                  | 0.869       |
|    value_loss           | 1.8e+03     |
-----------------------------------------
-------------------------------------
| reward             | [-3.7046645] |
| time/              |              |
|    fps             | 186          |
|    iterations      | 1            |
|    time_elapsed    | 10           |
|    total_timesteps | 43008        |
-------------------------------------
------------------------------------------
| reward                  | [-3.1864355] |
| time/                   |              |
|    fps                  | 153          |
|    iterations           | 4            |
|    time_elapsed         | 53           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 4.870161     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.53        |
|    explained_variance   | 0.0407       |
|    learning_rate        | 0.0003       |
|    loss                 | -34.9        |
|    n_updates            | 180          |
|    policy_gradient_loss | -29.2        |
|    std                  | 0.854        |
|    value_loss           | 1.62e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-3.1864355] |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 4            |
|    time_elapsed         | 53           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 4.870161     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.53        |
|    explained_variance   | 0.0407       |
|    learning_rate        | 0.0003       |
|    loss                 | -1.35        |
|    n_updates            | 180          |
|    policy_gradient_loss | -1.11        |
|    std                  | 0.854        |
|    value_loss           | 1.62e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-4.3835816] |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 2            |
|    time_elapsed         | 22           |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 8.994655     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.43        |
|    explained_variance   | -0.0325      |
|    learning_rate        | 0.0003       |
|    loss                 | -23.3        |
|    n_updates            | 210          |
|    policy_gradient_loss | -18.7        |
|    std                  | 0.811        |
|    value_loss           | 2.16e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-3.2194228] |
| time/                   |              |
|    fps                  | 152          |
|    iterations           | 5            |
|    time_elapsed         | 67           |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 4.0592093    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.51        |
|    explained_variance   | 0.0383       |
|    learning_rate        | 0.0003       |
|    loss                 | -56          |
|    n_updates            | 190          |
|    policy_gradient_loss | -14.5        |
|    std                  | 0.851        |
|    value_loss           | 1.78e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-3.2194228] |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 5            |
|    time_elapsed         | 66           |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 4.0592093    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.51        |
|    explained_variance   | 0.0383       |
|    learning_rate        | 0.0003       |
|    loss                 | -1.98        |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.578       |
|    std                  | 0.851        |
|    value_loss           | 1.78e+03     |
------------------------------------------
-------------------------------------------
| reward                  | [-0.40299225] |
| time/                   |               |
|    fps                  | 178           |
|    iterations           | 3             |
|    time_elapsed         | 34            |
|    total_timesteps      | 47104         |
| train/                  |               |
|    approx_kl            | 8.436071      |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.4          |
|    explained_variance   | -0.0332       |
|    learning_rate        | 0.0003        |
|    loss                 | -23.6         |
|    n_updates            | 220           |
|    policy_gradient_loss | -17.4         |
|    std                  | 0.8           |
|    value_loss           | 2.29e+03      |
-------------------------------------------
-------------------------------------
| reward             | [-3.7046645] |
| time/              |              |
|    fps             | 163          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 43008        |
-------------------------------------
-------------------------------------
| reward             | [-3.7046645] |
| time/              |              |
|    fps             | 160          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 43008        |
-------------------------------------
------------------------------------------
| reward                  | [-1.2357254] |
| time/                   |              |
|    fps                  | 177          |
|    iterations           | 4            |
|    time_elapsed         | 46           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 9.69503      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.36        |
|    explained_variance   | -0.0374      |
|    learning_rate        | 0.0003       |
|    loss                 | -33.9        |
|    n_updates            | 230          |
|    policy_gradient_loss | -15.4        |
|    std                  | 0.784        |
|    value_loss           | 2.23e+03     |
------------------------------------------
slurmstepd: error: *** STEP 76110.3 ON gail.ist.berkeley.edu CANCELLED AT 2023-10-17T16:04:45 ***
slurmstepd: error: *** JOB 76110 ON airl.ist.berkeley.edu CANCELLED AT 2023-10-17T23:04:45 ***
slurmstepd: error: *** STEP 76110.1 ON ddpg.ist.berkeley.edu CANCELLED AT 2023-10-17T16:04:45 ***
slurmstepd: error: *** STEP 76110.0 ON airl.ist.berkeley.edu CANCELLED AT 2023-10-17T23:04:45 ***
