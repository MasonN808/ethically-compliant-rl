wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231017_234445-94fcyydv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(1.0)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty/runs/94fcyydv
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231017_164445-g0er83w9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(10.0)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty/runs/g0er83w9
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231017_164445-ls79e1pt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(3.0)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty/runs/ls79e1pt
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231017_164445-zx9z2m86
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(0.3)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty/runs/zx9z2m86
Using cpu device
-------------------------------------
| reward             | [-1.4640868] |
| time/              |              |
|    fps             | 183          |
|    iterations      | 1            |
|    time_elapsed    | 11           |
|    total_timesteps | 2048         |
-------------------------------------
Using cpu device
-------------------------------------
| reward             | [-1.4640868] |
| time/              |              |
|    fps             | 160          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 2048         |
-------------------------------------
Using cpu device
-------------------------------------
| reward             | [-1.4640868] |
| time/              |              |
|    fps             | 161          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 2048         |
-------------------------------------
Using cpu device
-------------------------------------
| reward             | [-1.4640868] |
| time/              |              |
|    fps             | 162          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 2048         |
-------------------------------------
-------------------------------------------
| reward                  | [-0.58534485] |
| time/                   |               |
|    fps                  | 178           |
|    iterations           | 2             |
|    time_elapsed         | 22            |
|    total_timesteps      | 4096          |
| train/                  |               |
|    approx_kl            | 0.15166444    |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.83         |
|    explained_variance   | 0.0414        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.443        |
|    n_updates            | 10            |
|    policy_gradient_loss | -0.0793       |
|    std                  | 0.993         |
|    value_loss           | 298           |
-------------------------------------------
-------------------------------------------
| reward                  | [-0.58534485] |
| time/                   |               |
|    fps                  | 157           |
|    iterations           | 2             |
|    time_elapsed         | 26            |
|    total_timesteps      | 4096          |
| train/                  |               |
|    approx_kl            | 0.15166444    |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.83         |
|    explained_variance   | 0.0414        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.311        |
|    n_updates            | 10            |
|    policy_gradient_loss | -0.0434       |
|    std                  | 0.993         |
|    value_loss           | 298           |
-------------------------------------------
-------------------------------------------
| reward                  | [-0.58534485] |
| time/                   |               |
|    fps                  | 155           |
|    iterations           | 2             |
|    time_elapsed         | 26            |
|    total_timesteps      | 4096          |
| train/                  |               |
|    approx_kl            | 0.15166444    |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.83         |
|    explained_variance   | 0.0414        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.818        |
|    n_updates            | 10            |
|    policy_gradient_loss | -0.182        |
|    std                  | 0.993         |
|    value_loss           | 298           |
-------------------------------------------
-------------------------------------------
| reward                  | [-0.58534485] |
| time/                   |               |
|    fps                  | 154           |
|    iterations           | 2             |
|    time_elapsed         | 26            |
|    total_timesteps      | 4096          |
| train/                  |               |
|    approx_kl            | 0.15166444    |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.83         |
|    explained_variance   | 0.0414        |
|    learning_rate        | 0.0003        |
|    loss                 | -2.13         |
|    n_updates            | 10            |
|    policy_gradient_loss | -0.541        |
|    std                  | 0.993         |
|    value_loss           | 298           |
-------------------------------------------
-------------------------------------------
| reward                  | [-0.78479373] |
| time/                   |               |
|    fps                  | 177           |
|    iterations           | 3             |
|    time_elapsed         | 34            |
|    total_timesteps      | 6144          |
| train/                  |               |
|    approx_kl            | 0.54889315    |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.83         |
|    explained_variance   | 0.0594        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.791        |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.248        |
|    std                  | 0.996         |
|    value_loss           | 196           |
-------------------------------------------
-------------------------------------------
| reward                  | [-0.78479373] |
| time/                   |               |
|    fps                  | 155           |
|    iterations           | 3             |
|    time_elapsed         | 39            |
|    total_timesteps      | 6144          |
| train/                  |               |
|    approx_kl            | 0.54889315    |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.83         |
|    explained_variance   | 0.0594        |
|    learning_rate        | 0.0003        |
|    loss                 | -1.78         |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.626        |
|    std                  | 0.996         |
|    value_loss           | 196           |
-------------------------------------------
-------------------------------------------
| reward                  | [-0.78479373] |
| time/                   |               |
|    fps                  | 155           |
|    iterations           | 3             |
|    time_elapsed         | 39            |
|    total_timesteps      | 6144          |
| train/                  |               |
|    approx_kl            | 0.54889315    |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.83         |
|    explained_variance   | 0.0594        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.444        |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.116        |
|    std                  | 0.996         |
|    value_loss           | 196           |
-------------------------------------------
-------------------------------------------
| reward                  | [-0.78479373] |
| time/                   |               |
|    fps                  | 152           |
|    iterations           | 3             |
|    time_elapsed         | 40            |
|    total_timesteps      | 6144          |
| train/                  |               |
|    approx_kl            | 0.54889315    |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.83         |
|    explained_variance   | 0.0594        |
|    learning_rate        | 0.0003        |
|    loss                 | -5.25         |
|    n_updates            | 20            |
|    policy_gradient_loss | -1.95         |
|    std                  | 0.996         |
|    value_loss           | 196           |
-------------------------------------------
------------------------------------------
| reward                  | [-0.5839704] |
| time/                   |              |
|    fps                  | 176          |
|    iterations           | 4            |
|    time_elapsed         | 46           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.5152277    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | 0.102        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.634       |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.216       |
|    std                  | 1            |
|    value_loss           | 306          |
------------------------------------------
------------------------------------------
| reward                  | [-0.5839704] |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 4            |
|    time_elapsed         | 52           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.5152277    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | 0.102        |
|    learning_rate        | 0.0003       |
|    loss                 | -1.66        |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.588       |
|    std                  | 1            |
|    value_loss           | 306          |
------------------------------------------
------------------------------------------
| reward                  | [-0.5839704] |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 4            |
|    time_elapsed         | 52           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.5152277    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | 0.102        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.275       |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.0853      |
|    std                  | 1            |
|    value_loss           | 306          |
------------------------------------------
------------------------------------------
| reward                  | [-0.5839704] |
| time/                   |              |
|    fps                  | 152          |
|    iterations           | 4            |
|    time_elapsed         | 53           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.5152277    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | 0.102        |
|    learning_rate        | 0.0003       |
|    loss                 | -5.26        |
|    n_updates            | 30           |
|    policy_gradient_loss | -1.89        |
|    std                  | 1            |
|    value_loss           | 306          |
------------------------------------------
------------------------------------------
| reward                  | [-1.3529372] |
| time/                   |              |
|    fps                  | 176          |
|    iterations           | 5            |
|    time_elapsed         | 58           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.26126283   |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | -0.0076      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.331       |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.161       |
|    std                  | 0.998        |
|    value_loss           | 900          |
------------------------------------------
------------------------------------------
| reward                  | [-1.3529372] |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 5            |
|    time_elapsed         | 65           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.26126283   |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | -0.0076      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.814       |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.426       |
|    std                  | 0.998        |
|    value_loss           | 900          |
------------------------------------------
------------------------------------------
| reward                  | [-1.3529372] |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 5            |
|    time_elapsed         | 66           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.26126283   |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | -0.0076      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.162       |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.0678      |
|    std                  | 0.998        |
|    value_loss           | 900          |
------------------------------------------
------------------------------------------
| reward                  | [-1.3529372] |
| time/                   |              |
|    fps                  | 152          |
|    iterations           | 5            |
|    time_elapsed         | 67           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.26126283   |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | -0.0076      |
|    learning_rate        | 0.0003       |
|    loss                 | -2.5         |
|    n_updates            | 40           |
|    policy_gradient_loss | -1.35        |
|    std                  | 0.998        |
|    value_loss           | 900          |
------------------------------------------
-------------------------------------
| reward             | [-2.4382849] |
| time/              |              |
|    fps             | 183          |
|    iterations      | 1            |
|    time_elapsed    | 11           |
|    total_timesteps | 12288        |
-------------------------------------
-------------------------------------
| reward             | [-2.4382849] |
| time/              |              |
|    fps             | 166          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 12288        |
-------------------------------------
-------------------------------------
| reward             | [-2.4382849] |
| time/              |              |
|    fps             | 159          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 12288        |
-------------------------------------
-------------------------------------
| reward             | [-2.4382849] |
| time/              |              |
|    fps             | 162          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 12288        |
-------------------------------------
------------------------------------------
| reward                  | [-3.2339594] |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 2            |
|    time_elapsed         | 22           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 1.0407298    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | 0.0155       |
|    learning_rate        | 0.0003       |
|    loss                 | -1.27        |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.399       |
|    std                  | 1            |
|    value_loss           | 2.75e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-3.2339594] |
| time/                   |              |
|    fps                  | 160          |
|    iterations           | 2            |
|    time_elapsed         | 25           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 1.0407298    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | 0.0155       |
|    learning_rate        | 0.0003       |
|    loss                 | -3.56        |
|    n_updates            | 60           |
|    policy_gradient_loss | -1.09        |
|    std                  | 1            |
|    value_loss           | 2.75e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-3.1957934] |
| time/                   |              |
|    fps                  | 177          |
|    iterations           | 3            |
|    time_elapsed         | 34           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 2.9328527    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.83        |
|    explained_variance   | 0.00873      |
|    learning_rate        | 0.0003       |
|    loss                 | -3.36        |
|    n_updates            | 70           |
|    policy_gradient_loss | -1.29        |
|    std                  | 0.988        |
|    value_loss           | 2.57e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-3.2339594] |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 2            |
|    time_elapsed         | 26           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 1.0407298    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | 0.0155       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.463       |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.157       |
|    std                  | 1            |
|    value_loss           | 2.75e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-3.2339594] |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 2            |
|    time_elapsed         | 26           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 1.0407298    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | 0.0155       |
|    learning_rate        | 0.0003       |
|    loss                 | -11.6        |
|    n_updates            | 60           |
|    policy_gradient_loss | -3.52        |
|    std                  | 1            |
|    value_loss           | 2.75e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-4.1294026] |
| time/                   |              |
|    fps                  | 176          |
|    iterations           | 4            |
|    time_elapsed         | 46           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 4.496603     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.81        |
|    explained_variance   | 0.0195       |
|    learning_rate        | 0.0003       |
|    loss                 | -3.29        |
|    n_updates            | 80           |
|    policy_gradient_loss | -2.56        |
|    std                  | 0.981        |
|    value_loss           | 2.42e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-3.1957934] |
| time/                   |              |
|    fps                  | 158          |
|    iterations           | 3            |
|    time_elapsed         | 38           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 2.9328527    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.83        |
|    explained_variance   | 0.00873      |
|    learning_rate        | 0.0003       |
|    loss                 | -10.2        |
|    n_updates            | 70           |
|    policy_gradient_loss | -3.61        |
|    std                  | 0.988        |
|    value_loss           | 2.57e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-3.1957934] |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 3            |
|    time_elapsed         | 39           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 2.9328527    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.83        |
|    explained_variance   | 0.00873      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.978       |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.474       |
|    std                  | 0.988        |
|    value_loss           | 2.57e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-3.1957934] |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 3            |
|    time_elapsed         | 39           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 2.9328527    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.83        |
|    explained_variance   | 0.00873      |
|    learning_rate        | 0.0003       |
|    loss                 | -34.1        |
|    n_updates            | 70           |
|    policy_gradient_loss | -11.7        |
|    std                  | 0.988        |
|    value_loss           | 2.57e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-4.2938156] |
| time/                   |              |
|    fps                  | 176          |
|    iterations           | 5            |
|    time_elapsed         | 58           |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 4.079999     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.79        |
|    explained_variance   | 0.0147       |
|    learning_rate        | 0.0003       |
|    loss                 | -3.44        |
|    n_updates            | 90           |
|    policy_gradient_loss | -2.17        |
|    std                  | 0.967        |
|    value_loss           | 2.49e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-4.1294026] |
| time/                   |              |
|    fps                  | 158          |
|    iterations           | 4            |
|    time_elapsed         | 51           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 4.496603     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.81        |
|    explained_variance   | 0.0195       |
|    learning_rate        | 0.0003       |
|    loss                 | -9.35        |
|    n_updates            | 80           |
|    policy_gradient_loss | -7.34        |
|    std                  | 0.981        |
|    value_loss           | 2.42e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-4.1294026] |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 4            |
|    time_elapsed         | 52           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 4.496603     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.81        |
|    explained_variance   | 0.0195       |
|    learning_rate        | 0.0003       |
|    loss                 | -30.6        |
|    n_updates            | 80           |
|    policy_gradient_loss | -24.1        |
|    std                  | 0.981        |
|    value_loss           | 2.42e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-4.1294026] |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 4            |
|    time_elapsed         | 53           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 4.496603     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.81        |
|    explained_variance   | 0.0195       |
|    learning_rate        | 0.0003       |
|    loss                 | -1.17        |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.887       |
|    std                  | 0.981        |
|    value_loss           | 2.42e+03     |
------------------------------------------
-------------------------------------
| reward             | [-4.2858114] |
| time/              |              |
|    fps             | 186          |
|    iterations      | 1            |
|    time_elapsed    | 10           |
|    total_timesteps | 22528        |
-------------------------------------
------------------------------------------
| reward                  | [-4.2938156] |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 5            |
|    time_elapsed         | 65           |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 4.079999     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.79        |
|    explained_variance   | 0.0147       |
|    learning_rate        | 0.0003       |
|    loss                 | -9.69        |
|    n_updates            | 90           |
|    policy_gradient_loss | -6.05        |
|    std                  | 0.967        |
|    value_loss           | 2.49e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-4.2938156] |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 5            |
|    time_elapsed         | 65           |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 4.079999     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.79        |
|    explained_variance   | 0.0147       |
|    learning_rate        | 0.0003       |
|    loss                 | -31.6        |
|    n_updates            | 90           |
|    policy_gradient_loss | -19.6        |
|    std                  | 0.967        |
|    value_loss           | 2.49e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-4.2938156] |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 5            |
|    time_elapsed         | 66           |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 4.079999     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.79        |
|    explained_variance   | 0.0147       |
|    learning_rate        | 0.0003       |
|    loss                 | -1.25        |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.813       |
|    std                  | 0.967        |
|    value_loss           | 2.49e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-5.0819564] |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 2            |
|    time_elapsed         | 22           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 5.6577096    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.75        |
|    explained_variance   | 0.00196      |
|    learning_rate        | 0.0003       |
|    loss                 | -5.08        |
|    n_updates            | 110          |
|    policy_gradient_loss | -2.98        |
|    std                  | 0.952        |
|    value_loss           | 2.28e+03     |
------------------------------------------
-------------------------------------
| reward             | [-4.2858114] |
| time/              |              |
|    fps             | 165          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 22528        |
-------------------------------------
-------------------------------------
| reward             | [-4.2858114] |
| time/              |              |
|    fps             | 164          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 22528        |
-------------------------------------
-------------------------------------
| reward             | [-4.2858114] |
| time/              |              |
|    fps             | 162          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 22528        |
-------------------------------------
------------------------------------------
| reward                  | [-0.6590864] |
| time/                   |              |
|    fps                  | 178          |
|    iterations           | 3            |
|    time_elapsed         | 34           |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 7.4623985    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.73        |
|    explained_variance   | 0.0102       |
|    learning_rate        | 0.0003       |
|    loss                 | -9.75        |
|    n_updates            | 120          |
|    policy_gradient_loss | -4.27        |
|    std                  | 0.942        |
|    value_loss           | 3.04e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-5.0819564] |
| time/                   |              |
|    fps                  | 160          |
|    iterations           | 2            |
|    time_elapsed         | 25           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 5.6577096    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.75        |
|    explained_variance   | 0.00196      |
|    learning_rate        | 0.0003       |
|    loss                 | -14.5        |
|    n_updates            | 110          |
|    policy_gradient_loss | -8.61        |
|    std                  | 0.952        |
|    value_loss           | 2.28e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-5.0819564] |
| time/                   |              |
|    fps                  | 160          |
|    iterations           | 2            |
|    time_elapsed         | 25           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 5.6577096    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.75        |
|    explained_variance   | 0.00196      |
|    learning_rate        | 0.0003       |
|    loss                 | -47.6        |
|    n_updates            | 110          |
|    policy_gradient_loss | -28.3        |
|    std                  | 0.952        |
|    value_loss           | 2.28e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-5.0819564] |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 2            |
|    time_elapsed         | 26           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 5.6577096    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.75        |
|    explained_variance   | 0.00196      |
|    learning_rate        | 0.0003       |
|    loss                 | -1.77        |
|    n_updates            | 110          |
|    policy_gradient_loss | -1.01        |
|    std                  | 0.952        |
|    value_loss           | 2.28e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-1.4089038] |
| time/                   |              |
|    fps                  | 177          |
|    iterations           | 4            |
|    time_elapsed         | 46           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 6.0030413    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.69        |
|    explained_variance   | 0.00284      |
|    learning_rate        | 0.0003       |
|    loss                 | -9.06        |
|    n_updates            | 130          |
|    policy_gradient_loss | -3.68        |
|    std                  | 0.915        |
|    value_loss           | 1.78e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-0.6590864] |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 3            |
|    time_elapsed         | 38           |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 7.4623985    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.73        |
|    explained_variance   | 0.0102       |
|    learning_rate        | 0.0003       |
|    loss                 | -28.4        |
|    n_updates            | 120          |
|    policy_gradient_loss | -12.3        |
|    std                  | 0.942        |
|    value_loss           | 3.04e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-0.6590864] |
| time/                   |              |
|    fps                  | 158          |
|    iterations           | 3            |
|    time_elapsed         | 38           |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 7.4623985    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.73        |
|    explained_variance   | 0.0102       |
|    learning_rate        | 0.0003       |
|    loss                 | -93.8        |
|    n_updates            | 120          |
|    policy_gradient_loss | -40.5        |
|    std                  | 0.942        |
|    value_loss           | 3.04e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-0.6590864] |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 3            |
|    time_elapsed         | 39           |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 7.4623985    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.73        |
|    explained_variance   | 0.0102       |
|    learning_rate        | 0.0003       |
|    loss                 | -3.22        |
|    n_updates            | 120          |
|    policy_gradient_loss | -1.45        |
|    std                  | 0.942        |
|    value_loss           | 3.04e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-2.1252704] |
| time/                   |              |
|    fps                  | 176          |
|    iterations           | 5            |
|    time_elapsed         | 58           |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 5.0570273    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.65        |
|    explained_variance   | 0.0441       |
|    learning_rate        | 0.0003       |
|    loss                 | -3.48        |
|    n_updates            | 140          |
|    policy_gradient_loss | -3.11        |
|    std                  | 0.904        |
|    value_loss           | 1.39e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-1.4089038] |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 4            |
|    time_elapsed         | 52           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 6.0030413    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.69        |
|    explained_variance   | 0.00284      |
|    learning_rate        | 0.0003       |
|    loss                 | -26.3        |
|    n_updates            | 130          |
|    policy_gradient_loss | -10.6        |
|    std                  | 0.915        |
|    value_loss           | 1.78e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-1.4089038] |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 4            |
|    time_elapsed         | 52           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 6.0030413    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.69        |
|    explained_variance   | 0.00284      |
|    learning_rate        | 0.0003       |
|    loss                 | -86.8        |
|    n_updates            | 130          |
|    policy_gradient_loss | -34.9        |
|    std                  | 0.915        |
|    value_loss           | 1.78e+03     |
------------------------------------------
-------------------------------------
| reward             | [-2.5978532] |
| time/              |              |
|    fps             | 186          |
|    iterations      | 1            |
|    time_elapsed    | 10           |
|    total_timesteps | 32768        |
-------------------------------------
------------------------------------------
| reward                  | [-1.4089038] |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 4            |
|    time_elapsed         | 52           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 6.0030413    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.69        |
|    explained_variance   | 0.00284      |
|    learning_rate        | 0.0003       |
|    loss                 | -3.02        |
|    n_updates            | 130          |
|    policy_gradient_loss | -1.26        |
|    std                  | 0.915        |
|    value_loss           | 1.78e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-2.1252704] |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 5            |
|    time_elapsed         | 65           |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 5.0570273    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.65        |
|    explained_variance   | 0.0441       |
|    learning_rate        | 0.0003       |
|    loss                 | -10          |
|    n_updates            | 140          |
|    policy_gradient_loss | -8.89        |
|    std                  | 0.904        |
|    value_loss           | 1.39e+03     |
------------------------------------------
-----------------------------------------
| reward                  | [-2.720141] |
| time/                   |             |
|    fps                  | 180         |
|    iterations           | 2           |
|    time_elapsed         | 22          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 3.4222      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.61       |
|    explained_variance   | 0.0461      |
|    learning_rate        | 0.0003      |
|    loss                 | -4.38       |
|    n_updates            | 160         |
|    policy_gradient_loss | -2.24       |
|    std                  | 0.889       |
|    value_loss           | 1.62e+03    |
-----------------------------------------
------------------------------------------
| reward                  | [-2.1252704] |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 5            |
|    time_elapsed         | 65           |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 5.0570273    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.65        |
|    explained_variance   | 0.0441       |
|    learning_rate        | 0.0003       |
|    loss                 | -33          |
|    n_updates            | 140          |
|    policy_gradient_loss | -29.1        |
|    std                  | 0.904        |
|    value_loss           | 1.39e+03     |
------------------------------------------
------------------------------------------
| reward                  | [-2.1252704] |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 5            |
|    time_elapsed         | 66           |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 5.0570273    |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.65        |
|    explained_variance   | 0.0441       |
|    learning_rate        | 0.0003       |
|    loss                 | -1.18        |
|    n_updates            | 140          |
|    policy_gradient_loss | -1.08        |
|    std                  | 0.904        |
|    value_loss           | 1.39e+03     |
------------------------------------------
slurmstepd: error: *** STEP 76114.3 ON gail.ist.berkeley.edu CANCELLED AT 2023-10-17T16:48:10 ***
slurmstepd: error: *** STEP 76114.2 ON dqn.ist.berkeley.edu CANCELLED AT 2023-10-17T16:48:10 ***
slurmstepd: error: *** JOB 76114 ON airl.ist.berkeley.edu CANCELLED AT 2023-10-17T23:48:10 ***
slurmstepd: error: *** STEP 76114.1 ON ddpg.ist.berkeley.edu CANCELLED AT 2023-10-17T16:48:10 ***
slurmstepd: error: *** STEP 76114.0 ON airl.ist.berkeley.edu CANCELLED AT 2023-10-17T23:48:10 ***
