wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231120_143521-baompkgl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(dynamic)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty-Report5
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty-Report5/runs/baompkgl
Using cpu device
-------------------------------------
| reward             | [-0.5981351] |
| time/              |              |
|    fps             | 114          |
|    iterations      | 1            |
|    time_elapsed    | 17           |
|    total_timesteps | 2048         |
-------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47997946] |
| time/                       |               |
|    fps                      | 113           |
|    iterations               | 2             |
|    time_elapsed             | 36            |
|    total_timesteps          | 4096          |
| train/                      |               |
|    approx_kl                | 0.012262071   |
|    approx_ln(kl)            | -4.4012446    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.0115        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 4.48          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 87.8          |
|    n_updates                | 2             |
|    policy_gradient_loss     | -0.00312      |
|    std                      | 1             |
|    value_loss               | 276           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.0558343] |
| time/                       |              |
|    fps                      | 111          |
|    iterations               | 3            |
|    time_elapsed             | 55           |
|    total_timesteps          | 6144         |
| train/                      |              |
|    approx_kl                | 0.0044543673 |
|    approx_ln(kl)            | -5.4138703   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.84        |
|    explained_variance       | 0.0255       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.79         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 326          |
|    n_updates                | 3            |
|    policy_gradient_loss     | -0.000938    |
|    std                      | 0.999        |
|    value_loss               | 633          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.0751572] |
| time/                       |              |
|    fps                      | 115          |
|    iterations               | 4            |
|    time_elapsed             | 71           |
|    total_timesteps          | 8192         |
| train/                      |              |
|    approx_kl                | 0.003740559  |
|    approx_ln(kl)            | -5.58852     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.83        |
|    explained_variance       | 0.205        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.85         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 127          |
|    n_updates                | 4            |
|    policy_gradient_loss     | -0.00343     |
|    std                      | 0.999        |
|    value_loss               | 306          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.9408033] |
| time/                       |              |
|    fps                      | 117          |
|    iterations               | 5            |
|    time_elapsed             | 86           |
|    total_timesteps          | 10240        |
| train/                      |              |
|    approx_kl                | 0.0066614226 |
|    approx_ln(kl)            | -5.011422    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.84        |
|    explained_variance       | -0.0349      |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.88         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 48.6         |
|    n_updates                | 6            |
|    policy_gradient_loss     | -0.0025      |
|    std                      | 1            |
|    value_loss               | 195          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.60093534] |
| time/                       |               |
|    fps                      | 119           |
|    iterations               | 6             |
|    time_elapsed             | 103           |
|    total_timesteps          | 12288         |
| train/                      |               |
|    approx_kl                | 0.010548438   |
|    approx_ln(kl)            | -4.5517774    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.85         |
|    explained_variance       | 0.0105        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 4.7           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 110           |
|    n_updates                | 8             |
|    policy_gradient_loss     | -0.00467      |
|    std                      | 1.01          |
|    value_loss               | 304           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45975608] |
| time/                       |               |
|    fps                      | 120           |
|    iterations               | 7             |
|    time_elapsed             | 118           |
|    total_timesteps          | 14336         |
| train/                      |               |
|    approx_kl                | 0.011400257   |
|    approx_ln(kl)            | -4.474119     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.85         |
|    explained_variance       | 0.321         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 3.75          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 42.4          |
|    n_updates                | 10            |
|    policy_gradient_loss     | -0.00386      |
|    std                      | 1.01          |
|    value_loss               | 128           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.54617923] |
| time/                       |               |
|    fps                      | 121           |
|    iterations               | 8             |
|    time_elapsed             | 135           |
|    total_timesteps          | 16384         |
| train/                      |               |
|    approx_kl                | 0.009690394   |
|    approx_ln(kl)            | -4.63662      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.87         |
|    explained_variance       | 0.369         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 4.03          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 56.3          |
|    n_updates                | 13            |
|    policy_gradient_loss     | -0.00151      |
|    std                      | 1.02          |
|    value_loss               | 170           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.7078564] |
| time/                       |              |
|    fps                      | 115          |
|    iterations               | 9            |
|    time_elapsed             | 159          |
|    total_timesteps          | 18432        |
| train/                      |              |
|    approx_kl                | 0.0035439217 |
|    approx_ln(kl)            | -5.6425214   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.88        |
|    explained_variance       | 0.146        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.6          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 36.7         |
|    n_updates                | 14           |
|    policy_gradient_loss     | -0.00197     |
|    std                      | 1.02         |
|    value_loss               | 116          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.2613684] |
| time/                       |              |
|    fps                      | 116          |
|    iterations               | 10           |
|    time_elapsed             | 175          |
|    total_timesteps          | 20480        |
| train/                      |              |
|    approx_kl                | 0.00975047   |
|    approx_ln(kl)            | -4.6304398   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.89        |
|    explained_variance       | -1.64        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.43         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 30.8         |
|    n_updates                | 16           |
|    policy_gradient_loss     | -0.00178     |
|    std                      | 1.03         |
|    value_loss               | 128          |
----------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.3652319] |
| time/                       |              |
|    fps                      | 119          |
|    iterations               | 11           |
|    time_elapsed             | 188          |
|    total_timesteps          | 22528        |
| train/                      |              |
|    approx_kl                | 0.009418211  |
|    approx_ln(kl)            | -4.66511     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.89        |
|    explained_variance       | -0.00214     |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.08         |
|    ln(policy_gradient_loss) | -6.41        |
|    loss                     | 161          |
|    n_updates                | 18           |
|    policy_gradient_loss     | 0.00164      |
|    std                      | 1.03         |
|    value_loss               | 360          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.5203625] |
| time/                       |              |
|    fps                      | 121          |
|    iterations               | 12           |
|    time_elapsed             | 201          |
|    total_timesteps          | 24576        |
| train/                      |              |
|    approx_kl                | 0.0037313818 |
|    approx_ln(kl)            | -5.5909767   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.89        |
|    explained_variance       | 0.0188       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.35         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 210          |
|    n_updates                | 19           |
|    policy_gradient_loss     | -0.00154     |
|    std                      | 1.03         |
|    value_loss               | 415          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.60568774] |
| time/                       |               |
|    fps                      | 124           |
|    iterations               | 13            |
|    time_elapsed             | 214           |
|    total_timesteps          | 26624         |
| train/                      |               |
|    approx_kl                | 0.010154038   |
|    approx_ln(kl)            | -4.589884     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.89         |
|    explained_variance       | 0.0358        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 3.97          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 52.9          |
|    n_updates                | 21            |
|    policy_gradient_loss     | -0.000367     |
|    std                      | 1.03          |
|    value_loss               | 144           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.4460633] |
| time/                       |              |
|    fps                      | 125          |
|    iterations               | 14           |
|    time_elapsed             | 228          |
|    total_timesteps          | 28672        |
| train/                      |              |
|    approx_kl                | 0.00918308   |
|    approx_ln(kl)            | -4.6903925   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.9         |
|    explained_variance       | 0.0522       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.76         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 116          |
|    n_updates                | 23           |
|    policy_gradient_loss     | -0.00282     |
|    std                      | 1.03         |
|    value_loss               | 270          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.93491364] |
| time/                       |               |
|    fps                      | 126           |
|    iterations               | 15            |
|    time_elapsed             | 242           |
|    total_timesteps          | 30720         |
| train/                      |               |
|    approx_kl                | 0.0082093375  |
|    approx_ln(kl)            | -4.802483     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.637         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 3.33          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 28            |
|    n_updates                | 26            |
|    policy_gradient_loss     | -0.000127     |
|    std                      | 1.04          |
|    value_loss               | 129           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3444113] |
| time/                       |              |
|    fps                      | 128          |
|    iterations               | 16           |
|    time_elapsed             | 254          |
|    total_timesteps          | 32768        |
| train/                      |              |
|    approx_kl                | 0.009478373  |
|    approx_ln(kl)            | -4.6587424   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.386        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.8          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 44.7         |
|    n_updates                | 28           |
|    policy_gradient_loss     | -0.0024      |
|    std                      | 1.04         |
|    value_loss               | 122          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.7942503] |
| time/                       |              |
|    fps                      | 129          |
|    iterations               | 17           |
|    time_elapsed             | 268          |
|    total_timesteps          | 34816        |
| train/                      |              |
|    approx_kl                | 0.01511017   |
|    approx_ln(kl)            | -4.192387    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.00637      |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.06         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 57.9         |
|    n_updates                | 30           |
|    policy_gradient_loss     | -0.00327     |
|    std                      | 1.04         |
|    value_loss               | 139          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43470123] |
| time/                       |               |
|    fps                      | 130           |
|    iterations               | 18            |
|    time_elapsed             | 282           |
|    total_timesteps          | 36864         |
| train/                      |               |
|    approx_kl                | 0.0046166587  |
|    approx_ln(kl)            | -5.378084     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.93         |
|    explained_variance       | -0.0184       |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 3.49          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 32.7          |
|    n_updates                | 31            |
|    policy_gradient_loss     | -0.00329      |
|    std                      | 1.05          |
|    value_loss               | 105           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.50200176] |
| time/                       |               |
|    fps                      | 131           |
|    iterations               | 19            |
|    time_elapsed             | 295           |
|    total_timesteps          | 38912         |
| train/                      |               |
|    approx_kl                | 0.0037602566  |
|    approx_ln(kl)            | -5.583268     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.94         |
|    explained_variance       | 0.0139        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 2.84          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 17.1          |
|    n_updates                | 32            |
|    policy_gradient_loss     | -0.00172      |
|    std                      | 1.05          |
|    value_loss               | 37.5          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.80175924] |
| time/                       |               |
|    fps                      | 132           |
|    iterations               | 20            |
|    time_elapsed             | 308           |
|    total_timesteps          | 40960         |
| train/                      |               |
|    approx_kl                | 0.00743625    |
|    approx_ln(kl)            | -4.9013886    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.94         |
|    explained_variance       | 0.00522       |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.492         |
|    ln(policy_gradient_loss) | -5.71         |
|    loss                     | 1.64          |
|    n_updates                | 33            |
|    policy_gradient_loss     | 0.0033        |
|    std                      | 1.05          |
|    value_loss               | 3.32          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-2.0941842] |
| time/                       |              |
|    fps                      | 133          |
|    iterations               | 21           |
|    time_elapsed             | 321          |
|    total_timesteps          | 43008        |
| train/                      |              |
|    approx_kl                | 0.0063594556 |
|    approx_ln(kl)            | -5.0578127   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | 0.00279      |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.6          |
|    ln(policy_gradient_loss) | -9.91        |
|    loss                     | 4.94         |
|    n_updates                | 34           |
|    policy_gradient_loss     | 4.98e-05     |
|    std                      | 1.05         |
|    value_loss               | 11           |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-2.4147823] |
| time/                       |              |
|    fps                      | 134          |
|    iterations               | 22           |
|    time_elapsed             | 334          |
|    total_timesteps          | 45056        |
| train/                      |              |
|    approx_kl                | 0.0054597086 |
|    approx_ln(kl)            | -5.21036     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | -0.0443      |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.2          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 66.4         |
|    n_updates                | 35           |
|    policy_gradient_loss     | -0.00134     |
|    std                      | 1.06         |
|    value_loss               | 136          |
----------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-2.4665298] |
| time/                       |              |
|    fps                      | 135          |
|    iterations               | 23           |
|    time_elapsed             | 348          |
|    total_timesteps          | 47104        |
| train/                      |              |
|    approx_kl                | 0.009801198  |
|    approx_ln(kl)            | -4.625251    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | -0.0561      |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.14         |
|    ln(policy_gradient_loss) | -6.07        |
|    loss                     | 171          |
|    n_updates                | 37           |
|    policy_gradient_loss     | 0.00231      |
|    std                      | 1.06         |
|    value_loss               | 370          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 2 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-2.316559] |
| time/                       |             |
|    fps                      | 134         |
|    iterations               | 24          |
|    time_elapsed             | 364         |
|    total_timesteps          | 49152       |
| train/                      |             |
|    approx_kl                | 0.009812331 |
|    approx_ln(kl)            | -4.6241155  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.95       |
|    explained_variance       | 0.00511     |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 5.15        |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 172         |
|    n_updates                | 40          |
|    policy_gradient_loss     | -0.00301    |
|    std                      | 1.06        |
|    value_loss               | 451         |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.7682415] |
| time/                       |              |
|    fps                      | 132          |
|    iterations               | 25           |
|    time_elapsed             | 387          |
|    total_timesteps          | 51200        |
| train/                      |              |
|    approx_kl                | 0.0042698397 |
|    approx_ln(kl)            | -5.456179    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | 0.00423      |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.24         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 189          |
|    n_updates                | 42           |
|    policy_gradient_loss     | -0.00212     |
|    std                      | 1.06         |
|    value_loss               | 389          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47526717] |
| time/                       |               |
|    fps                      | 127           |
|    iterations               | 26            |
|    time_elapsed             | 418           |
|    total_timesteps          | 53248         |
| train/                      |               |
|    approx_kl                | 0.006578303   |
|    approx_ln(kl)            | -5.0239787    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | 0.818         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 5.15          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 172           |
|    n_updates                | 44            |
|    policy_gradient_loss     | -0.00494      |
|    std                      | 1.06          |
|    value_loss               | 420           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 2 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.6226779] |
| time/                       |              |
|    fps                      | 121          |
|    iterations               | 27           |
|    time_elapsed             | 455          |
|    total_timesteps          | 55296        |
| train/                      |              |
|    approx_kl                | 0.01078762   |
|    approx_ln(kl)            | -4.529356    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | 0.818        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.71         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 111          |
|    n_updates                | 47           |
|    policy_gradient_loss     | -0.00136     |
|    std                      | 1.06         |
|    value_loss               | 250          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 2 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.7736716] |
| time/                       |              |
|    fps                      | 112          |
|    iterations               | 28           |
|    time_elapsed             | 511          |
|    total_timesteps          | 57344        |
| train/                      |              |
|    approx_kl                | 0.011878369  |
|    approx_ln(kl)            | -4.4330363   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | 0.481        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.21         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 67.6         |
|    n_updates                | 50           |
|    policy_gradient_loss     | -0.00284     |
|    std                      | 1.06         |
|    value_loss               | 201          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.0657024] |
| time/                       |              |
|    fps                      | 111          |
|    iterations               | 29           |
|    time_elapsed             | 530          |
|    total_timesteps          | 59392        |
| train/                      |              |
|    approx_kl                | 0.005299503  |
|    approx_ln(kl)            | -5.2401423   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | -0.0147      |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.92         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 137          |
|    n_updates                | 52           |
|    policy_gradient_loss     | -0.00189     |
|    std                      | 1.06         |
|    value_loss               | 345          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 3 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.8996748] |
| time/                       |              |
|    fps                      | 106          |
|    iterations               | 30           |
|    time_elapsed             | 574          |
|    total_timesteps          | 61440        |
| train/                      |              |
|    approx_kl                | 0.010920529  |
|    approx_ln(kl)            | -4.517111    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | 0.736        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.62         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 101          |
|    n_updates                | 56           |
|    policy_gradient_loss     | -0.00221     |
|    std                      | 1.06         |
|    value_loss               | 247          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 1 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-2.088416] |
| time/                       |             |
|    fps                      | 107         |
|    iterations               | 31          |
|    time_elapsed             | 587         |
|    total_timesteps          | 63488       |
| train/                      |             |
|    approx_kl                | 0.004159932 |
|    approx_ln(kl)            | -5.4822564  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.95       |
|    explained_variance       | -0.783      |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 4.09        |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 59.5        |
|    n_updates                | 58          |
|    policy_gradient_loss     | -0.00141    |
|    std                      | 1.06        |
|    value_loss               | 169         |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 4 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-2.5169127] |
| time/                       |              |
|    fps                      | 108          |
|    iterations               | 32           |
|    time_elapsed             | 601          |
|    total_timesteps          | 65536        |
| train/                      |              |
|    approx_kl                | 0.011046848  |
|    approx_ln(kl)            | -4.50561     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | 0.63         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.37         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 79.1         |
|    n_updates                | 63           |
|    policy_gradient_loss     | -0.00519     |
|    std                      | 1.06         |
|    value_loss               | 163          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 2 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-2.3659394] |
| time/                       |              |
|    fps                      | 109          |
|    iterations               | 33           |
|    time_elapsed             | 614          |
|    total_timesteps          | 67584        |
| train/                      |              |
|    approx_kl                | 0.013448626  |
|    approx_ln(kl)            | -4.3088784   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | -0.000442    |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.91         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 136          |
|    n_updates                | 66           |
|    policy_gradient_loss     | -0.00127     |
|    std                      | 1.06         |
|    value_loss               | 346          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 2 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-2.7879608] |
| time/                       |              |
|    fps                      | 110          |
|    iterations               | 34           |
|    time_elapsed             | 627          |
|    total_timesteps          | 69632        |
| train/                      |              |
|    approx_kl                | 0.010136644  |
|    approx_ln(kl)            | -4.5915985   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | 0.851        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.17         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 64.4         |
|    n_updates                | 69           |
|    policy_gradient_loss     | -0.00203     |
|    std                      | 1.06         |
|    value_loss               | 315          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 2 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-2.2378907] |
| time/                       |              |
|    fps                      | 111          |
|    iterations               | 35           |
|    time_elapsed             | 640          |
|    total_timesteps          | 71680        |
| train/                      |              |
|    approx_kl                | 0.013056715  |
|    approx_ln(kl)            | -4.338453    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | 0.754        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 5.53         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 252          |
|    n_updates                | 72           |
|    policy_gradient_loss     | -0.00392     |
|    std                      | 1.06         |
|    value_loss               | 452          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.8476459] |
| time/                       |              |
|    fps                      | 112          |
|    iterations               | 36           |
|    time_elapsed             | 653          |
|    total_timesteps          | 73728        |
| train/                      |              |
|    approx_kl                | 0.0071803257 |
|    approx_ln(kl)            | -4.9364104   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | 0.152        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.1          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 60.2         |
|    n_updates                | 74           |
|    policy_gradient_loss     | -0.0027      |
|    std                      | 1.06         |
|    value_loss               | 252          |
----------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5839937] |
| time/                       |              |
|    fps                      | 113          |
|    iterations               | 37           |
|    time_elapsed             | 666          |
|    total_timesteps          | 75776        |
| train/                      |              |
|    approx_kl                | 0.013877524  |
|    approx_ln(kl)            | -4.277485    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | -1.31        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.51         |
|    ln(policy_gradient_loss) | -5.94        |
|    loss                     | 33.4         |
|    n_updates                | 76           |
|    policy_gradient_loss     | 0.00263      |
|    std                      | 1.06         |
|    value_loss               | 128          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5374097] |
| time/                       |              |
|    fps                      | 114          |
|    iterations               | 38           |
|    time_elapsed             | 679          |
|    total_timesteps          | 77824        |
| train/                      |              |
|    approx_kl                | 0.005329759  |
|    approx_ln(kl)            | -5.2344494   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | -0.0142      |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.75         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 115          |
|    n_updates                | 77           |
|    policy_gradient_loss     | -0.00438     |
|    std                      | 1.06         |
|    value_loss               | 270          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.97234774] |
| time/                       |               |
|    fps                      | 115           |
|    iterations               | 39            |
|    time_elapsed             | 692           |
|    total_timesteps          | 79872         |
| train/                      |               |
|    approx_kl                | 0.0047377655  |
|    approx_ln(kl)            | -5.3521895    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | -0.0118       |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 4.66          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 105           |
|    n_updates                | 78            |
|    policy_gradient_loss     | -0.0033       |
|    std                      | 1.06          |
|    value_loss               | 217           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.268757]  |
| time/                       |              |
|    fps                      | 116          |
|    iterations               | 40           |
|    time_elapsed             | 705          |
|    total_timesteps          | 81920        |
| train/                      |              |
|    approx_kl                | 0.0035792547 |
|    approx_ln(kl)            | -5.632601    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | -0.0168      |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.92         |
|    ln(policy_gradient_loss) | -7.3         |
|    loss                     | 50.6         |
|    n_updates                | 79           |
|    policy_gradient_loss     | 0.000678     |
|    std                      | 1.06         |
|    value_loss               | 116          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.2069907] |
| time/                       |              |
|    fps                      | 116          |
|    iterations               | 41           |
|    time_elapsed             | 718          |
|    total_timesteps          | 83968        |
| train/                      |              |
|    approx_kl                | 0.004525788  |
|    approx_ln(kl)            | -5.3979635   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | -0.00291     |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.19         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 66           |
|    n_updates                | 80           |
|    policy_gradient_loss     | -0.00319     |
|    std                      | 1.06         |
|    value_loss               | 144          |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.5737772] |
| time/                       |              |
|    fps                      | 117          |
|    iterations               | 42           |
|    time_elapsed             | 731          |
|    total_timesteps          | 86016        |
| train/                      |              |
|    approx_kl                | 0.00478504   |
|    approx_ln(kl)            | -5.342261    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | -0.00228     |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.36         |
|    ln(policy_gradient_loss) | -6.49        |
|    loss                     | 28.8         |
|    n_updates                | 81           |
|    policy_gradient_loss     | 0.00151      |
|    std                      | 1.06         |
|    value_loss               | 91.1         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-1.789362] |
| time/                       |             |
|    fps                      | 118         |
|    iterations               | 43          |
|    time_elapsed             | 745         |
|    total_timesteps          | 88064       |
| train/                      |             |
|    approx_kl                | 0.005493673 |
|    approx_ln(kl)            | -5.2041583  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.95       |
|    explained_variance       | -0.00106    |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 3.37        |
|    ln(policy_gradient_loss) | -5.12       |
|    loss                     | 29.1        |
|    n_updates                | 82          |
|    policy_gradient_loss     | 0.00597     |
|    std                      | 1.06        |
|    value_loss               | 123         |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-2.0162225] |
| time/                       |              |
|    fps                      | 118          |
|    iterations               | 44           |
|    time_elapsed             | 758          |
|    total_timesteps          | 90112        |
| train/                      |              |
|    approx_kl                | 0.008766793  |
|    approx_ln(kl)            | -4.7367845   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | -0.00235     |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.39         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 80.3         |
|    n_updates                | 83           |
|    policy_gradient_loss     | -0.00299     |
|    std                      | 1.06         |
|    value_loss               | 195          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.4040818] |
| time/                       |              |
|    fps                      | 119          |
|    iterations               | 45           |
|    time_elapsed             | 771          |
|    total_timesteps          | 92160        |
| train/                      |              |
|    approx_kl                | 0.004905655  |
|    approx_ln(kl)            | -5.3173666   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.96        |
|    explained_variance       | -0.00286     |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.75         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 115          |
|    n_updates                | 84           |
|    policy_gradient_loss     | -0.00296     |
|    std                      | 1.06         |
|    value_loss               | 196          |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.2604271] |
| time/                       |              |
|    fps                      | 120          |
|    iterations               | 46           |
|    time_elapsed             | 784          |
|    total_timesteps          | 94208        |
| train/                      |              |
|    approx_kl                | 0.005622767  |
|    approx_ln(kl)            | -5.1809316   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.96        |
|    explained_variance       | -0.00208     |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 4.58         |
|    ln(policy_gradient_loss) | -5.42        |
|    loss                     | 97.8         |
|    n_updates                | 85           |
|    policy_gradient_loss     | 0.00444      |
|    std                      | 1.06         |
|    value_loss               | 165          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.97201836] |
| time/                       |               |
|    fps                      | 120           |
|    iterations               | 47            |
|    time_elapsed             | 797           |
|    total_timesteps          | 96256         |
| train/                      |               |
|    approx_kl                | 0.004936044   |
|    approx_ln(kl)            | -5.311191     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | 0.00057       |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 3.56          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 35.1          |
|    n_updates                | 86            |
|    policy_gradient_loss     | -0.00794      |
|    std                      | 1.06          |
|    value_loss               | 75            |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45393816] |
| time/                       |               |
|    fps                      | 121           |
|    iterations               | 48            |
|    time_elapsed             | 810           |
|    total_timesteps          | 98304         |
| train/                      |               |
|    approx_kl                | 0.007851829   |
|    approx_ln(kl)            | -4.8470087    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | -0.000169     |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 3.73          |
|    ln(policy_gradient_loss) | -4.79         |
|    loss                     | 41.8          |
|    n_updates                | 87            |
|    policy_gradient_loss     | 0.00834       |
|    std                      | 1.06          |
|    value_loss               | 86.4          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38700622] |
| time/                       |               |
|    fps                      | 121           |
|    iterations               | 49            |
|    time_elapsed             | 823           |
|    total_timesteps          | 100352        |
| train/                      |               |
|    approx_kl                | 0.006533594   |
|    approx_ln(kl)            | -5.030798     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | -0.000909     |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 2.64          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 14            |
|    n_updates                | 89            |
|    policy_gradient_loss     | -0.00112      |
|    std                      | 1.06          |
|    value_loss               | 48.1          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 2 due to reaching max kl: 0.02
--------------------------------------
| reward             | [-0.43902737] |
| time/              |               |
|    fps             | 163           |
|    iterations      | 1             |
|    time_elapsed    | 12            |
|    total_timesteps | 102400        |
--------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5393673] |
| time/                       |              |
|    fps                      | 158          |
|    iterations               | 2            |
|    time_elapsed             | 25           |
|    total_timesteps          | 104448       |
| train/                      |              |
|    approx_kl                | 0.00625158   |
|    approx_ln(kl)            | -5.074921    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | -0.000306    |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.48         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 12           |
|    n_updates                | 94           |
|    policy_gradient_loss     | -0.00116     |
|    std                      | 1.05         |
|    value_loss               | 40.5         |
----------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32930797] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 3             |
|    time_elapsed             | 39            |
|    total_timesteps          | 106496        |
| train/                      |               |
|    approx_kl                | 0.007220964   |
|    approx_ln(kl)            | -4.930767     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.93         |
|    explained_variance       | 0.00486       |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.77          |
|    ln(policy_gradient_loss) | -6.45         |
|    loss                     | 5.85          |
|    n_updates                | 96            |
|    policy_gradient_loss     | 0.00158       |
|    std                      | 1.05          |
|    value_loss               | 36.2          |
-----------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.8457236] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 4            |
|    time_elapsed             | 52           |
|    total_timesteps          | 108544       |
| train/                      |              |
|    approx_kl                | 0.012645955  |
|    approx_ln(kl)            | -4.370418    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | 6.8e-05      |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.53         |
|    ln(policy_gradient_loss) | -7.64        |
|    loss                     | 4.61         |
|    n_updates                | 98           |
|    policy_gradient_loss     | 0.000482     |
|    std                      | 1.05         |
|    value_loss               | 15.1         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.71042895] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 5             |
|    time_elapsed             | 65            |
|    total_timesteps          | 110592        |
| train/                      |               |
|    approx_kl                | 0.006721552   |
|    approx_ln(kl)            | -5.002436     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.94         |
|    explained_variance       | -0.00404      |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 2.4           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 11            |
|    n_updates                | 99            |
|    policy_gradient_loss     | -0.00196      |
|    std                      | 1.05          |
|    value_loss               | 34.4          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4977221] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 6            |
|    time_elapsed             | 78           |
|    total_timesteps          | 112640       |
| train/                      |              |
|    approx_kl                | 0.00376888   |
|    approx_ln(kl)            | -5.5809774   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.0489       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.32         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 3.74         |
|    n_updates                | 100          |
|    policy_gradient_loss     | -0.00281     |
|    std                      | 1.05         |
|    value_loss               | 13           |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4931917] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 7            |
|    time_elapsed             | 91           |
|    total_timesteps          | 114688       |
| train/                      |              |
|    approx_kl                | 0.0050707464 |
|    approx_ln(kl)            | -5.2842674   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | -0.0452      |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.172        |
|    ln(policy_gradient_loss) | -5.18        |
|    loss                     | 1.19         |
|    n_updates                | 101          |
|    policy_gradient_loss     | 0.00564      |
|    std                      | 1.05         |
|    value_loss               | 6.71         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5748925] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 8            |
|    time_elapsed             | 103          |
|    total_timesteps          | 116736       |
| train/                      |              |
|    approx_kl                | 0.0045428267 |
|    approx_ln(kl)            | -5.394206    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | 0.605        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.256        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.29         |
|    n_updates                | 102          |
|    policy_gradient_loss     | -0.00302     |
|    std                      | 1.05         |
|    value_loss               | 5.95         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.60549194] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 9             |
|    time_elapsed             | 116           |
|    total_timesteps          | 118784        |
| train/                      |               |
|    approx_kl                | 0.0062668975  |
|    approx_ln(kl)            | -5.072474     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.94         |
|    explained_variance       | 0.403         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.479         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.61          |
|    n_updates                | 103           |
|    policy_gradient_loss     | -0.000703     |
|    std                      | 1.05          |
|    value_loss               | 6.11          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35492525] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 10            |
|    time_elapsed             | 130           |
|    total_timesteps          | 120832        |
| train/                      |               |
|    approx_kl                | 0.005246773   |
|    approx_ln(kl)            | -5.250142     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.94         |
|    explained_variance       | 0.614         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.32          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 3.76          |
|    n_updates                | 104           |
|    policy_gradient_loss     | -9.82e-05     |
|    std                      | 1.05          |
|    value_loss               | 9.19          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.537143] |
| time/                       |             |
|    fps                      | 157         |
|    iterations               | 11          |
|    time_elapsed             | 143         |
|    total_timesteps          | 122880      |
| train/                      |             |
|    approx_kl                | 0.007528736 |
|    approx_ln(kl)            | -4.889028   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.93       |
|    explained_variance       | 0.896       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -0.227      |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.797       |
|    n_updates                | 105         |
|    policy_gradient_loss     | -0.00245    |
|    std                      | 1.05        |
|    value_loss               | 6.55        |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32993302] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 12            |
|    time_elapsed             | 156           |
|    total_timesteps          | 124928        |
| train/                      |               |
|    approx_kl                | 0.005434223   |
|    approx_ln(kl)            | -5.215039     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.93         |
|    explained_variance       | -0.385        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.298         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.35          |
|    n_updates                | 106           |
|    policy_gradient_loss     | -0.00418      |
|    std                      | 1.05          |
|    value_loss               | 7.9           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39211014] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 13            |
|    time_elapsed             | 169           |
|    total_timesteps          | 126976        |
| train/                      |               |
|    approx_kl                | 0.008665039   |
|    approx_ln(kl)            | -4.748459     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.93         |
|    explained_variance       | 0.908         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.73          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 2.07          |
|    n_updates                | 107           |
|    policy_gradient_loss     | -0.00169      |
|    std                      | 1.05          |
|    value_loss               | 7.69          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
---------------------------------------------
| reward                      | [-0.330106] |
| time/                       |             |
|    fps                      | 157         |
|    iterations               | 14          |
|    time_elapsed             | 181         |
|    total_timesteps          | 129024      |
| train/                      |             |
|    approx_kl                | 0.008858188 |
|    approx_ln(kl)            | -4.7264132  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.93       |
|    explained_variance       | 0.87        |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 0.407       |
|    ln(policy_gradient_loss) | -7.01       |
|    loss                     | 1.5         |
|    n_updates                | 108         |
|    policy_gradient_loss     | 0.000906    |
|    std                      | 1.05        |
|    value_loss               | 3.63        |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.6260819] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 15           |
|    time_elapsed             | 194          |
|    total_timesteps          | 131072       |
| train/                      |              |
|    approx_kl                | 0.006285478  |
|    approx_ln(kl)            | -5.0695133   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | 0.947        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.6          |
|    ln(policy_gradient_loss) | -5.26        |
|    loss                     | 4.94         |
|    n_updates                | 109          |
|    policy_gradient_loss     | 0.0052       |
|    std                      | 1.05         |
|    value_loss               | 6.06         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.6622615] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 16           |
|    time_elapsed             | 207          |
|    total_timesteps          | 133120       |
| train/                      |              |
|    approx_kl                | 0.0038744553 |
|    approx_ln(kl)            | -5.55335     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | 0.895        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.18         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.2          |
|    n_updates                | 110          |
|    policy_gradient_loss     | -0.000238    |
|    std                      | 1.05         |
|    value_loss               | 3.2          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45411122] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 17            |
|    time_elapsed             | 220           |
|    total_timesteps          | 135168        |
| train/                      |               |
|    approx_kl                | 0.0055955066  |
|    approx_ln(kl)            | -5.1857915    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.94         |
|    explained_variance       | 0.932         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.1           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 3.01          |
|    n_updates                | 111           |
|    policy_gradient_loss     | -0.00269      |
|    std                      | 1.05          |
|    value_loss               | 5.88          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5352562] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 18           |
|    time_elapsed             | 233          |
|    total_timesteps          | 137216       |
| train/                      |              |
|    approx_kl                | 0.0047654086 |
|    approx_ln(kl)            | -5.346372    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.814        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.0756      |
|    ln(policy_gradient_loss) | -7.42        |
|    loss                     | 0.927        |
|    n_updates                | 112          |
|    policy_gradient_loss     | 0.000598     |
|    std                      | 1.05         |
|    value_loss               | 3.45         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45681328] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 19            |
|    time_elapsed             | 246           |
|    total_timesteps          | 139264        |
| train/                      |               |
|    approx_kl                | 0.0060800803  |
|    approx_ln(kl)            | -5.1027374    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.93         |
|    explained_variance       | 0.966         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.583        |
|    ln(policy_gradient_loss) | -4.47         |
|    loss                     | 0.558         |
|    n_updates                | 113           |
|    policy_gradient_loss     | 0.0114        |
|    std                      | 1.05          |
|    value_loss               | 3.39          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44166386] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 20            |
|    time_elapsed             | 259           |
|    total_timesteps          | 141312        |
| train/                      |               |
|    approx_kl                | 0.005964866   |
|    approx_ln(kl)            | -5.1218686    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.93         |
|    explained_variance       | 0.843         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.778         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 2.18          |
|    n_updates                | 114           |
|    policy_gradient_loss     | -0.00899      |
|    std                      | 1.05          |
|    value_loss               | 8.43          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4406878] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 21           |
|    time_elapsed             | 272          |
|    total_timesteps          | 143360       |
| train/                      |              |
|    approx_kl                | 0.0044533955 |
|    approx_ln(kl)            | -5.4140882   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.752        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.121        |
|    ln(policy_gradient_loss) | -9.12        |
|    loss                     | 1.13         |
|    n_updates                | 115          |
|    policy_gradient_loss     | 0.000109     |
|    std                      | 1.04         |
|    value_loss               | 4.55         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26595256] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 22            |
|    time_elapsed             | 285           |
|    total_timesteps          | 145408        |
| train/                      |               |
|    approx_kl                | 0.0065829093  |
|    approx_ln(kl)            | -5.0232787    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.775         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.0465        |
|    ln(policy_gradient_loss) | -6.34         |
|    loss                     | 1.05          |
|    n_updates                | 116           |
|    policy_gradient_loss     | 0.00176       |
|    std                      | 1.04          |
|    value_loss               | 3.1           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.5883548] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 23           |
|    time_elapsed             | 298          |
|    total_timesteps          | 147456       |
| train/                      |              |
|    approx_kl                | 0.008002272  |
|    approx_ln(kl)            | -4.8280296   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.128        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.775       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.461        |
|    n_updates                | 117          |
|    policy_gradient_loss     | -0.00381     |
|    std                      | 1.04         |
|    value_loss               | 1.3          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41194218] |
| time/                       |               |
|    fps                      | 158           |
|    iterations               | 24            |
|    time_elapsed             | 310           |
|    total_timesteps          | 149504        |
| train/                      |               |
|    approx_kl                | 0.007353166   |
|    approx_ln(kl)            | -4.9126244    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.852         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.405         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.5           |
|    n_updates                | 118           |
|    policy_gradient_loss     | -0.00756      |
|    std                      | 1.04          |
|    value_loss               | 4.6           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.27485797] |
| time/                       |               |
|    fps                      | 158           |
|    iterations               | 25            |
|    time_elapsed             | 323           |
|    total_timesteps          | 151552        |
| train/                      |               |
|    approx_kl                | 0.009664674   |
|    approx_ln(kl)            | -4.639278     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.898         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.115        |
|    ln(policy_gradient_loss) | -5.87         |
|    loss                     | 0.892         |
|    n_updates                | 119           |
|    policy_gradient_loss     | 0.00283       |
|    std                      | 1.04          |
|    value_loss               | 1.51          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5008378] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 26           |
|    time_elapsed             | 337          |
|    total_timesteps          | 153600       |
| train/                      |              |
|    approx_kl                | 0.007133916  |
|    approx_ln(kl)            | -4.942895    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.803        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.0235       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.02         |
|    n_updates                | 120          |
|    policy_gradient_loss     | -0.013       |
|    std                      | 1.04         |
|    value_loss               | 3.25         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.6091818] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 27           |
|    time_elapsed             | 350          |
|    total_timesteps          | 155648       |
| train/                      |              |
|    approx_kl                | 0.010149677  |
|    approx_ln(kl)            | -4.5903134   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.922        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.21        |
|    ln(policy_gradient_loss) | -8.14        |
|    loss                     | 0.81         |
|    n_updates                | 121          |
|    policy_gradient_loss     | 0.000291     |
|    std                      | 1.04         |
|    value_loss               | 1.98         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.62269634] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 28            |
|    time_elapsed             | 363           |
|    total_timesteps          | 157696        |
| train/                      |               |
|    approx_kl                | 0.0052227955  |
|    approx_ln(kl)            | -5.2547226    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.836         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.109         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.11          |
|    n_updates                | 122           |
|    policy_gradient_loss     | -0.00157      |
|    std                      | 1.04          |
|    value_loss               | 3.27          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.214535] |
| time/                       |             |
|    fps                      | 157         |
|    iterations               | 29          |
|    time_elapsed             | 376         |
|    total_timesteps          | 159744      |
| train/                      |             |
|    approx_kl                | 0.004763514 |
|    approx_ln(kl)            | -5.34677    |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.91       |
|    explained_variance       | 0.955       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 0.232       |
|    ln(policy_gradient_loss) | -6.25       |
|    loss                     | 1.26        |
|    n_updates                | 123         |
|    policy_gradient_loss     | 0.00192     |
|    std                      | 1.04        |
|    value_loss               | 3.74        |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37729898] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 30            |
|    time_elapsed             | 389           |
|    total_timesteps          | 161792        |
| train/                      |               |
|    approx_kl                | 0.0050475034  |
|    approx_ln(kl)            | -5.2888618    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.834         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.457         |
|    ln(policy_gradient_loss) | -9.4          |
|    loss                     | 1.58          |
|    n_updates                | 124           |
|    policy_gradient_loss     | 8.25e-05      |
|    std                      | 1.04          |
|    value_loss               | 3.13          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4514296] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 31           |
|    time_elapsed             | 402          |
|    total_timesteps          | 163840       |
| train/                      |              |
|    approx_kl                | 0.0048679444 |
|    approx_ln(kl)            | -5.3250837   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.91        |
|    explained_variance       | 0.934        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.836        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 2.31         |
|    n_updates                | 125          |
|    policy_gradient_loss     | -0.00243     |
|    std                      | 1.04         |
|    value_loss               | 4.3          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41164657] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 32            |
|    time_elapsed             | 415           |
|    total_timesteps          | 165888        |
| train/                      |               |
|    approx_kl                | 0.008828556   |
|    approx_ln(kl)            | -4.729764     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.481         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.107         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.11          |
|    n_updates                | 126           |
|    policy_gradient_loss     | -0.00443      |
|    std                      | 1.03          |
|    value_loss               | 2.22          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.6271093] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 33           |
|    time_elapsed             | 428          |
|    total_timesteps          | 167936       |
| train/                      |              |
|    approx_kl                | 0.0084463    |
|    approx_ln(kl)            | -4.774027    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.9         |
|    explained_variance       | 0.95         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.00474      |
|    ln(policy_gradient_loss) | -6.74        |
|    loss                     | 1            |
|    n_updates                | 127          |
|    policy_gradient_loss     | 0.00119      |
|    std                      | 1.03         |
|    value_loss               | 1.85         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.42305055] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 34            |
|    time_elapsed             | 441           |
|    total_timesteps          | 169984        |
| train/                      |               |
|    approx_kl                | 0.011297211   |
|    approx_ln(kl)            | -4.4831996    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.9          |
|    explained_variance       | 0.593         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.12         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.326         |
|    n_updates                | 128           |
|    policy_gradient_loss     | -0.00468      |
|    std                      | 1.03          |
|    value_loss               | 0.663         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
---------------------------------------------
| reward                      | [-0.420575] |
| time/                       |             |
|    fps                      | 157         |
|    iterations               | 35          |
|    time_elapsed             | 454         |
|    total_timesteps          | 172032      |
| train/                      |             |
|    approx_kl                | 0.015569314 |
|    approx_ln(kl)            | -4.162453   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.9        |
|    explained_variance       | 0.788       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -0.141      |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.868       |
|    n_updates                | 129         |
|    policy_gradient_loss     | -0.00863    |
|    std                      | 1.03        |
|    value_loss               | 1.48        |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.33827198] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 36            |
|    time_elapsed             | 467           |
|    total_timesteps          | 174080        |
| train/                      |               |
|    approx_kl                | 0.010239147   |
|    approx_ln(kl)            | -4.5815372    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.9          |
|    explained_variance       | 0.831         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.266        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.767         |
|    n_updates                | 130           |
|    policy_gradient_loss     | -0.00148      |
|    std                      | 1.03          |
|    value_loss               | 10.6          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2920432] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 37           |
|    time_elapsed             | 480          |
|    total_timesteps          | 176128       |
| train/                      |              |
|    approx_kl                | 0.008021246  |
|    approx_ln(kl)            | -4.8256617   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.9         |
|    explained_variance       | 0.939        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.355        |
|    ln(policy_gradient_loss) | -4.39        |
|    loss                     | 1.43         |
|    n_updates                | 131          |
|    policy_gradient_loss     | 0.0124       |
|    std                      | 1.03         |
|    value_loss               | 3.02         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.6765912] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 38           |
|    time_elapsed             | 493          |
|    total_timesteps          | 178176       |
| train/                      |              |
|    approx_kl                | 0.006898141  |
|    approx_ln(kl)            | -4.9765034   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.9         |
|    explained_variance       | 0.881        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.381        |
|    ln(policy_gradient_loss) | -5.37        |
|    loss                     | 1.46         |
|    n_updates                | 132          |
|    policy_gradient_loss     | 0.00466      |
|    std                      | 1.03         |
|    value_loss               | 3.63         |
----------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.5238852] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 39           |
|    time_elapsed             | 507          |
|    total_timesteps          | 180224       |
| train/                      |              |
|    approx_kl                | 0.02787643   |
|    approx_ln(kl)            | -3.5799737   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.89        |
|    explained_variance       | 0.809        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.797        |
|    ln(policy_gradient_loss) | -6.1         |
|    loss                     | 2.22         |
|    n_updates                | 134          |
|    policy_gradient_loss     | 0.00224      |
|    std                      | 1.03         |
|    value_loss               | 6.52         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5438974] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 40           |
|    time_elapsed             | 520          |
|    total_timesteps          | 182272       |
| train/                      |              |
|    approx_kl                | 0.004151771  |
|    approx_ln(kl)            | -5.4842205   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.89        |
|    explained_variance       | 0.815        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.355        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.43         |
|    n_updates                | 135          |
|    policy_gradient_loss     | -0.00947     |
|    std                      | 1.03         |
|    value_loss               | 5.03         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.55277765] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 41            |
|    time_elapsed             | 534           |
|    total_timesteps          | 184320        |
| train/                      |               |
|    approx_kl                | 0.0074090976  |
|    approx_ln(kl)            | -4.9050465    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.89         |
|    explained_variance       | 0.839         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.384         |
|    ln(policy_gradient_loss) | -5.88         |
|    loss                     | 1.47          |
|    n_updates                | 136           |
|    policy_gradient_loss     | 0.0028        |
|    std                      | 1.02          |
|    value_loss               | 3.55          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35818997] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 42            |
|    time_elapsed             | 546           |
|    total_timesteps          | 186368        |
| train/                      |               |
|    approx_kl                | 0.0066343686  |
|    approx_ln(kl)            | -5.015492     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.89         |
|    explained_variance       | 0.65          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0917       |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.912         |
|    n_updates                | 137           |
|    policy_gradient_loss     | -0.00702      |
|    std                      | 1.02          |
|    value_loss               | 1.82          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.46864027] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 43            |
|    time_elapsed             | 559           |
|    total_timesteps          | 188416        |
| train/                      |               |
|    approx_kl                | 0.007528695   |
|    approx_ln(kl)            | -4.889034     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.89         |
|    explained_variance       | -0.324        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.548         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.73          |
|    n_updates                | 138           |
|    policy_gradient_loss     | -0.000535     |
|    std                      | 1.03          |
|    value_loss               | 5.03          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33798817] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 44            |
|    time_elapsed             | 572           |
|    total_timesteps          | 190464        |
| train/                      |               |
|    approx_kl                | 0.0037996077  |
|    approx_ln(kl)            | -5.5728574    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.89         |
|    explained_variance       | 0.00251       |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.412        |
|    ln(policy_gradient_loss) | -6.16         |
|    loss                     | 0.662         |
|    n_updates                | 139           |
|    policy_gradient_loss     | 0.00211       |
|    std                      | 1.03          |
|    value_loss               | 2.96          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40156868] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 45            |
|    time_elapsed             | 585           |
|    total_timesteps          | 192512        |
| train/                      |               |
|    approx_kl                | 0.00840575    |
|    approx_ln(kl)            | -4.778839     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.89         |
|    explained_variance       | -0.574        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.658         |
|    ln(policy_gradient_loss) | -4.64         |
|    loss                     | 1.93          |
|    n_updates                | 140           |
|    policy_gradient_loss     | 0.00965       |
|    std                      | 1.03          |
|    value_loss               | 4             |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.59663916] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 46            |
|    time_elapsed             | 598           |
|    total_timesteps          | 194560        |
| train/                      |               |
|    approx_kl                | 0.0062788837  |
|    approx_ln(kl)            | -5.070563     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.9          |
|    explained_variance       | -1.24         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.17         |
|    ln(policy_gradient_loss) | -5.11         |
|    loss                     | 0.844         |
|    n_updates                | 141           |
|    policy_gradient_loss     | 0.00606       |
|    std                      | 1.04          |
|    value_loss               | 2.51          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42099524] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 47            |
|    time_elapsed             | 611           |
|    total_timesteps          | 196608        |
| train/                      |               |
|    approx_kl                | 0.005747999   |
|    approx_ln(kl)            | -5.1589036    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | -0.11         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.286        |
|    ln(policy_gradient_loss) | -6.16         |
|    loss                     | 0.751         |
|    n_updates                | 142           |
|    policy_gradient_loss     | 0.00211       |
|    std                      | 1.04          |
|    value_loss               | 1.45          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3364765] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 48           |
|    time_elapsed             | 625          |
|    total_timesteps          | 198656       |
| train/                      |              |
|    approx_kl                | 0.009624047  |
|    approx_ln(kl)            | -4.6434903   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | -0.195       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.0607       |
|    ln(policy_gradient_loss) | -6.63        |
|    loss                     | 1.06         |
|    n_updates                | 143          |
|    policy_gradient_loss     | 0.00132      |
|    std                      | 1.04         |
|    value_loss               | 2.27         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3400705] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 49           |
|    time_elapsed             | 638          |
|    total_timesteps          | 200704       |
| train/                      |              |
|    approx_kl                | 0.0071893246 |
|    approx_ln(kl)            | -4.9351583   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.124        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.32         |
|    ln(policy_gradient_loss) | -7.06        |
|    loss                     | 1.38         |
|    n_updates                | 144          |
|    policy_gradient_loss     | 0.00086      |
|    std                      | 1.05         |
|    value_loss               | 2.57         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------
| reward             | [-0.624896] |
| time/              |             |
|    fps             | 162         |
|    iterations      | 1           |
|    time_elapsed    | 12          |
|    total_timesteps | 202752      |
------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44454533] |
| time/                       |               |
|    fps                      | 158           |
|    iterations               | 2             |
|    time_elapsed             | 25            |
|    total_timesteps          | 204800        |
| train/                      |               |
|    approx_kl                | 0.0067202384  |
|    approx_ln(kl)            | -5.0026317    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | -0.118        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0178       |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.982         |
|    n_updates                | 146           |
|    policy_gradient_loss     | -0.00595      |
|    std                      | 1.04          |
|    value_loss               | 2.29          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.51398075] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 3             |
|    time_elapsed             | 39            |
|    total_timesteps          | 206848        |
| train/                      |               |
|    approx_kl                | 0.007158601   |
|    approx_ln(kl)            | -4.9394407    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | -0.389        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.536        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.585         |
|    n_updates                | 147           |
|    policy_gradient_loss     | -0.00355      |
|    std                      | 1.04          |
|    value_loss               | 1.46          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.52498406] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 4             |
|    time_elapsed             | 51            |
|    total_timesteps          | 208896        |
| train/                      |               |
|    approx_kl                | 0.014188861   |
|    approx_ln(kl)            | -4.255298     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.00869       |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.144         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.15          |
|    n_updates                | 148           |
|    policy_gradient_loss     | -0.00605      |
|    std                      | 1.04          |
|    value_loss               | 2.11          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.44088602] |
| time/                       |               |
|    fps                      | 158           |
|    iterations               | 5             |
|    time_elapsed             | 64            |
|    total_timesteps          | 210944        |
| train/                      |               |
|    approx_kl                | 0.012216004   |
|    approx_ln(kl)            | -4.4050083    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.0956        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.684         |
|    ln(policy_gradient_loss) | -4.76         |
|    loss                     | 1.98          |
|    n_updates                | 149           |
|    policy_gradient_loss     | 0.00861       |
|    std                      | 1.04          |
|    value_loss               | 3.7           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.42930833] |
| time/                       |               |
|    fps                      | 158           |
|    iterations               | 6             |
|    time_elapsed             | 77            |
|    total_timesteps          | 212992        |
| train/                      |               |
|    approx_kl                | 0.009362002   |
|    approx_ln(kl)            | -4.6710963    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.156         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.12         |
|    ln(policy_gradient_loss) | -5            |
|    loss                     | 0.326         |
|    n_updates                | 150           |
|    policy_gradient_loss     | 0.00675       |
|    std                      | 1.04          |
|    value_loss               | 0.564         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31053573] |
| time/                       |               |
|    fps                      | 158           |
|    iterations               | 7             |
|    time_elapsed             | 90            |
|    total_timesteps          | 215040        |
| train/                      |               |
|    approx_kl                | 0.006960532   |
|    approx_ln(kl)            | -4.9674993    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | -0.0491       |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.968        |
|    ln(policy_gradient_loss) | -5.66         |
|    loss                     | 0.38          |
|    n_updates                | 151           |
|    policy_gradient_loss     | 0.00348       |
|    std                      | 1.03          |
|    value_loss               | 0.803         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43404773] |
| time/                       |               |
|    fps                      | 158           |
|    iterations               | 8             |
|    time_elapsed             | 103           |
|    total_timesteps          | 217088        |
| train/                      |               |
|    approx_kl                | 0.006947046   |
|    approx_ln(kl)            | -4.9694386    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.9          |
|    explained_variance       | 0.0801        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.018         |
|    ln(policy_gradient_loss) | -5.43         |
|    loss                     | 1.02          |
|    n_updates                | 152           |
|    policy_gradient_loss     | 0.0044        |
|    std                      | 1.03          |
|    value_loss               | 2.22          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44963738] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 9             |
|    time_elapsed             | 116           |
|    total_timesteps          | 219136        |
| train/                      |               |
|    approx_kl                | 0.0071591353  |
|    approx_ln(kl)            | -4.939366     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.9          |
|    explained_variance       | -0.00124      |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0557       |
|    ln(policy_gradient_loss) | -5.52         |
|    loss                     | 0.946         |
|    n_updates                | 153           |
|    policy_gradient_loss     | 0.00402       |
|    std                      | 1.03          |
|    value_loss               | 2.37          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3395758] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 10           |
|    time_elapsed             | 129          |
|    total_timesteps          | 221184       |
| train/                      |              |
|    approx_kl                | 0.00527717   |
|    approx_ln(kl)            | -5.244365    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.91        |
|    explained_variance       | 0.0249       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.105       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.9          |
|    n_updates                | 154          |
|    policy_gradient_loss     | -0.00687     |
|    std                      | 1.04         |
|    value_loss               | 2.05         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.7843341] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 11           |
|    time_elapsed             | 143          |
|    total_timesteps          | 223232       |
| train/                      |              |
|    approx_kl                | 0.013281061  |
|    approx_ln(kl)            | -4.3214164   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.91        |
|    explained_variance       | 0.13         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.382        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.47         |
|    n_updates                | 155          |
|    policy_gradient_loss     | -0.0222      |
|    std                      | 1.04         |
|    value_loss               | 2.96         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5410877] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 12           |
|    time_elapsed             | 156          |
|    total_timesteps          | 225280       |
| train/                      |              |
|    approx_kl                | 0.008607305  |
|    approx_ln(kl)            | -4.755144    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.91        |
|    explained_variance       | -0.0208      |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.28         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 3.59         |
|    n_updates                | 156          |
|    policy_gradient_loss     | -0.00619     |
|    std                      | 1.04         |
|    value_loss               | 8.5          |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.47568628] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 13            |
|    time_elapsed             | 172           |
|    total_timesteps          | 227328        |
| train/                      |               |
|    approx_kl                | 0.010438301   |
|    approx_ln(kl)            | -4.5622735    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.0335        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.677         |
|    ln(policy_gradient_loss) | -4.58         |
|    loss                     | 1.97          |
|    n_updates                | 157           |
|    policy_gradient_loss     | 0.0102        |
|    std                      | 1.04          |
|    value_loss               | 3.31          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48975837] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 14            |
|    time_elapsed             | 189           |
|    total_timesteps          | 229376        |
| train/                      |               |
|    approx_kl                | 0.0060201418  |
|    approx_ln(kl)            | -5.1126447    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.0143        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.995         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 2.71          |
|    n_updates                | 158           |
|    policy_gradient_loss     | -0.00425      |
|    std                      | 1.04          |
|    value_loss               | 4.98          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4999541] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 15           |
|    time_elapsed             | 205          |
|    total_timesteps          | 231424       |
| train/                      |              |
|    approx_kl                | 0.009031309  |
|    approx_ln(kl)            | -4.707058    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | -0.376       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.51         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 4.55         |
|    n_updates                | 159          |
|    policy_gradient_loss     | -0.0019      |
|    std                      | 1.04         |
|    value_loss               | 9.17         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.6403558] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 16           |
|    time_elapsed             | 221          |
|    total_timesteps          | 233472       |
| train/                      |              |
|    approx_kl                | 0.0072491504 |
|    approx_ln(kl)            | -4.926871    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.236        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.334       |
|    ln(policy_gradient_loss) | -5.29        |
|    loss                     | 0.716        |
|    n_updates                | 160          |
|    policy_gradient_loss     | 0.00506      |
|    std                      | 1.04         |
|    value_loss               | 1.17         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.7300962] |
| time/                       |              |
|    fps                      | 146          |
|    iterations               | 17           |
|    time_elapsed             | 237          |
|    total_timesteps          | 235520       |
| train/                      |              |
|    approx_kl                | 0.007532299  |
|    approx_ln(kl)            | -4.888555    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | -0.108       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.289        |
|    ln(policy_gradient_loss) | -5.36        |
|    loss                     | 1.33         |
|    n_updates                | 161          |
|    policy_gradient_loss     | 0.00471      |
|    std                      | 1.04         |
|    value_loss               | 3.37         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41554826] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 18            |
|    time_elapsed             | 253           |
|    total_timesteps          | 237568        |
| train/                      |               |
|    approx_kl                | 0.0057040225  |
|    approx_ln(kl)            | -5.1665835    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.026         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 2.07          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 7.9           |
|    n_updates                | 162           |
|    policy_gradient_loss     | -0.00542      |
|    std                      | 1.05          |
|    value_loss               | 22.9          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.72838676] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 19            |
|    time_elapsed             | 269           |
|    total_timesteps          | 239616        |
| train/                      |               |
|    approx_kl                | 0.0056212177  |
|    approx_ln(kl)            | -5.181207     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.0396        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.642         |
|    ln(policy_gradient_loss) | -7            |
|    loss                     | 1.9           |
|    n_updates                | 163           |
|    policy_gradient_loss     | 0.000913      |
|    std                      | 1.04          |
|    value_loss               | 8.56          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.7707271] |
| time/                       |              |
|    fps                      | 143          |
|    iterations               | 20           |
|    time_elapsed             | 285          |
|    total_timesteps          | 241664       |
| train/                      |              |
|    approx_kl                | 0.009388959  |
|    approx_ln(kl)            | -4.668221    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.0749       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.9          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 6.72         |
|    n_updates                | 164          |
|    policy_gradient_loss     | -0.0166      |
|    std                      | 1.04         |
|    value_loss               | 14.2         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.7060917] |
| time/                       |              |
|    fps                      | 142          |
|    iterations               | 21           |
|    time_elapsed             | 301          |
|    total_timesteps          | 243712       |
| train/                      |              |
|    approx_kl                | 0.008362572  |
|    approx_ln(kl)            | -4.7839894   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | -0.275       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.99         |
|    ln(policy_gradient_loss) | -4.9         |
|    loss                     | 7.32         |
|    n_updates                | 165          |
|    policy_gradient_loss     | 0.00744      |
|    std                      | 1.04         |
|    value_loss               | 15.7         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.0495096] |
| time/                       |              |
|    fps                      | 141          |
|    iterations               | 22           |
|    time_elapsed             | 317          |
|    total_timesteps          | 245760       |
| train/                      |              |
|    approx_kl                | 0.011167655  |
|    approx_ln(kl)            | -4.494734    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | -0.0747      |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.57         |
|    ln(policy_gradient_loss) | -4.57        |
|    loss                     | 13.1         |
|    n_updates                | 166          |
|    policy_gradient_loss     | 0.0104       |
|    std                      | 1.04         |
|    value_loss               | 26           |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.1123481] |
| time/                       |              |
|    fps                      | 141          |
|    iterations               | 23           |
|    time_elapsed             | 333          |
|    total_timesteps          | 247808       |
| train/                      |              |
|    approx_kl                | 0.007196678  |
|    approx_ln(kl)            | -4.934136    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.0301       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.57         |
|    ln(policy_gradient_loss) | -4.71        |
|    loss                     | 13           |
|    n_updates                | 167          |
|    policy_gradient_loss     | 0.00897      |
|    std                      | 1.04         |
|    value_loss               | 34.1         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.0110897] |
| time/                       |              |
|    fps                      | 140          |
|    iterations               | 24           |
|    time_elapsed             | 349          |
|    total_timesteps          | 249856       |
| train/                      |              |
|    approx_kl                | 0.0075285025 |
|    approx_ln(kl)            | -4.889059    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | -0.0255      |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.03         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 7.61         |
|    n_updates                | 168          |
|    policy_gradient_loss     | -0.00158     |
|    std                      | 1.04         |
|    value_loss               | 23.3         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.0584866] |
| time/                       |              |
|    fps                      | 140          |
|    iterations               | 25           |
|    time_elapsed             | 365          |
|    total_timesteps          | 251904       |
| train/                      |              |
|    approx_kl                | 0.006088272  |
|    approx_ln(kl)            | -5.101391    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.342        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.58         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 4.83         |
|    n_updates                | 169          |
|    policy_gradient_loss     | -0.00134     |
|    std                      | 1.05         |
|    value_loss               | 11.4         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3672134] |
| time/                       |              |
|    fps                      | 139          |
|    iterations               | 26           |
|    time_elapsed             | 381          |
|    total_timesteps          | 253952       |
| train/                      |              |
|    approx_kl                | 0.005293045  |
|    approx_ln(kl)            | -5.2413616   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.607        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.01         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 2.75         |
|    n_updates                | 170          |
|    policy_gradient_loss     | -0.000371    |
|    std                      | 1.05         |
|    value_loss               | 9.54         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.829831]  |
| time/                       |              |
|    fps                      | 139          |
|    iterations               | 27           |
|    time_elapsed             | 396          |
|    total_timesteps          | 256000       |
| train/                      |              |
|    approx_kl                | 0.0055471766 |
|    approx_ln(kl)            | -5.194466    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.744        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.73         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 2.08         |
|    n_updates                | 171          |
|    policy_gradient_loss     | -0.00193     |
|    std                      | 1.05         |
|    value_loss               | 7.28         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.7041929] |
| time/                       |              |
|    fps                      | 139          |
|    iterations               | 28           |
|    time_elapsed             | 412          |
|    total_timesteps          | 258048       |
| train/                      |              |
|    approx_kl                | 0.0054788874 |
|    approx_ln(kl)            | -5.2068534   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.876        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.17         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 3.22         |
|    n_updates                | 172          |
|    policy_gradient_loss     | -0.0025      |
|    std                      | 1.05         |
|    value_loss               | 9.79         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.641232] |
| time/                       |             |
|    fps                      | 138         |
|    iterations               | 29          |
|    time_elapsed             | 427         |
|    total_timesteps          | 260096      |
| train/                      |             |
|    approx_kl                | 0.007275838 |
|    approx_ln(kl)            | -4.9231963  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.93       |
|    explained_variance       | 0.814       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 0.851       |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 2.34        |
|    n_updates                | 173         |
|    policy_gradient_loss     | -0.00445    |
|    std                      | 1.05        |
|    value_loss               | 6.84        |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.9242908] |
| time/                       |              |
|    fps                      | 138          |
|    iterations               | 30           |
|    time_elapsed             | 443          |
|    total_timesteps          | 262144       |
| train/                      |              |
|    approx_kl                | 0.0067524733 |
|    approx_ln(kl)            | -4.9978466   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.935        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.0462      |
|    ln(policy_gradient_loss) | -6.16        |
|    loss                     | 0.955        |
|    n_updates                | 174          |
|    policy_gradient_loss     | 0.00212      |
|    std                      | 1.04         |
|    value_loss               | 2.13         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.92467976] |
| time/                       |               |
|    fps                      | 136           |
|    iterations               | 31            |
|    time_elapsed             | 463           |
|    total_timesteps          | 264192        |
| train/                      |               |
|    approx_kl                | 0.0067567714  |
|    approx_ln(kl)            | -4.99721      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.949         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.187         |
|    ln(policy_gradient_loss) | -5.63         |
|    loss                     | 1.21          |
|    n_updates                | 175           |
|    policy_gradient_loss     | 0.00359       |
|    std                      | 1.04          |
|    value_loss               | 3.36          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.8544876] |
| time/                       |              |
|    fps                      | 135          |
|    iterations               | 32           |
|    time_elapsed             | 482          |
|    total_timesteps          | 266240       |
| train/                      |              |
|    approx_kl                | 0.005901908  |
|    approx_ln(kl)            | -5.1324797   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.941        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.404        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.5          |
|    n_updates                | 176          |
|    policy_gradient_loss     | -0.00347     |
|    std                      | 1.04         |
|    value_loss               | 4.04         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.92483205] |
| time/                       |               |
|    fps                      | 136           |
|    iterations               | 33            |
|    time_elapsed             | 495           |
|    total_timesteps          | 268288        |
| train/                      |               |
|    approx_kl                | 0.013466972   |
|    approx_ln(kl)            | -4.307515     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.39         |
|    ln(policy_gradient_loss) | -5.14         |
|    loss                     | 0.677         |
|    n_updates                | 177           |
|    policy_gradient_loss     | 0.00586       |
|    std                      | 1.04          |
|    value_loss               | 1.54          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.9992026] |
| time/                       |              |
|    fps                      | 136          |
|    iterations               | 34           |
|    time_elapsed             | 508          |
|    total_timesteps          | 270336       |
| train/                      |              |
|    approx_kl                | 0.016207686  |
|    approx_ln(kl)            | -4.1222696   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.971        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.704        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 2.02         |
|    n_updates                | 178          |
|    policy_gradient_loss     | -0.0171      |
|    std                      | 1.04         |
|    value_loss               | 5            |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.2332711] |
| time/                       |              |
|    fps                      | 137          |
|    iterations               | 35           |
|    time_elapsed             | 521          |
|    total_timesteps          | 272384       |
| train/                      |              |
|    approx_kl                | 0.009068037  |
|    approx_ln(kl)            | -4.7029996   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.974        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.55         |
|    ln(policy_gradient_loss) | -5.94        |
|    loss                     | 1.73         |
|    n_updates                | 179          |
|    policy_gradient_loss     | 0.00264      |
|    std                      | 1.04         |
|    value_loss               | 4.06         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.52830625] |
| time/                       |               |
|    fps                      | 137           |
|    iterations               | 36            |
|    time_elapsed             | 534           |
|    total_timesteps          | 274432        |
| train/                      |               |
|    approx_kl                | 0.0073076435  |
|    approx_ln(kl)            | -4.918834     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.967         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.79          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 5.97          |
|    n_updates                | 180           |
|    policy_gradient_loss     | -5.23e-05     |
|    std                      | 1.04          |
|    value_loss               | 12.1          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42510995] |
| time/                       |               |
|    fps                      | 138           |
|    iterations               | 37            |
|    time_elapsed             | 547           |
|    total_timesteps          | 276480        |
| train/                      |               |
|    approx_kl                | 0.009204547   |
|    approx_ln(kl)            | -4.688058     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.959         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.61          |
|    ln(policy_gradient_loss) | -4.39         |
|    loss                     | 5.02          |
|    n_updates                | 181           |
|    policy_gradient_loss     | 0.0124        |
|    std                      | 1.04          |
|    value_loss               | 11.2          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5859436] |
| time/                       |              |
|    fps                      | 138          |
|    iterations               | 38           |
|    time_elapsed             | 561          |
|    total_timesteps          | 278528       |
| train/                      |              |
|    approx_kl                | 0.005169108  |
|    approx_ln(kl)            | -5.265055    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.931        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.21         |
|    ln(policy_gradient_loss) | -4.92        |
|    loss                     | 9.15         |
|    n_updates                | 182          |
|    policy_gradient_loss     | 0.00731      |
|    std                      | 1.04         |
|    value_loss               | 15           |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.92082435] |
| time/                       |               |
|    fps                      | 138           |
|    iterations               | 39            |
|    time_elapsed             | 575           |
|    total_timesteps          | 280576        |
| train/                      |               |
|    approx_kl                | 0.009901402   |
|    approx_ln(kl)            | -4.615079     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.892         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.52          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 4.58          |
|    n_updates                | 183           |
|    policy_gradient_loss     | -0.0102       |
|    std                      | 1.04          |
|    value_loss               | 11.3          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.8097922] |
| time/                       |              |
|    fps                      | 139          |
|    iterations               | 40           |
|    time_elapsed             | 588          |
|    total_timesteps          | 282624       |
| train/                      |              |
|    approx_kl                | 0.012828712  |
|    approx_ln(kl)            | -4.3560696   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.923        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.09         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 2.98         |
|    n_updates                | 184          |
|    policy_gradient_loss     | -0.0139      |
|    std                      | 1.04         |
|    value_loss               | 6.4          |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.2181627] |
| time/                       |              |
|    fps                      | 139          |
|    iterations               | 41           |
|    time_elapsed             | 601          |
|    total_timesteps          | 284672       |
| train/                      |              |
|    approx_kl                | 0.011244695  |
|    approx_ln(kl)            | -4.487859    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.823        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.17         |
|    ln(policy_gradient_loss) | -4.18        |
|    loss                     | 8.8          |
|    n_updates                | 185          |
|    policy_gradient_loss     | 0.0153       |
|    std                      | 1.04         |
|    value_loss               | 20.3         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-1.1664672] |
| time/                       |              |
|    fps                      | 139          |
|    iterations               | 42           |
|    time_elapsed             | 615          |
|    total_timesteps          | 286720       |
| train/                      |              |
|    approx_kl                | 0.012093897  |
|    approx_ln(kl)            | -4.4150543   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.855        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.76         |
|    ln(policy_gradient_loss) | -4.61        |
|    loss                     | 15.8         |
|    n_updates                | 186          |
|    policy_gradient_loss     | 0.00997      |
|    std                      | 1.04         |
|    value_loss               | 28.1         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.3462738] |
| time/                       |              |
|    fps                      | 140          |
|    iterations               | 43           |
|    time_elapsed             | 628          |
|    total_timesteps          | 288768       |
| train/                      |              |
|    approx_kl                | 0.0076601855 |
|    approx_ln(kl)            | -4.871719    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.848        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.76         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 15.7         |
|    n_updates                | 187          |
|    policy_gradient_loss     | -0.0111      |
|    std                      | 1.04         |
|    value_loss               | 30.9         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.5201616] |
| time/                       |              |
|    fps                      | 140          |
|    iterations               | 44           |
|    time_elapsed             | 640          |
|    total_timesteps          | 290816       |
| train/                      |              |
|    approx_kl                | 0.0077642    |
|    approx_ln(kl)            | -4.858232    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.764        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.02         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 20.5         |
|    n_updates                | 188          |
|    policy_gradient_loss     | -0.00239     |
|    std                      | 1.04         |
|    value_loss               | 43.9         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.5498161] |
| time/                       |              |
|    fps                      | 140          |
|    iterations               | 45           |
|    time_elapsed             | 653          |
|    total_timesteps          | 292864       |
| train/                      |              |
|    approx_kl                | 0.006306979  |
|    approx_ln(kl)            | -5.0660987   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.755        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.98         |
|    ln(policy_gradient_loss) | -7.61        |
|    loss                     | 19.6         |
|    n_updates                | 189          |
|    policy_gradient_loss     | 0.000497     |
|    std                      | 1.04         |
|    value_loss               | 41.4         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.52736443] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 46            |
|    time_elapsed             | 666           |
|    total_timesteps          | 294912        |
| train/                      |               |
|    approx_kl                | 0.008532969   |
|    approx_ln(kl)            | -4.763818     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.93         |
|    explained_variance       | 0.685         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 3             |
|    ln(policy_gradient_loss) | -5.96         |
|    loss                     | 20.1          |
|    n_updates                | 190           |
|    policy_gradient_loss     | 0.00258       |
|    std                      | 1.04          |
|    value_loss               | 41.8          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.6590867] |
| time/                       |              |
|    fps                      | 141          |
|    iterations               | 47           |
|    time_elapsed             | 679          |
|    total_timesteps          | 296960       |
| train/                      |              |
|    approx_kl                | 0.0063337116 |
|    approx_ln(kl)            | -5.0618687   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.656        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.3          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 27           |
|    n_updates                | 191          |
|    policy_gradient_loss     | -0.00123     |
|    std                      | 1.04         |
|    value_loss               | 56.9         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.9219293] |
| time/                       |              |
|    fps                      | 141          |
|    iterations               | 48           |
|    time_elapsed             | 692          |
|    total_timesteps          | 299008       |
| train/                      |              |
|    approx_kl                | 0.008548525  |
|    approx_ln(kl)            | -4.7619967   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.669        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.97         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 19.6         |
|    n_updates                | 192          |
|    policy_gradient_loss     | -0.0094      |
|    std                      | 1.04         |
|    value_loss               | 40.5         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-1.0696313] |
| time/                       |              |
|    fps                      | 142          |
|    iterations               | 49           |
|    time_elapsed             | 705          |
|    total_timesteps          | 301056       |
| train/                      |              |
|    approx_kl                | 0.009061882  |
|    approx_ln(kl)            | -4.7036786   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.591        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.56         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 35.2         |
|    n_updates                | 193          |
|    policy_gradient_loss     | -0.00694     |
|    std                      | 1.04         |
|    value_loss               | 61.2         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------
| reward             | [-1.2259686] |
| time/              |              |
|    fps             | 161          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 303104       |
-------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-1.2920866] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 2            |
|    time_elapsed             | 26           |
|    total_timesteps          | 305152       |
| train/                      |              |
|    approx_kl                | 0.008125004  |
|    approx_ln(kl)            | -4.812809    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.545        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.47         |
|    ln(policy_gradient_loss) | -5.18        |
|    loss                     | 32.1         |
|    n_updates                | 195          |
|    policy_gradient_loss     | 0.00564      |
|    std                      | 1.04         |
|    value_loss               | 63.5         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-1.6100782] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 3            |
|    time_elapsed             | 39           |
|    total_timesteps          | 307200       |
| train/                      |              |
|    approx_kl                | 0.00728991   |
|    approx_ln(kl)            | -4.921264    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.573        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.4          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 30.1         |
|    n_updates                | 196          |
|    policy_gradient_loss     | -0.000896    |
|    std                      | 1.04         |
|    value_loss               | 61.1         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.3770064] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 4            |
|    time_elapsed             | 52           |
|    total_timesteps          | 309248       |
| train/                      |              |
|    approx_kl                | 0.0073598404 |
|    approx_ln(kl)            | -4.911717    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.594        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.38         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 29.4         |
|    n_updates                | 197          |
|    policy_gradient_loss     | -0.00658     |
|    std                      | 1.04         |
|    value_loss               | 68.8         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-1.5736032] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 5            |
|    time_elapsed             | 65           |
|    total_timesteps          | 311296       |
| train/                      |              |
|    approx_kl                | 0.010831326  |
|    approx_ln(kl)            | -4.525313    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.69         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.42         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 30.7         |
|    n_updates                | 198          |
|    policy_gradient_loss     | -0.0139      |
|    std                      | 1.04         |
|    value_loss               | 53           |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-1.6058207] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 6            |
|    time_elapsed             | 79           |
|    total_timesteps          | 313344       |
| train/                      |              |
|    approx_kl                | 0.013115994  |
|    approx_ln(kl)            | -4.333923    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.671        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.33         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 27.9         |
|    n_updates                | 199          |
|    policy_gradient_loss     | -0.0131      |
|    std                      | 1.04         |
|    value_loss               | 54.6         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-1.5994211] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 7            |
|    time_elapsed             | 92           |
|    total_timesteps          | 315392       |
| train/                      |              |
|    approx_kl                | 0.010407803  |
|    approx_ln(kl)            | -4.5651994   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.699        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 3.34         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 28.1         |
|    n_updates                | 200          |
|    policy_gradient_loss     | -0.00891     |
|    std                      | 1.04         |
|    value_loss               | 47.3         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5512665] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 8            |
|    time_elapsed             | 106          |
|    total_timesteps          | 317440       |
| train/                      |              |
|    approx_kl                | 0.007454225  |
|    approx_ln(kl)            | -4.8989744   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.759        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.85         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 17.3         |
|    n_updates                | 201          |
|    policy_gradient_loss     | -0.00187     |
|    std                      | 1.04         |
|    value_loss               | 37.9         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.7165858] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 9            |
|    time_elapsed             | 118          |
|    total_timesteps          | 319488       |
| train/                      |              |
|    approx_kl                | 0.006582847  |
|    approx_ln(kl)            | -5.023288    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.733        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.92         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 18.6         |
|    n_updates                | 202          |
|    policy_gradient_loss     | -0.00129     |
|    std                      | 1.04         |
|    value_loss               | 38.2         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.9334053] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 10           |
|    time_elapsed             | 131          |
|    total_timesteps          | 321536       |
| train/                      |              |
|    approx_kl                | 0.007172114  |
|    approx_ln(kl)            | -4.937555    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.773        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.41         |
|    ln(policy_gradient_loss) | -6.92        |
|    loss                     | 11.2         |
|    n_updates                | 203          |
|    policy_gradient_loss     | 0.000986     |
|    std                      | 1.04         |
|    value_loss               | 21.7         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.0215173] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 11           |
|    time_elapsed             | 144          |
|    total_timesteps          | 323584       |
| train/                      |              |
|    approx_kl                | 0.008692038  |
|    approx_ln(kl)            | -4.745348    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.786        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.21         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 9.14         |
|    n_updates                | 204          |
|    policy_gradient_loss     | -0.00122     |
|    std                      | 1.04         |
|    value_loss               | 19.9         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.1419288] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 12           |
|    time_elapsed             | 157          |
|    total_timesteps          | 325632       |
| train/                      |              |
|    approx_kl                | 0.005356131  |
|    approx_ln(kl)            | -5.229513    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.803        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.11         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 8.24         |
|    n_updates                | 205          |
|    policy_gradient_loss     | -0.00607     |
|    std                      | 1.04         |
|    value_loss               | 19.3         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.2915952] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 13           |
|    time_elapsed             | 170          |
|    total_timesteps          | 327680       |
| train/                      |              |
|    approx_kl                | 0.007716923  |
|    approx_ln(kl)            | -4.8643394   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.807        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.44         |
|    ln(policy_gradient_loss) | -5.03        |
|    loss                     | 11.5         |
|    n_updates                | 206          |
|    policy_gradient_loss     | 0.00652      |
|    std                      | 1.04         |
|    value_loss               | 20.6         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.3854082] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 14           |
|    time_elapsed             | 183          |
|    total_timesteps          | 329728       |
| train/                      |              |
|    approx_kl                | 0.005929294  |
|    approx_ln(kl)            | -5.12785     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.82         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.08         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 8.01         |
|    n_updates                | 207          |
|    policy_gradient_loss     | -0.0078      |
|    std                      | 1.05         |
|    value_loss               | 19.3         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-1.1795745] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 15           |
|    time_elapsed             | 196          |
|    total_timesteps          | 331776       |
| train/                      |              |
|    approx_kl                | 0.006181442  |
|    approx_ln(kl)            | -5.0862036   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.816        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 2.48         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 12           |
|    n_updates                | 208          |
|    policy_gradient_loss     | -0.00544     |
|    std                      | 1.05         |
|    value_loss               | 26.3         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-1.4662278] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 16           |
|    time_elapsed             | 209          |
|    total_timesteps          | 333824       |
| train/                      |              |
|    approx_kl                | 0.007786613  |
|    approx_ln(kl)            | -4.8553495   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.873        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.91         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 6.76         |
|    n_updates                | 209          |
|    policy_gradient_loss     | -0.00116     |
|    std                      | 1.05         |
|    value_loss               | 13.5         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-1.4630154] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 17           |
|    time_elapsed             | 222          |
|    total_timesteps          | 335872       |
| train/                      |              |
|    approx_kl                | 0.014314211  |
|    approx_ln(kl)            | -4.2465024   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.886        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.92         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 6.85         |
|    n_updates                | 210          |
|    policy_gradient_loss     | -0.0004      |
|    std                      | 1.05         |
|    value_loss               | 16.5         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48371565] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 18            |
|    time_elapsed             | 235           |
|    total_timesteps          | 337920        |
| train/                      |               |
|    approx_kl                | 0.006926909   |
|    approx_ln(kl)            | -4.9723415    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.93         |
|    explained_variance       | 0.907         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.91          |
|    ln(policy_gradient_loss) | -6.51         |
|    loss                     | 6.75          |
|    n_updates                | 211           |
|    policy_gradient_loss     | 0.00149       |
|    std                      | 1.05          |
|    value_loss               | 13.5          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45117638] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 19            |
|    time_elapsed             | 248           |
|    total_timesteps          | 339968        |
| train/                      |               |
|    approx_kl                | 0.0076569137  |
|    approx_ln(kl)            | -4.872146     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.93         |
|    explained_variance       | 0.935         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.44          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 4.23          |
|    n_updates                | 212           |
|    policy_gradient_loss     | -0.000228     |
|    std                      | 1.05          |
|    value_loss               | 8.29          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.89880365] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 20            |
|    time_elapsed             | 261           |
|    total_timesteps          | 342016        |
| train/                      |               |
|    approx_kl                | 0.0070616915  |
|    approx_ln(kl)            | -4.9530706    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.93         |
|    explained_variance       | 0.946         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.3           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 3.65          |
|    n_updates                | 213           |
|    policy_gradient_loss     | -0.00725      |
|    std                      | 1.05          |
|    value_loss               | 7.83          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.92537624] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 21            |
|    time_elapsed             | 274           |
|    total_timesteps          | 344064        |
| train/                      |               |
|    approx_kl                | 0.0045277365  |
|    approx_ln(kl)            | -5.397533     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.93         |
|    explained_variance       | 0.952         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.275         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.32          |
|    n_updates                | 214           |
|    policy_gradient_loss     | -0.00128      |
|    std                      | 1.05          |
|    value_loss               | 3.61          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.77407974] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 22            |
|    time_elapsed             | 288           |
|    total_timesteps          | 346112        |
| train/                      |               |
|    approx_kl                | 0.0058223205  |
|    approx_ln(kl)            | -5.146056     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.93         |
|    explained_variance       | 0.957         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.756         |
|    ln(policy_gradient_loss) | -5.3          |
|    loss                     | 2.13          |
|    n_updates                | 215           |
|    policy_gradient_loss     | 0.005         |
|    std                      | 1.05          |
|    value_loss               | 4.83          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-1.0914333] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 23           |
|    time_elapsed             | 300          |
|    total_timesteps          | 348160       |
| train/                      |              |
|    approx_kl                | 0.009137288  |
|    approx_ln(kl)            | -4.6953917   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.97         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.166       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.847        |
|    n_updates                | 216          |
|    policy_gradient_loss     | -0.00734     |
|    std                      | 1.05         |
|    value_loss               | 2.71         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-1.1438037] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 24           |
|    time_elapsed             | 313          |
|    total_timesteps          | 350208       |
| train/                      |              |
|    approx_kl                | 0.0077030906 |
|    approx_ln(kl)            | -4.8661337   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.956        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.857        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 2.36         |
|    n_updates                | 217          |
|    policy_gradient_loss     | -0.00346     |
|    std                      | 1.05         |
|    value_loss               | 5.19         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.0182326] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 25           |
|    time_elapsed             | 326          |
|    total_timesteps          | 352256       |
| train/                      |              |
|    approx_kl                | 0.0066793584 |
|    approx_ln(kl)            | -5.0087333   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | 0.969        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.278        |
|    ln(policy_gradient_loss) | -5.84        |
|    loss                     | 1.32         |
|    n_updates                | 218          |
|    policy_gradient_loss     | 0.0029       |
|    std                      | 1.05         |
|    value_loss               | 2.42         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.05
----------------------------------------------
| reward                      | [-0.8190917] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 26           |
|    time_elapsed             | 339          |
|    total_timesteps          | 354304       |
| train/                      |              |
|    approx_kl                | 0.022688745  |
|    approx_ln(kl)            | -3.7858863   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | 0.983        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.253        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.29         |
|    n_updates                | 219          |
|    policy_gradient_loss     | -0.0164      |
|    std                      | 1.05         |
|    value_loss               | 2.49         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-1.0709597] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 27           |
|    time_elapsed             | 353          |
|    total_timesteps          | 356352       |
| train/                      |              |
|    approx_kl                | 0.0066786204 |
|    approx_ln(kl)            | -5.008844    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | 0.979        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.144        |
|    ln(policy_gradient_loss) | -6.72        |
|    loss                     | 1.15         |
|    n_updates                | 220          |
|    policy_gradient_loss     | 0.0012       |
|    std                      | 1.05         |
|    value_loss               | 2.27         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.7805719] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 28           |
|    time_elapsed             | 366          |
|    total_timesteps          | 358400       |
| train/                      |              |
|    approx_kl                | 0.008134915  |
|    approx_ln(kl)            | -4.81159     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | 0.988        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.746       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.474        |
|    n_updates                | 221          |
|    policy_gradient_loss     | -0.0046      |
|    std                      | 1.05         |
|    value_loss               | 1.08         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.65028167] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 29            |
|    time_elapsed             | 379           |
|    total_timesteps          | 360448        |
| train/                      |               |
|    approx_kl                | 0.008476945   |
|    approx_ln(kl)            | -4.7704053    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.94         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0547       |
|    ln(policy_gradient_loss) | -7            |
|    loss                     | 0.947         |
|    n_updates                | 222           |
|    policy_gradient_loss     | 0.000909      |
|    std                      | 1.05          |
|    value_loss               | 3.02          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.71010315] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 30            |
|    time_elapsed             | 393           |
|    total_timesteps          | 362496        |
| train/                      |               |
|    approx_kl                | 0.007786367   |
|    approx_ln(kl)            | -4.855381     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.94         |
|    explained_variance       | 0.975         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.169         |
|    ln(policy_gradient_loss) | -5.23         |
|    loss                     | 1.18          |
|    n_updates                | 223           |
|    policy_gradient_loss     | 0.00538       |
|    std                      | 1.05          |
|    value_loss               | 2.69          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.6276323] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 31           |
|    time_elapsed             | 409          |
|    total_timesteps          | 364544       |
| train/                      |              |
|    approx_kl                | 0.0042205565 |
|    approx_ln(kl)            | -5.467788    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | 0.98         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.422       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.656        |
|    n_updates                | 224          |
|    policy_gradient_loss     | -3.49e-05    |
|    std                      | 1.06         |
|    value_loss               | 1.87         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.6779573] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 32           |
|    time_elapsed             | 428          |
|    total_timesteps          | 366592       |
| train/                      |              |
|    approx_kl                | 0.006697835  |
|    approx_ln(kl)            | -5.005971    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | 0.988        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.03        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.357        |
|    n_updates                | 225          |
|    policy_gradient_loss     | -0.00061     |
|    std                      | 1.05         |
|    value_loss               | 1.02         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.61695904] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 33            |
|    time_elapsed             | 445           |
|    total_timesteps          | 368640        |
| train/                      |               |
|    approx_kl                | 0.008486977   |
|    approx_ln(kl)            | -4.7692223    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.94         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.18         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.836         |
|    n_updates                | 226           |
|    policy_gradient_loss     | -0.00449      |
|    std                      | 1.05          |
|    value_loss               | 1.35          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5617689] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 34           |
|    time_elapsed             | 465          |
|    total_timesteps          | 370688       |
| train/                      |              |
|    approx_kl                | 0.006810571  |
|    approx_ln(kl)            | -4.9892793   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | 0.983        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.719       |
|    ln(policy_gradient_loss) | -5.67        |
|    loss                     | 0.487        |
|    n_updates                | 227          |
|    policy_gradient_loss     | 0.00344      |
|    std                      | 1.05         |
|    value_loss               | 1.62         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.63108146] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 35            |
|    time_elapsed             | 478           |
|    total_timesteps          | 372736        |
| train/                      |               |
|    approx_kl                | 0.006334324   |
|    approx_ln(kl)            | -5.0617723    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.479        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.619         |
|    n_updates                | 228           |
|    policy_gradient_loss     | -0.00574      |
|    std                      | 1.06          |
|    value_loss               | 2.7           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.88899416] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 36            |
|    time_elapsed             | 496           |
|    total_timesteps          | 374784        |
| train/                      |               |
|    approx_kl                | 0.0074886223  |
|    approx_ln(kl)            | -4.8943706    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | 0.977         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.675        |
|    ln(policy_gradient_loss) | -6.87         |
|    loss                     | 0.509         |
|    n_updates                | 229           |
|    policy_gradient_loss     | 0.00104       |
|    std                      | 1.06          |
|    value_loss               | 1.87          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.57562846] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 37            |
|    time_elapsed             | 509           |
|    total_timesteps          | 376832        |
| train/                      |               |
|    approx_kl                | 0.008464608   |
|    approx_ln(kl)            | -4.7718616    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | 0.974         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.633        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.531         |
|    n_updates                | 230           |
|    policy_gradient_loss     | -0.000604     |
|    std                      | 1.06          |
|    value_loss               | 2.67          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.599498] |
| time/                       |             |
|    fps                      | 149         |
|    iterations               | 38          |
|    time_elapsed             | 521         |
|    total_timesteps          | 378880      |
| train/                      |             |
|    approx_kl                | 0.006844376 |
|    approx_ln(kl)            | -4.984328   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.95       |
|    explained_variance       | 0.979       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 0.094       |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 1.1         |
|    n_updates                | 231         |
|    policy_gradient_loss     | -0.00985    |
|    std                      | 1.06        |
|    value_loss               | 3.02        |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.54135734] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 39            |
|    time_elapsed             | 534           |
|    total_timesteps          | 380928        |
| train/                      |               |
|    approx_kl                | 0.0063361498  |
|    approx_ln(kl)            | -5.061484     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | 0.932         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.329         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.39          |
|    n_updates                | 232           |
|    policy_gradient_loss     | -0.00117      |
|    std                      | 1.06          |
|    value_loss               | 3.16          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.53342575] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 40            |
|    time_elapsed             | 547           |
|    total_timesteps          | 382976        |
| train/                      |               |
|    approx_kl                | 0.0059788297  |
|    approx_ln(kl)            | -5.1195307    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | 0.956         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.259         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.3           |
|    n_updates                | 233           |
|    policy_gradient_loss     | -0.00268      |
|    std                      | 1.06          |
|    value_loss               | 3.36          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5787309] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 41           |
|    time_elapsed             | 560          |
|    total_timesteps          | 385024       |
| train/                      |              |
|    approx_kl                | 0.0069501568 |
|    approx_ln(kl)            | -4.9689913   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | 0.96         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.0087      |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.991        |
|    n_updates                | 234          |
|    policy_gradient_loss     | -2.83e-07    |
|    std                      | 1.06         |
|    value_loss               | 1.94         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.62531894] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 42            |
|    time_elapsed             | 573           |
|    total_timesteps          | 387072        |
| train/                      |               |
|    approx_kl                | 0.006464489   |
|    approx_ln(kl)            | -5.0414314    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | 0.932         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.255         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.29          |
|    n_updates                | 235           |
|    policy_gradient_loss     | -0.00184      |
|    std                      | 1.06          |
|    value_loss               | 2.97          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.7033236] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 43           |
|    time_elapsed             | 586          |
|    total_timesteps          | 389120       |
| train/                      |              |
|    approx_kl                | 0.0066726045 |
|    approx_ln(kl)            | -5.009745    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | 0.917        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.14        |
|    ln(policy_gradient_loss) | -5.97        |
|    loss                     | 0.319        |
|    n_updates                | 236          |
|    policy_gradient_loss     | 0.00256      |
|    std                      | 1.06         |
|    value_loss               | 1.25         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.6962089] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 44           |
|    time_elapsed             | 599          |
|    total_timesteps          | 391168       |
| train/                      |              |
|    approx_kl                | 0.0058385134 |
|    approx_ln(kl)            | -5.143279    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | 0.95         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.25        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.287        |
|    n_updates                | 237          |
|    policy_gradient_loss     | -0.00509     |
|    std                      | 1.05         |
|    value_loss               | 0.806        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5560683] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 45           |
|    time_elapsed             | 611          |
|    total_timesteps          | 393216       |
| train/                      |              |
|    approx_kl                | 0.0053255586 |
|    approx_ln(kl)            | -5.2352376   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | 0.899        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.65         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.92         |
|    n_updates                | 238          |
|    policy_gradient_loss     | -0.00585     |
|    std                      | 1.05         |
|    value_loss               | 4.39         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.6232914] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 46           |
|    time_elapsed             | 624          |
|    total_timesteps          | 395264       |
| train/                      |              |
|    approx_kl                | 0.0049729072 |
|    approx_ln(kl)            | -5.3037505   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | 0.91         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.0658       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.07         |
|    n_updates                | 239          |
|    policy_gradient_loss     | -0.00284     |
|    std                      | 1.05         |
|    value_loss               | 3.06         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.46110332] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 47            |
|    time_elapsed             | 638           |
|    total_timesteps          | 397312        |
| train/                      |               |
|    approx_kl                | 0.0042410404  |
|    approx_ln(kl)            | -5.462947     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.94         |
|    explained_variance       | 0.813         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.553        |
|    ln(policy_gradient_loss) | -8.62         |
|    loss                     | 0.575         |
|    n_updates                | 240           |
|    policy_gradient_loss     | 0.00018       |
|    std                      | 1.05          |
|    value_loss               | 1.98          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38124648] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 48            |
|    time_elapsed             | 653           |
|    total_timesteps          | 399360        |
| train/                      |               |
|    approx_kl                | 0.0058347015  |
|    approx_ln(kl)            | -5.1439323    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.94         |
|    explained_variance       | 0.758         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.181         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.2           |
|    n_updates                | 241           |
|    policy_gradient_loss     | -0.000268     |
|    std                      | 1.05          |
|    value_loss               | 2.93          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3765839] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 49           |
|    time_elapsed             | 669          |
|    total_timesteps          | 401408       |
| train/                      |              |
|    approx_kl                | 0.0069296793 |
|    approx_ln(kl)            | -4.971942    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | 0.652        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.955        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 2.6          |
|    n_updates                | 242          |
|    policy_gradient_loss     | -0.00226     |
|    std                      | 1.05         |
|    value_loss               | 6.29         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------
| reward             | [-0.5423251] |
| time/              |              |
|    fps             | 122          |
|    iterations      | 1            |
|    time_elapsed    | 16           |
|    total_timesteps | 403456       |
-------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.59620404] |
| time/                       |               |
|    fps                      | 122           |
|    iterations               | 2             |
|    time_elapsed             | 33            |
|    total_timesteps          | 405504        |
| train/                      |               |
|    approx_kl                | 0.007613923   |
|    approx_ln(kl)            | -4.8777766    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | 0.668         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.321        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.725         |
|    n_updates                | 244           |
|    policy_gradient_loss     | -0.0111       |
|    std                      | 1.06          |
|    value_loss               | 2.27          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.6803397] |
| time/                       |              |
|    fps                      | 123          |
|    iterations               | 3            |
|    time_elapsed             | 49           |
|    total_timesteps          | 407552       |
| train/                      |              |
|    approx_kl                | 0.0069286744 |
|    approx_ln(kl)            | -4.972087    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | 0.287        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.227        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.26         |
|    n_updates                | 245          |
|    policy_gradient_loss     | -0.00897     |
|    std                      | 1.06         |
|    value_loss               | 3.07         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.50490713] |
| time/                       |               |
|    fps                      | 125           |
|    iterations               | 4             |
|    time_elapsed             | 65            |
|    total_timesteps          | 409600        |
| train/                      |               |
|    approx_kl                | 0.005853764   |
|    approx_ln(kl)            | -5.1406703    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | 0.796         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.3          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.273         |
|    n_updates                | 246           |
|    policy_gradient_loss     | -0.00352      |
|    std                      | 1.06          |
|    value_loss               | 0.727         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35384852] |
| time/                       |               |
|    fps                      | 127           |
|    iterations               | 5             |
|    time_elapsed             | 80            |
|    total_timesteps          | 411648        |
| train/                      |               |
|    approx_kl                | 0.0074963444  |
|    approx_ln(kl)            | -4.8933396    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | 0.927         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.1          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.334         |
|    n_updates                | 247           |
|    policy_gradient_loss     | -0.0146       |
|    std                      | 1.06          |
|    value_loss               | 0.745         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.71255827] |
| time/                       |               |
|    fps                      | 128           |
|    iterations               | 6             |
|    time_elapsed             | 95            |
|    total_timesteps          | 413696        |
| train/                      |               |
|    approx_kl                | 0.005370151   |
|    approx_ln(kl)            | -5.226899     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | 0.811         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.475         |
|    ln(policy_gradient_loss) | -6.67         |
|    loss                     | 1.61          |
|    n_updates                | 248           |
|    policy_gradient_loss     | 0.00126       |
|    std                      | 1.06          |
|    value_loss               | 4.74          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.6832552] |
| time/                       |              |
|    fps                      | 129          |
|    iterations               | 7            |
|    time_elapsed             | 110          |
|    total_timesteps          | 415744       |
| train/                      |              |
|    approx_kl                | 0.006291722  |
|    approx_ln(kl)            | -5.0685205   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | 0.887        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.14         |
|    ln(policy_gradient_loss) | -6.56        |
|    loss                     | 1.15         |
|    n_updates                | 249          |
|    policy_gradient_loss     | 0.00142      |
|    std                      | 1.05         |
|    value_loss               | 3.04         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.69690645] |
| time/                       |               |
|    fps                      | 127           |
|    iterations               | 8             |
|    time_elapsed             | 128           |
|    total_timesteps          | 417792        |
| train/                      |               |
|    approx_kl                | 0.005221388   |
|    approx_ln(kl)            | -5.254992     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.94         |
|    explained_variance       | 0.935         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.518        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.596         |
|    n_updates                | 250           |
|    policy_gradient_loss     | -0.000342     |
|    std                      | 1.05          |
|    value_loss               | 1.43          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.73194003] |
| time/                       |               |
|    fps                      | 126           |
|    iterations               | 9             |
|    time_elapsed             | 146           |
|    total_timesteps          | 419840        |
| train/                      |               |
|    approx_kl                | 0.0067122243  |
|    approx_ln(kl)            | -5.0038247    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.94         |
|    explained_variance       | 0.964         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.816        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.442         |
|    n_updates                | 251           |
|    policy_gradient_loss     | -3.46e-06     |
|    std                      | 1.05          |
|    value_loss               | 0.858         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5139159] |
| time/                       |              |
|    fps                      | 128          |
|    iterations               | 10           |
|    time_elapsed             | 158          |
|    total_timesteps          | 421888       |
| train/                      |              |
|    approx_kl                | 0.007821417  |
|    approx_ln(kl)            | -4.8508897   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | 0.982        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.54        |
|    ln(policy_gradient_loss) | -6.12        |
|    loss                     | 0.215        |
|    n_updates                | 252          |
|    policy_gradient_loss     | 0.0022       |
|    std                      | 1.06         |
|    value_loss               | 0.625        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.394766] |
| time/                       |             |
|    fps                      | 130         |
|    iterations               | 11          |
|    time_elapsed             | 172         |
|    total_timesteps          | 423936      |
| train/                      |             |
|    approx_kl                | 0.006123454 |
|    approx_ln(kl)            | -5.095629   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.95       |
|    explained_variance       | 0.979       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -0.307      |
|    ln(policy_gradient_loss) | -7.14       |
|    loss                     | 0.736       |
|    n_updates                | 253         |
|    policy_gradient_loss     | 0.000789    |
|    std                      | 1.06        |
|    value_loss               | 1.87        |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38194028] |
| time/                       |               |
|    fps                      | 132           |
|    iterations               | 12            |
|    time_elapsed             | 185           |
|    total_timesteps          | 425984        |
| train/                      |               |
|    approx_kl                | 0.0077053183  |
|    approx_ln(kl)            | -4.8658447    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | 0.909         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.339         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.4           |
|    n_updates                | 254           |
|    policy_gradient_loss     | -0.01         |
|    std                      | 1.06          |
|    value_loss               | 2.49          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.493315] |
| time/                       |             |
|    fps                      | 133         |
|    iterations               | 13          |
|    time_elapsed             | 198         |
|    total_timesteps          | 428032      |
| train/                      |             |
|    approx_kl                | 0.008161841 |
|    approx_ln(kl)            | -4.8082857  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.95       |
|    explained_variance       | 0.964       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -0.863      |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.422       |
|    n_updates                | 255         |
|    policy_gradient_loss     | -0.00341    |
|    std                      | 1.06        |
|    value_loss               | 0.913       |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5435604] |
| time/                       |              |
|    fps                      | 134          |
|    iterations               | 14           |
|    time_elapsed             | 212          |
|    total_timesteps          | 430080       |
| train/                      |              |
|    approx_kl                | 0.0077454047 |
|    approx_ln(kl)            | -4.860656    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.95        |
|    explained_variance       | 0.357        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.705        |
|    ln(policy_gradient_loss) | -4.97        |
|    loss                     | 2.02         |
|    n_updates                | 256          |
|    policy_gradient_loss     | 0.00696      |
|    std                      | 1.06         |
|    value_loss               | 7.04         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37662175] |
| time/                       |               |
|    fps                      | 136           |
|    iterations               | 15            |
|    time_elapsed             | 225           |
|    total_timesteps          | 432128        |
| train/                      |               |
|    approx_kl                | 0.0053702486  |
|    approx_ln(kl)            | -5.226881     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | -0.667        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.258         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.29          |
|    n_updates                | 257           |
|    policy_gradient_loss     | -0.00431      |
|    std                      | 1.06          |
|    value_loss               | 8.24          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.62353355] |
| time/                       |               |
|    fps                      | 137           |
|    iterations               | 16            |
|    time_elapsed             | 238           |
|    total_timesteps          | 434176        |
| train/                      |               |
|    approx_kl                | 0.00826334    |
|    approx_ln(kl)            | -4.7959266    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | 0.0239        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.99          |
|    ln(policy_gradient_loss) | -7.36         |
|    loss                     | 7.3           |
|    n_updates                | 258           |
|    policy_gradient_loss     | 0.000634      |
|    std                      | 1.06          |
|    value_loss               | 15.3          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.8800069] |
| time/                       |              |
|    fps                      | 138          |
|    iterations               | 17           |
|    time_elapsed             | 251          |
|    total_timesteps          | 436224       |
| train/                      |              |
|    approx_kl                | 0.0070491703 |
|    approx_ln(kl)            | -4.9548454   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.96        |
|    explained_variance       | 0.472        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.424        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.53         |
|    n_updates                | 259          |
|    policy_gradient_loss     | -0.00661     |
|    std                      | 1.06         |
|    value_loss               | 3.92         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.39222983] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 18            |
|    time_elapsed             | 264           |
|    total_timesteps          | 438272        |
| train/                      |               |
|    approx_kl                | 0.009564057   |
|    approx_ln(kl)            | -4.649743     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.96         |
|    explained_variance       | 0.428         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.48          |
|    ln(policy_gradient_loss) | -5.07         |
|    loss                     | 4.38          |
|    n_updates                | 260           |
|    policy_gradient_loss     | 0.00626       |
|    std                      | 1.06          |
|    value_loss               | 12.6          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.67287964] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 19            |
|    time_elapsed             | 278           |
|    total_timesteps          | 440320        |
| train/                      |               |
|    approx_kl                | 0.007751754   |
|    approx_ln(kl)            | -4.859836     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.96         |
|    explained_variance       | 0.247         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.48          |
|    ln(policy_gradient_loss) | -8.08         |
|    loss                     | 4.41          |
|    n_updates                | 261           |
|    policy_gradient_loss     | 0.000311      |
|    std                      | 1.06          |
|    value_loss               | 9.15          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.45607352] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 20            |
|    time_elapsed             | 291           |
|    total_timesteps          | 442368        |
| train/                      |               |
|    approx_kl                | 0.009432606   |
|    approx_ln(kl)            | -4.663583     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.96         |
|    explained_variance       | -0.0112       |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.826         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 2.28          |
|    n_updates                | 262           |
|    policy_gradient_loss     | -0.0123       |
|    std                      | 1.07          |
|    value_loss               | 4.68          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5430501] |
| time/                       |              |
|    fps                      | 141          |
|    iterations               | 21           |
|    time_elapsed             | 304          |
|    total_timesteps          | 444416       |
| train/                      |              |
|    approx_kl                | 0.007706235  |
|    approx_ln(kl)            | -4.8657255   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.97        |
|    explained_variance       | -0.19        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.06         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 2.88         |
|    n_updates                | 263          |
|    policy_gradient_loss     | -0.00217     |
|    std                      | 1.07         |
|    value_loss               | 5.85         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43159148] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 22            |
|    time_elapsed             | 318           |
|    total_timesteps          | 446464        |
| train/                      |               |
|    approx_kl                | 0.0060484214  |
|    approx_ln(kl)            | -5.107958     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.97         |
|    explained_variance       | -0.0114       |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.979         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 2.66          |
|    n_updates                | 264           |
|    policy_gradient_loss     | -0.00411      |
|    std                      | 1.07          |
|    value_loss               | 7.22          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5683459] |
| time/                       |              |
|    fps                      | 142          |
|    iterations               | 23           |
|    time_elapsed             | 331          |
|    total_timesteps          | 448512       |
| train/                      |              |
|    approx_kl                | 0.0061561875 |
|    approx_ln(kl)            | -5.0902977   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.97        |
|    explained_variance       | 0.25         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.933        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 2.54         |
|    n_updates                | 265          |
|    policy_gradient_loss     | -0.00471     |
|    std                      | 1.07         |
|    value_loss               | 7.32         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.56424385] |
| time/                       |               |
|    fps                      | 142           |
|    iterations               | 24            |
|    time_elapsed             | 345           |
|    total_timesteps          | 450560        |
| train/                      |               |
|    approx_kl                | 0.0074997     |
|    approx_ln(kl)            | -4.8928924    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.97         |
|    explained_variance       | 0.292         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.02          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 2.77          |
|    n_updates                | 266           |
|    policy_gradient_loss     | -0.00831      |
|    std                      | 1.07          |
|    value_loss               | 5.66          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.7952668] |
| time/                       |              |
|    fps                      | 143          |
|    iterations               | 25           |
|    time_elapsed             | 358          |
|    total_timesteps          | 452608       |
| train/                      |              |
|    approx_kl                | 0.0075432127 |
|    approx_ln(kl)            | -4.887107    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.97        |
|    explained_variance       | 0.0836       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.74         |
|    ln(policy_gradient_loss) | -6.35        |
|    loss                     | 5.68         |
|    n_updates                | 267          |
|    policy_gradient_loss     | 0.00175      |
|    std                      | 1.07         |
|    value_loss               | 13.2         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29567316] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 26            |
|    time_elapsed             | 370           |
|    total_timesteps          | 454656        |
| train/                      |               |
|    approx_kl                | 0.004961242   |
|    approx_ln(kl)            | -5.3060994    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.97         |
|    explained_variance       | 0.0682        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.07          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 2.91          |
|    n_updates                | 268           |
|    policy_gradient_loss     | -0.00214      |
|    std                      | 1.07          |
|    value_loss               | 6.66          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.38140157] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 27            |
|    time_elapsed             | 383           |
|    total_timesteps          | 456704        |
| train/                      |               |
|    approx_kl                | 0.010565901   |
|    approx_ln(kl)            | -4.550123     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.97         |
|    explained_variance       | 0.351         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.39          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 4.01          |
|    n_updates                | 269           |
|    policy_gradient_loss     | -0.00479      |
|    std                      | 1.07          |
|    value_loss               | 7.76          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.54727477] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 28            |
|    time_elapsed             | 396           |
|    total_timesteps          | 458752        |
| train/                      |               |
|    approx_kl                | 0.008309101   |
|    approx_ln(kl)            | -4.790404     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.97         |
|    explained_variance       | 0.474         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.1           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 3.01          |
|    n_updates                | 270           |
|    policy_gradient_loss     | -0.00366      |
|    std                      | 1.07          |
|    value_loss               | 6             |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5758899] |
| time/                       |              |
|    fps                      | 145          |
|    iterations               | 29           |
|    time_elapsed             | 409          |
|    total_timesteps          | 460800       |
| train/                      |              |
|    approx_kl                | 0.008370103  |
|    approx_ln(kl)            | -4.783089    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.97        |
|    explained_variance       | 0.598        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.148        |
|    ln(policy_gradient_loss) | -4.93        |
|    loss                     | 1.16         |
|    n_updates                | 271          |
|    policy_gradient_loss     | 0.0072       |
|    std                      | 1.07         |
|    value_loss               | 2.88         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.4252969] |
| time/                       |              |
|    fps                      | 145          |
|    iterations               | 30           |
|    time_elapsed             | 422          |
|    total_timesteps          | 462848       |
| train/                      |              |
|    approx_kl                | 0.008573158  |
|    approx_ln(kl)            | -4.759119    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.98        |
|    explained_variance       | 0.79         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.0107       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.01         |
|    n_updates                | 272          |
|    policy_gradient_loss     | -0.000541    |
|    std                      | 1.07         |
|    value_loss               | 2.92         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.6646645] |
| time/                       |              |
|    fps                      | 145          |
|    iterations               | 31           |
|    time_elapsed             | 436          |
|    total_timesteps          | 464896       |
| train/                      |              |
|    approx_kl                | 0.0047420035 |
|    approx_ln(kl)            | -5.3512955   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.98        |
|    explained_variance       | 0.676        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.271       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.762        |
|    n_updates                | 273          |
|    policy_gradient_loss     | -0.000563    |
|    std                      | 1.07         |
|    value_loss               | 2.4          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27062055] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 32            |
|    time_elapsed             | 449           |
|    total_timesteps          | 466944        |
| train/                      |               |
|    approx_kl                | 0.006562475   |
|    approx_ln(kl)            | -5.026387     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.98         |
|    explained_variance       | 0.848         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.128         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.14          |
|    n_updates                | 274           |
|    policy_gradient_loss     | -0.00273      |
|    std                      | 1.07          |
|    value_loss               | 2.4           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43887287] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 33            |
|    time_elapsed             | 462           |
|    total_timesteps          | 468992        |
| train/                      |               |
|    approx_kl                | 0.007470202   |
|    approx_ln(kl)            | -4.8968334    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.98         |
|    explained_variance       | 0.545         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.119         |
|    ln(policy_gradient_loss) | -5.68         |
|    loss                     | 1.13          |
|    n_updates                | 275           |
|    policy_gradient_loss     | 0.00342       |
|    std                      | 1.07          |
|    value_loss               | 3.24          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3829528] |
| time/                       |              |
|    fps                      | 146          |
|    iterations               | 34           |
|    time_elapsed             | 475          |
|    total_timesteps          | 471040       |
| train/                      |              |
|    approx_kl                | 0.0038684276 |
|    approx_ln(kl)            | -5.5549073   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.98        |
|    explained_variance       | 0.628        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.211        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.24         |
|    n_updates                | 276          |
|    policy_gradient_loss     | -0.00213     |
|    std                      | 1.08         |
|    value_loss               | 1.96         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.33867553] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 35            |
|    time_elapsed             | 488           |
|    total_timesteps          | 473088        |
| train/                      |               |
|    approx_kl                | 0.0072917445  |
|    approx_ln(kl)            | -4.9210124    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.759         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1            |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.367         |
|    n_updates                | 277           |
|    policy_gradient_loss     | -0.00382      |
|    std                      | 1.08          |
|    value_loss               | 0.871         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31037736] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 36            |
|    time_elapsed             | 501           |
|    total_timesteps          | 475136        |
| train/                      |               |
|    approx_kl                | 0.0062518697  |
|    approx_ln(kl)            | -5.074875     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.794         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.274        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.76          |
|    n_updates                | 278           |
|    policy_gradient_loss     | -0.00528      |
|    std                      | 1.08          |
|    value_loss               | 1.57          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47726172] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 37            |
|    time_elapsed             | 514           |
|    total_timesteps          | 477184        |
| train/                      |               |
|    approx_kl                | 0.00851359    |
|    approx_ln(kl)            | -4.7660913    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.459         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.138        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.871         |
|    n_updates                | 279           |
|    policy_gradient_loss     | -0.00696      |
|    std                      | 1.08          |
|    value_loss               | 2.12          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.67046034] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 38            |
|    time_elapsed             | 527           |
|    total_timesteps          | 479232        |
| train/                      |               |
|    approx_kl                | 0.0137343565  |
|    approx_ln(kl)            | -4.2878547    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.354         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.0511        |
|    ln(policy_gradient_loss) | -4.28         |
|    loss                     | 1.05          |
|    n_updates                | 280           |
|    policy_gradient_loss     | 0.0139        |
|    std                      | 1.09          |
|    value_loss               | 2.1           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.53480124] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 39            |
|    time_elapsed             | 540           |
|    total_timesteps          | 481280        |
| train/                      |               |
|    approx_kl                | 0.00701943    |
|    approx_ln(kl)            | -4.959073     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.687         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.282        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.754         |
|    n_updates                | 281           |
|    policy_gradient_loss     | -0.00373      |
|    std                      | 1.09          |
|    value_loss               | 1.63          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.48830667] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 40            |
|    time_elapsed             | 553           |
|    total_timesteps          | 483328        |
| train/                      |               |
|    approx_kl                | 0.008340051   |
|    approx_ln(kl)            | -4.786686     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.594         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.247        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.781         |
|    n_updates                | 282           |
|    policy_gradient_loss     | -0.00655      |
|    std                      | 1.09          |
|    value_loss               | 1.65          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5709037] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 41           |
|    time_elapsed             | 566          |
|    total_timesteps          | 485376       |
| train/                      |              |
|    approx_kl                | 0.007186436  |
|    approx_ln(kl)            | -4.9355597   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.426        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.564        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.76         |
|    n_updates                | 283          |
|    policy_gradient_loss     | -0.00182     |
|    std                      | 1.09         |
|    value_loss               | 3.29         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42655873] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 42            |
|    time_elapsed             | 579           |
|    total_timesteps          | 487424        |
| train/                      |               |
|    approx_kl                | 0.0068556964  |
|    approx_ln(kl)            | -4.9826756    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.275         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.334        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.716         |
|    n_updates                | 284           |
|    policy_gradient_loss     | -0.000647     |
|    std                      | 1.09          |
|    value_loss               | 1.83          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36921212] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 43            |
|    time_elapsed             | 592           |
|    total_timesteps          | 489472        |
| train/                      |               |
|    approx_kl                | 0.006135655   |
|    approx_ln(kl)            | -5.0936384    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | -0.795        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.307        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.736         |
|    n_updates                | 285           |
|    policy_gradient_loss     | -0.00216      |
|    std                      | 1.09          |
|    value_loss               | 1.91          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36274335] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 44            |
|    time_elapsed             | 605           |
|    total_timesteps          | 491520        |
| train/                      |               |
|    approx_kl                | 0.005323631   |
|    approx_ln(kl)            | -5.2355995    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.101         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.0812        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.08          |
|    n_updates                | 286           |
|    policy_gradient_loss     | -0.0046       |
|    std                      | 1.1           |
|    value_loss               | 3.07          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4506537] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 45           |
|    time_elapsed             | 617          |
|    total_timesteps          | 493568       |
| train/                      |              |
|    approx_kl                | 0.006491029  |
|    approx_ln(kl)            | -5.0373344   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | -0.11        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.264        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.3          |
|    n_updates                | 287          |
|    policy_gradient_loss     | -0.00236     |
|    std                      | 1.1          |
|    value_loss               | 3.33         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4971478] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 46           |
|    time_elapsed             | 631          |
|    total_timesteps          | 495616       |
| train/                      |              |
|    approx_kl                | 0.008423743  |
|    approx_ln(kl)            | -4.776701    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.245        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.0518       |
|    ln(policy_gradient_loss) | -5.57        |
|    loss                     | 1.05         |
|    n_updates                | 288          |
|    policy_gradient_loss     | 0.00382      |
|    std                      | 1.1          |
|    value_loss               | 2.56         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3687053] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 47           |
|    time_elapsed             | 644          |
|    total_timesteps          | 497664       |
| train/                      |              |
|    approx_kl                | 0.005697347  |
|    approx_ln(kl)            | -5.1677547   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.178        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.278        |
|    ln(policy_gradient_loss) | -8.3         |
|    loss                     | 1.32         |
|    n_updates                | 289          |
|    policy_gradient_loss     | 0.000247     |
|    std                      | 1.1          |
|    value_loss               | 3            |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41213727] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 48            |
|    time_elapsed             | 657           |
|    total_timesteps          | 499712        |
| train/                      |               |
|    approx_kl                | 0.005541439   |
|    approx_ln(kl)            | -5.1955013    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.667         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0218       |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.978         |
|    n_updates                | 290           |
|    policy_gradient_loss     | -0.00796      |
|    std                      | 1.1           |
|    value_loss               | 2.55          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.52142036] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 49            |
|    time_elapsed             | 670           |
|    total_timesteps          | 501760        |
| train/                      |               |
|    approx_kl                | 0.006318872   |
|    approx_ln(kl)            | -5.0642147    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.362         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.329        |
|    ln(policy_gradient_loss) | -5.62         |
|    loss                     | 0.719         |
|    n_updates                | 291           |
|    policy_gradient_loss     | 0.00363       |
|    std                      | 1.1           |
|    value_loss               | 1.67          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
--------------------------------------
| reward             | [-0.48696187] |
| time/              |               |
|    fps             | 146           |
|    iterations      | 1             |
|    time_elapsed    | 13            |
|    total_timesteps | 503808        |
--------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3991288] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 2            |
|    time_elapsed             | 26           |
|    total_timesteps          | 505856       |
| train/                      |              |
|    approx_kl                | 0.0076802247 |
|    approx_ln(kl)            | -4.8691063   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.509        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.479       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.62         |
|    n_updates                | 293          |
|    policy_gradient_loss     | -0.00674     |
|    std                      | 1.1          |
|    value_loss               | 1.29         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48149878] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 3             |
|    time_elapsed             | 39            |
|    total_timesteps          | 507904        |
| train/                      |               |
|    approx_kl                | 0.0059097204  |
|    approx_ln(kl)            | -5.131157     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.475         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.201         |
|    ln(policy_gradient_loss) | -4.71         |
|    loss                     | 1.22          |
|    n_updates                | 294           |
|    policy_gradient_loss     | 0.00902       |
|    std                      | 1.1           |
|    value_loss               | 2.52          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40024203] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 4             |
|    time_elapsed             | 52            |
|    total_timesteps          | 509952        |
| train/                      |               |
|    approx_kl                | 0.005047048   |
|    approx_ln(kl)            | -5.288952     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.477         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.505         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.66          |
|    n_updates                | 295           |
|    policy_gradient_loss     | -0.00766      |
|    std                      | 1.1           |
|    value_loss               | 4.23          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.52247447] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 5             |
|    time_elapsed             | 65            |
|    total_timesteps          | 512000        |
| train/                      |               |
|    approx_kl                | 0.0041955286  |
|    approx_ln(kl)            | -5.473736     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.368         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.273         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.31          |
|    n_updates                | 296           |
|    policy_gradient_loss     | -0.00228      |
|    std                      | 1.1           |
|    value_loss               | 2.74          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29734936] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 6             |
|    time_elapsed             | 78            |
|    total_timesteps          | 514048        |
| train/                      |               |
|    approx_kl                | 0.008049439   |
|    approx_ln(kl)            | -4.822153     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | -0.112        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.862        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.422         |
|    n_updates                | 297           |
|    policy_gradient_loss     | -0.00241      |
|    std                      | 1.09          |
|    value_loss               | 1.1           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.4610693] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 7            |
|    time_elapsed             | 92           |
|    total_timesteps          | 516096       |
| train/                      |              |
|    approx_kl                | 0.009551483  |
|    approx_ln(kl)            | -4.6510587   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.345        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.269       |
|    ln(policy_gradient_loss) | -5.49        |
|    loss                     | 0.764        |
|    n_updates                | 298          |
|    policy_gradient_loss     | 0.00414      |
|    std                      | 1.09         |
|    value_loss               | 1.76         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4810977] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 8            |
|    time_elapsed             | 105          |
|    total_timesteps          | 518144       |
| train/                      |              |
|    approx_kl                | 0.0053978455 |
|    approx_ln(kl)            | -5.2217555   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.56         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.266        |
|    ln(policy_gradient_loss) | -6.18        |
|    loss                     | 1.3          |
|    n_updates                | 299          |
|    policy_gradient_loss     | 0.00207      |
|    std                      | 1.09         |
|    value_loss               | 3.02         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49627337] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 9             |
|    time_elapsed             | 119           |
|    total_timesteps          | 520192        |
| train/                      |               |
|    approx_kl                | 0.0061676763  |
|    approx_ln(kl)            | -5.0884333    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.678         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.22         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.295         |
|    n_updates                | 300           |
|    policy_gradient_loss     | -9.57e-05     |
|    std                      | 1.09          |
|    value_loss               | 1.43          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4648688] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 10           |
|    time_elapsed             | 132          |
|    total_timesteps          | 522240       |
| train/                      |              |
|    approx_kl                | 0.0037930498 |
|    approx_ln(kl)            | -5.574585    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.738        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.195        |
|    ln(policy_gradient_loss) | -7.19        |
|    loss                     | 1.21         |
|    n_updates                | 301          |
|    policy_gradient_loss     | 0.000757     |
|    std                      | 1.09         |
|    value_loss               | 2.81         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43322268] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 11            |
|    time_elapsed             | 145           |
|    total_timesteps          | 524288        |
| train/                      |               |
|    approx_kl                | 0.006998554   |
|    approx_ln(kl)            | -4.962052     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.949         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.397        |
|    ln(policy_gradient_loss) | -5.48         |
|    loss                     | 0.672         |
|    n_updates                | 302           |
|    policy_gradient_loss     | 0.00417       |
|    std                      | 1.09          |
|    value_loss               | 1.09          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3699978] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 12           |
|    time_elapsed             | 159          |
|    total_timesteps          | 526336       |
| train/                      |              |
|    approx_kl                | 0.0058446415 |
|    approx_ln(kl)            | -5.14223     |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.956        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.16        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.314        |
|    n_updates                | 303          |
|    policy_gradient_loss     | -0.0135      |
|    std                      | 1.09         |
|    value_loss               | 1.07         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.503255] |
| time/                       |             |
|    fps                      | 154         |
|    iterations               | 13          |
|    time_elapsed             | 172         |
|    total_timesteps          | 528384      |
| train/                      |             |
|    approx_kl                | 0.008390455 |
|    approx_ln(kl)            | -4.7806606  |
|    clip_range               | 0.2         |
|    entropy_loss             | -3.02       |
|    explained_variance       | 0.702       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -1.45       |
|    ln(policy_gradient_loss) | -5.75       |
|    loss                     | 0.234       |
|    n_updates                | 304         |
|    policy_gradient_loss     | 0.0032      |
|    std                      | 1.09        |
|    value_loss               | 1.13        |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.5628194] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 14           |
|    time_elapsed             | 186          |
|    total_timesteps          | 530432       |
| train/                      |              |
|    approx_kl                | 0.009505483  |
|    approx_ln(kl)            | -4.6558867   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.887        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.79        |
|    ln(policy_gradient_loss) | -6.52        |
|    loss                     | 0.167        |
|    n_updates                | 305          |
|    policy_gradient_loss     | 0.00147      |
|    std                      | 1.09         |
|    value_loss               | 0.51         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40618673] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 15            |
|    time_elapsed             | 199           |
|    total_timesteps          | 532480        |
| train/                      |               |
|    approx_kl                | 0.0060955198  |
|    approx_ln(kl)            | -5.100201     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.613         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0652       |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.937         |
|    n_updates                | 306           |
|    policy_gradient_loss     | -0.00331      |
|    std                      | 1.09          |
|    value_loss               | 2.51          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.380964] |
| time/                       |             |
|    fps                      | 151         |
|    iterations               | 16          |
|    time_elapsed             | 216         |
|    total_timesteps          | 534528      |
| train/                      |             |
|    approx_kl                | 0.005032441 |
|    approx_ln(kl)            | -5.29185    |
|    clip_range               | 0.2         |
|    entropy_loss             | -3.01       |
|    explained_variance       | 0.835       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -1.46       |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.231       |
|    n_updates                | 307         |
|    policy_gradient_loss     | -0.00436    |
|    std                      | 1.09        |
|    value_loss               | 0.956       |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.6459468] |
| time/                       |              |
|    fps                      | 143          |
|    iterations               | 17           |
|    time_elapsed             | 241          |
|    total_timesteps          | 536576       |
| train/                      |              |
|    approx_kl                | 0.004395063  |
|    approx_ln(kl)            | -5.4272733   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.906        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.642       |
|    ln(policy_gradient_loss) | -5.3         |
|    loss                     | 0.526        |
|    n_updates                | 308          |
|    policy_gradient_loss     | 0.00498      |
|    std                      | 1.1          |
|    value_loss               | 1.56         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5234684] |
| time/                       |              |
|    fps                      | 140          |
|    iterations               | 18           |
|    time_elapsed             | 262          |
|    total_timesteps          | 538624       |
| train/                      |              |
|    approx_kl                | 0.006249185  |
|    approx_ln(kl)            | -5.075304    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.913        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.145        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.16         |
|    n_updates                | 309          |
|    policy_gradient_loss     | -0.00031     |
|    std                      | 1.1          |
|    value_loss               | 2.74         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48152182] |
| time/                       |               |
|    fps                      | 137           |
|    iterations               | 19            |
|    time_elapsed             | 282           |
|    total_timesteps          | 540672        |
| train/                      |               |
|    approx_kl                | 0.0066379746  |
|    approx_ln(kl)            | -5.0149484    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.549         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.28         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.756         |
|    n_updates                | 310           |
|    policy_gradient_loss     | -0.00137      |
|    std                      | 1.11          |
|    value_loss               | 1.8           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.516469] |
| time/                       |             |
|    fps                      | 136         |
|    iterations               | 20          |
|    time_elapsed             | 299         |
|    total_timesteps          | 542720      |
| train/                      |             |
|    approx_kl                | 0.006720956 |
|    approx_ln(kl)            | -5.002525   |
|    clip_range               | 0.2         |
|    entropy_loss             | -3.04       |
|    explained_variance       | 0.933       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -0.249      |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.779       |
|    n_updates                | 311         |
|    policy_gradient_loss     | -0.00173    |
|    std                      | 1.11        |
|    value_loss               | 1.81        |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2841363] |
| time/                       |              |
|    fps                      | 135          |
|    iterations               | 21           |
|    time_elapsed             | 316          |
|    total_timesteps          | 544768       |
| train/                      |              |
|    approx_kl                | 0.0061679957 |
|    approx_ln(kl)            | -5.0883813   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.04        |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.87        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.155        |
|    n_updates                | 312          |
|    policy_gradient_loss     | -0.00136     |
|    std                      | 1.11         |
|    value_loss               | 0.365        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30846515] |
| time/                       |               |
|    fps                      | 134           |
|    iterations               | 22            |
|    time_elapsed             | 333           |
|    total_timesteps          | 546816        |
| train/                      |               |
|    approx_kl                | 0.0076750736  |
|    approx_ln(kl)            | -4.869777     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.79          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.11          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.12          |
|    n_updates                | 313           |
|    policy_gradient_loss     | -0.00265      |
|    std                      | 1.11          |
|    value_loss               | 3.29          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5718959] |
| time/                       |              |
|    fps                      | 134          |
|    iterations               | 23           |
|    time_elapsed             | 351          |
|    total_timesteps          | 548864       |
| train/                      |              |
|    approx_kl                | 0.005548222  |
|    approx_ln(kl)            | -5.194278    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.04        |
|    explained_variance       | 0.412        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.844        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 2.33         |
|    n_updates                | 314          |
|    policy_gradient_loss     | -0.00482     |
|    std                      | 1.11         |
|    value_loss               | 9.51         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45823938] |
| time/                       |               |
|    fps                      | 133           |
|    iterations               | 24            |
|    time_elapsed             | 367           |
|    total_timesteps          | 550912        |
| train/                      |               |
|    approx_kl                | 0.003112407   |
|    approx_ln(kl)            | -5.772359     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.819         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.648        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.523         |
|    n_updates                | 315           |
|    policy_gradient_loss     | -0.00172      |
|    std                      | 1.1           |
|    value_loss               | 2.66          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.57162255] |
| time/                       |               |
|    fps                      | 133           |
|    iterations               | 25            |
|    time_elapsed             | 384           |
|    total_timesteps          | 552960        |
| train/                      |               |
|    approx_kl                | 0.0058900192  |
|    approx_ln(kl)            | -5.134496     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.516         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.332         |
|    ln(policy_gradient_loss) | -7.41         |
|    loss                     | 1.39          |
|    n_updates                | 316           |
|    policy_gradient_loss     | 0.000607      |
|    std                      | 1.1           |
|    value_loss               | 3.05          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5186352] |
| time/                       |              |
|    fps                      | 132          |
|    iterations               | 26           |
|    time_elapsed             | 401          |
|    total_timesteps          | 555008       |
| train/                      |              |
|    approx_kl                | 0.0042213453 |
|    approx_ln(kl)            | -5.4676013   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.756        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.41        |
|    ln(policy_gradient_loss) | -5.55        |
|    loss                     | 0.244        |
|    n_updates                | 317          |
|    policy_gradient_loss     | 0.00388      |
|    std                      | 1.1          |
|    value_loss               | 0.545        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.46374723] |
| time/                       |               |
|    fps                      | 131           |
|    iterations               | 27            |
|    time_elapsed             | 419           |
|    total_timesteps          | 557056        |
| train/                      |               |
|    approx_kl                | 0.0057573873  |
|    approx_ln(kl)            | -5.1572714    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.86          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.513        |
|    ln(policy_gradient_loss) | -6.09         |
|    loss                     | 0.598         |
|    n_updates                | 318           |
|    policy_gradient_loss     | 0.00228       |
|    std                      | 1.1           |
|    value_loss               | 1.7           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3143196] |
| time/                       |              |
|    fps                      | 131          |
|    iterations               | 28           |
|    time_elapsed             | 435          |
|    total_timesteps          | 559104       |
| train/                      |              |
|    approx_kl                | 0.0050003473 |
|    approx_ln(kl)            | -5.298248    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.594        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.188        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.21         |
|    n_updates                | 319          |
|    policy_gradient_loss     | -0.0084      |
|    std                      | 1.1          |
|    value_loss               | 2.97         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.51484627] |
| time/                       |               |
|    fps                      | 131           |
|    iterations               | 29            |
|    time_elapsed             | 450           |
|    total_timesteps          | 561152        |
| train/                      |               |
|    approx_kl                | 0.0053586974  |
|    approx_ln(kl)            | -5.2290344    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.763         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0848       |
|    ln(policy_gradient_loss) | -7.24         |
|    loss                     | 0.919         |
|    n_updates                | 320           |
|    policy_gradient_loss     | 0.000718      |
|    std                      | 1.1           |
|    value_loss               | 2.51          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40918973] |
| time/                       |               |
|    fps                      | 129           |
|    iterations               | 30            |
|    time_elapsed             | 473           |
|    total_timesteps          | 563200        |
| train/                      |               |
|    approx_kl                | 0.017983677   |
|    approx_ln(kl)            | -4.018291     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.413         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.02         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.359         |
|    n_updates                | 322           |
|    policy_gradient_loss     | -0.000409     |
|    std                      | 1.1           |
|    value_loss               | 1.59          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5084323] |
| time/                       |              |
|    fps                      | 128          |
|    iterations               | 31           |
|    time_elapsed             | 493          |
|    total_timesteps          | 565248       |
| train/                      |              |
|    approx_kl                | 0.0053098546 |
|    approx_ln(kl)            | -5.2381907   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.04        |
|    explained_variance       | 0.912        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.264       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.768        |
|    n_updates                | 323          |
|    policy_gradient_loss     | -0.000556    |
|    std                      | 1.1          |
|    value_loss               | 2.2          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.14167334] |
| time/                       |               |
|    fps                      | 129           |
|    iterations               | 32            |
|    time_elapsed             | 507           |
|    total_timesteps          | 567296        |
| train/                      |               |
|    approx_kl                | 0.0055027353  |
|    approx_ln(kl)            | -5.20251      |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.926         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.763        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.466         |
|    n_updates                | 324           |
|    policy_gradient_loss     | -0.00495      |
|    std                      | 1.1           |
|    value_loss               | 1.2           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37310115] |
| time/                       |               |
|    fps                      | 129           |
|    iterations               | 33            |
|    time_elapsed             | 520           |
|    total_timesteps          | 569344        |
| train/                      |               |
|    approx_kl                | 0.0070291753  |
|    approx_ln(kl)            | -4.957686     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.875         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.293         |
|    ln(policy_gradient_loss) | -5.22         |
|    loss                     | 1.34          |
|    n_updates                | 325           |
|    policy_gradient_loss     | 0.00539       |
|    std                      | 1.1           |
|    value_loss               | 3.73          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.63419306] |
| time/                       |               |
|    fps                      | 130           |
|    iterations               | 34            |
|    time_elapsed             | 532           |
|    total_timesteps          | 571392        |
| train/                      |               |
|    approx_kl                | 0.0068041566  |
|    approx_ln(kl)            | -4.9902215    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.717         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.593         |
|    ln(policy_gradient_loss) | -7.43         |
|    loss                     | 1.81          |
|    n_updates                | 326           |
|    policy_gradient_loss     | 0.000595      |
|    std                      | 1.1           |
|    value_loss               | 15.4          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49555025] |
| time/                       |               |
|    fps                      | 131           |
|    iterations               | 35            |
|    time_elapsed             | 545           |
|    total_timesteps          | 573440        |
| train/                      |               |
|    approx_kl                | 0.010564077   |
|    approx_ln(kl)            | -4.550296     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.785         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.46         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.231         |
|    n_updates                | 328           |
|    policy_gradient_loss     | -0.00172      |
|    std                      | 1.1           |
|    value_loss               | 1.48          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22955091] |
| time/                       |               |
|    fps                      | 131           |
|    iterations               | 36            |
|    time_elapsed             | 558           |
|    total_timesteps          | 575488        |
| train/                      |               |
|    approx_kl                | 0.0046648923  |
|    approx_ln(kl)            | -5.3676906    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.921         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.838        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.433         |
|    n_updates                | 329           |
|    policy_gradient_loss     | -0.00438      |
|    std                      | 1.1           |
|    value_loss               | 1.16          |
-----------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.526755] |
| time/                       |             |
|    fps                      | 132         |
|    iterations               | 37          |
|    time_elapsed             | 572         |
|    total_timesteps          | 577536      |
| train/                      |             |
|    approx_kl                | 0.006844882 |
|    approx_ln(kl)            | -4.984254   |
|    clip_range               | 0.2         |
|    entropy_loss             | -3.02       |
|    explained_variance       | 0.926       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -1.18       |
|    ln(policy_gradient_loss) | -5.38       |
|    loss                     | 0.308       |
|    n_updates                | 331         |
|    policy_gradient_loss     | 0.00463     |
|    std                      | 1.09        |
|    value_loss               | 1.59        |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43482825] |
| time/                       |               |
|    fps                      | 132           |
|    iterations               | 38            |
|    time_elapsed             | 585           |
|    total_timesteps          | 579584        |
| train/                      |               |
|    approx_kl                | 0.006844094   |
|    approx_ln(kl)            | -4.9843693    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.926         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.931         |
|    ln(policy_gradient_loss) | -6.72         |
|    loss                     | 2.54          |
|    n_updates                | 332           |
|    policy_gradient_loss     | 0.00121       |
|    std                      | 1.09          |
|    value_loss               | 5.14          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.21544023] |
| time/                       |               |
|    fps                      | 133           |
|    iterations               | 39            |
|    time_elapsed             | 598           |
|    total_timesteps          | 581632        |
| train/                      |               |
|    approx_kl                | 0.005767226   |
|    approx_ln(kl)            | -5.155564     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.971         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.747        |
|    ln(policy_gradient_loss) | -5.25         |
|    loss                     | 0.474         |
|    n_updates                | 333           |
|    policy_gradient_loss     | 0.00525       |
|    std                      | 1.09          |
|    value_loss               | 1.63          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5247494] |
| time/                       |              |
|    fps                      | 133          |
|    iterations               | 40           |
|    time_elapsed             | 611          |
|    total_timesteps          | 583680       |
| train/                      |              |
|    approx_kl                | 0.0051262346 |
|    approx_ln(kl)            | -5.273384    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.949        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.505       |
|    ln(policy_gradient_loss) | -7.93        |
|    loss                     | 0.604        |
|    n_updates                | 334          |
|    policy_gradient_loss     | 0.000359     |
|    std                      | 1.09         |
|    value_loss               | 1.49         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38422424] |
| time/                       |               |
|    fps                      | 134           |
|    iterations               | 41            |
|    time_elapsed             | 624           |
|    total_timesteps          | 585728        |
| train/                      |               |
|    approx_kl                | 0.0052506016  |
|    approx_ln(kl)            | -5.2494125    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.911         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.06         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.345         |
|    n_updates                | 335           |
|    policy_gradient_loss     | -0.00469      |
|    std                      | 1.08          |
|    value_loss               | 1.4           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25032535] |
| time/                       |               |
|    fps                      | 134           |
|    iterations               | 42            |
|    time_elapsed             | 637           |
|    total_timesteps          | 587776        |
| train/                      |               |
|    approx_kl                | 0.0040673963  |
|    approx_ln(kl)            | -5.504752     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.96         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.141         |
|    n_updates                | 336           |
|    policy_gradient_loss     | -0.00132      |
|    std                      | 1.08          |
|    value_loss               | 0.458         |
-----------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.5652504] |
| time/                       |              |
|    fps                      | 135          |
|    iterations               | 43           |
|    time_elapsed             | 650          |
|    total_timesteps          | 589824       |
| train/                      |              |
|    approx_kl                | 0.015659736  |
|    approx_ln(kl)            | -4.1566625   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.99        |
|    explained_variance       | 0.977        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.94        |
|    ln(policy_gradient_loss) | -5.92        |
|    loss                     | 0.144        |
|    n_updates                | 338          |
|    policy_gradient_loss     | 0.00268      |
|    std                      | 1.08         |
|    value_loss               | 0.626        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22990675] |
| time/                       |               |
|    fps                      | 135           |
|    iterations               | 44            |
|    time_elapsed             | 664           |
|    total_timesteps          | 591872        |
| train/                      |               |
|    approx_kl                | 0.0061527193  |
|    approx_ln(kl)            | -5.0908613    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.967         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.29         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.275         |
|    n_updates                | 339           |
|    policy_gradient_loss     | -0.00268      |
|    std                      | 1.08          |
|    value_loss               | 0.706         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.50622517] |
| time/                       |               |
|    fps                      | 135           |
|    iterations               | 45            |
|    time_elapsed             | 677           |
|    total_timesteps          | 593920        |
| train/                      |               |
|    approx_kl                | 0.0055286265  |
|    approx_ln(kl)            | -5.197816     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.935         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.02         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.361         |
|    n_updates                | 340           |
|    policy_gradient_loss     | -0.00312      |
|    std                      | 1.08          |
|    value_loss               | 0.737         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4623003] |
| time/                       |              |
|    fps                      | 136          |
|    iterations               | 46           |
|    time_elapsed             | 690          |
|    total_timesteps          | 595968       |
| train/                      |              |
|    approx_kl                | 0.004556546  |
|    approx_ln(kl)            | -5.3911905   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.98        |
|    explained_variance       | 0.867        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.25        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.286        |
|    n_updates                | 341          |
|    policy_gradient_loss     | -0.00574     |
|    std                      | 1.07         |
|    value_loss               | 0.753        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.61190945] |
| time/                       |               |
|    fps                      | 136           |
|    iterations               | 47            |
|    time_elapsed             | 703           |
|    total_timesteps          | 598016        |
| train/                      |               |
|    approx_kl                | 0.0049613435  |
|    approx_ln(kl)            | -5.306079     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.98         |
|    explained_variance       | 0.958         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.54         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.214         |
|    n_updates                | 342           |
|    policy_gradient_loss     | -0.00251      |
|    std                      | 1.07          |
|    value_loss               | 1.09          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43979245] |
| time/                       |               |
|    fps                      | 137           |
|    iterations               | 48            |
|    time_elapsed             | 717           |
|    total_timesteps          | 600064        |
| train/                      |               |
|    approx_kl                | 0.009332182   |
|    approx_ln(kl)            | -4.6742864    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.98         |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.79         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.167         |
|    n_updates                | 344           |
|    policy_gradient_loss     | -0.00387      |
|    std                      | 1.08          |
|    value_loss               | 0.482         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43662813] |
| time/                       |               |
|    fps                      | 137           |
|    iterations               | 49            |
|    time_elapsed             | 731           |
|    total_timesteps          | 602112        |
| train/                      |               |
|    approx_kl                | 0.005321627   |
|    approx_ln(kl)            | -5.235976     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.971         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.45         |
|    ln(policy_gradient_loss) | -6.38         |
|    loss                     | 0.235         |
|    n_updates                | 345           |
|    policy_gradient_loss     | 0.00169       |
|    std                      | 1.08          |
|    value_loss               | 0.673         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
--------------------------------------
| reward             | [-0.27382982] |
| time/              |               |
|    fps             | 148           |
|    iterations      | 1             |
|    time_elapsed    | 13            |
|    total_timesteps | 604160        |
--------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47479016] |
| time/                       |               |
|    fps                      | 136           |
|    iterations               | 2             |
|    time_elapsed             | 30            |
|    total_timesteps          | 606208        |
| train/                      |               |
|    approx_kl                | 0.0063185175  |
|    approx_ln(kl)            | -5.0642705    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.558        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.572         |
|    n_updates                | 347           |
|    policy_gradient_loss     | -0.00142      |
|    std                      | 1.08          |
|    value_loss               | 1.19          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49646148] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 3             |
|    time_elapsed             | 42            |
|    total_timesteps          | 608256        |
| train/                      |               |
|    approx_kl                | 0.006627923   |
|    approx_ln(kl)            | -5.0164638    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.949         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.718        |
|    ln(policy_gradient_loss) | -7.09         |
|    loss                     | 0.488         |
|    n_updates                | 348           |
|    policy_gradient_loss     | 0.000832      |
|    std                      | 1.08          |
|    value_loss               | 2.39          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2048897] |
| time/                       |              |
|    fps                      | 147          |
|    iterations               | 4            |
|    time_elapsed             | 55           |
|    total_timesteps          | 610304       |
| train/                      |              |
|    approx_kl                | 0.0038685708 |
|    approx_ln(kl)            | -5.55487     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.99        |
|    explained_variance       | 0.959        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.991       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.371        |
|    n_updates                | 349          |
|    policy_gradient_loss     | -0.000357    |
|    std                      | 1.08         |
|    value_loss               | 0.93         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40129665] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 5             |
|    time_elapsed             | 68            |
|    total_timesteps          | 612352        |
| train/                      |               |
|    approx_kl                | 0.0053954883  |
|    approx_ln(kl)            | -5.2221923    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.937         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.46          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 4.3           |
|    n_updates                | 350           |
|    policy_gradient_loss     | -0.00383      |
|    std                      | 1.08          |
|    value_loss               | 7.49          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.55170095] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 6             |
|    time_elapsed             | 81            |
|    total_timesteps          | 614400        |
| train/                      |               |
|    approx_kl                | 0.003714888   |
|    approx_ln(kl)            | -5.595407     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.878         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.83         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.436         |
|    n_updates                | 351           |
|    policy_gradient_loss     | -0.00122      |
|    std                      | 1.08          |
|    value_loss               | 1.62          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39223394] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 7             |
|    time_elapsed             | 94            |
|    total_timesteps          | 616448        |
| train/                      |               |
|    approx_kl                | 0.00466916    |
|    approx_ln(kl)            | -5.366776     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.955         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.3          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.272         |
|    n_updates                | 352           |
|    policy_gradient_loss     | -0.00157      |
|    std                      | 1.08          |
|    value_loss               | 2.55          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.20738858] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 8             |
|    time_elapsed             | 107           |
|    total_timesteps          | 618496        |
| train/                      |               |
|    approx_kl                | 0.006416403   |
|    approx_ln(kl)            | -5.0488977    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.973         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.12         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.327         |
|    n_updates                | 353           |
|    policy_gradient_loss     | -0.00537      |
|    std                      | 1.08          |
|    value_loss               | 1.02          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47440413] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 9             |
|    time_elapsed             | 120           |
|    total_timesteps          | 620544        |
| train/                      |               |
|    approx_kl                | 0.0041405684  |
|    approx_ln(kl)            | -5.4869223    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.72         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.178         |
|    n_updates                | 354           |
|    policy_gradient_loss     | -0.00123      |
|    std                      | 1.08          |
|    value_loss               | 0.493         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.56708485] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 10            |
|    time_elapsed             | 133           |
|    total_timesteps          | 622592        |
| train/                      |               |
|    approx_kl                | 0.0070682913  |
|    approx_ln(kl)            | -4.9521365    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.12         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.326         |
|    n_updates                | 355           |
|    policy_gradient_loss     | -0.0039       |
|    std                      | 1.08          |
|    value_loss               | 0.554         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3759264] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 11           |
|    time_elapsed             | 146          |
|    total_timesteps          | 624640       |
| train/                      |              |
|    approx_kl                | 0.008001863  |
|    approx_ln(kl)            | -4.828081    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.98        |
|    explained_variance       | 0.923        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.11        |
|    ln(policy_gradient_loss) | -5.82        |
|    loss                     | 0.329        |
|    n_updates                | 356          |
|    policy_gradient_loss     | 0.00296      |
|    std                      | 1.08         |
|    value_loss               | 0.908        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34800905] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 12            |
|    time_elapsed             | 159           |
|    total_timesteps          | 626688        |
| train/                      |               |
|    approx_kl                | 0.0036090694  |
|    approx_ln(kl)            | -5.6243052    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.23         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.108         |
|    n_updates                | 357           |
|    policy_gradient_loss     | -0.00253      |
|    std                      | 1.08          |
|    value_loss               | 0.499         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48996824] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 13            |
|    time_elapsed             | 172           |
|    total_timesteps          | 628736        |
| train/                      |               |
|    approx_kl                | 0.004860483   |
|    approx_ln(kl)            | -5.3266172    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.67         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.188         |
|    n_updates                | 358           |
|    policy_gradient_loss     | -0.00376      |
|    std                      | 1.08          |
|    value_loss               | 0.509         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39126042] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 14            |
|    time_elapsed             | 185           |
|    total_timesteps          | 630784        |
| train/                      |               |
|    approx_kl                | 0.0057886457  |
|    approx_ln(kl)            | -5.151857     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.3          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.101         |
|    n_updates                | 359           |
|    policy_gradient_loss     | -0.00892      |
|    std                      | 1.09          |
|    value_loss               | 0.395         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29588106] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 15            |
|    time_elapsed             | 198           |
|    total_timesteps          | 632832        |
| train/                      |               |
|    approx_kl                | 0.006517248   |
|    approx_ln(kl)            | -5.0333033    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.46         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0856        |
|    n_updates                | 360           |
|    policy_gradient_loss     | -0.000271     |
|    std                      | 1.09          |
|    value_loss               | 0.194         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42405388] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 16            |
|    time_elapsed             | 210           |
|    total_timesteps          | 634880        |
| train/                      |               |
|    approx_kl                | 0.006837331   |
|    approx_ln(kl)            | -4.9853578    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.93         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.145         |
|    n_updates                | 361           |
|    policy_gradient_loss     | -0.000722     |
|    std                      | 1.09          |
|    value_loss               | 0.304         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40324667] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 17            |
|    time_elapsed             | 223           |
|    total_timesteps          | 636928        |
| train/                      |               |
|    approx_kl                | 0.004035952   |
|    approx_ln(kl)            | -5.512513     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.65         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.193         |
|    n_updates                | 362           |
|    policy_gradient_loss     | -0.00061      |
|    std                      | 1.09          |
|    value_loss               | 0.316         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49489412] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 18            |
|    time_elapsed             | 236           |
|    total_timesteps          | 638976        |
| train/                      |               |
|    approx_kl                | 0.0063742474  |
|    approx_ln(kl)            | -5.055489     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.57         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0762        |
|    n_updates                | 363           |
|    policy_gradient_loss     | -0.00259      |
|    std                      | 1.1           |
|    value_loss               | 0.433         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3965473] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 19           |
|    time_elapsed             | 249          |
|    total_timesteps          | 641024       |
| train/                      |              |
|    approx_kl                | 0.0054591303 |
|    approx_ln(kl)            | -5.210466    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.881        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.68        |
|    ln(policy_gradient_loss) | -5.66        |
|    loss                     | 0.0683       |
|    n_updates                | 364          |
|    policy_gradient_loss     | 0.00349      |
|    std                      | 1.1          |
|    value_loss               | 0.484        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45384043] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 20            |
|    time_elapsed             | 263           |
|    total_timesteps          | 643072        |
| train/                      |               |
|    approx_kl                | 0.0054484745  |
|    approx_ln(kl)            | -5.2124195    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.89         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.152         |
|    n_updates                | 365           |
|    policy_gradient_loss     | -0.000744     |
|    std                      | 1.1           |
|    value_loss               | 0.354         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-46.1505]   |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 21           |
|    time_elapsed             | 277          |
|    total_timesteps          | 645120       |
| train/                      |              |
|    approx_kl                | 0.0057767453 |
|    approx_ln(kl)            | -5.153915    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.973        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2           |
|    ln(policy_gradient_loss) | -5.65        |
|    loss                     | 0.135        |
|    n_updates                | 366          |
|    policy_gradient_loss     | 0.00352      |
|    std                      | 1.1          |
|    value_loss               | 0.427        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5780748] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 22           |
|    time_elapsed             | 290          |
|    total_timesteps          | 647168       |
| train/                      |              |
|    approx_kl                | 0.0055019404 |
|    approx_ln(kl)            | -5.2026544   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.26        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.104        |
|    n_updates                | 367          |
|    policy_gradient_loss     | -0.00857     |
|    std                      | 1.1          |
|    value_loss               | 0.323        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32382947] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 23            |
|    time_elapsed             | 303           |
|    total_timesteps          | 649216        |
| train/                      |               |
|    approx_kl                | 0.0046042227  |
|    approx_ln(kl)            | -5.3807817    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.53         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0798        |
|    n_updates                | 368           |
|    policy_gradient_loss     | -0.00383      |
|    std                      | 1.1           |
|    value_loss               | 0.291         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37364677] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 24            |
|    time_elapsed             | 317           |
|    total_timesteps          | 651264        |
| train/                      |               |
|    approx_kl                | 0.006272435   |
|    approx_ln(kl)            | -5.0715904    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.41         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0895        |
|    n_updates                | 369           |
|    policy_gradient_loss     | -0.00586      |
|    std                      | 1.1           |
|    value_loss               | 0.173         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48953593] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 25            |
|    time_elapsed             | 330           |
|    total_timesteps          | 653312        |
| train/                      |               |
|    approx_kl                | 0.009469449   |
|    approx_ln(kl)            | -4.6596847    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.55         |
|    ln(policy_gradient_loss) | -4.75         |
|    loss                     | 0.212         |
|    n_updates                | 370           |
|    policy_gradient_loss     | 0.00865       |
|    std                      | 1.1           |
|    value_loss               | 0.581         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2568936] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 26           |
|    time_elapsed             | 342          |
|    total_timesteps          | 655360       |
| train/                      |              |
|    approx_kl                | 0.007497675  |
|    approx_ln(kl)            | -4.8931623   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.04        |
|    explained_variance       | 0.995        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.12        |
|    ln(policy_gradient_loss) | -6           |
|    loss                     | 0.12         |
|    n_updates                | 371          |
|    policy_gradient_loss     | 0.00248      |
|    std                      | 1.1          |
|    value_loss               | 0.265        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49356177] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 27            |
|    time_elapsed             | 355           |
|    total_timesteps          | 657408        |
| train/                      |               |
|    approx_kl                | 0.006155108   |
|    approx_ln(kl)            | -5.090473     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.53         |
|    ln(policy_gradient_loss) | -6.35         |
|    loss                     | 0.217         |
|    n_updates                | 372           |
|    policy_gradient_loss     | 0.00175       |
|    std                      | 1.11          |
|    value_loss               | 0.341         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.52828634] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 28            |
|    time_elapsed             | 368           |
|    total_timesteps          | 659456        |
| train/                      |               |
|    approx_kl                | 0.0042641405  |
|    approx_ln(kl)            | -5.457515     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.81         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.163         |
|    n_updates                | 373           |
|    policy_gradient_loss     | -0.00692      |
|    std                      | 1.11          |
|    value_loss               | 0.485         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38385776] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 29            |
|    time_elapsed             | 381           |
|    total_timesteps          | 661504        |
| train/                      |               |
|    approx_kl                | 0.0051684347  |
|    approx_ln(kl)            | -5.2651854    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.45         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0864        |
|    n_updates                | 374           |
|    policy_gradient_loss     | -0.00352      |
|    std                      | 1.1           |
|    value_loss               | 0.283         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44785923] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 30            |
|    time_elapsed             | 394           |
|    total_timesteps          | 663552        |
| train/                      |               |
|    approx_kl                | 0.006870298   |
|    approx_ln(kl)            | -4.980548     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.19         |
|    ln(policy_gradient_loss) | -5.06         |
|    loss                     | 0.304         |
|    n_updates                | 375           |
|    policy_gradient_loss     | 0.00634       |
|    std                      | 1.1           |
|    value_loss               | 0.59          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35707283] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 31            |
|    time_elapsed             | 408           |
|    total_timesteps          | 665600        |
| train/                      |               |
|    approx_kl                | 0.005483903   |
|    approx_ln(kl)            | -5.2059383    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.951         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.831        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.436         |
|    n_updates                | 376           |
|    policy_gradient_loss     | -0.00334      |
|    std                      | 1.11          |
|    value_loss               | 0.938         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.6500046] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 32           |
|    time_elapsed             | 421          |
|    total_timesteps          | 667648       |
| train/                      |              |
|    approx_kl                | 0.0047251442 |
|    approx_ln(kl)            | -5.3548574   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.04        |
|    explained_variance       | 0.974        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.41        |
|    ln(policy_gradient_loss) | -6.86        |
|    loss                     | 0.0903       |
|    n_updates                | 377          |
|    policy_gradient_loss     | 0.00105      |
|    std                      | 1.11         |
|    value_loss               | 0.283        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.350839]  |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 33           |
|    time_elapsed             | 434          |
|    total_timesteps          | 669696       |
| train/                      |              |
|    approx_kl                | 0.0058613406 |
|    approx_ln(kl)            | -5.139377    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.04        |
|    explained_variance       | 0.963        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.59        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.204        |
|    n_updates                | 378          |
|    policy_gradient_loss     | -0.00404     |
|    std                      | 1.11         |
|    value_loss               | 4.2          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.48644456] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 34            |
|    time_elapsed             | 447           |
|    total_timesteps          | 671744        |
| train/                      |               |
|    approx_kl                | 0.007044011   |
|    approx_ln(kl)            | -4.9555774    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.93         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.145         |
|    n_updates                | 379           |
|    policy_gradient_loss     | -0.00872      |
|    std                      | 1.11          |
|    value_loss               | 0.41          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29458392] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 35            |
|    time_elapsed             | 461           |
|    total_timesteps          | 673792        |
| train/                      |               |
|    approx_kl                | 0.009430413   |
|    approx_ln(kl)            | -4.6638155    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.892         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.4          |
|    ln(policy_gradient_loss) | -5.77         |
|    loss                     | 0.247         |
|    n_updates                | 380           |
|    policy_gradient_loss     | 0.00311       |
|    std                      | 1.11          |
|    value_loss               | 0.718         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45869216] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 36            |
|    time_elapsed             | 474           |
|    total_timesteps          | 675840        |
| train/                      |               |
|    approx_kl                | 0.009086196   |
|    approx_ln(kl)            | -4.700999     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.05         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.29         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.101         |
|    n_updates                | 381           |
|    policy_gradient_loss     | -0.00781      |
|    std                      | 1.11          |
|    value_loss               | 0.421         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5291312] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 37           |
|    time_elapsed             | 487          |
|    total_timesteps          | 677888       |
| train/                      |              |
|    approx_kl                | 0.008829955  |
|    approx_ln(kl)            | -4.729605    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.05        |
|    explained_variance       | 0.941        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.74         |
|    ln(policy_gradient_loss) | -5.66        |
|    loss                     | 5.71         |
|    n_updates                | 382          |
|    policy_gradient_loss     | 0.00347      |
|    std                      | 1.11         |
|    value_loss               | 8.54         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25082037] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 38            |
|    time_elapsed             | 500           |
|    total_timesteps          | 679936        |
| train/                      |               |
|    approx_kl                | 0.006923996   |
|    approx_ln(kl)            | -4.972762     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.05         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.38         |
|    ln(policy_gradient_loss) | -5.93         |
|    loss                     | 0.0923        |
|    n_updates                | 383           |
|    policy_gradient_loss     | 0.00266       |
|    std                      | 1.11          |
|    value_loss               | 0.212         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.32793653] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 39            |
|    time_elapsed             | 513           |
|    total_timesteps          | 681984        |
| train/                      |               |
|    approx_kl                | 0.005449128   |
|    approx_ln(kl)            | -5.2123       |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.05         |
|    explained_variance       | 0.961         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.16         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0156        |
|    n_updates                | 384           |
|    policy_gradient_loss     | -0.00763      |
|    std                      | 1.11          |
|    value_loss               | 0.328         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.51764095] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 40            |
|    time_elapsed             | 526           |
|    total_timesteps          | 684032        |
| train/                      |               |
|    approx_kl                | 0.007978318   |
|    approx_ln(kl)            | -4.8310275    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.05         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.17         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.114         |
|    n_updates                | 385           |
|    policy_gradient_loss     | -0.00233      |
|    std                      | 1.11          |
|    value_loss               | 0.226         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4701078] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 41           |
|    time_elapsed             | 539          |
|    total_timesteps          | 686080       |
| train/                      |              |
|    approx_kl                | 0.0065854355 |
|    approx_ln(kl)            | -5.022895    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.05        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.9         |
|    ln(policy_gradient_loss) | -4.34        |
|    loss                     | 0.149        |
|    n_updates                | 386          |
|    policy_gradient_loss     | 0.013        |
|    std                      | 1.11         |
|    value_loss               | 0.293        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35683128] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 42            |
|    time_elapsed             | 552           |
|    total_timesteps          | 688128        |
| train/                      |               |
|    approx_kl                | 0.0061463104  |
|    approx_ln(kl)            | -5.091903     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.05         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.28         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0378        |
|    n_updates                | 387           |
|    policy_gradient_loss     | -0.00325      |
|    std                      | 1.11          |
|    value_loss               | 0.222         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.4607782] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 43           |
|    time_elapsed             | 565          |
|    total_timesteps          | 690176       |
| train/                      |              |
|    approx_kl                | 0.006749515  |
|    approx_ln(kl)            | -4.998285    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.05        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.34        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0961       |
|    n_updates                | 388          |
|    policy_gradient_loss     | -0.00809     |
|    std                      | 1.11         |
|    value_loss               | 0.258        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22614819] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 44            |
|    time_elapsed             | 578           |
|    total_timesteps          | 692224        |
| train/                      |               |
|    approx_kl                | 0.007533424   |
|    approx_ln(kl)            | -4.888406     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.05         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.79         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0617        |
|    n_updates                | 389           |
|    policy_gradient_loss     | -0.000787     |
|    std                      | 1.11          |
|    value_loss               | 0.273         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.49577844] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 45            |
|    time_elapsed             | 591           |
|    total_timesteps          | 694272        |
| train/                      |               |
|    approx_kl                | 0.006406351   |
|    approx_ln(kl)            | -5.0504656    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.05         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.21         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.109         |
|    n_updates                | 390           |
|    policy_gradient_loss     | -0.00972      |
|    std                      | 1.11          |
|    value_loss               | 0.289         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4878023] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 46           |
|    time_elapsed             | 604          |
|    total_timesteps          | 696320       |
| train/                      |              |
|    approx_kl                | 0.0053999955 |
|    approx_ln(kl)            | -5.2213573   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.05        |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.5         |
|    ln(policy_gradient_loss) | -6.51        |
|    loss                     | 0.224        |
|    n_updates                | 391          |
|    policy_gradient_loss     | 0.00149      |
|    std                      | 1.11         |
|    value_loss               | 0.596        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26517957] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 47            |
|    time_elapsed             | 617           |
|    total_timesteps          | 698368        |
| train/                      |               |
|    approx_kl                | 0.0030955237  |
|    approx_ln(kl)            | -5.777798     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.05         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.61         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.00991       |
|    n_updates                | 392           |
|    policy_gradient_loss     | -0.00378      |
|    std                      | 1.11          |
|    value_loss               | 0.166         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47466692] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 48            |
|    time_elapsed             | 631           |
|    total_timesteps          | 700416        |
| train/                      |               |
|    approx_kl                | 0.0057861186  |
|    approx_ln(kl)            | -5.1522937    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.05         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.92         |
|    ln(policy_gradient_loss) | -6.33         |
|    loss                     | 0.0541        |
|    n_updates                | 393           |
|    policy_gradient_loss     | 0.00179       |
|    std                      | 1.11          |
|    value_loss               | 0.145         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4310161] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 49           |
|    time_elapsed             | 644          |
|    total_timesteps          | 702464       |
| train/                      |              |
|    approx_kl                | 0.006968869  |
|    approx_ln(kl)            | -4.9663024   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.05        |
|    explained_variance       | 0.99         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.49        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0827       |
|    n_updates                | 394          |
|    policy_gradient_loss     | -0.00263     |
|    std                      | 1.11         |
|    value_loss               | 0.473        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------
| reward             | [-0.5149744] |
| time/              |              |
|    fps             | 157          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 704512       |
-------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47122294] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 2             |
|    time_elapsed             | 26            |
|    total_timesteps          | 706560        |
| train/                      |               |
|    approx_kl                | 0.004559787   |
|    approx_ln(kl)            | -5.3904796    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.05         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.6          |
|    ln(policy_gradient_loss) | -5.56         |
|    loss                     | 0.0745        |
|    n_updates                | 396           |
|    policy_gradient_loss     | 0.00387       |
|    std                      | 1.11          |
|    value_loss               | 0.101         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33438173] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 3             |
|    time_elapsed             | 39            |
|    total_timesteps          | 708608        |
| train/                      |               |
|    approx_kl                | 0.0070858584  |
|    approx_ln(kl)            | -4.949654     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.05         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.46         |
|    ln(policy_gradient_loss) | -6.26         |
|    loss                     | 0.0313        |
|    n_updates                | 397           |
|    policy_gradient_loss     | 0.00191       |
|    std                      | 1.11          |
|    value_loss               | 0.0993        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5468857] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 4            |
|    time_elapsed             | 52           |
|    total_timesteps          | 710656       |
| train/                      |              |
|    approx_kl                | 0.004640204  |
|    approx_ln(kl)            | -5.372997    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.05        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.18        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.114        |
|    n_updates                | 398          |
|    policy_gradient_loss     | -0.00549     |
|    std                      | 1.11         |
|    value_loss               | 0.212        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5179071] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 5            |
|    time_elapsed             | 65           |
|    total_timesteps          | 712704       |
| train/                      |              |
|    approx_kl                | 0.0073249405 |
|    approx_ln(kl)            | -4.91647     |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.05        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.35        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0352       |
|    n_updates                | 399          |
|    policy_gradient_loss     | -0.0115      |
|    std                      | 1.11         |
|    value_loss               | 0.229        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.52812445] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 6             |
|    time_elapsed             | 78            |
|    total_timesteps          | 714752        |
| train/                      |               |
|    approx_kl                | 0.0059456467  |
|    approx_ln(kl)            | -5.125096     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.05         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.36         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0348        |
|    n_updates                | 400           |
|    policy_gradient_loss     | -0.00352      |
|    std                      | 1.11          |
|    value_loss               | 0.165         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.532318] |
| time/                       |             |
|    fps                      | 157         |
|    iterations               | 7           |
|    time_elapsed             | 91          |
|    total_timesteps          | 716800      |
| train/                      |             |
|    approx_kl                | 0.006247688 |
|    approx_ln(kl)            | -5.075544   |
|    clip_range               | 0.2         |
|    entropy_loss             | -3.05       |
|    explained_variance       | 0.998       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -2.37       |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.0934      |
|    n_updates                | 401         |
|    policy_gradient_loss     | -0.00772    |
|    std                      | 1.11        |
|    value_loss               | 0.168       |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.5522151] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 8            |
|    time_elapsed             | 104          |
|    total_timesteps          | 718848       |
| train/                      |              |
|    approx_kl                | 0.010430606  |
|    approx_ln(kl)            | -4.5630107   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.05        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.51        |
|    ln(policy_gradient_loss) | -6.2         |
|    loss                     | 0.03         |
|    n_updates                | 402          |
|    policy_gradient_loss     | 0.00202      |
|    std                      | 1.11         |
|    value_loss               | 0.031        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3960102] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 9            |
|    time_elapsed             | 117          |
|    total_timesteps          | 720896       |
| train/                      |              |
|    approx_kl                | 0.007903419  |
|    approx_ln(kl)            | -4.84046     |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.05        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.07        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0466       |
|    n_updates                | 403          |
|    policy_gradient_loss     | -0.000801    |
|    std                      | 1.11         |
|    value_loss               | 0.226        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3528128] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 10           |
|    time_elapsed             | 129          |
|    total_timesteps          | 722944       |
| train/                      |              |
|    approx_kl                | 0.006576293  |
|    approx_ln(kl)            | -5.024284    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.04        |
|    explained_variance       | 0.985        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -4.3         |
|    ln(policy_gradient_loss) | -5.44        |
|    loss                     | 0.0135       |
|    n_updates                | 404          |
|    policy_gradient_loss     | 0.00433      |
|    std                      | 1.11         |
|    value_loss               | 0.207        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24366605] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 11            |
|    time_elapsed             | 143           |
|    total_timesteps          | 724992        |
| train/                      |               |
|    approx_kl                | 0.0060243453  |
|    approx_ln(kl)            | -5.1119466    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.13         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.119         |
|    n_updates                | 405           |
|    policy_gradient_loss     | -0.000615     |
|    std                      | 1.11          |
|    value_loss               | 0.183         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.51150376] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 12            |
|    time_elapsed             | 156           |
|    total_timesteps          | 727040        |
| train/                      |               |
|    approx_kl                | 0.0049247504  |
|    approx_ln(kl)            | -5.313482     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.12         |
|    ln(policy_gradient_loss) | -6.64         |
|    loss                     | 0.12          |
|    n_updates                | 406           |
|    policy_gradient_loss     | 0.0013        |
|    std                      | 1.11          |
|    value_loss               | 0.444         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.57824934] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 13            |
|    time_elapsed             | 169           |
|    total_timesteps          | 729088        |
| train/                      |               |
|    approx_kl                | 0.005632003   |
|    approx_ln(kl)            | -5.1792903    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.1          |
|    ln(policy_gradient_loss) | -5.02         |
|    loss                     | 0.123         |
|    n_updates                | 407           |
|    policy_gradient_loss     | 0.00661       |
|    std                      | 1.11          |
|    value_loss               | 0.197         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31633934] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 14            |
|    time_elapsed             | 182           |
|    total_timesteps          | 731136        |
| train/                      |               |
|    approx_kl                | 0.006428097   |
|    approx_ln(kl)            | -5.0470767    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.58         |
|    ln(policy_gradient_loss) | -7.57         |
|    loss                     | 0.076         |
|    n_updates                | 408           |
|    policy_gradient_loss     | 0.000518      |
|    std                      | 1.11          |
|    value_loss               | 0.139         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5708554] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 15           |
|    time_elapsed             | 195          |
|    total_timesteps          | 733184       |
| train/                      |              |
|    approx_kl                | 0.005438824  |
|    approx_ln(kl)            | -5.2141924   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.04        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.01        |
|    ln(policy_gradient_loss) | -4.95        |
|    loss                     | 0.134        |
|    n_updates                | 409          |
|    policy_gradient_loss     | 0.00709      |
|    std                      | 1.11         |
|    value_loss               | 0.192        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.54860044] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 16            |
|    time_elapsed             | 208           |
|    total_timesteps          | 735232        |
| train/                      |               |
|    approx_kl                | 0.008667189   |
|    approx_ln(kl)            | -4.748211     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.9          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0548        |
|    n_updates                | 410           |
|    policy_gradient_loss     | -0.000232     |
|    std                      | 1.11          |
|    value_loss               | 0.219         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48343492] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 17            |
|    time_elapsed             | 221           |
|    total_timesteps          | 737280        |
| train/                      |               |
|    approx_kl                | 0.005834045   |
|    approx_ln(kl)            | -5.144045     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.63         |
|    ln(policy_gradient_loss) | -6.03         |
|    loss                     | 0.0719        |
|    n_updates                | 411           |
|    policy_gradient_loss     | 0.00241       |
|    std                      | 1.11          |
|    value_loss               | 0.275         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47407004] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 18            |
|    time_elapsed             | 234           |
|    total_timesteps          | 739328        |
| train/                      |               |
|    approx_kl                | 0.0037426853  |
|    approx_ln(kl)            | -5.587952     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.3          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0371        |
|    n_updates                | 412           |
|    policy_gradient_loss     | -0.00221      |
|    std                      | 1.11          |
|    value_loss               | 0.173         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.55287135] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 19            |
|    time_elapsed             | 247           |
|    total_timesteps          | 741376        |
| train/                      |               |
|    approx_kl                | 0.006815235   |
|    approx_ln(kl)            | -4.9885945    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.33         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0131        |
|    n_updates                | 413           |
|    policy_gradient_loss     | -0.00258      |
|    std                      | 1.11          |
|    value_loss               | 0.138         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43296918] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 20            |
|    time_elapsed             | 260           |
|    total_timesteps          | 743424        |
| train/                      |               |
|    approx_kl                | 0.008424727   |
|    approx_ln(kl)            | -4.776584     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.71         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0669        |
|    n_updates                | 414           |
|    policy_gradient_loss     | -0.00738      |
|    std                      | 1.11          |
|    value_loss               | 0.183         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32661524] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 21            |
|    time_elapsed             | 274           |
|    total_timesteps          | 745472        |
| train/                      |               |
|    approx_kl                | 0.008866189   |
|    approx_ln(kl)            | -4.72551      |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.76         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0636        |
|    n_updates                | 415           |
|    policy_gradient_loss     | -0.00818      |
|    std                      | 1.11          |
|    value_loss               | 0.209         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42161587] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 22            |
|    time_elapsed             | 289           |
|    total_timesteps          | 747520        |
| train/                      |               |
|    approx_kl                | 0.0065733157  |
|    approx_ln(kl)            | -5.024737     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.24         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0392        |
|    n_updates                | 416           |
|    policy_gradient_loss     | -0.00198      |
|    std                      | 1.1           |
|    value_loss               | 0.1           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28973275] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 23            |
|    time_elapsed             | 302           |
|    total_timesteps          | 749568        |
| train/                      |               |
|    approx_kl                | 0.0048698364  |
|    approx_ln(kl)            | -5.324695     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.06         |
|    ln(policy_gradient_loss) | -5.64         |
|    loss                     | 0.128         |
|    n_updates                | 417           |
|    policy_gradient_loss     | 0.00354       |
|    std                      | 1.1           |
|    value_loss               | 0.22          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37558737] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 24            |
|    time_elapsed             | 316           |
|    total_timesteps          | 751616        |
| train/                      |               |
|    approx_kl                | 0.0055726515  |
|    approx_ln(kl)            | -5.189884     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.976         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3            |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0497        |
|    n_updates                | 418           |
|    policy_gradient_loss     | -0.012        |
|    std                      | 1.1           |
|    value_loss               | 0.302         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28844464] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 25            |
|    time_elapsed             | 330           |
|    total_timesteps          | 753664        |
| train/                      |               |
|    approx_kl                | 0.0066414634  |
|    approx_ln(kl)            | -5.014423     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.18         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0415        |
|    n_updates                | 419           |
|    policy_gradient_loss     | -0.00519      |
|    std                      | 1.1           |
|    value_loss               | 0.17          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47519955] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 26            |
|    time_elapsed             | 344           |
|    total_timesteps          | 755712        |
| train/                      |               |
|    approx_kl                | 0.008442489   |
|    approx_ln(kl)            | -4.774478     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.78         |
|    ln(policy_gradient_loss) | -4.99         |
|    loss                     | 0.0623        |
|    n_updates                | 420           |
|    policy_gradient_loss     | 0.00678       |
|    std                      | 1.09          |
|    value_loss               | 0.0994        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42273965] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 27            |
|    time_elapsed             | 357           |
|    total_timesteps          | 757760        |
| train/                      |               |
|    approx_kl                | 0.007409428   |
|    approx_ln(kl)            | -4.905002     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.75         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.064         |
|    n_updates                | 421           |
|    policy_gradient_loss     | -0.00282      |
|    std                      | 1.09          |
|    value_loss               | 0.122         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.57194453] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 28            |
|    time_elapsed             | 371           |
|    total_timesteps          | 759808        |
| train/                      |               |
|    approx_kl                | 0.008093288   |
|    approx_ln(kl)            | -4.81672      |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.72         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.00892       |
|    n_updates                | 422           |
|    policy_gradient_loss     | -0.0147       |
|    std                      | 1.09          |
|    value_loss               | 0.0551        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4524974] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 29           |
|    time_elapsed             | 384          |
|    total_timesteps          | 761856       |
| train/                      |              |
|    approx_kl                | 0.008548362  |
|    approx_ln(kl)            | -4.762016    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.26        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0383       |
|    n_updates                | 423          |
|    policy_gradient_loss     | -0.00628     |
|    std                      | 1.09         |
|    value_loss               | 0.137        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4846337] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 30           |
|    time_elapsed             | 397          |
|    total_timesteps          | 763904       |
| train/                      |              |
|    approx_kl                | 0.0074365884 |
|    approx_ln(kl)            | -4.901343    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.29        |
|    ln(policy_gradient_loss) | -8.3         |
|    loss                     | 0.0373       |
|    n_updates                | 424          |
|    policy_gradient_loss     | 0.000249     |
|    std                      | 1.09         |
|    value_loss               | 0.105        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.532458]  |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 31           |
|    time_elapsed             | 409          |
|    total_timesteps          | 765952       |
| train/                      |              |
|    approx_kl                | 0.0060690036 |
|    approx_ln(kl)            | -5.104561    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.982        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.87        |
|    ln(policy_gradient_loss) | -4.93        |
|    loss                     | 0.154        |
|    n_updates                | 425          |
|    policy_gradient_loss     | 0.00722      |
|    std                      | 1.09         |
|    value_loss               | 0.236        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32520026] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 32            |
|    time_elapsed             | 423           |
|    total_timesteps          | 768000        |
| train/                      |               |
|    approx_kl                | 0.006872333   |
|    approx_ln(kl)            | -4.980252     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.44         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0871        |
|    n_updates                | 426           |
|    policy_gradient_loss     | -0.00781      |
|    std                      | 1.09          |
|    value_loss               | 0.211         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5433668] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 33           |
|    time_elapsed             | 436          |
|    total_timesteps          | 770048       |
| train/                      |              |
|    approx_kl                | 0.0074401926 |
|    approx_ln(kl)            | -4.9008584   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.4         |
|    ln(policy_gradient_loss) | -6.32        |
|    loss                     | 0.0906       |
|    n_updates                | 427          |
|    policy_gradient_loss     | 0.0018       |
|    std                      | 1.09         |
|    value_loss               | 0.162        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.24888414] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 34            |
|    time_elapsed             | 449           |
|    total_timesteps          | 772096        |
| train/                      |               |
|    approx_kl                | 0.009834434   |
|    approx_ln(kl)            | -4.6218653    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.43         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.239         |
|    n_updates                | 428           |
|    policy_gradient_loss     | -0.00174      |
|    std                      | 1.09          |
|    value_loss               | 0.612         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30898318] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 35            |
|    time_elapsed             | 462           |
|    total_timesteps          | 774144        |
| train/                      |               |
|    approx_kl                | 0.0076518604  |
|    approx_ln(kl)            | -4.8728065    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.75         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.174         |
|    n_updates                | 429           |
|    policy_gradient_loss     | -0.00575      |
|    std                      | 1.09          |
|    value_loss               | 0.491         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32981184] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 36            |
|    time_elapsed             | 475           |
|    total_timesteps          | 776192        |
| train/                      |               |
|    approx_kl                | 0.006158319   |
|    approx_ln(kl)            | -5.0899515    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.868         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.92         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.146         |
|    n_updates                | 430           |
|    policy_gradient_loss     | -8.14e-05     |
|    std                      | 1.09          |
|    value_loss               | 0.388         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.53490406] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 37            |
|    time_elapsed             | 489           |
|    total_timesteps          | 778240        |
| train/                      |               |
|    approx_kl                | 0.0068640206  |
|    approx_ln(kl)            | -4.981462     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.87         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.154         |
|    n_updates                | 431           |
|    policy_gradient_loss     | -0.007        |
|    std                      | 1.1           |
|    value_loss               | 0.505         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4354338] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 38           |
|    time_elapsed             | 502          |
|    total_timesteps          | 780288       |
| train/                      |              |
|    approx_kl                | 0.0043111215 |
|    approx_ln(kl)            | -5.446557    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.985        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.32        |
|    ln(policy_gradient_loss) | -7.13        |
|    loss                     | 0.0979       |
|    n_updates                | 432          |
|    policy_gradient_loss     | 0.000804     |
|    std                      | 1.1          |
|    value_loss               | 0.785        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.58175004] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 39            |
|    time_elapsed             | 516           |
|    total_timesteps          | 782336        |
| train/                      |               |
|    approx_kl                | 0.0055524274  |
|    approx_ln(kl)            | -5.19352      |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.8          |
|    ln(policy_gradient_loss) | -6.59         |
|    loss                     | 0.165         |
|    n_updates                | 433           |
|    policy_gradient_loss     | 0.00138       |
|    std                      | 1.1           |
|    value_loss               | 0.46          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5686209] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 40           |
|    time_elapsed             | 529          |
|    total_timesteps          | 784384       |
| train/                      |              |
|    approx_kl                | 0.0065372624 |
|    approx_ln(kl)            | -5.0302367   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.97        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0511       |
|    n_updates                | 434          |
|    policy_gradient_loss     | -0.00375     |
|    std                      | 1.1          |
|    value_loss               | 0.219        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.51836747] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 41            |
|    time_elapsed             | 542           |
|    total_timesteps          | 786432        |
| train/                      |               |
|    approx_kl                | 0.0055524623  |
|    approx_ln(kl)            | -5.193514     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.78         |
|    ln(policy_gradient_loss) | -5.37         |
|    loss                     | 0.168         |
|    n_updates                | 435           |
|    policy_gradient_loss     | 0.00466       |
|    std                      | 1.1           |
|    value_loss               | 0.351         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3963051] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 42           |
|    time_elapsed             | 555          |
|    total_timesteps          | 788480       |
| train/                      |              |
|    approx_kl                | 0.0049429447 |
|    approx_ln(kl)            | -5.309794    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.52        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0803       |
|    n_updates                | 436          |
|    policy_gradient_loss     | -0.00511     |
|    std                      | 1.1          |
|    value_loss               | 0.167        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5174288] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 43           |
|    time_elapsed             | 568          |
|    total_timesteps          | 790528       |
| train/                      |              |
|    approx_kl                | 0.007301531  |
|    approx_ln(kl)            | -4.919671    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.993        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.33        |
|    ln(policy_gradient_loss) | -5.93        |
|    loss                     | 0.0972       |
|    n_updates                | 437          |
|    policy_gradient_loss     | 0.00265      |
|    std                      | 1.1          |
|    value_loss               | 0.21         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29478252] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 44            |
|    time_elapsed             | 581           |
|    total_timesteps          | 792576        |
| train/                      |               |
|    approx_kl                | 0.0059808767  |
|    approx_ln(kl)            | -5.1191883    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.71         |
|    ln(policy_gradient_loss) | -5.39         |
|    loss                     | 0.0667        |
|    n_updates                | 438           |
|    policy_gradient_loss     | 0.00458       |
|    std                      | 1.09          |
|    value_loss               | 0.0711        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30392945] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 45            |
|    time_elapsed             | 595           |
|    total_timesteps          | 794624        |
| train/                      |               |
|    approx_kl                | 0.007977294   |
|    approx_ln(kl)            | -4.8311563    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.61         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.201         |
|    n_updates                | 439           |
|    policy_gradient_loss     | -0.00203      |
|    std                      | 1.09          |
|    value_loss               | 0.351         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25015408] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 46            |
|    time_elapsed             | 608           |
|    total_timesteps          | 796672        |
| train/                      |               |
|    approx_kl                | 0.0057514072  |
|    approx_ln(kl)            | -5.158311     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.94         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0529        |
|    n_updates                | 440           |
|    policy_gradient_loss     | -0.00654      |
|    std                      | 1.09          |
|    value_loss               | 0.177         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.52001214] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 47            |
|    time_elapsed             | 621           |
|    total_timesteps          | 798720        |
| train/                      |               |
|    approx_kl                | 0.0062151332  |
|    approx_ln(kl)            | -5.080768     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.57         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0768        |
|    n_updates                | 441           |
|    policy_gradient_loss     | -0.0139       |
|    std                      | 1.09          |
|    value_loss               | 0.296         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.497106]  |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 48           |
|    time_elapsed             | 634          |
|    total_timesteps          | 800768       |
| train/                      |              |
|    approx_kl                | 0.0070009637 |
|    approx_ln(kl)            | -4.9617076   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.58        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0278       |
|    n_updates                | 442          |
|    policy_gradient_loss     | -0.00263     |
|    std                      | 1.09         |
|    value_loss               | 0.0869       |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40200743] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 49            |
|    time_elapsed             | 648           |
|    total_timesteps          | 802816        |
| train/                      |               |
|    approx_kl                | 0.005780655   |
|    approx_ln(kl)            | -5.1532383    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.95         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0525        |
|    n_updates                | 443           |
|    policy_gradient_loss     | -0.0099       |
|    std                      | 1.09          |
|    value_loss               | 0.221         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
--------------------------------------
| reward             | [-0.41396424] |
| time/              |               |
|    fps             | 157           |
|    iterations      | 1             |
|    time_elapsed    | 12            |
|    total_timesteps | 804864        |
--------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.529045] |
| time/                       |             |
|    fps                      | 155         |
|    iterations               | 2           |
|    time_elapsed             | 26          |
|    total_timesteps          | 806912      |
| train/                      |             |
|    approx_kl                | 0.006880355 |
|    approx_ln(kl)            | -4.979085   |
|    clip_range               | 0.2         |
|    entropy_loss             | -3          |
|    explained_variance       | 0.997       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -2.9        |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.0551      |
|    n_updates                | 445         |
|    policy_gradient_loss     | -0.00313    |
|    std                      | 1.09        |
|    value_loss               | 0.12        |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.4935326] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 3            |
|    time_elapsed             | 39           |
|    total_timesteps          | 808960       |
| train/                      |              |
|    approx_kl                | 0.009778826  |
|    approx_ln(kl)            | -4.627536    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.39        |
|    ln(policy_gradient_loss) | -5.86        |
|    loss                     | 0.248        |
|    n_updates                | 446          |
|    policy_gradient_loss     | 0.00284      |
|    std                      | 1.09         |
|    value_loss               | 0.383        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37535465] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 4             |
|    time_elapsed             | 52            |
|    total_timesteps          | 811008        |
| train/                      |               |
|    approx_kl                | 0.0074618966  |
|    approx_ln(kl)            | -4.897946     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.86         |
|    ln(policy_gradient_loss) | -5.66         |
|    loss                     | 0.0572        |
|    n_updates                | 447           |
|    policy_gradient_loss     | 0.00347       |
|    std                      | 1.09          |
|    value_loss               | 0.157         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2899905] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 5            |
|    time_elapsed             | 66           |
|    total_timesteps          | 813056       |
| train/                      |              |
|    approx_kl                | 0.007931426  |
|    approx_ln(kl)            | -4.8369226   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.36        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0948       |
|    n_updates                | 448          |
|    policy_gradient_loss     | -0.00151     |
|    std                      | 1.09         |
|    value_loss               | 0.284        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40957382] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 6             |
|    time_elapsed             | 79            |
|    total_timesteps          | 815104        |
| train/                      |               |
|    approx_kl                | 0.00637876    |
|    approx_ln(kl)            | -5.0547814    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.961         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.01         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.134         |
|    n_updates                | 449           |
|    policy_gradient_loss     | -0.0041       |
|    std                      | 1.09          |
|    value_loss               | 0.33          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35857335] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 7             |
|    time_elapsed             | 93            |
|    total_timesteps          | 817152        |
| train/                      |               |
|    approx_kl                | 0.0052411556  |
|    approx_ln(kl)            | -5.251213     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.6          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.01          |
|    n_updates                | 450           |
|    policy_gradient_loss     | -0.0019       |
|    std                      | 1.09          |
|    value_loss               | 0.258         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49395737] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 8             |
|    time_elapsed             | 106           |
|    total_timesteps          | 819200        |
| train/                      |               |
|    approx_kl                | 0.0070031993  |
|    approx_ln(kl)            | -4.961388     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.98         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0507        |
|    n_updates                | 451           |
|    policy_gradient_loss     | -0.00336      |
|    std                      | 1.09          |
|    value_loss               | 0.172         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.42253408] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 9             |
|    time_elapsed             | 119           |
|    total_timesteps          | 821248        |
| train/                      |               |
|    approx_kl                | 0.007067296   |
|    approx_ln(kl)            | -4.952277     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.72         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0661        |
|    n_updates                | 452           |
|    policy_gradient_loss     | -0.00922      |
|    std                      | 1.1           |
|    value_loss               | 0.226         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44691738] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 10            |
|    time_elapsed             | 133           |
|    total_timesteps          | 823296        |
| train/                      |               |
|    approx_kl                | 0.006585732   |
|    approx_ln(kl)            | -5.0228496    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.778        |
|    ln(policy_gradient_loss) | -4.42         |
|    loss                     | 0.459         |
|    n_updates                | 453           |
|    policy_gradient_loss     | 0.0121        |
|    std                      | 1.1           |
|    value_loss               | 0.715         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4593337] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 11           |
|    time_elapsed             | 151          |
|    total_timesteps          | 825344       |
| train/                      |              |
|    approx_kl                | 0.006518621  |
|    approx_ln(kl)            | -5.0330925   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.945        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.47        |
|    ln(policy_gradient_loss) | -4.41        |
|    loss                     | 0.23         |
|    n_updates                | 454          |
|    policy_gradient_loss     | 0.0121       |
|    std                      | 1.1          |
|    value_loss               | 0.88         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3874529] |
| time/                       |              |
|    fps                      | 144          |
|    iterations               | 12           |
|    time_elapsed             | 169          |
|    total_timesteps          | 827392       |
| train/                      |              |
|    approx_kl                | 0.0066807573 |
|    approx_ln(kl)            | -5.008524    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.2         |
|    ln(policy_gradient_loss) | -8.17        |
|    loss                     | 0.301        |
|    n_updates                | 455          |
|    policy_gradient_loss     | 0.000283     |
|    std                      | 1.1          |
|    value_loss               | 0.741        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.440984] |
| time/                       |             |
|    fps                      | 139         |
|    iterations               | 13          |
|    time_elapsed             | 190         |
|    total_timesteps          | 829440      |
| train/                      |             |
|    approx_kl                | 0.006198271 |
|    approx_ln(kl)            | -5.0834846  |
|    clip_range               | 0.2         |
|    entropy_loss             | -3.03       |
|    explained_variance       | 0.988       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -1.7        |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.183       |
|    n_updates                | 456         |
|    policy_gradient_loss     | -0.00108    |
|    std                      | 1.1         |
|    value_loss               | 0.541       |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.6451956] |
| time/                       |              |
|    fps                      | 136          |
|    iterations               | 14           |
|    time_elapsed             | 209          |
|    total_timesteps          | 831488       |
| train/                      |              |
|    approx_kl                | 0.0071126446 |
|    approx_ln(kl)            | -4.9458814   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.983        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.957       |
|    ln(policy_gradient_loss) | -5.67        |
|    loss                     | 0.384        |
|    n_updates                | 457          |
|    policy_gradient_loss     | 0.00344      |
|    std                      | 1.1          |
|    value_loss               | 0.898        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.43292946] |
| time/                       |               |
|    fps                      | 137           |
|    iterations               | 15            |
|    time_elapsed             | 222           |
|    total_timesteps          | 833536        |
| train/                      |               |
|    approx_kl                | 0.0072640395  |
|    approx_ln(kl)            | -4.924819     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.98         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.137         |
|    n_updates                | 458           |
|    policy_gradient_loss     | -0.0107       |
|    std                      | 1.1           |
|    value_loss               | 0.344         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.53317]  |
| time/                       |             |
|    fps                      | 138         |
|    iterations               | 16          |
|    time_elapsed             | 235         |
|    total_timesteps          | 835584      |
| train/                      |             |
|    approx_kl                | 0.005981435 |
|    approx_ln(kl)            | -5.119095   |
|    clip_range               | 0.2         |
|    entropy_loss             | -3.02       |
|    explained_variance       | 0.988       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -1.44       |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.237       |
|    n_updates                | 459         |
|    policy_gradient_loss     | -0.00895    |
|    std                      | 1.1         |
|    value_loss               | 0.488       |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41830403] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 17            |
|    time_elapsed             | 249           |
|    total_timesteps          | 837632        |
| train/                      |               |
|    approx_kl                | 0.0063055735  |
|    approx_ln(kl)            | -5.0663214    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.68         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.186         |
|    n_updates                | 460           |
|    policy_gradient_loss     | -0.00255      |
|    std                      | 1.1           |
|    value_loss               | 0.48          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49123746] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 18            |
|    time_elapsed             | 262           |
|    total_timesteps          | 839680        |
| train/                      |               |
|    approx_kl                | 0.008604964   |
|    approx_ln(kl)            | -4.755416     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.7          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.183         |
|    n_updates                | 461           |
|    policy_gradient_loss     | -0.012        |
|    std                      | 1.1           |
|    value_loss               | 0.44          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40482184] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 19            |
|    time_elapsed             | 275           |
|    total_timesteps          | 841728        |
| train/                      |               |
|    approx_kl                | 0.007628903   |
|    approx_ln(kl)            | -4.875811     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.873        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.418         |
|    n_updates                | 462           |
|    policy_gradient_loss     | -0.00985      |
|    std                      | 1.1           |
|    value_loss               | 0.454         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27219033] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 20            |
|    time_elapsed             | 288           |
|    total_timesteps          | 843776        |
| train/                      |               |
|    approx_kl                | 0.007727807   |
|    approx_ln(kl)            | -4.8629303    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.17         |
|    ln(policy_gradient_loss) | -7.83         |
|    loss                     | 0.114         |
|    n_updates                | 463           |
|    policy_gradient_loss     | 0.000398      |
|    std                      | 1.1           |
|    value_loss               | 0.42          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24656399] |
| time/                       |               |
|    fps                      | 142           |
|    iterations               | 21            |
|    time_elapsed             | 301           |
|    total_timesteps          | 845824        |
| train/                      |               |
|    approx_kl                | 0.0057190377  |
|    approx_ln(kl)            | -5.1639547    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.79         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0615        |
|    n_updates                | 464           |
|    policy_gradient_loss     | -0.000388     |
|    std                      | 1.1           |
|    value_loss               | 0.239         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5061394] |
| time/                       |              |
|    fps                      | 143          |
|    iterations               | 22           |
|    time_elapsed             | 314          |
|    total_timesteps          | 847872       |
| train/                      |              |
|    approx_kl                | 0.0050616497 |
|    approx_ln(kl)            | -5.2860627   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.87        |
|    ln(policy_gradient_loss) | -5.97        |
|    loss                     | 0.155        |
|    n_updates                | 465          |
|    policy_gradient_loss     | 0.00257      |
|    std                      | 1.1          |
|    value_loss               | 0.262        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.493145]  |
| time/                       |              |
|    fps                      | 143          |
|    iterations               | 23           |
|    time_elapsed             | 327          |
|    total_timesteps          | 849920       |
| train/                      |              |
|    approx_kl                | 0.0042570625 |
|    approx_ln(kl)            | -5.459176    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.84        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0584       |
|    n_updates                | 466          |
|    policy_gradient_loss     | -0.000935    |
|    std                      | 1.1          |
|    value_loss               | 0.189        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.6209593] |
| time/                       |              |
|    fps                      | 144          |
|    iterations               | 24           |
|    time_elapsed             | 340          |
|    total_timesteps          | 851968       |
| train/                      |              |
|    approx_kl                | 0.0063526086 |
|    approx_ln(kl)            | -5.05889     |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.995        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.63        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.195        |
|    n_updates                | 467          |
|    policy_gradient_loss     | -0.00617     |
|    std                      | 1.1          |
|    value_loss               | 0.575        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4476468] |
| time/                       |              |
|    fps                      | 144          |
|    iterations               | 25           |
|    time_elapsed             | 353          |
|    total_timesteps          | 854016       |
| train/                      |              |
|    approx_kl                | 0.0068171984 |
|    approx_ln(kl)            | -4.9883065   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.4         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0912       |
|    n_updates                | 468          |
|    policy_gradient_loss     | -0.00734     |
|    std                      | 1.1          |
|    value_loss               | 0.239        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43474552] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 26            |
|    time_elapsed             | 366           |
|    total_timesteps          | 856064        |
| train/                      |               |
|    approx_kl                | 0.0073072584  |
|    approx_ln(kl)            | -4.918887     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.46         |
|    ln(policy_gradient_loss) | -5.26         |
|    loss                     | 0.0316        |
|    n_updates                | 469           |
|    policy_gradient_loss     | 0.00519       |
|    std                      | 1.09          |
|    value_loss               | 0.168         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.50877744] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 27            |
|    time_elapsed             | 379           |
|    total_timesteps          | 858112        |
| train/                      |               |
|    approx_kl                | 0.0052568642  |
|    approx_ln(kl)            | -5.2482204    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.11         |
|    ln(policy_gradient_loss) | -8.87         |
|    loss                     | 0.122         |
|    n_updates                | 470           |
|    policy_gradient_loss     | 0.000141      |
|    std                      | 1.09          |
|    value_loss               | 0.35          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28964815] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 28            |
|    time_elapsed             | 392           |
|    total_timesteps          | 860160        |
| train/                      |               |
|    approx_kl                | 0.005421521   |
|    approx_ln(kl)            | -5.2173786    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.05         |
|    ln(policy_gradient_loss) | -6.72         |
|    loss                     | 0.0475        |
|    n_updates                | 471           |
|    policy_gradient_loss     | 0.00121       |
|    std                      | 1.09          |
|    value_loss               | 0.202         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38632813] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 29            |
|    time_elapsed             | 405           |
|    total_timesteps          | 862208        |
| train/                      |               |
|    approx_kl                | 0.007084701   |
|    approx_ln(kl)            | -4.9498177    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.76         |
|    ln(policy_gradient_loss) | -5.08         |
|    loss                     | 0.0635        |
|    n_updates                | 472           |
|    policy_gradient_loss     | 0.00621       |
|    std                      | 1.09          |
|    value_loss               | 0.165         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.55893606] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 30            |
|    time_elapsed             | 418           |
|    total_timesteps          | 864256        |
| train/                      |               |
|    approx_kl                | 0.005941712   |
|    approx_ln(kl)            | -5.125758     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.61         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.2           |
|    n_updates                | 473           |
|    policy_gradient_loss     | -0.00329      |
|    std                      | 1.09          |
|    value_loss               | 0.313         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25313807] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 31            |
|    time_elapsed             | 431           |
|    total_timesteps          | 866304        |
| train/                      |               |
|    approx_kl                | 0.0063446676  |
|    approx_ln(kl)            | -5.0601406    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.52         |
|    ln(policy_gradient_loss) | -6.85         |
|    loss                     | 0.0802        |
|    n_updates                | 474           |
|    policy_gradient_loss     | 0.00106       |
|    std                      | 1.09          |
|    value_loss               | 0.237         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.55450684] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 32            |
|    time_elapsed             | 444           |
|    total_timesteps          | 868352        |
| train/                      |               |
|    approx_kl                | 0.007960927   |
|    approx_ln(kl)            | -4.83321      |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.47         |
|    ln(policy_gradient_loss) | -4.67         |
|    loss                     | 0.0842        |
|    n_updates                | 475           |
|    policy_gradient_loss     | 0.00937       |
|    std                      | 1.09          |
|    value_loss               | 0.165         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47387946] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 33            |
|    time_elapsed             | 457           |
|    total_timesteps          | 870400        |
| train/                      |               |
|    approx_kl                | 0.0035249637  |
|    approx_ln(kl)            | -5.6478853    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.47         |
|    ln(policy_gradient_loss) | -5.98         |
|    loss                     | 0.0846        |
|    n_updates                | 476           |
|    policy_gradient_loss     | 0.00253       |
|    std                      | 1.09          |
|    value_loss               | 0.277         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29213396] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 34            |
|    time_elapsed             | 470           |
|    total_timesteps          | 872448        |
| train/                      |               |
|    approx_kl                | 0.008297056   |
|    approx_ln(kl)            | -4.7918544    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.28         |
|    ln(policy_gradient_loss) | -7.43         |
|    loss                     | 0.102         |
|    n_updates                | 477           |
|    policy_gradient_loss     | 0.000592      |
|    std                      | 1.09          |
|    value_loss               | 0.185         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4003153] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 35           |
|    time_elapsed             | 483          |
|    total_timesteps          | 874496       |
| train/                      |              |
|    approx_kl                | 0.006841369  |
|    approx_ln(kl)            | -4.9847674   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.08        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.046        |
|    n_updates                | 478          |
|    policy_gradient_loss     | -0.00412     |
|    std                      | 1.09         |
|    value_loss               | 0.0978       |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39904994] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 36            |
|    time_elapsed             | 496           |
|    total_timesteps          | 876544        |
| train/                      |               |
|    approx_kl                | 0.006738124   |
|    approx_ln(kl)            | -4.999974     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.52         |
|    ln(policy_gradient_loss) | -5.65         |
|    loss                     | 0.0803        |
|    n_updates                | 479           |
|    policy_gradient_loss     | 0.00351       |
|    std                      | 1.09          |
|    value_loss               | 0.28          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3997998] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 37           |
|    time_elapsed             | 509          |
|    total_timesteps          | 878592       |
| train/                      |              |
|    approx_kl                | 0.00797221   |
|    approx_ln(kl)            | -4.831794    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -4.06        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0173       |
|    n_updates                | 480          |
|    policy_gradient_loss     | -0.00366     |
|    std                      | 1.09         |
|    value_loss               | 0.0932       |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.54074216] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 38            |
|    time_elapsed             | 523           |
|    total_timesteps          | 880640        |
| train/                      |               |
|    approx_kl                | 0.007174386   |
|    approx_ln(kl)            | -4.937238     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.76         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.172         |
|    n_updates                | 481           |
|    policy_gradient_loss     | -0.0101       |
|    std                      | 1.09          |
|    value_loss               | 0.437         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4796575] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 39           |
|    time_elapsed             | 536          |
|    total_timesteps          | 882688       |
| train/                      |              |
|    approx_kl                | 0.008386808  |
|    approx_ln(kl)            | -4.781095    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.6         |
|    ln(policy_gradient_loss) | -5.82        |
|    loss                     | 0.0746       |
|    n_updates                | 482          |
|    policy_gradient_loss     | 0.00296      |
|    std                      | 1.09         |
|    value_loss               | 0.193        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4693307] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 40           |
|    time_elapsed             | 550          |
|    total_timesteps          | 884736       |
| train/                      |              |
|    approx_kl                | 0.007605904  |
|    approx_ln(kl)            | -4.8788304   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.2         |
|    ln(policy_gradient_loss) | -4.48        |
|    loss                     | 0.111        |
|    n_updates                | 483          |
|    policy_gradient_loss     | 0.0114       |
|    std                      | 1.09         |
|    value_loss               | 0.173        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28026417] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 41            |
|    time_elapsed             | 563           |
|    total_timesteps          | 886784        |
| train/                      |               |
|    approx_kl                | 0.0062115267  |
|    approx_ln(kl)            | -5.0813484    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.72         |
|    ln(policy_gradient_loss) | -6.85         |
|    loss                     | 0.0662        |
|    n_updates                | 484           |
|    policy_gradient_loss     | 0.00106       |
|    std                      | 1.09          |
|    value_loss               | 0.206         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4530543] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 42           |
|    time_elapsed             | 576          |
|    total_timesteps          | 888832       |
| train/                      |              |
|    approx_kl                | 0.0075702253 |
|    approx_ln(kl)            | -4.8835325   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.51        |
|    ln(policy_gradient_loss) | -6.42        |
|    loss                     | 0.221        |
|    n_updates                | 485          |
|    policy_gradient_loss     | 0.00162      |
|    std                      | 1.09         |
|    value_loss               | 0.512        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5114283] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 43           |
|    time_elapsed             | 589          |
|    total_timesteps          | 890880       |
| train/                      |              |
|    approx_kl                | 0.0059268153 |
|    approx_ln(kl)            | -5.1282682   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.98        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0187       |
|    n_updates                | 486          |
|    policy_gradient_loss     | -0.00348     |
|    std                      | 1.09         |
|    value_loss               | 0.173        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4995134] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 44           |
|    time_elapsed             | 602          |
|    total_timesteps          | 892928       |
| train/                      |              |
|    approx_kl                | 0.005234915  |
|    approx_ln(kl)            | -5.2524047   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.65        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0708       |
|    n_updates                | 487          |
|    policy_gradient_loss     | -0.000105    |
|    std                      | 1.09         |
|    value_loss               | 0.157        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3706602] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 45           |
|    time_elapsed             | 615          |
|    total_timesteps          | 894976       |
| train/                      |              |
|    approx_kl                | 0.009472417  |
|    approx_ln(kl)            | -4.6593714   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -5.05        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.00641      |
|    n_updates                | 488          |
|    policy_gradient_loss     | -0.00397     |
|    std                      | 1.09         |
|    value_loss               | 0.0974       |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5164097] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 46           |
|    time_elapsed             | 628          |
|    total_timesteps          | 897024       |
| train/                      |              |
|    approx_kl                | 0.0074492567 |
|    approx_ln(kl)            | -4.899641    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.25        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0386       |
|    n_updates                | 489          |
|    policy_gradient_loss     | -0.00998     |
|    std                      | 1.09         |
|    value_loss               | 0.0867       |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5065758] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 47           |
|    time_elapsed             | 641          |
|    total_timesteps          | 899072       |
| train/                      |              |
|    approx_kl                | 0.007670545  |
|    approx_ln(kl)            | -4.8703675   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.82        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0597       |
|    n_updates                | 490          |
|    policy_gradient_loss     | -0.00232     |
|    std                      | 1.09         |
|    value_loss               | 0.198        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.33999377] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 48            |
|    time_elapsed             | 654           |
|    total_timesteps          | 901120        |
| train/                      |               |
|    approx_kl                | 0.0135203935  |
|    approx_ln(kl)            | -4.303556     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.966         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.63         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0719        |
|    n_updates                | 491           |
|    policy_gradient_loss     | -0.0135       |
|    std                      | 1.09          |
|    value_loss               | 0.27          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5679378] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 49           |
|    time_elapsed             | 667          |
|    total_timesteps          | 903168       |
| train/                      |              |
|    approx_kl                | 0.008235309  |
|    approx_ln(kl)            | -4.7993245   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.984        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.44        |
|    ln(policy_gradient_loss) | -4.38        |
|    loss                     | 0.236        |
|    n_updates                | 492          |
|    policy_gradient_loss     | 0.0125       |
|    std                      | 1.09         |
|    value_loss               | 0.582        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------
| reward             | [-0.5882766] |
| time/              |              |
|    fps             | 161          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 905216       |
-------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4107056] |
| time/                       |              |
|    fps                      | 159          |
|    iterations               | 2            |
|    time_elapsed             | 25           |
|    total_timesteps          | 907264       |
| train/                      |              |
|    approx_kl                | 0.008820165  |
|    approx_ln(kl)            | -4.730715    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.76        |
|    ln(policy_gradient_loss) | -4.61        |
|    loss                     | 0.172        |
|    n_updates                | 494          |
|    policy_gradient_loss     | 0.00993      |
|    std                      | 1.09         |
|    value_loss               | 0.422        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.38744268] |
| time/                       |               |
|    fps                      | 159           |
|    iterations               | 3             |
|    time_elapsed             | 38            |
|    total_timesteps          | 909312        |
| train/                      |               |
|    approx_kl                | 0.008257268   |
|    approx_ln(kl)            | -4.7966614    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.44         |
|    ln(policy_gradient_loss) | -10.2         |
|    loss                     | 0.0868        |
|    n_updates                | 495           |
|    policy_gradient_loss     | 3.84e-05      |
|    std                      | 1.09          |
|    value_loss               | 0.212         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.55578285] |
| time/                       |               |
|    fps                      | 159           |
|    iterations               | 4             |
|    time_elapsed             | 51            |
|    total_timesteps          | 911360        |
| train/                      |               |
|    approx_kl                | 0.006529869   |
|    approx_ln(kl)            | -5.0313683    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.78         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0229        |
|    n_updates                | 496           |
|    policy_gradient_loss     | -0.0122       |
|    std                      | 1.09          |
|    value_loss               | 0.22          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23323114] |
| time/                       |               |
|    fps                      | 159           |
|    iterations               | 5             |
|    time_elapsed             | 64            |
|    total_timesteps          | 913408        |
| train/                      |               |
|    approx_kl                | 0.0071306154  |
|    approx_ln(kl)            | -4.943358     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.6          |
|    ln(policy_gradient_loss) | -5.67         |
|    loss                     | 0.0746        |
|    n_updates                | 497           |
|    policy_gradient_loss     | 0.00345       |
|    std                      | 1.09          |
|    value_loss               | 0.127         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4888296] |
| time/                       |              |
|    fps                      | 158          |
|    iterations               | 6            |
|    time_elapsed             | 77           |
|    total_timesteps          | 915456       |
| train/                      |              |
|    approx_kl                | 0.005206833  |
|    approx_ln(kl)            | -5.2577834   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.18        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.113        |
|    n_updates                | 498          |
|    policy_gradient_loss     | -0.00811     |
|    std                      | 1.09         |
|    value_loss               | 0.208        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.51172745] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 7             |
|    time_elapsed             | 90            |
|    total_timesteps          | 917504        |
| train/                      |               |
|    approx_kl                | 0.0075601595  |
|    approx_ln(kl)            | -4.884863     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.43         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0322        |
|    n_updates                | 499           |
|    policy_gradient_loss     | -0.00365      |
|    std                      | 1.09          |
|    value_loss               | 0.125         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.56432307] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 8             |
|    time_elapsed             | 104           |
|    total_timesteps          | 919552        |
| train/                      |               |
|    approx_kl                | 0.0060276305  |
|    approx_ln(kl)            | -5.1114016    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.24         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0391        |
|    n_updates                | 500           |
|    policy_gradient_loss     | -6.29e-05     |
|    std                      | 1.09          |
|    value_loss               | 0.174         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.53358907] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 9             |
|    time_elapsed             | 116           |
|    total_timesteps          | 921600        |
| train/                      |               |
|    approx_kl                | 0.007321264   |
|    approx_ln(kl)            | -4.916972     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.6          |
|    ln(policy_gradient_loss) | -6.48         |
|    loss                     | 0.0743        |
|    n_updates                | 501           |
|    policy_gradient_loss     | 0.00154       |
|    std                      | 1.09          |
|    value_loss               | 0.201         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43770477] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 10            |
|    time_elapsed             | 129           |
|    total_timesteps          | 923648        |
| train/                      |               |
|    approx_kl                | 0.004202548   |
|    approx_ln(kl)            | -5.4720645    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.65         |
|    ln(policy_gradient_loss) | -6.9          |
|    loss                     | 0.0703        |
|    n_updates                | 502           |
|    policy_gradient_loss     | 0.00101       |
|    std                      | 1.09          |
|    value_loss               | 0.142         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4539369] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 11           |
|    time_elapsed             | 142          |
|    total_timesteps          | 925696       |
| train/                      |              |
|    approx_kl                | 0.006592758  |
|    approx_ln(kl)            | -5.0217834   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.89        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0555       |
|    n_updates                | 503          |
|    policy_gradient_loss     | -3.22e-05    |
|    std                      | 1.09         |
|    value_loss               | 0.206        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41113868] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 12            |
|    time_elapsed             | 155           |
|    total_timesteps          | 927744        |
| train/                      |               |
|    approx_kl                | 0.008194229   |
|    approx_ln(kl)            | -4.804325     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.25         |
|    ln(policy_gradient_loss) | -5.92         |
|    loss                     | 0.105         |
|    n_updates                | 504           |
|    policy_gradient_loss     | 0.00268       |
|    std                      | 1.09          |
|    value_loss               | 0.16          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41145322] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 13            |
|    time_elapsed             | 168           |
|    total_timesteps          | 929792        |
| train/                      |               |
|    approx_kl                | 0.006094709   |
|    approx_ln(kl)            | -5.100334     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.895         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.79          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 5.99          |
|    n_updates                | 505           |
|    policy_gradient_loss     | -0.00444      |
|    std                      | 1.09          |
|    value_loss               | 9.96          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.46840698] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 14            |
|    time_elapsed             | 181           |
|    total_timesteps          | 931840        |
| train/                      |               |
|    approx_kl                | 0.006864913   |
|    approx_ln(kl)            | -4.981332     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.65         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0709        |
|    n_updates                | 506           |
|    policy_gradient_loss     | -0.000335     |
|    std                      | 1.09          |
|    value_loss               | 0.125         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3654972] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 15           |
|    time_elapsed             | 194          |
|    total_timesteps          | 933888       |
| train/                      |              |
|    approx_kl                | 0.0031732502 |
|    approx_ln(kl)            | -5.752999    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.71        |
|    ln(policy_gradient_loss) | -5.46        |
|    loss                     | 0.181        |
|    n_updates                | 507          |
|    policy_gradient_loss     | 0.00426      |
|    std                      | 1.1          |
|    value_loss               | 0.265        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38413167] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 16            |
|    time_elapsed             | 207           |
|    total_timesteps          | 935936        |
| train/                      |               |
|    approx_kl                | 0.007459791   |
|    approx_ln(kl)            | -4.8982277    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.4          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0908        |
|    n_updates                | 508           |
|    policy_gradient_loss     | -0.00142      |
|    std                      | 1.1           |
|    value_loss               | 0.206         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.5240701] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 17           |
|    time_elapsed             | 220          |
|    total_timesteps          | 937984       |
| train/                      |              |
|    approx_kl                | 0.011538034  |
|    approx_ln(kl)            | -4.462106    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.993        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.05        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0471       |
|    n_updates                | 509          |
|    policy_gradient_loss     | -0.00422     |
|    std                      | 1.1          |
|    value_loss               | 0.0907       |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4794398] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 18           |
|    time_elapsed             | 234          |
|    total_timesteps          | 940032       |
| train/                      |              |
|    approx_kl                | 0.0058256593 |
|    approx_ln(kl)            | -5.145483    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.65        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0261       |
|    n_updates                | 510          |
|    policy_gradient_loss     | -0.00996     |
|    std                      | 1.1          |
|    value_loss               | 0.117        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.53428704] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 19            |
|    time_elapsed             | 247           |
|    total_timesteps          | 942080        |
| train/                      |               |
|    approx_kl                | 0.0076284944  |
|    approx_ln(kl)            | -4.875865     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.968         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.51         |
|    ln(policy_gradient_loss) | -9.97         |
|    loss                     | 0.03          |
|    n_updates                | 511           |
|    policy_gradient_loss     | 4.68e-05      |
|    std                      | 1.1           |
|    value_loss               | 0.201         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47691604] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 20            |
|    time_elapsed             | 261           |
|    total_timesteps          | 944128        |
| train/                      |               |
|    approx_kl                | 0.006138305   |
|    approx_ln(kl)            | -5.0932064    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.4          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0906        |
|    n_updates                | 512           |
|    policy_gradient_loss     | -0.00269      |
|    std                      | 1.1           |
|    value_loss               | 0.156         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37223524] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 21            |
|    time_elapsed             | 274           |
|    total_timesteps          | 946176        |
| train/                      |               |
|    approx_kl                | 0.006602382   |
|    approx_ln(kl)            | -5.0203247    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -5.55         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.00387       |
|    n_updates                | 513           |
|    policy_gradient_loss     | -0.00334      |
|    std                      | 1.1           |
|    value_loss               | 0.115         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.55666834] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 22            |
|    time_elapsed             | 287           |
|    total_timesteps          | 948224        |
| train/                      |               |
|    approx_kl                | 0.005172371   |
|    approx_ln(kl)            | -5.2644243    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -5.97         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.00257       |
|    n_updates                | 514           |
|    policy_gradient_loss     | -0.00138      |
|    std                      | 1.1           |
|    value_loss               | 0.0768        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45881775] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 23            |
|    time_elapsed             | 301           |
|    total_timesteps          | 950272        |
| train/                      |               |
|    approx_kl                | 0.007535291   |
|    approx_ln(kl)            | -4.888158     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.22         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0147        |
|    n_updates                | 515           |
|    policy_gradient_loss     | -0.00784      |
|    std                      | 1.1           |
|    value_loss               | 0.126         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.534818]  |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 24           |
|    time_elapsed             | 314          |
|    total_timesteps          | 952320       |
| train/                      |              |
|    approx_kl                | 0.0052253823 |
|    approx_ln(kl)            | -5.254227    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -4.06        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0172       |
|    n_updates                | 516          |
|    policy_gradient_loss     | -0.00806     |
|    std                      | 1.1          |
|    value_loss               | 0.221        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34554672] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 25            |
|    time_elapsed             | 327           |
|    total_timesteps          | 954368        |
| train/                      |               |
|    approx_kl                | 0.007446771   |
|    approx_ln(kl)            | -4.899975     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.963         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.392        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.676         |
|    n_updates                | 517           |
|    policy_gradient_loss     | -0.000218     |
|    std                      | 1.1           |
|    value_loss               | 4.41          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26988629] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 26            |
|    time_elapsed             | 340           |
|    total_timesteps          | 956416        |
| train/                      |               |
|    approx_kl                | 0.0038069983  |
|    approx_ln(kl)            | -5.5709143    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -5.91         |
|    ln(policy_gradient_loss) | -6.55         |
|    loss                     | 0.00271       |
|    n_updates                | 518           |
|    policy_gradient_loss     | 0.00143       |
|    std                      | 1.09          |
|    value_loss               | 0.132         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.4519681] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 27           |
|    time_elapsed             | 353          |
|    total_timesteps          | 958464       |
| train/                      |              |
|    approx_kl                | 0.012694553  |
|    approx_ln(kl)            | -4.3665824   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.91        |
|    ln(policy_gradient_loss) | -5.33        |
|    loss                     | 0.0544       |
|    n_updates                | 519          |
|    policy_gradient_loss     | 0.00484      |
|    std                      | 1.09         |
|    value_loss               | 0.169        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32230228] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 28            |
|    time_elapsed             | 367           |
|    total_timesteps          | 960512        |
| train/                      |               |
|    approx_kl                | 0.006506486   |
|    approx_ln(kl)            | -5.034956     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.83         |
|    ln(policy_gradient_loss) | -4.75         |
|    loss                     | 0.0588        |
|    n_updates                | 520           |
|    policy_gradient_loss     | 0.00863       |
|    std                      | 1.09          |
|    value_loss               | 0.144         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39792946] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 29            |
|    time_elapsed             | 380           |
|    total_timesteps          | 962560        |
| train/                      |               |
|    approx_kl                | 0.0071115266  |
|    approx_ln(kl)            | -4.9460382    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.48         |
|    ln(policy_gradient_loss) | -5.1          |
|    loss                     | 0.084         |
|    n_updates                | 521           |
|    policy_gradient_loss     | 0.00607       |
|    std                      | 1.09          |
|    value_loss               | 0.111         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41726515] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 30            |
|    time_elapsed             | 395           |
|    total_timesteps          | 964608        |
| train/                      |               |
|    approx_kl                | 0.0045626643  |
|    approx_ln(kl)            | -5.3898487    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.85         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0581        |
|    n_updates                | 522           |
|    policy_gradient_loss     | -0.00162      |
|    std                      | 1.09          |
|    value_loss               | 0.191         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.19490884] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 31            |
|    time_elapsed             | 414           |
|    total_timesteps          | 966656        |
| train/                      |               |
|    approx_kl                | 0.0072445613  |
|    approx_ln(kl)            | -4.927504     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.922         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.34          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 3.82          |
|    n_updates                | 523           |
|    policy_gradient_loss     | -0.00558      |
|    std                      | 1.09          |
|    value_loss               | 3.24          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5802221] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 32           |
|    time_elapsed             | 431          |
|    total_timesteps          | 968704       |
| train/                      |              |
|    approx_kl                | 0.006985807  |
|    approx_ln(kl)            | -4.963875    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.988        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.39        |
|    ln(policy_gradient_loss) | -5.57        |
|    loss                     | 0.25         |
|    n_updates                | 524          |
|    policy_gradient_loss     | 0.00381      |
|    std                      | 1.09         |
|    value_loss               | 0.5          |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.5124585] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 33           |
|    time_elapsed             | 448          |
|    total_timesteps          | 970752       |
| train/                      |              |
|    approx_kl                | 0.008007045  |
|    approx_ln(kl)            | -4.8274336   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.35        |
|    ln(policy_gradient_loss) | -5.83        |
|    loss                     | 0.035        |
|    n_updates                | 525          |
|    policy_gradient_loss     | 0.00294      |
|    std                      | 1.09         |
|    value_loss               | 0.124        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5405186] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 34           |
|    time_elapsed             | 464          |
|    total_timesteps          | 972800       |
| train/                      |              |
|    approx_kl                | 0.004993488  |
|    approx_ln(kl)            | -5.2996206   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.48        |
|    ln(policy_gradient_loss) | -6.85        |
|    loss                     | 0.0307       |
|    n_updates                | 526          |
|    policy_gradient_loss     | 0.00106      |
|    std                      | 1.09         |
|    value_loss               | 0.115        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.28436017] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 35            |
|    time_elapsed             | 481           |
|    total_timesteps          | 974848        |
| train/                      |               |
|    approx_kl                | 0.007817516   |
|    approx_ln(kl)            | -4.8513885    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.9          |
|    ln(policy_gradient_loss) | -5.36         |
|    loss                     | 0.00745       |
|    n_updates                | 527           |
|    policy_gradient_loss     | 0.0047        |
|    std                      | 1.09          |
|    value_loss               | 0.042         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44014534] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 36            |
|    time_elapsed             | 497           |
|    total_timesteps          | 976896        |
| train/                      |               |
|    approx_kl                | 0.0054109693  |
|    approx_ln(kl)            | -5.219327     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.66         |
|    ln(policy_gradient_loss) | -5.2          |
|    loss                     | 0.0701        |
|    n_updates                | 528           |
|    policy_gradient_loss     | 0.00552       |
|    std                      | 1.09          |
|    value_loss               | 0.193         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32845208] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 37            |
|    time_elapsed             | 513           |
|    total_timesteps          | 978944        |
| train/                      |               |
|    approx_kl                | 0.0075982995  |
|    approx_ln(kl)            | -4.879831     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.07         |
|    ln(policy_gradient_loss) | -4.22         |
|    loss                     | 0.127         |
|    n_updates                | 529           |
|    policy_gradient_loss     | 0.0147        |
|    std                      | 1.09          |
|    value_loss               | 0.299         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.45400184] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 38            |
|    time_elapsed             | 529           |
|    total_timesteps          | 980992        |
| train/                      |               |
|    approx_kl                | 0.0068976185  |
|    approx_ln(kl)            | -4.976579     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.23         |
|    ln(policy_gradient_loss) | -5.91         |
|    loss                     | 0.107         |
|    n_updates                | 530           |
|    policy_gradient_loss     | 0.00271       |
|    std                      | 1.09          |
|    value_loss               | 0.281         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5276856] |
| time/                       |              |
|    fps                      | 146          |
|    iterations               | 39           |
|    time_elapsed             | 544          |
|    total_timesteps          | 983040       |
| train/                      |              |
|    approx_kl                | 0.004867012  |
|    approx_ln(kl)            | -5.325275    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.99        |
|    ln(policy_gradient_loss) | -5.41        |
|    loss                     | 0.136        |
|    n_updates                | 531          |
|    policy_gradient_loss     | 0.00449      |
|    std                      | 1.08         |
|    value_loss               | 0.547        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48344606] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 40            |
|    time_elapsed             | 560           |
|    total_timesteps          | 985088        |
| train/                      |               |
|    approx_kl                | 0.005319965   |
|    approx_ln(kl)            | -5.2362885    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.11         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0446        |
|    n_updates                | 532           |
|    policy_gradient_loss     | -0.00755      |
|    std                      | 1.08          |
|    value_loss               | 0.287         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.45625284] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 41            |
|    time_elapsed             | 576           |
|    total_timesteps          | 987136        |
| train/                      |               |
|    approx_kl                | 0.01032535    |
|    approx_ln(kl)            | -4.5731535    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.79         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0226        |
|    n_updates                | 533           |
|    policy_gradient_loss     | -0.0113       |
|    std                      | 1.08          |
|    value_loss               | 0.0945        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.41334605] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 42            |
|    time_elapsed             | 592           |
|    total_timesteps          | 989184        |
| train/                      |               |
|    approx_kl                | 0.011151325   |
|    approx_ln(kl)            | -4.4961967    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.923         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.38          |
|    ln(policy_gradient_loss) | -5.22         |
|    loss                     | 3.96          |
|    n_updates                | 534           |
|    policy_gradient_loss     | 0.00542       |
|    std                      | 1.08          |
|    value_loss               | 3.94          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.5147088] |
| time/                       |              |
|    fps                      | 144          |
|    iterations               | 43           |
|    time_elapsed             | 608          |
|    total_timesteps          | 991232       |
| train/                      |              |
|    approx_kl                | 0.009308579  |
|    approx_ln(kl)            | -4.676819    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.31        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0366       |
|    n_updates                | 535          |
|    policy_gradient_loss     | -0.0151      |
|    std                      | 1.08         |
|    value_loss               | 0.194        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.46895275] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 44            |
|    time_elapsed             | 624           |
|    total_timesteps          | 993280        |
| train/                      |               |
|    approx_kl                | 0.008377765   |
|    approx_ln(kl)            | -4.782174     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.8          |
|    ln(policy_gradient_loss) | -6            |
|    loss                     | 0.0224        |
|    n_updates                | 536           |
|    policy_gradient_loss     | 0.00249       |
|    std                      | 1.08          |
|    value_loss               | 0.0607        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5837603] |
| time/                       |              |
|    fps                      | 143          |
|    iterations               | 45           |
|    time_elapsed             | 640          |
|    total_timesteps          | 995328       |
| train/                      |              |
|    approx_kl                | 0.006844845  |
|    approx_ln(kl)            | -4.9842596   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.43        |
|    ln(policy_gradient_loss) | -7.5         |
|    loss                     | 0.0325       |
|    n_updates                | 537          |
|    policy_gradient_loss     | 0.000552     |
|    std                      | 1.09         |
|    value_loss               | 0.0878       |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40873063] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 46            |
|    time_elapsed             | 656           |
|    total_timesteps          | 997376        |
| train/                      |               |
|    approx_kl                | 0.00574088    |
|    approx_ln(kl)            | -5.160143     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.96         |
|    ln(policy_gradient_loss) | -5.78         |
|    loss                     | 0.0518        |
|    n_updates                | 538           |
|    policy_gradient_loss     | 0.00308       |
|    std                      | 1.09          |
|    value_loss               | 0.061         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49781466] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 47            |
|    time_elapsed             | 671           |
|    total_timesteps          | 999424        |
| train/                      |               |
|    approx_kl                | 0.005470719   |
|    approx_ln(kl)            | -5.2083454    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.64         |
|    ln(policy_gradient_loss) | -5.16         |
|    loss                     | 0.0715        |
|    n_updates                | 539           |
|    policy_gradient_loss     | 0.00575       |
|    std                      | 1.09          |
|    value_loss               | 0.186         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4201959] |
| time/                       |              |
|    fps                      | 143          |
|    iterations               | 48           |
|    time_elapsed             | 684          |
|    total_timesteps          | 1001472      |
| train/                      |              |
|    approx_kl                | 0.007466675  |
|    approx_ln(kl)            | -4.8973055   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.35        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.095        |
|    n_updates                | 540          |
|    policy_gradient_loss     | -0.00201     |
|    std                      | 1.09         |
|    value_loss               | 0.161        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3925113] |
| time/                       |              |
|    fps                      | 143          |
|    iterations               | 49           |
|    time_elapsed             | 697          |
|    total_timesteps          | 1003520      |
| train/                      |              |
|    approx_kl                | 0.008089918  |
|    approx_ln(kl)            | -4.817137    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.86        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0572       |
|    n_updates                | 541          |
|    policy_gradient_loss     | -0.00415     |
|    std                      | 1.09         |
|    value_loss               | 0.162        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
--------------------------------------
| reward             | [-0.57360244] |
| time/              |               |
|    fps             | 159           |
|    iterations      | 1             |
|    time_elapsed    | 12            |
|    total_timesteps | 1005568       |
--------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3717041] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 2            |
|    time_elapsed             | 26           |
|    total_timesteps          | 1007616      |
| train/                      |              |
|    approx_kl                | 0.0046379725 |
|    approx_ln(kl)            | -5.373478    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.77        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0624       |
|    n_updates                | 543          |
|    policy_gradient_loss     | -0.00347     |
|    std                      | 1.09         |
|    value_loss               | 0.0759       |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.29933897] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 3             |
|    time_elapsed             | 39            |
|    total_timesteps          | 1009664       |
| train/                      |               |
|    approx_kl                | 0.009355189   |
|    approx_ln(kl)            | -4.671824     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.25         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0389        |
|    n_updates                | 544           |
|    policy_gradient_loss     | -0.0172       |
|    std                      | 1.09          |
|    value_loss               | 0.199         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3792067] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 4            |
|    time_elapsed             | 51           |
|    total_timesteps          | 1011712      |
| train/                      |              |
|    approx_kl                | 0.0062649436 |
|    approx_ln(kl)            | -5.072786    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.02        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.133        |
|    n_updates                | 545          |
|    policy_gradient_loss     | -0.00263     |
|    std                      | 1.09         |
|    value_loss               | 0.238        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38956487] |
| time/                       |               |
|    fps                      | 158           |
|    iterations               | 5             |
|    time_elapsed             | 64            |
|    total_timesteps          | 1013760       |
| train/                      |               |
|    approx_kl                | 0.006748418   |
|    approx_ln(kl)            | -4.9984474    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.69         |
|    ln(policy_gradient_loss) | -5.87         |
|    loss                     | 0.068         |
|    n_updates                | 546           |
|    policy_gradient_loss     | 0.00282       |
|    std                      | 1.09          |
|    value_loss               | 0.197         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.5834972] |
| time/                       |              |
|    fps                      | 158          |
|    iterations               | 6            |
|    time_elapsed             | 77           |
|    total_timesteps          | 1015808      |
| train/                      |              |
|    approx_kl                | 0.0084520755 |
|    approx_ln(kl)            | -4.773343    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.01        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0494       |
|    n_updates                | 547          |
|    policy_gradient_loss     | -0.00276     |
|    std                      | 1.09         |
|    value_loss               | 0.189        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5031889] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 7            |
|    time_elapsed             | 90           |
|    total_timesteps          | 1017856      |
| train/                      |              |
|    approx_kl                | 0.0056181187 |
|    approx_ln(kl)            | -5.1817584   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.991        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.41        |
|    ln(policy_gradient_loss) | -5.71        |
|    loss                     | 0.0899       |
|    n_updates                | 548          |
|    policy_gradient_loss     | 0.0033       |
|    std                      | 1.09         |
|    value_loss               | 0.188        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5808868] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 8            |
|    time_elapsed             | 104          |
|    total_timesteps          | 1019904      |
| train/                      |              |
|    approx_kl                | 0.0060503976 |
|    approx_ln(kl)            | -5.107631    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.35        |
|    ln(policy_gradient_loss) | -6.19        |
|    loss                     | 0.0949       |
|    n_updates                | 549          |
|    policy_gradient_loss     | 0.00205      |
|    std                      | 1.09         |
|    value_loss               | 0.285        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.46891013] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 9             |
|    time_elapsed             | 117           |
|    total_timesteps          | 1021952       |
| train/                      |               |
|    approx_kl                | 0.006914592   |
|    approx_ln(kl)            | -4.9741216    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.974         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.759        |
|    ln(policy_gradient_loss) | -5.06         |
|    loss                     | 0.468         |
|    n_updates                | 550           |
|    policy_gradient_loss     | 0.00633       |
|    std                      | 1.09          |
|    value_loss               | 4.25          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5307255] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 10           |
|    time_elapsed             | 130          |
|    total_timesteps          | 1024000      |
| train/                      |              |
|    approx_kl                | 0.007339694  |
|    approx_ln(kl)            | -4.9144583   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.52        |
|    ln(policy_gradient_loss) | -5.67        |
|    loss                     | 0.22         |
|    n_updates                | 551          |
|    policy_gradient_loss     | 0.00343      |
|    std                      | 1.09         |
|    value_loss               | 0.415        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4169912] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 11           |
|    time_elapsed             | 143          |
|    total_timesteps          | 1026048      |
| train/                      |              |
|    approx_kl                | 0.0073364372 |
|    approx_ln(kl)            | -4.9149017   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.22        |
|    ln(policy_gradient_loss) | -6.85        |
|    loss                     | 0.108        |
|    n_updates                | 552          |
|    policy_gradient_loss     | 0.00106      |
|    std                      | 1.09         |
|    value_loss               | 0.469        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4977524] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 12           |
|    time_elapsed             | 156          |
|    total_timesteps          | 1028096      |
| train/                      |              |
|    approx_kl                | 0.0037115286 |
|    approx_ln(kl)            | -5.5963116   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.979        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.81        |
|    ln(policy_gradient_loss) | -7.78        |
|    loss                     | 0.164        |
|    n_updates                | 553          |
|    policy_gradient_loss     | 0.000418     |
|    std                      | 1.09         |
|    value_loss               | 0.429        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36113665] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 13            |
|    time_elapsed             | 169           |
|    total_timesteps          | 1030144       |
| train/                      |               |
|    approx_kl                | 0.0050319773  |
|    approx_ln(kl)            | -5.291942     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.5          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.223         |
|    n_updates                | 554           |
|    policy_gradient_loss     | -0.00116      |
|    std                      | 1.09          |
|    value_loss               | 0.518         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48662224] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 14            |
|    time_elapsed             | 182           |
|    total_timesteps          | 1032192       |
| train/                      |               |
|    approx_kl                | 0.007956889   |
|    approx_ln(kl)            | -4.8337173    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.22         |
|    ln(policy_gradient_loss) | -6.46         |
|    loss                     | 0.296         |
|    n_updates                | 555           |
|    policy_gradient_loss     | 0.00156       |
|    std                      | 1.09          |
|    value_loss               | 0.522         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48119015] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 15            |
|    time_elapsed             | 196           |
|    total_timesteps          | 1034240       |
| train/                      |               |
|    approx_kl                | 0.0037108224  |
|    approx_ln(kl)            | -5.596502     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.96         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.141         |
|    n_updates                | 556           |
|    policy_gradient_loss     | -0.00137      |
|    std                      | 1.09          |
|    value_loss               | 0.332         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.37800246] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 16            |
|    time_elapsed             | 208           |
|    total_timesteps          | 1036288       |
| train/                      |               |
|    approx_kl                | 0.008072762   |
|    approx_ln(kl)            | -4.8192596    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | nan           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | -0.0125       |
|    n_updates                | 557           |
|    policy_gradient_loss     | -0.00603      |
|    std                      | 1.09          |
|    value_loss               | 0.16          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.55480236] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 17            |
|    time_elapsed             | 221           |
|    total_timesteps          | 1038336       |
| train/                      |               |
|    approx_kl                | 0.0049670483  |
|    approx_ln(kl)            | -5.3049297    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.29         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.102         |
|    n_updates                | 558           |
|    policy_gradient_loss     | -0.00271      |
|    std                      | 1.09          |
|    value_loss               | 0.305         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.51794386] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 18            |
|    time_elapsed             | 234           |
|    total_timesteps          | 1040384       |
| train/                      |               |
|    approx_kl                | 0.007211436   |
|    approx_ln(kl)            | -4.932087     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.66         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0702        |
|    n_updates                | 559           |
|    policy_gradient_loss     | -0.0138       |
|    std                      | 1.09          |
|    value_loss               | 0.248         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5183414] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 19           |
|    time_elapsed             | 247          |
|    total_timesteps          | 1042432      |
| train/                      |              |
|    approx_kl                | 0.006252431  |
|    approx_ln(kl)            | -5.0747848   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.61        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0735       |
|    n_updates                | 560          |
|    policy_gradient_loss     | -0.00248     |
|    std                      | 1.09         |
|    value_loss               | 0.181        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2512598] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 20           |
|    time_elapsed             | 260          |
|    total_timesteps          | 1044480      |
| train/                      |              |
|    approx_kl                | 0.006917592  |
|    approx_ln(kl)            | -4.9736876   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.979        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.7         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.184        |
|    n_updates                | 561          |
|    policy_gradient_loss     | -0.00438     |
|    std                      | 1.09         |
|    value_loss               | 0.447        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30335408] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 21            |
|    time_elapsed             | 274           |
|    total_timesteps          | 1046528       |
| train/                      |               |
|    approx_kl                | 0.006110297   |
|    approx_ln(kl)            | -5.0977798    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.88         |
|    ln(policy_gradient_loss) | -4.78         |
|    loss                     | 0.153         |
|    n_updates                | 562           |
|    policy_gradient_loss     | 0.0084        |
|    std                      | 1.09          |
|    value_loss               | 0.221         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.53052133] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 22            |
|    time_elapsed             | 287           |
|    total_timesteps          | 1048576       |
| train/                      |               |
|    approx_kl                | 0.0053586434  |
|    approx_ln(kl)            | -5.2290444    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.48         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0839        |
|    n_updates                | 563           |
|    policy_gradient_loss     | -0.00115      |
|    std                      | 1.09          |
|    value_loss               | 0.25          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33575445] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 23            |
|    time_elapsed             | 301           |
|    total_timesteps          | 1050624       |
| train/                      |               |
|    approx_kl                | 0.008252238   |
|    approx_ln(kl)            | -4.797271     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.1          |
|    ln(policy_gradient_loss) | -5.72         |
|    loss                     | 0.334         |
|    n_updates                | 564           |
|    policy_gradient_loss     | 0.00326       |
|    std                      | 1.09          |
|    value_loss               | 2.38          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.58574367] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 24            |
|    time_elapsed             | 315           |
|    total_timesteps          | 1052672       |
| train/                      |               |
|    approx_kl                | 0.006835912   |
|    approx_ln(kl)            | -4.985565     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.68         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.187         |
|    n_updates                | 565           |
|    policy_gradient_loss     | -0.00312      |
|    std                      | 1.09          |
|    value_loss               | 0.336         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.49037355] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 25            |
|    time_elapsed             | 328           |
|    total_timesteps          | 1054720       |
| train/                      |               |
|    approx_kl                | 0.0074579925  |
|    approx_ln(kl)            | -4.898469     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.64         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0714        |
|    n_updates                | 566           |
|    policy_gradient_loss     | -0.00637      |
|    std                      | 1.09          |
|    value_loss               | 0.182         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4762477] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 26           |
|    time_elapsed             | 341          |
|    total_timesteps          | 1056768      |
| train/                      |              |
|    approx_kl                | 0.009435488  |
|    approx_ln(kl)            | -4.6632776   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.988        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.34        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0959       |
|    n_updates                | 567          |
|    policy_gradient_loss     | -0.00309     |
|    std                      | 1.09         |
|    value_loss               | 0.216        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4650048] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 27           |
|    time_elapsed             | 353          |
|    total_timesteps          | 1058816      |
| train/                      |              |
|    approx_kl                | 0.009317755  |
|    approx_ln(kl)            | -4.6758337   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.51        |
|    ln(policy_gradient_loss) | -4.93        |
|    loss                     | 0.0816       |
|    n_updates                | 568          |
|    policy_gradient_loss     | 0.00726      |
|    std                      | 1.09         |
|    value_loss               | 0.154        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.5199519] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 28           |
|    time_elapsed             | 366          |
|    total_timesteps          | 1060864      |
| train/                      |              |
|    approx_kl                | 0.013077354  |
|    approx_ln(kl)            | -4.336873    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.995        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.66        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0702       |
|    n_updates                | 569          |
|    policy_gradient_loss     | -0.0014      |
|    std                      | 1.1          |
|    value_loss               | 0.246        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39261916] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 29            |
|    time_elapsed             | 380           |
|    total_timesteps          | 1062912       |
| train/                      |               |
|    approx_kl                | 0.0074176257  |
|    approx_ln(kl)            | -4.9038963    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.39         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0336        |
|    n_updates                | 570           |
|    policy_gradient_loss     | -0.0097       |
|    std                      | 1.1           |
|    value_loss               | 0.174         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5532496] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 30           |
|    time_elapsed             | 393          |
|    total_timesteps          | 1064960      |
| train/                      |              |
|    approx_kl                | 0.007745699  |
|    approx_ln(kl)            | -4.8606176   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.995        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.28        |
|    ln(policy_gradient_loss) | -4.32        |
|    loss                     | 0.102        |
|    n_updates                | 571          |
|    policy_gradient_loss     | 0.0133       |
|    std                      | 1.1          |
|    value_loss               | 0.212        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44458726] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 31            |
|    time_elapsed             | 406           |
|    total_timesteps          | 1067008       |
| train/                      |               |
|    approx_kl                | 0.004377769   |
|    approx_ln(kl)            | -5.4312162    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.82         |
|    ln(policy_gradient_loss) | -6.24         |
|    loss                     | 0.0596        |
|    n_updates                | 572           |
|    policy_gradient_loss     | 0.00195       |
|    std                      | 1.1           |
|    value_loss               | 0.0956        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35742766] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 32            |
|    time_elapsed             | 419           |
|    total_timesteps          | 1069056       |
| train/                      |               |
|    approx_kl                | 0.008983082   |
|    approx_ln(kl)            | -4.7124124    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.05         |
|    ln(policy_gradient_loss) | -3.88         |
|    loss                     | 0.129         |
|    n_updates                | 573           |
|    policy_gradient_loss     | 0.0207        |
|    std                      | 1.1           |
|    value_loss               | 0.147         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38847265] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 33            |
|    time_elapsed             | 432           |
|    total_timesteps          | 1071104       |
| train/                      |               |
|    approx_kl                | 0.007458808   |
|    approx_ln(kl)            | -4.89836      |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.79         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0615        |
|    n_updates                | 574           |
|    policy_gradient_loss     | -0.00624      |
|    std                      | 1.1           |
|    value_loss               | 0.155         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.3538908] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 34           |
|    time_elapsed             | 446          |
|    total_timesteps          | 1073152      |
| train/                      |              |
|    approx_kl                | 0.007645746  |
|    approx_ln(kl)            | -4.8736057   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.26        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.104        |
|    n_updates                | 575          |
|    policy_gradient_loss     | -0.000341    |
|    std                      | 1.1          |
|    value_loss               | 0.121        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
---------------------------------------------
| reward                      | [-0.506615] |
| time/                       |             |
|    fps                      | 155         |
|    iterations               | 35          |
|    time_elapsed             | 460         |
|    total_timesteps          | 1075200     |
| train/                      |             |
|    approx_kl                | 0.011558455 |
|    approx_ln(kl)            | -4.460338   |
|    clip_range               | 0.2         |
|    entropy_loss             | -3.02       |
|    explained_variance       | 0.998       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -2.3        |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.0998      |
|    n_updates                | 576         |
|    policy_gradient_loss     | -0.00479    |
|    std                      | 1.1         |
|    value_loss               | 0.161       |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35472384] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 36            |
|    time_elapsed             | 473           |
|    total_timesteps          | 1077248       |
| train/                      |               |
|    approx_kl                | 0.0064414283  |
|    approx_ln(kl)            | -5.045005     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.5          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0817        |
|    n_updates                | 577           |
|    policy_gradient_loss     | -0.00567      |
|    std                      | 1.1           |
|    value_loss               | 0.324         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5046331] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 37           |
|    time_elapsed             | 487          |
|    total_timesteps          | 1079296      |
| train/                      |              |
|    approx_kl                | 0.006985967  |
|    approx_ln(kl)            | -4.963852    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.34        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0353       |
|    n_updates                | 578          |
|    policy_gradient_loss     | -0.0113      |
|    std                      | 1.09         |
|    value_loss               | 0.192        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44922754] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 38            |
|    time_elapsed             | 500           |
|    total_timesteps          | 1081344       |
| train/                      |               |
|    approx_kl                | 0.007384266   |
|    approx_ln(kl)            | -4.908404     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.85         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.157         |
|    n_updates                | 579           |
|    policy_gradient_loss     | -0.000158     |
|    std                      | 1.09          |
|    value_loss               | 0.231         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.29860556] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 39            |
|    time_elapsed             | 512           |
|    total_timesteps          | 1083392       |
| train/                      |               |
|    approx_kl                | 0.013002197   |
|    approx_ln(kl)            | -4.342637     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.44         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0868        |
|    n_updates                | 580           |
|    policy_gradient_loss     | -0.014        |
|    std                      | 1.09          |
|    value_loss               | 0.215         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.57083714] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 40            |
|    time_elapsed             | 525           |
|    total_timesteps          | 1085440       |
| train/                      |               |
|    approx_kl                | 0.008842797   |
|    approx_ln(kl)            | -4.7281523    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.96         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.141         |
|    n_updates                | 581           |
|    policy_gradient_loss     | -0.00489      |
|    std                      | 1.09          |
|    value_loss               | 0.278         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3731607] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 41           |
|    time_elapsed             | 538          |
|    total_timesteps          | 1087488      |
| train/                      |              |
|    approx_kl                | 0.007224095  |
|    approx_ln(kl)            | -4.930333    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.67        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0694       |
|    n_updates                | 582          |
|    policy_gradient_loss     | -0.0106      |
|    std                      | 1.09         |
|    value_loss               | 0.217        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.15971]  |
| time/                       |             |
|    fps                      | 155         |
|    iterations               | 42          |
|    time_elapsed             | 552         |
|    total_timesteps          | 1089536     |
| train/                      |             |
|    approx_kl                | 0.008083399 |
|    approx_ln(kl)            | -4.8179426  |
|    clip_range               | 0.2         |
|    entropy_loss             | -3.01       |
|    explained_variance       | 0.976       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -2.7        |
|    ln(policy_gradient_loss) | -6.95       |
|    loss                     | 0.0672      |
|    n_updates                | 583         |
|    policy_gradient_loss     | 0.000959    |
|    std                      | 1.09        |
|    value_loss               | 0.261       |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43633097] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 43            |
|    time_elapsed             | 565           |
|    total_timesteps          | 1091584       |
| train/                      |               |
|    approx_kl                | 0.006897683   |
|    approx_ln(kl)            | -4.9765697    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.38         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0926        |
|    n_updates                | 584           |
|    policy_gradient_loss     | -0.0031       |
|    std                      | 1.09          |
|    value_loss               | 0.394         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.50323105] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 44            |
|    time_elapsed             | 578           |
|    total_timesteps          | 1093632       |
| train/                      |               |
|    approx_kl                | 0.0055134743  |
|    approx_ln(kl)            | -5.2005606    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.973         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.173         |
|    ln(policy_gradient_loss) | -6.31         |
|    loss                     | 1.19          |
|    n_updates                | 585           |
|    policy_gradient_loss     | 0.00181       |
|    std                      | 1.09          |
|    value_loss               | 3.96          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26659697] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 45            |
|    time_elapsed             | 592           |
|    total_timesteps          | 1095680       |
| train/                      |               |
|    approx_kl                | 0.004859202   |
|    approx_ln(kl)            | -5.326881     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.96          |
|    ln(policy_gradient_loss) | -5.77         |
|    loss                     | 2.61          |
|    n_updates                | 586           |
|    policy_gradient_loss     | 0.00311       |
|    std                      | 1.09          |
|    value_loss               | 2.27          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23667419] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 46            |
|    time_elapsed             | 605           |
|    total_timesteps          | 1097728       |
| train/                      |               |
|    approx_kl                | 0.004423713   |
|    approx_ln(kl)            | -5.420776     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2            |
|    ln(policy_gradient_loss) | -5.97         |
|    loss                     | 0.135         |
|    n_updates                | 587           |
|    policy_gradient_loss     | 0.00255       |
|    std                      | 1.1           |
|    value_loss               | 0.616         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5710246] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 47           |
|    time_elapsed             | 618          |
|    total_timesteps          | 1099776      |
| train/                      |              |
|    approx_kl                | 0.0069389334 |
|    approx_ln(kl)            | -4.9706073   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.27        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.103        |
|    n_updates                | 588          |
|    policy_gradient_loss     | -0.000155    |
|    std                      | 1.1          |
|    value_loss               | 0.224        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.42148927] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 48            |
|    time_elapsed             | 631           |
|    total_timesteps          | 1101824       |
| train/                      |               |
|    approx_kl                | 0.005884381   |
|    approx_ln(kl)            | -5.1354537    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.3          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.273         |
|    n_updates                | 589           |
|    policy_gradient_loss     | -0.00604      |
|    std                      | 1.1           |
|    value_loss               | 0.603         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5029134] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 49           |
|    time_elapsed             | 644          |
|    total_timesteps          | 1103872      |
| train/                      |              |
|    approx_kl                | 0.0084827645 |
|    approx_ln(kl)            | -4.769719    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.06        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.128        |
|    n_updates                | 590          |
|    policy_gradient_loss     | -0.0105      |
|    std                      | 1.1          |
|    value_loss               | 0.277        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
--------------------------------------
| reward             | [-0.26232916] |
| time/              |               |
|    fps             | 159           |
|    iterations      | 1             |
|    time_elapsed    | 12            |
|    total_timesteps | 1105920       |
--------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.26137516] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 2             |
|    time_elapsed             | 27            |
|    total_timesteps          | 1107968       |
| train/                      |               |
|    approx_kl                | 0.009643631   |
|    approx_ln(kl)            | -4.6414576    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.46         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0853        |
|    n_updates                | 592           |
|    policy_gradient_loss     | -0.00198      |
|    std                      | 1.1           |
|    value_loss               | 0.142         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48617953] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 3             |
|    time_elapsed             | 40            |
|    total_timesteps          | 1110016       |
| train/                      |               |
|    approx_kl                | 0.007667619   |
|    approx_ln(kl)            | -4.870749     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.962         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.111        |
|    ln(policy_gradient_loss) | -4.51         |
|    loss                     | 0.895         |
|    n_updates                | 593           |
|    policy_gradient_loss     | 0.011         |
|    std                      | 1.1           |
|    value_loss               | 2.87          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24452496] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 4             |
|    time_elapsed             | 56            |
|    total_timesteps          | 1112064       |
| train/                      |               |
|    approx_kl                | 0.007590872   |
|    approx_ln(kl)            | -4.880809     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.15         |
|    ln(policy_gradient_loss) | -4.53         |
|    loss                     | 0.117         |
|    n_updates                | 594           |
|    policy_gradient_loss     | 0.0108        |
|    std                      | 1.1           |
|    value_loss               | 0.153         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45463204] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 5             |
|    time_elapsed             | 69            |
|    total_timesteps          | 1114112       |
| train/                      |               |
|    approx_kl                | 0.0052493373  |
|    approx_ln(kl)            | -5.2496533    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.25         |
|    ln(policy_gradient_loss) | -5.15         |
|    loss                     | 0.105         |
|    n_updates                | 595           |
|    policy_gradient_loss     | 0.00582       |
|    std                      | 1.1           |
|    value_loss               | 0.188         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4588756] |
| time/                       |              |
|    fps                      | 146          |
|    iterations               | 6            |
|    time_elapsed             | 83           |
|    total_timesteps          | 1116160      |
| train/                      |              |
|    approx_kl                | 0.005898601  |
|    approx_ln(kl)            | -5.13304     |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.04        |
|    ln(policy_gradient_loss) | -5.93        |
|    loss                     | 0.13         |
|    n_updates                | 596          |
|    policy_gradient_loss     | 0.00265      |
|    std                      | 1.1          |
|    value_loss               | 0.196        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.51016575] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 7             |
|    time_elapsed             | 97            |
|    total_timesteps          | 1118208       |
| train/                      |               |
|    approx_kl                | 0.0074540214  |
|    approx_ln(kl)            | -4.8990016    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.67         |
|    ln(policy_gradient_loss) | -4.68         |
|    loss                     | 0.188         |
|    n_updates                | 597           |
|    policy_gradient_loss     | 0.0093        |
|    std                      | 1.1           |
|    value_loss               | 0.228         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36461082] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 8             |
|    time_elapsed             | 110           |
|    total_timesteps          | 1120256       |
| train/                      |               |
|    approx_kl                | 0.0066586374  |
|    approx_ln(kl)            | -5.0118403    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.28         |
|    ln(policy_gradient_loss) | -5.21         |
|    loss                     | 0.103         |
|    n_updates                | 598           |
|    policy_gradient_loss     | 0.00547       |
|    std                      | 1.1           |
|    value_loss               | 0.165         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40019977] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 9             |
|    time_elapsed             | 123           |
|    total_timesteps          | 1122304       |
| train/                      |               |
|    approx_kl                | 0.006109158   |
|    approx_ln(kl)            | -5.097966     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.81         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0221        |
|    n_updates                | 599           |
|    policy_gradient_loss     | -0.00182      |
|    std                      | 1.1           |
|    value_loss               | 0.0999        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30859253] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 10            |
|    time_elapsed             | 137           |
|    total_timesteps          | 1124352       |
| train/                      |               |
|    approx_kl                | 0.0051322016  |
|    approx_ln(kl)            | -5.2722206    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.65         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0708        |
|    n_updates                | 600           |
|    policy_gradient_loss     | -0.00193      |
|    std                      | 1.1           |
|    value_loss               | 0.196         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.5202223] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 11           |
|    time_elapsed             | 149          |
|    total_timesteps          | 1126400      |
| train/                      |              |
|    approx_kl                | 0.00707576   |
|    approx_ln(kl)            | -4.9510803   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.86        |
|    ln(policy_gradient_loss) | -7.67        |
|    loss                     | 0.0572       |
|    n_updates                | 601          |
|    policy_gradient_loss     | 0.000465     |
|    std                      | 1.1          |
|    value_loss               | 0.152        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.51309407] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 12            |
|    time_elapsed             | 163           |
|    total_timesteps          | 1128448       |
| train/                      |               |
|    approx_kl                | 0.005759833   |
|    approx_ln(kl)            | -5.156847     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.42         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0121        |
|    n_updates                | 602           |
|    policy_gradient_loss     | -0.00338      |
|    std                      | 1.1           |
|    value_loss               | 0.0979        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3229582] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 13           |
|    time_elapsed             | 176          |
|    total_timesteps          | 1130496      |
| train/                      |              |
|    approx_kl                | 0.007318357  |
|    approx_ln(kl)            | -4.9173694   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.24        |
|    ln(policy_gradient_loss) | -5.16        |
|    loss                     | 0.107        |
|    n_updates                | 603          |
|    policy_gradient_loss     | 0.00573      |
|    std                      | 1.1          |
|    value_loss               | 0.218        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.34866753] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 14            |
|    time_elapsed             | 189           |
|    total_timesteps          | 1132544       |
| train/                      |               |
|    approx_kl                | 0.008810258   |
|    approx_ln(kl)            | -4.7318387    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | nan           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | -0.00135      |
|    n_updates                | 604           |
|    policy_gradient_loss     | -0.00831      |
|    std                      | 1.1           |
|    value_loss               | 0.0818        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4409878] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 15           |
|    time_elapsed             | 202          |
|    total_timesteps          | 1134592      |
| train/                      |              |
|    approx_kl                | 0.0052691093 |
|    approx_ln(kl)            | -5.245894    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -4.59        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0102       |
|    n_updates                | 605          |
|    policy_gradient_loss     | -0.00523     |
|    std                      | 1.1          |
|    value_loss               | 0.152        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4416403] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 16           |
|    time_elapsed             | 214          |
|    total_timesteps          | 1136640      |
| train/                      |              |
|    approx_kl                | 0.0064399713 |
|    approx_ln(kl)            | -5.0452313   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.04        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.89        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0554       |
|    n_updates                | 606          |
|    policy_gradient_loss     | -0.00235     |
|    std                      | 1.11         |
|    value_loss               | 0.106        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48332992] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 17            |
|    time_elapsed             | 228           |
|    total_timesteps          | 1138688       |
| train/                      |               |
|    approx_kl                | 0.007752318   |
|    approx_ln(kl)            | -4.8597636    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.36         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0128        |
|    n_updates                | 607           |
|    policy_gradient_loss     | -0.00108      |
|    std                      | 1.11          |
|    value_loss               | 0.0619        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5861828] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 18           |
|    time_elapsed             | 241          |
|    total_timesteps          | 1140736      |
| train/                      |              |
|    approx_kl                | 0.0079085715 |
|    approx_ln(kl)            | -4.839808    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.04        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.85        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0213       |
|    n_updates                | 608          |
|    policy_gradient_loss     | -0.00491     |
|    std                      | 1.1          |
|    value_loss               | 0.17         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5339018] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 19           |
|    time_elapsed             | 254          |
|    total_timesteps          | 1142784      |
| train/                      |              |
|    approx_kl                | 0.005746139  |
|    approx_ln(kl)            | -5.1592274   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.04        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.47        |
|    ln(policy_gradient_loss) | -6.62        |
|    loss                     | 0.031        |
|    n_updates                | 609          |
|    policy_gradient_loss     | 0.00133      |
|    std                      | 1.1          |
|    value_loss               | 0.136        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49771392] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 20            |
|    time_elapsed             | 267           |
|    total_timesteps          | 1144832       |
| train/                      |               |
|    approx_kl                | 0.006773424   |
|    approx_ln(kl)            | -4.9947486    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.45         |
|    ln(policy_gradient_loss) | -5.13         |
|    loss                     | 0.236         |
|    n_updates                | 610           |
|    policy_gradient_loss     | 0.00591       |
|    std                      | 1.11          |
|    value_loss               | 1.99          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4888867] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 21           |
|    time_elapsed             | 280          |
|    total_timesteps          | 1146880      |
| train/                      |              |
|    approx_kl                | 0.0070391586 |
|    approx_ln(kl)            | -4.9562664   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.04        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.85        |
|    ln(policy_gradient_loss) | -4.8         |
|    loss                     | 0.0577       |
|    n_updates                | 611          |
|    policy_gradient_loss     | 0.00821      |
|    std                      | 1.11         |
|    value_loss               | 0.101        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36259976] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 22            |
|    time_elapsed             | 293           |
|    total_timesteps          | 1148928       |
| train/                      |               |
|    approx_kl                | 0.005742489   |
|    approx_ln(kl)            | -5.1598625    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.7          |
|    ln(policy_gradient_loss) | -6.15         |
|    loss                     | 0.183         |
|    n_updates                | 612           |
|    policy_gradient_loss     | 0.00213       |
|    std                      | 1.11          |
|    value_loss               | 0.303         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25100705] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 23            |
|    time_elapsed             | 306           |
|    total_timesteps          | 1150976       |
| train/                      |               |
|    approx_kl                | 0.005998151   |
|    approx_ln(kl)            | -5.116304     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.04         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.92         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0542        |
|    n_updates                | 613           |
|    policy_gradient_loss     | -0.00622      |
|    std                      | 1.11          |
|    value_loss               | 0.279         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5101968] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 24           |
|    time_elapsed             | 319          |
|    total_timesteps          | 1153024      |
| train/                      |              |
|    approx_kl                | 0.0047306907 |
|    approx_ln(kl)            | -5.353684    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.04        |
|    explained_variance       | 0.989        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.39        |
|    ln(policy_gradient_loss) | -6.16        |
|    loss                     | 0.0336       |
|    n_updates                | 614          |
|    policy_gradient_loss     | 0.00211      |
|    std                      | 1.1          |
|    value_loss               | 0.108        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5128483] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 25           |
|    time_elapsed             | 332          |
|    total_timesteps          | 1155072      |
| train/                      |              |
|    approx_kl                | 0.0075225024 |
|    approx_ln(kl)            | -4.8898563   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.76        |
|    ln(policy_gradient_loss) | -7.05        |
|    loss                     | 0.0631       |
|    n_updates                | 615          |
|    policy_gradient_loss     | 0.000869     |
|    std                      | 1.1          |
|    value_loss               | 0.0828       |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
---------------------------------------------
| reward                      | [-0.311952] |
| time/                       |             |
|    fps                      | 153         |
|    iterations               | 26          |
|    time_elapsed             | 346         |
|    total_timesteps          | 1157120     |
| train/                      |             |
|    approx_kl                | 0.008390975 |
|    approx_ln(kl)            | -4.7805986  |
|    clip_range               | 0.2         |
|    entropy_loss             | -3.03       |
|    explained_variance       | 0.998       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -2.96       |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.052       |
|    n_updates                | 616         |
|    policy_gradient_loss     | -0.00995    |
|    std                      | 1.1         |
|    value_loss               | 0.144       |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4585776] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 27           |
|    time_elapsed             | 359          |
|    total_timesteps          | 1159168      |
| train/                      |              |
|    approx_kl                | 0.0076219467 |
|    approx_ln(kl)            | -4.8767233   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.06        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0469       |
|    n_updates                | 617          |
|    policy_gradient_loss     | -0.00462     |
|    std                      | 1.1          |
|    value_loss               | 0.144        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5051437] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 28           |
|    time_elapsed             | 372          |
|    total_timesteps          | 1161216      |
| train/                      |              |
|    approx_kl                | 0.0046871244 |
|    approx_ln(kl)            | -5.362936    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.57        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0764       |
|    n_updates                | 618          |
|    policy_gradient_loss     | -0.00318     |
|    std                      | 1.1          |
|    value_loss               | 0.136        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4955395] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 29           |
|    time_elapsed             | 386          |
|    total_timesteps          | 1163264      |
| train/                      |              |
|    approx_kl                | 0.0050358884 |
|    approx_ln(kl)            | -5.2911654   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.985        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.84        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.159        |
|    n_updates                | 619          |
|    policy_gradient_loss     | -0.00707     |
|    std                      | 1.1          |
|    value_loss               | 1.68         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37467495] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 30            |
|    time_elapsed             | 399           |
|    total_timesteps          | 1165312       |
| train/                      |               |
|    approx_kl                | 0.0062990724  |
|    approx_ln(kl)            | -5.067353     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.969         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.901        |
|    ln(policy_gradient_loss) | -5.13         |
|    loss                     | 0.406         |
|    n_updates                | 620           |
|    policy_gradient_loss     | 0.00594       |
|    std                      | 1.1           |
|    value_loss               | 5.09          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4613385] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 31           |
|    time_elapsed             | 413          |
|    total_timesteps          | 1167360      |
| train/                      |              |
|    approx_kl                | 0.0049444605 |
|    approx_ln(kl)            | -5.3094873   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.981        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.55        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.212        |
|    n_updates                | 621          |
|    policy_gradient_loss     | -0.00596     |
|    std                      | 1.1          |
|    value_loss               | 1.8          |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30938262] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 32            |
|    time_elapsed             | 426           |
|    total_timesteps          | 1169408       |
| train/                      |               |
|    approx_kl                | 0.005358305   |
|    approx_ln(kl)            | -5.229108     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.81         |
|    ln(policy_gradient_loss) | -5.99         |
|    loss                     | 0.0602        |
|    n_updates                | 622           |
|    policy_gradient_loss     | 0.00251       |
|    std                      | 1.1           |
|    value_loss               | 0.135         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.2948724] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 33           |
|    time_elapsed             | 439          |
|    total_timesteps          | 1171456      |
| train/                      |              |
|    approx_kl                | 0.007919595  |
|    approx_ln(kl)            | -4.838415    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.74        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0237       |
|    n_updates                | 623          |
|    policy_gradient_loss     | -0.00251     |
|    std                      | 1.1          |
|    value_loss               | 0.128        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39295414] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 34            |
|    time_elapsed             | 452           |
|    total_timesteps          | 1173504       |
| train/                      |               |
|    approx_kl                | 0.006726642   |
|    approx_ln(kl)            | -5.0016794    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.88         |
|    ln(policy_gradient_loss) | -4.78         |
|    loss                     | 0.0562        |
|    n_updates                | 624           |
|    policy_gradient_loss     | 0.00838       |
|    std                      | 1.1           |
|    value_loss               | 0.0889        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3466281] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 35           |
|    time_elapsed             | 465          |
|    total_timesteps          | 1175552      |
| train/                      |              |
|    approx_kl                | 0.007164376  |
|    approx_ln(kl)            | -4.9386344   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.86        |
|    ln(policy_gradient_loss) | -5.23        |
|    loss                     | 0.0211       |
|    n_updates                | 625          |
|    policy_gradient_loss     | 0.00537      |
|    std                      | 1.1          |
|    value_loss               | 0.126        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45588306] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 36            |
|    time_elapsed             | 478           |
|    total_timesteps          | 1177600       |
| train/                      |               |
|    approx_kl                | 0.004916981   |
|    approx_ln(kl)            | -5.3150606    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.36         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0948        |
|    n_updates                | 626           |
|    policy_gradient_loss     | -0.00395      |
|    std                      | 1.1           |
|    value_loss               | 0.245         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.46191773] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 37            |
|    time_elapsed             | 491           |
|    total_timesteps          | 1179648       |
| train/                      |               |
|    approx_kl                | 0.006932641   |
|    approx_ln(kl)            | -4.9715147    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | nan           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | -0.000669     |
|    n_updates                | 627           |
|    policy_gradient_loss     | -0.00516      |
|    std                      | 1.1           |
|    value_loss               | 0.0548        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41061583] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 38            |
|    time_elapsed             | 504           |
|    total_timesteps          | 1181696       |
| train/                      |               |
|    approx_kl                | 0.0075768754  |
|    approx_ln(kl)            | -4.882654     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.38         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0341        |
|    n_updates                | 628           |
|    policy_gradient_loss     | -0.00772      |
|    std                      | 1.1           |
|    value_loss               | 0.0951        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.426796]  |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 39           |
|    time_elapsed             | 517          |
|    total_timesteps          | 1183744      |
| train/                      |              |
|    approx_kl                | 0.0055329143 |
|    approx_ln(kl)            | -5.1970406   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.66        |
|    ln(policy_gradient_loss) | -7.37        |
|    loss                     | 0.0702       |
|    n_updates                | 629          |
|    policy_gradient_loss     | 0.000628     |
|    std                      | 1.09         |
|    value_loss               | 0.0833       |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40690735] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 40            |
|    time_elapsed             | 530           |
|    total_timesteps          | 1185792       |
| train/                      |               |
|    approx_kl                | 0.007825573   |
|    approx_ln(kl)            | -4.8503585    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | nan           |
|    ln(policy_gradient_loss) | -5.78         |
|    loss                     | -0.0316       |
|    n_updates                | 630           |
|    policy_gradient_loss     | 0.0031        |
|    std                      | 1.1           |
|    value_loss               | 0.0482        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.570483]  |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 41           |
|    time_elapsed             | 544          |
|    total_timesteps          | 1187840      |
| train/                      |              |
|    approx_kl                | 0.0074690194 |
|    approx_ln(kl)            | -4.8969917   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 1            |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.09        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0456       |
|    n_updates                | 631          |
|    policy_gradient_loss     | -0.00283     |
|    std                      | 1.1          |
|    value_loss               | 0.0685       |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.41293812] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 42            |
|    time_elapsed             | 557           |
|    total_timesteps          | 1189888       |
| train/                      |               |
|    approx_kl                | 0.013721231   |
|    approx_ln(kl)            | -4.2888107    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.69         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.068         |
|    n_updates                | 632           |
|    policy_gradient_loss     | -0.00893      |
|    std                      | 1.1           |
|    value_loss               | 0.129         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41462216] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 43            |
|    time_elapsed             | 570           |
|    total_timesteps          | 1191936       |
| train/                      |               |
|    approx_kl                | 0.0077347276  |
|    approx_ln(kl)            | -4.862035     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.8          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0224        |
|    n_updates                | 633           |
|    policy_gradient_loss     | -0.0128       |
|    std                      | 1.1           |
|    value_loss               | 0.0838        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38676673] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 44            |
|    time_elapsed             | 583           |
|    total_timesteps          | 1193984       |
| train/                      |               |
|    approx_kl                | 0.0070276684  |
|    approx_ln(kl)            | -4.9579005    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.71         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0665        |
|    n_updates                | 634           |
|    policy_gradient_loss     | -0.000946     |
|    std                      | 1.1           |
|    value_loss               | 0.156         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40529984] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 45            |
|    time_elapsed             | 596           |
|    total_timesteps          | 1196032       |
| train/                      |               |
|    approx_kl                | 0.009909265   |
|    approx_ln(kl)            | -4.614285     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.47         |
|    ln(policy_gradient_loss) | -6.04         |
|    loss                     | 0.0849        |
|    n_updates                | 635           |
|    policy_gradient_loss     | 0.00238       |
|    std                      | 1.1           |
|    value_loss               | 0.133         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3627004] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 46           |
|    time_elapsed             | 609          |
|    total_timesteps          | 1198080      |
| train/                      |              |
|    approx_kl                | 0.0064663226 |
|    approx_ln(kl)            | -5.0411477   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.29        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0372       |
|    n_updates                | 636          |
|    policy_gradient_loss     | -0.00523     |
|    std                      | 1.1          |
|    value_loss               | 0.0928       |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27896816] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 47            |
|    time_elapsed             | 622           |
|    total_timesteps          | 1200128       |
| train/                      |               |
|    approx_kl                | 0.007725366   |
|    approx_ln(kl)            | -4.863246     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.5          |
|    ln(policy_gradient_loss) | -4.6          |
|    loss                     | 0.0822        |
|    n_updates                | 637           |
|    policy_gradient_loss     | 0.0101        |
|    std                      | 1.1           |
|    value_loss               | 0.0732        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29482588] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 48            |
|    time_elapsed             | 635           |
|    total_timesteps          | 1202176       |
| train/                      |               |
|    approx_kl                | 0.006388962   |
|    approx_ln(kl)            | -5.0531836    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.909         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.54          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 4.66          |
|    n_updates                | 638           |
|    policy_gradient_loss     | -0.000109     |
|    std                      | 1.1           |
|    value_loss               | 1.91          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.15674986] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 49            |
|    time_elapsed             | 649           |
|    total_timesteps          | 1204224       |
| train/                      |               |
|    approx_kl                | 0.007152448   |
|    approx_ln(kl)            | -4.9403005    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.18         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.113         |
|    n_updates                | 639           |
|    policy_gradient_loss     | -0.0036       |
|    std                      | 1.1           |
|    value_loss               | 0.181         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
--------------------------------------
| reward             | [-0.27810022] |
| time/              |               |
|    fps             | 158           |
|    iterations      | 1             |
|    time_elapsed    | 12            |
|    total_timesteps | 1206272       |
--------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43780607] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 2             |
|    time_elapsed             | 26            |
|    total_timesteps          | 1208320       |
| train/                      |               |
|    approx_kl                | 0.008403386   |
|    approx_ln(kl)            | -4.7791204    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.84         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.158         |
|    n_updates                | 641           |
|    policy_gradient_loss     | -0.00345      |
|    std                      | 1.1           |
|    value_loss               | 1.2           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44431403] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 3             |
|    time_elapsed             | 39            |
|    total_timesteps          | 1210368       |
| train/                      |               |
|    approx_kl                | 0.007675239   |
|    approx_ln(kl)            | -4.8697557    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.21         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.109         |
|    n_updates                | 642           |
|    policy_gradient_loss     | -0.00707      |
|    std                      | 1.1           |
|    value_loss               | 0.245         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27544594] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 4             |
|    time_elapsed             | 52            |
|    total_timesteps          | 1212416       |
| train/                      |               |
|    approx_kl                | 0.0060901376  |
|    approx_ln(kl)            | -5.1010847    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.77         |
|    ln(policy_gradient_loss) | -5.87         |
|    loss                     | 0.063         |
|    n_updates                | 643           |
|    policy_gradient_loss     | 0.00282       |
|    std                      | 1.1           |
|    value_loss               | 0.157         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.28055194] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 5             |
|    time_elapsed             | 65            |
|    total_timesteps          | 1214464       |
| train/                      |               |
|    approx_kl                | 0.008993982   |
|    approx_ln(kl)            | -4.7111998    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.03         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.132         |
|    n_updates                | 644           |
|    policy_gradient_loss     | -0.00273      |
|    std                      | 1.1           |
|    value_loss               | 0.206         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39919528] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 6             |
|    time_elapsed             | 79            |
|    total_timesteps          | 1216512       |
| train/                      |               |
|    approx_kl                | 0.006289006   |
|    approx_ln(kl)            | -5.068952     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.2          |
|    ln(policy_gradient_loss) | -5.57         |
|    loss                     | 0.111         |
|    n_updates                | 645           |
|    policy_gradient_loss     | 0.00381       |
|    std                      | 1.1           |
|    value_loss               | 0.234         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42526647] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 7             |
|    time_elapsed             | 92            |
|    total_timesteps          | 1218560       |
| train/                      |               |
|    approx_kl                | 0.0066822446  |
|    approx_ln(kl)            | -5.0083013    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.03         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.48         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0839        |
|    n_updates                | 646           |
|    policy_gradient_loss     | -0.00399      |
|    std                      | 1.1           |
|    value_loss               | 0.154         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2959776] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 8            |
|    time_elapsed             | 104          |
|    total_timesteps          | 1220608      |
| train/                      |              |
|    approx_kl                | 0.007974559  |
|    approx_ln(kl)            | -4.831499    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.03        |
|    explained_variance       | 0.995        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.47        |
|    ln(policy_gradient_loss) | -4.72        |
|    loss                     | 0.0842       |
|    n_updates                | 647          |
|    policy_gradient_loss     | 0.00888      |
|    std                      | 1.1          |
|    value_loss               | 0.102        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49852017] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 9             |
|    time_elapsed             | 117           |
|    total_timesteps          | 1222656       |
| train/                      |               |
|    approx_kl                | 0.002922393   |
|    approx_ln(kl)            | -5.8353524    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.28         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0376        |
|    n_updates                | 648           |
|    policy_gradient_loss     | -0.00253      |
|    std                      | 1.1           |
|    value_loss               | 0.112         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5418235] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 10           |
|    time_elapsed             | 130          |
|    total_timesteps          | 1224704      |
| train/                      |              |
|    approx_kl                | 0.0043067276 |
|    approx_ln(kl)            | -5.447577    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.86        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0573       |
|    n_updates                | 649          |
|    policy_gradient_loss     | -0.00932     |
|    std                      | 1.1          |
|    value_loss               | 0.184        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31978872] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 11            |
|    time_elapsed             | 143           |
|    total_timesteps          | 1226752       |
| train/                      |               |
|    approx_kl                | 0.005774962   |
|    approx_ln(kl)            | -5.1542234    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.45         |
|    ln(policy_gradient_loss) | -5.48         |
|    loss                     | 0.0863        |
|    n_updates                | 650           |
|    policy_gradient_loss     | 0.00417       |
|    std                      | 1.09          |
|    value_loss               | 0.134         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.34718734] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 12            |
|    time_elapsed             | 157           |
|    total_timesteps          | 1228800       |
| train/                      |               |
|    approx_kl                | 0.008283165   |
|    approx_ln(kl)            | -4.79353      |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.51         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0299        |
|    n_updates                | 651           |
|    policy_gradient_loss     | -0.0168       |
|    std                      | 1.09          |
|    value_loss               | 0.119         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3382093] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 13           |
|    time_elapsed             | 171          |
|    total_timesteps          | 1230848      |
| train/                      |              |
|    approx_kl                | 0.008207286  |
|    approx_ln(kl)            | -4.802733    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.974        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.22         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 3.39         |
|    n_updates                | 652          |
|    policy_gradient_loss     | -0.00246     |
|    std                      | 1.09         |
|    value_loss               | 1.98         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5490553] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 14           |
|    time_elapsed             | 184          |
|    total_timesteps          | 1232896      |
| train/                      |              |
|    approx_kl                | 0.006868054  |
|    approx_ln(kl)            | -4.9808745   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.29        |
|    ln(policy_gradient_loss) | -6.04        |
|    loss                     | 0.101        |
|    n_updates                | 653          |
|    policy_gradient_loss     | 0.00239      |
|    std                      | 1.09         |
|    value_loss               | 0.142        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39704034] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 15            |
|    time_elapsed             | 197           |
|    total_timesteps          | 1234944       |
| train/                      |               |
|    approx_kl                | 0.0065239808  |
|    approx_ln(kl)            | -5.0322704    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.89         |
|    ln(policy_gradient_loss) | -7.56         |
|    loss                     | 0.0555        |
|    n_updates                | 654           |
|    policy_gradient_loss     | 0.000521      |
|    std                      | 1.09          |
|    value_loss               | 0.144         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26099697] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 16            |
|    time_elapsed             | 210           |
|    total_timesteps          | 1236992       |
| train/                      |               |
|    approx_kl                | 0.007372716   |
|    approx_ln(kl)            | -4.9099693    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.37         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0127        |
|    n_updates                | 655           |
|    policy_gradient_loss     | -0.0104       |
|    std                      | 1.09          |
|    value_loss               | 0.109         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48601022] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 17            |
|    time_elapsed             | 223           |
|    total_timesteps          | 1239040       |
| train/                      |               |
|    approx_kl                | 0.007014123   |
|    approx_ln(kl)            | -4.95983      |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.7          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.183         |
|    n_updates                | 656           |
|    policy_gradient_loss     | -0.00256      |
|    std                      | 1.09          |
|    value_loss               | 1.3           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.5222239] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 18           |
|    time_elapsed             | 236          |
|    total_timesteps          | 1241088      |
| train/                      |              |
|    approx_kl                | 0.010102704  |
|    approx_ln(kl)            | -4.594952    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.23        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0395       |
|    n_updates                | 657          |
|    policy_gradient_loss     | -0.0214      |
|    std                      | 1.09         |
|    value_loss               | 0.118        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45618874] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 19            |
|    time_elapsed             | 249           |
|    total_timesteps          | 1243136       |
| train/                      |               |
|    approx_kl                | 0.0065050498  |
|    approx_ln(kl)            | -5.0351763    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.77         |
|    ln(policy_gradient_loss) | -5.87         |
|    loss                     | 0.0625        |
|    n_updates                | 658           |
|    policy_gradient_loss     | 0.00283       |
|    std                      | 1.09          |
|    value_loss               | 0.125         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33377406] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 20            |
|    time_elapsed             | 262           |
|    total_timesteps          | 1245184       |
| train/                      |               |
|    approx_kl                | 0.00825183    |
|    approx_ln(kl)            | -4.7973204    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.58         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0755        |
|    n_updates                | 659           |
|    policy_gradient_loss     | -0.000561     |
|    std                      | 1.09          |
|    value_loss               | 0.159         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5202532] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 21           |
|    time_elapsed             | 275          |
|    total_timesteps          | 1247232      |
| train/                      |              |
|    approx_kl                | 0.0076119155 |
|    approx_ln(kl)            | -4.8780403   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.13        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.119        |
|    n_updates                | 660          |
|    policy_gradient_loss     | -0.00301     |
|    std                      | 1.09         |
|    value_loss               | 0.244        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.19743332] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 22            |
|    time_elapsed             | 288           |
|    total_timesteps          | 1249280       |
| train/                      |               |
|    approx_kl                | 0.0058472175  |
|    approx_ln(kl)            | -5.1417894    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.3          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0367        |
|    n_updates                | 661           |
|    policy_gradient_loss     | -0.00249      |
|    std                      | 1.09          |
|    value_loss               | 0.0688        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3525066] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 23           |
|    time_elapsed             | 301          |
|    total_timesteps          | 1251328      |
| train/                      |              |
|    approx_kl                | 0.0058496534 |
|    approx_ln(kl)            | -5.1413727   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.26        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.104        |
|    n_updates                | 662          |
|    policy_gradient_loss     | -0.00264     |
|    std                      | 1.09         |
|    value_loss               | 0.229        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31168726] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 24            |
|    time_elapsed             | 314           |
|    total_timesteps          | 1253376       |
| train/                      |               |
|    approx_kl                | 0.0058763837  |
|    approx_ln(kl)            | -5.1368136    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.975         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.205         |
|    ln(policy_gradient_loss) | -5.66         |
|    loss                     | 1.23          |
|    n_updates                | 663           |
|    policy_gradient_loss     | 0.00347       |
|    std                      | 1.09          |
|    value_loss               | 3.34          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37349504] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 25            |
|    time_elapsed             | 327           |
|    total_timesteps          | 1255424       |
| train/                      |               |
|    approx_kl                | 0.0077736913  |
|    approx_ln(kl)            | -4.8570104    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.52         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0109        |
|    n_updates                | 664           |
|    policy_gradient_loss     | -0.00565      |
|    std                      | 1.09          |
|    value_loss               | 0.126         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34188432] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 26            |
|    time_elapsed             | 340           |
|    total_timesteps          | 1257472       |
| train/                      |               |
|    approx_kl                | 0.007109628   |
|    approx_ln(kl)            | -4.9463053    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.46         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0315        |
|    n_updates                | 665           |
|    policy_gradient_loss     | -0.0129       |
|    std                      | 1.09          |
|    value_loss               | 0.0953        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.2799449] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 27           |
|    time_elapsed             | 353          |
|    total_timesteps          | 1259520      |
| train/                      |              |
|    approx_kl                | 0.008680829  |
|    approx_ln(kl)            | -4.7466383   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -4.22        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0147       |
|    n_updates                | 666          |
|    policy_gradient_loss     | -0.00306     |
|    std                      | 1.09         |
|    value_loss               | 0.0371       |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22588103] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 28            |
|    time_elapsed             | 367           |
|    total_timesteps          | 1261568       |
| train/                      |               |
|    approx_kl                | 0.0048764823  |
|    approx_ln(kl)            | -5.3233314    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.19         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0411        |
|    n_updates                | 667           |
|    policy_gradient_loss     | -0.00242      |
|    std                      | 1.09          |
|    value_loss               | 0.112         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33994558] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 29            |
|    time_elapsed             | 380           |
|    total_timesteps          | 1263616       |
| train/                      |               |
|    approx_kl                | 0.0076130484  |
|    approx_ln(kl)            | -4.8778915    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.24         |
|    ln(policy_gradient_loss) | -5.82         |
|    loss                     | 0.039         |
|    n_updates                | 668           |
|    policy_gradient_loss     | 0.00298       |
|    std                      | 1.1           |
|    value_loss               | 0.0978        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4243084] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 30           |
|    time_elapsed             | 393          |
|    total_timesteps          | 1265664      |
| train/                      |              |
|    approx_kl                | 0.0067225825 |
|    approx_ln(kl)            | -5.002283    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3           |
|    ln(policy_gradient_loss) | -5.52        |
|    loss                     | 0.0497       |
|    n_updates                | 669          |
|    policy_gradient_loss     | 0.00401      |
|    std                      | 1.09         |
|    value_loss               | 0.209        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41142944] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 31            |
|    time_elapsed             | 406           |
|    total_timesteps          | 1267712       |
| train/                      |               |
|    approx_kl                | 0.005917108   |
|    approx_ln(kl)            | -5.1299076    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | nan           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | -0.0111       |
|    n_updates                | 670           |
|    policy_gradient_loss     | -0.0126       |
|    std                      | 1.09          |
|    value_loss               | 0.0403        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25353974] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 32            |
|    time_elapsed             | 420           |
|    total_timesteps          | 1269760       |
| train/                      |               |
|    approx_kl                | 0.007207314   |
|    approx_ln(kl)            | -4.932659     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -8.46         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.000212      |
|    n_updates                | 671           |
|    policy_gradient_loss     | -0.0145       |
|    std                      | 1.09          |
|    value_loss               | 0.159         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.48242214] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 33            |
|    time_elapsed             | 433           |
|    total_timesteps          | 1271808       |
| train/                      |               |
|    approx_kl                | 0.009882138   |
|    approx_ln(kl)            | -4.6170263    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.02         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.53         |
|    ln(policy_gradient_loss) | -6.74         |
|    loss                     | 0.0795        |
|    n_updates                | 672           |
|    policy_gradient_loss     | 0.00119       |
|    std                      | 1.09          |
|    value_loss               | 0.173         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5005846] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 34           |
|    time_elapsed             | 445          |
|    total_timesteps          | 1273856      |
| train/                      |              |
|    approx_kl                | 0.007710366  |
|    approx_ln(kl)            | -4.8651896   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.02        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.28        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.102        |
|    n_updates                | 673          |
|    policy_gradient_loss     | -0.00972     |
|    std                      | 1.09         |
|    value_loss               | 0.203        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.4712305] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 35           |
|    time_elapsed             | 458          |
|    total_timesteps          | 1275904      |
| train/                      |              |
|    approx_kl                | 0.010196934  |
|    approx_ln(kl)            | -4.585668    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.16        |
|    ln(policy_gradient_loss) | -5.37        |
|    loss                     | 0.116        |
|    n_updates                | 674          |
|    policy_gradient_loss     | 0.00463      |
|    std                      | 1.09         |
|    value_loss               | 0.237        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3484748] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 36           |
|    time_elapsed             | 471          |
|    total_timesteps          | 1277952      |
| train/                      |              |
|    approx_kl                | 0.004626385  |
|    approx_ln(kl)            | -5.3759794   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.89        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0553       |
|    n_updates                | 675          |
|    policy_gradient_loss     | -0.00482     |
|    std                      | 1.09         |
|    value_loss               | 0.132        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47534043] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 37            |
|    time_elapsed             | 485           |
|    total_timesteps          | 1280000       |
| train/                      |               |
|    approx_kl                | 0.0077130236  |
|    approx_ln(kl)            | -4.864845     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.53         |
|    ln(policy_gradient_loss) | -5.42         |
|    loss                     | 0.0793        |
|    n_updates                | 676           |
|    policy_gradient_loss     | 0.00442       |
|    std                      | 1.09          |
|    value_loss               | 0.119         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.44027418] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 38            |
|    time_elapsed             | 498           |
|    total_timesteps          | 1282048       |
| train/                      |               |
|    approx_kl                | 0.011155764   |
|    approx_ln(kl)            | -4.495799     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.71         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0665        |
|    n_updates                | 677           |
|    policy_gradient_loss     | -0.00877      |
|    std                      | 1.09          |
|    value_loss               | 0.147         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37851673] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 39            |
|    time_elapsed             | 511           |
|    total_timesteps          | 1284096       |
| train/                      |               |
|    approx_kl                | 0.0077047534  |
|    approx_ln(kl)            | -4.8659177    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.71         |
|    ln(policy_gradient_loss) | -5.16         |
|    loss                     | 0.0245        |
|    n_updates                | 678           |
|    policy_gradient_loss     | 0.00573       |
|    std                      | 1.09          |
|    value_loss               | 0.0943        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42252648] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 40            |
|    time_elapsed             | 525           |
|    total_timesteps          | 1286144       |
| train/                      |               |
|    approx_kl                | 0.008006917   |
|    approx_ln(kl)            | -4.8274493    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.48         |
|    ln(policy_gradient_loss) | -6.21         |
|    loss                     | 0.0113        |
|    n_updates                | 679           |
|    policy_gradient_loss     | 0.002         |
|    std                      | 1.09          |
|    value_loss               | 0.0928        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27456397] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 41            |
|    time_elapsed             | 538           |
|    total_timesteps          | 1288192       |
| train/                      |               |
|    approx_kl                | 0.0075268457  |
|    approx_ln(kl)            | -4.8892794    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.18         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0415        |
|    n_updates                | 680           |
|    policy_gradient_loss     | -0.0135       |
|    std                      | 1.09          |
|    value_loss               | 0.109         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.51381284] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 42            |
|    time_elapsed             | 552           |
|    total_timesteps          | 1290240       |
| train/                      |               |
|    approx_kl                | 0.0060063303  |
|    approx_ln(kl)            | -5.1149416    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.29         |
|    ln(policy_gradient_loss) | -6.3          |
|    loss                     | 0.0373        |
|    n_updates                | 681           |
|    policy_gradient_loss     | 0.00184       |
|    std                      | 1.08          |
|    value_loss               | 0.105         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.5532444] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 43           |
|    time_elapsed             | 565          |
|    total_timesteps          | 1292288      |
| train/                      |              |
|    approx_kl                | 0.007867988  |
|    approx_ln(kl)            | -4.844953    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.3         |
|    ln(policy_gradient_loss) | -6.57        |
|    loss                     | 0.0999       |
|    n_updates                | 682          |
|    policy_gradient_loss     | 0.0014       |
|    std                      | 1.09         |
|    value_loss               | 0.132        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30943075] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 44            |
|    time_elapsed             | 578           |
|    total_timesteps          | 1294336       |
| train/                      |               |
|    approx_kl                | 0.006816785   |
|    approx_ln(kl)            | -4.9883676    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.78         |
|    ln(policy_gradient_loss) | -5.83         |
|    loss                     | 0.0621        |
|    n_updates                | 683           |
|    policy_gradient_loss     | 0.00294       |
|    std                      | 1.09          |
|    value_loss               | 0.0663        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5374583] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 45           |
|    time_elapsed             | 591          |
|    total_timesteps          | 1296384      |
| train/                      |              |
|    approx_kl                | 0.009333305  |
|    approx_ln(kl)            | -4.674166    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | nan          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | -0.049       |
|    n_updates                | 684          |
|    policy_gradient_loss     | -0.0203      |
|    std                      | 1.09         |
|    value_loss               | 0.0682       |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
---------------------------------------------
| reward                      | [-0.454716] |
| time/                       |             |
|    fps                      | 155         |
|    iterations               | 46          |
|    time_elapsed             | 604         |
|    total_timesteps          | 1298432     |
| train/                      |             |
|    approx_kl                | 0.007676834 |
|    approx_ln(kl)            | -4.8695483  |
|    clip_range               | 0.2         |
|    entropy_loss             | -3          |
|    explained_variance       | 0.997       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -1.93       |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.145       |
|    n_updates                | 685         |
|    policy_gradient_loss     | -0.00703    |
|    std                      | 1.09        |
|    value_loss               | 0.228       |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2936738] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 47           |
|    time_elapsed             | 617          |
|    total_timesteps          | 1300480      |
| train/                      |              |
|    approx_kl                | 0.0080910055 |
|    approx_ln(kl)            | -4.8170023   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.25        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0386       |
|    n_updates                | 686          |
|    policy_gradient_loss     | -0.00564     |
|    std                      | 1.09         |
|    value_loss               | 0.113        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48556107] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 48            |
|    time_elapsed             | 631           |
|    total_timesteps          | 1302528       |
| train/                      |               |
|    approx_kl                | 0.006051559   |
|    approx_ln(kl)            | -5.1074395    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.977         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.32         |
|    ln(policy_gradient_loss) | -5.1          |
|    loss                     | 0.268         |
|    n_updates                | 687           |
|    policy_gradient_loss     | 0.00611       |
|    std                      | 1.09          |
|    value_loss               | 0.451         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48190522] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 49            |
|    time_elapsed             | 644           |
|    total_timesteps          | 1304576       |
| train/                      |               |
|    approx_kl                | 0.0064194007  |
|    approx_ln(kl)            | -5.0484304    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.82         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.441         |
|    n_updates                | 688           |
|    policy_gradient_loss     | -0.0112       |
|    std                      | 1.09          |
|    value_loss               | 3.52          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
--------------------------------------
| reward             | [-0.30822778] |
| time/              |               |
|    fps             | 157           |
|    iterations      | 1             |
|    time_elapsed    | 12            |
|    total_timesteps | 1306624       |
--------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37075368] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 2             |
|    time_elapsed             | 26            |
|    total_timesteps          | 1308672       |
| train/                      |               |
|    approx_kl                | 0.006597447   |
|    approx_ln(kl)            | -5.0210724    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.965         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.76         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.172         |
|    n_updates                | 690           |
|    policy_gradient_loss     | -0.00469      |
|    std                      | 1.09          |
|    value_loss               | 0.54          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.45140547] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 3             |
|    time_elapsed             | 40            |
|    total_timesteps          | 1310720       |
| train/                      |               |
|    approx_kl                | 0.0060972925  |
|    approx_ln(kl)            | -5.0999107    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.5          |
|    ln(policy_gradient_loss) | -7.06         |
|    loss                     | 0.0821        |
|    n_updates                | 691           |
|    policy_gradient_loss     | 0.00086       |
|    std                      | 1.09          |
|    value_loss               | 0.189         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30808714] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 4             |
|    time_elapsed             | 53            |
|    total_timesteps          | 1312768       |
| train/                      |               |
|    approx_kl                | 0.006479286   |
|    approx_ln(kl)            | -5.039145     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.36         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0946        |
|    n_updates                | 692           |
|    policy_gradient_loss     | -0.000462     |
|    std                      | 1.09          |
|    value_loss               | 0.273         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3279877] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 5            |
|    time_elapsed             | 65           |
|    total_timesteps          | 1314816      |
| train/                      |              |
|    approx_kl                | 0.0048021046 |
|    approx_ln(kl)            | -5.3387012   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.72        |
|    ln(policy_gradient_loss) | -5.98        |
|    loss                     | 0.179        |
|    n_updates                | 693          |
|    policy_gradient_loss     | 0.00252      |
|    std                      | 1.09         |
|    value_loss               | 0.255        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5478572] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 6            |
|    time_elapsed             | 78           |
|    total_timesteps          | 1316864      |
| train/                      |              |
|    approx_kl                | 0.007937887  |
|    approx_ln(kl)            | -4.836108    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.39        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0919       |
|    n_updates                | 694          |
|    policy_gradient_loss     | -0.00755     |
|    std                      | 1.09         |
|    value_loss               | 0.183        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.44548833] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 7             |
|    time_elapsed             | 92            |
|    total_timesteps          | 1318912       |
| train/                      |               |
|    approx_kl                | 0.008243507   |
|    approx_ln(kl)            | -4.7983294    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.943         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.54         |
|    ln(policy_gradient_loss) | -5.07         |
|    loss                     | 0.215         |
|    n_updates                | 695           |
|    policy_gradient_loss     | 0.00627       |
|    std                      | 1.09          |
|    value_loss               | 0.375         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3308528] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 8            |
|    time_elapsed             | 105          |
|    total_timesteps          | 1320960      |
| train/                      |              |
|    approx_kl                | 0.0068428167 |
|    approx_ln(kl)            | -4.9845557   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3.01        |
|    explained_variance       | 0.981        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.0236       |
|    ln(policy_gradient_loss) | -5.41        |
|    loss                     | 1.02         |
|    n_updates                | 696          |
|    policy_gradient_loss     | 0.00445      |
|    std                      | 1.09         |
|    value_loss               | 3.92         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41925296] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 9             |
|    time_elapsed             | 118           |
|    total_timesteps          | 1323008       |
| train/                      |               |
|    approx_kl                | 0.0063453764  |
|    approx_ln(kl)            | -5.060029     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -6.65         |
|    ln(policy_gradient_loss) | -7.5          |
|    loss                     | 0.00129       |
|    n_updates                | 697           |
|    policy_gradient_loss     | 0.000551      |
|    std                      | 1.09          |
|    value_loss               | 0.108         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3571717] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 10           |
|    time_elapsed             | 131          |
|    total_timesteps          | 1325056      |
| train/                      |              |
|    approx_kl                | 0.008739377  |
|    approx_ln(kl)            | -4.7399163   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.17        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.114        |
|    n_updates                | 698          |
|    policy_gradient_loss     | -0.000877    |
|    std                      | 1.09         |
|    value_loss               | 0.214        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36291313] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 11            |
|    time_elapsed             | 144           |
|    total_timesteps          | 1327104       |
| train/                      |               |
|    approx_kl                | 0.008922047   |
|    approx_ln(kl)            | -4.7192297    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.4          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0908        |
|    n_updates                | 699           |
|    policy_gradient_loss     | -0.00268      |
|    std                      | 1.09          |
|    value_loss               | 0.17          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.1966197] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 12           |
|    time_elapsed             | 157          |
|    total_timesteps          | 1329152      |
| train/                      |              |
|    approx_kl                | 0.0070827105 |
|    approx_ln(kl)            | -4.9500985   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.11        |
|    ln(policy_gradient_loss) | -5.48        |
|    loss                     | 0.0447       |
|    n_updates                | 700          |
|    policy_gradient_loss     | 0.00416      |
|    std                      | 1.08         |
|    value_loss               | 0.0696       |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48440087] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 13            |
|    time_elapsed             | 171           |
|    total_timesteps          | 1331200       |
| train/                      |               |
|    approx_kl                | 0.00538388    |
|    approx_ln(kl)            | -5.224346     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.804         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 2.23          |
|    n_updates                | 701           |
|    policy_gradient_loss     | -0.00383      |
|    std                      | 1.08          |
|    value_loss               | 1.58          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.58555406] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 14            |
|    time_elapsed             | 184           |
|    total_timesteps          | 1333248       |
| train/                      |               |
|    approx_kl                | 0.0040613497  |
|    approx_ln(kl)            | -5.50624      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.9          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0551        |
|    n_updates                | 702           |
|    policy_gradient_loss     | -0.00229      |
|    std                      | 1.08          |
|    value_loss               | 1.15          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30885646] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 15            |
|    time_elapsed             | 197           |
|    total_timesteps          | 1335296       |
| train/                      |               |
|    approx_kl                | 0.005118929   |
|    approx_ln(kl)            | -5.27481      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.97          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.705        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.494         |
|    n_updates                | 703           |
|    policy_gradient_loss     | -0.000763     |
|    std                      | 1.08          |
|    value_loss               | 0.977         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37922472] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 16            |
|    time_elapsed             | 209           |
|    total_timesteps          | 1337344       |
| train/                      |               |
|    approx_kl                | 0.006438723   |
|    approx_ln(kl)            | -5.045425     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 1             |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.97         |
|    ln(policy_gradient_loss) | -6.3          |
|    loss                     | 0.0189        |
|    n_updates                | 704           |
|    policy_gradient_loss     | 0.00183       |
|    std                      | 1.08          |
|    value_loss               | 0.118         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5023094] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 17           |
|    time_elapsed             | 223          |
|    total_timesteps          | 1339392      |
| train/                      |              |
|    approx_kl                | 0.0039045822 |
|    approx_ln(kl)            | -5.5456047   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.99        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | nan          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | -0.00932     |
|    n_updates                | 705          |
|    policy_gradient_loss     | -0.00489     |
|    std                      | 1.08         |
|    value_loss               | 0.141        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2968671] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 18           |
|    time_elapsed             | 236          |
|    total_timesteps          | 1341440      |
| train/                      |              |
|    approx_kl                | 0.0062175686 |
|    approx_ln(kl)            | -5.080376    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.99        |
|    explained_variance       | 0.993        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -4.3         |
|    ln(policy_gradient_loss) | -6.29        |
|    loss                     | 0.0136       |
|    n_updates                | 706          |
|    policy_gradient_loss     | 0.00185      |
|    std                      | 1.08         |
|    value_loss               | 0.149        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.21875471] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 19            |
|    time_elapsed             | 249           |
|    total_timesteps          | 1343488       |
| train/                      |               |
|    approx_kl                | 0.0059259883  |
|    approx_ln(kl)            | -5.128408     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.1          |
|    ln(policy_gradient_loss) | -4.96         |
|    loss                     | 0.122         |
|    n_updates                | 707           |
|    policy_gradient_loss     | 0.00704       |
|    std                      | 1.08          |
|    value_loss               | 0.0751        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4922839] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 20           |
|    time_elapsed             | 262          |
|    total_timesteps          | 1345536      |
| train/                      |              |
|    approx_kl                | 0.007309814  |
|    approx_ln(kl)            | -4.9185376   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.99        |
|    explained_variance       | 0.995        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.3         |
|    ln(policy_gradient_loss) | -6.66        |
|    loss                     | 0.0367       |
|    n_updates                | 708          |
|    policy_gradient_loss     | 0.00128      |
|    std                      | 1.08         |
|    value_loss               | 0.125        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32990354] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 21            |
|    time_elapsed             | 275           |
|    total_timesteps          | 1347584       |
| train/                      |               |
|    approx_kl                | 0.0053313863  |
|    approx_ln(kl)            | -5.234144     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.98         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -5.55         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.00388       |
|    n_updates                | 709           |
|    policy_gradient_loss     | -0.00219      |
|    std                      | 1.08          |
|    value_loss               | 0.0444        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2930043] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 22           |
|    time_elapsed             | 288          |
|    total_timesteps          | 1349632      |
| train/                      |              |
|    approx_kl                | 0.007000694  |
|    approx_ln(kl)            | -4.961746    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.99        |
|    explained_variance       | 1            |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.45        |
|    ln(policy_gradient_loss) | -5.43        |
|    loss                     | 0.0318       |
|    n_updates                | 710          |
|    policy_gradient_loss     | 0.00437      |
|    std                      | 1.08         |
|    value_loss               | 0.0485       |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.58709854] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 23            |
|    time_elapsed             | 301           |
|    total_timesteps          | 1351680       |
| train/                      |               |
|    approx_kl                | 0.006046127   |
|    approx_ln(kl)            | -5.1083374    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.12         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0162        |
|    n_updates                | 711           |
|    policy_gradient_loss     | -0.000553     |
|    std                      | 1.08          |
|    value_loss               | 0.0815        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5041639] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 24           |
|    time_elapsed             | 314          |
|    total_timesteps          | 1353728      |
| train/                      |              |
|    approx_kl                | 0.0074603343 |
|    approx_ln(kl)            | -4.898155    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.99        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | nan          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | -0.0482      |
|    n_updates                | 712          |
|    policy_gradient_loss     | -0.0163      |
|    std                      | 1.08         |
|    value_loss               | 0.0985       |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3595185] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 25           |
|    time_elapsed             | 327          |
|    total_timesteps          | 1355776      |
| train/                      |              |
|    approx_kl                | 0.006531284  |
|    approx_ln(kl)            | -5.031152    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.99        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.99        |
|    ln(policy_gradient_loss) | -5.91        |
|    loss                     | 0.0501       |
|    n_updates                | 713          |
|    policy_gradient_loss     | 0.00271      |
|    std                      | 1.08         |
|    value_loss               | 0.0917       |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5535182] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 26           |
|    time_elapsed             | 340          |
|    total_timesteps          | 1357824      |
| train/                      |              |
|    approx_kl                | 0.0071680765 |
|    approx_ln(kl)            | -4.938118    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.99        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -5.1         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.00612      |
|    n_updates                | 714          |
|    policy_gradient_loss     | -0.00376     |
|    std                      | 1.08         |
|    value_loss               | 0.0513       |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.3802743] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 27           |
|    time_elapsed             | 354          |
|    total_timesteps          | 1359872      |
| train/                      |              |
|    approx_kl                | 0.0121914    |
|    approx_ln(kl)            | -4.4070244   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -4.28        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0138       |
|    n_updates                | 715          |
|    policy_gradient_loss     | -0.00825     |
|    std                      | 1.08         |
|    value_loss               | 0.0766       |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5198343] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 28           |
|    time_elapsed             | 367          |
|    total_timesteps          | 1361920      |
| train/                      |              |
|    approx_kl                | 0.0055112317 |
|    approx_ln(kl)            | -5.2009673   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.44        |
|    ln(policy_gradient_loss) | -8.29        |
|    loss                     | 0.0319       |
|    n_updates                | 716          |
|    policy_gradient_loss     | 0.000251     |
|    std                      | 1.09         |
|    value_loss               | 0.0821       |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.35440937] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 29            |
|    time_elapsed             | 380           |
|    total_timesteps          | 1363968       |
| train/                      |               |
|    approx_kl                | 0.008079057   |
|    approx_ln(kl)            | -4.81848      |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.06         |
|    ln(policy_gradient_loss) | -7.1          |
|    loss                     | 0.0173        |
|    n_updates                | 717           |
|    policy_gradient_loss     | 0.000829      |
|    std                      | 1.09          |
|    value_loss               | 0.0767        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5233456] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 30           |
|    time_elapsed             | 394          |
|    total_timesteps          | 1366016      |
| train/                      |              |
|    approx_kl                | 0.0071029365 |
|    approx_ln(kl)            | -4.947247    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | nan          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | -0.0145      |
|    n_updates                | 718          |
|    policy_gradient_loss     | -0.0194      |
|    std                      | 1.09         |
|    value_loss               | 0.0554       |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.52556026] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 31            |
|    time_elapsed             | 407           |
|    total_timesteps          | 1368064       |
| train/                      |               |
|    approx_kl                | 0.01019212    |
|    approx_ln(kl)            | -4.58614      |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.95         |
|    ln(policy_gradient_loss) | -4.53         |
|    loss                     | 0.142         |
|    n_updates                | 719           |
|    policy_gradient_loss     | 0.0108        |
|    std                      | 1.09          |
|    value_loss               | 0.136         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44784087] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 32            |
|    time_elapsed             | 420           |
|    total_timesteps          | 1370112       |
| train/                      |               |
|    approx_kl                | 0.0073621348  |
|    approx_ln(kl)            | -4.9114056    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 1             |
|    learning_rate            | 0.001         |
|    ln(loss)                 | nan           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | -0.00726      |
|    n_updates                | 720           |
|    policy_gradient_loss     | -0.0075       |
|    std                      | 1.09          |
|    value_loss               | 0.0531        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37823123] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 33            |
|    time_elapsed             | 433           |
|    total_timesteps          | 1372160       |
| train/                      |               |
|    approx_kl                | 0.0077582486  |
|    approx_ln(kl)            | -4.858999     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.17         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0418        |
|    n_updates                | 721           |
|    policy_gradient_loss     | -0.00501      |
|    std                      | 1.08          |
|    value_loss               | 1.6           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37554738] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 34            |
|    time_elapsed             | 446           |
|    total_timesteps          | 1374208       |
| train/                      |               |
|    approx_kl                | 0.007218739   |
|    approx_ln(kl)            | -4.931075     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.133         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.14          |
|    n_updates                | 722           |
|    policy_gradient_loss     | -0.0155       |
|    std                      | 1.08          |
|    value_loss               | 0.768         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4204241] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 35           |
|    time_elapsed             | 460          |
|    total_timesteps          | 1376256      |
| train/                      |              |
|    approx_kl                | 0.008561849  |
|    approx_ln(kl)            | -4.7604394   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.964        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.677       |
|    ln(policy_gradient_loss) | -4.81        |
|    loss                     | 0.508        |
|    n_updates                | 723          |
|    policy_gradient_loss     | 0.00814      |
|    std                      | 1.08         |
|    value_loss               | 1.9          |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.55300784] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 36            |
|    time_elapsed             | 473           |
|    total_timesteps          | 1378304       |
| train/                      |               |
|    approx_kl                | 0.0082233995  |
|    approx_ln(kl)            | -4.8007717    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.935        |
|    ln(policy_gradient_loss) | -5.32         |
|    loss                     | 0.393         |
|    n_updates                | 724           |
|    policy_gradient_loss     | 0.00491       |
|    std                      | 1.08          |
|    value_loss               | 0.719         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.5188967] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 37           |
|    time_elapsed             | 486          |
|    total_timesteps          | 1380352      |
| train/                      |              |
|    approx_kl                | 0.008197223  |
|    approx_ln(kl)            | -4.80396     |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | nan          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | -0.00872     |
|    n_updates                | 725          |
|    policy_gradient_loss     | -0.00872     |
|    std                      | 1.09         |
|    value_loss               | 0.107        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48945278] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 38            |
|    time_elapsed             | 499           |
|    total_timesteps          | 1382400       |
| train/                      |               |
|    approx_kl                | 0.0068424107  |
|    approx_ln(kl)            | -4.9846153    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.08         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0461        |
|    n_updates                | 726           |
|    policy_gradient_loss     | -0.00458      |
|    std                      | 1.09          |
|    value_loss               | 0.167         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48351562] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 39            |
|    time_elapsed             | 512           |
|    total_timesteps          | 1384448       |
| train/                      |               |
|    approx_kl                | 0.0045475797  |
|    approx_ln(kl)            | -5.3931603    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.42         |
|    ln(policy_gradient_loss) | -7.38         |
|    loss                     | 0.012         |
|    n_updates                | 727           |
|    policy_gradient_loss     | 0.000622      |
|    std                      | 1.08          |
|    value_loss               | 0.0317        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.4743813] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 40           |
|    time_elapsed             | 525          |
|    total_timesteps          | 1386496      |
| train/                      |              |
|    approx_kl                | 0.01032661   |
|    approx_ln(kl)            | -4.5730314   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.895        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.67        |
|    ln(policy_gradient_loss) | -5.58        |
|    loss                     | 0.0254       |
|    n_updates                | 728          |
|    policy_gradient_loss     | 0.00377      |
|    std                      | 1.08         |
|    value_loss               | 0.0558       |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3873967] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 41           |
|    time_elapsed             | 538          |
|    total_timesteps          | 1388544      |
| train/                      |              |
|    approx_kl                | 0.006997534  |
|    approx_ln(kl)            | -4.9621973   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.984        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.05        |
|    ln(policy_gradient_loss) | -7.29        |
|    loss                     | 0.129        |
|    n_updates                | 729          |
|    policy_gradient_loss     | 0.00068      |
|    std                      | 1.08         |
|    value_loss               | 1.74         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.31220248] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 42            |
|    time_elapsed             | 551           |
|    total_timesteps          | 1390592       |
| train/                      |               |
|    approx_kl                | 0.007843076   |
|    approx_ln(kl)            | -4.848124     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.95         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.142         |
|    n_updates                | 730           |
|    policy_gradient_loss     | -0.0121       |
|    std                      | 1.08          |
|    value_loss               | 0.477         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.47523656] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 43            |
|    time_elapsed             | 564           |
|    total_timesteps          | 1392640       |
| train/                      |               |
|    approx_kl                | 0.010333816   |
|    approx_ln(kl)            | -4.572334     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0525       |
|    ln(policy_gradient_loss) | -4.97         |
|    loss                     | 0.949         |
|    n_updates                | 731           |
|    policy_gradient_loss     | 0.00693       |
|    std                      | 1.08          |
|    value_loss               | 2.75          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32475504] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 44            |
|    time_elapsed             | 577           |
|    total_timesteps          | 1394688       |
| train/                      |               |
|    approx_kl                | 0.0072268415  |
|    approx_ln(kl)            | -4.929953     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.777        |
|    ln(policy_gradient_loss) | -5.52         |
|    loss                     | 0.46          |
|    n_updates                | 732           |
|    policy_gradient_loss     | 0.004         |
|    std                      | 1.08          |
|    value_loss               | 0.439         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3332689] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 45           |
|    time_elapsed             | 590          |
|    total_timesteps          | 1396736      |
| train/                      |              |
|    approx_kl                | 0.0049680136 |
|    approx_ln(kl)            | -5.304735    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.22        |
|    ln(policy_gradient_loss) | -5.89        |
|    loss                     | 0.0399       |
|    n_updates                | 733          |
|    policy_gradient_loss     | 0.00276      |
|    std                      | 1.08         |
|    value_loss               | 0.0938       |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5193159] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 46           |
|    time_elapsed             | 603          |
|    total_timesteps          | 1398784      |
| train/                      |              |
|    approx_kl                | 0.0069626    |
|    approx_ln(kl)            | -4.967202    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.99        |
|    explained_variance       | 0.98         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.306       |
|    ln(policy_gradient_loss) | -4.48        |
|    loss                     | 0.736        |
|    n_updates                | 734          |
|    policy_gradient_loss     | 0.0113       |
|    std                      | 1.08         |
|    value_loss               | 1.72         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.466687]  |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 47           |
|    time_elapsed             | 616          |
|    total_timesteps          | 1400832      |
| train/                      |              |
|    approx_kl                | 0.0057181874 |
|    approx_ln(kl)            | -5.1641035   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.99        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.44        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0876       |
|    n_updates                | 735          |
|    policy_gradient_loss     | -0.00787     |
|    std                      | 1.08         |
|    value_loss               | 0.187        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25511128] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 48            |
|    time_elapsed             | 629           |
|    total_timesteps          | 1402880       |
| train/                      |               |
|    approx_kl                | 0.009593624   |
|    approx_ln(kl)            | -4.6466565    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.974         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.258        |
|    ln(policy_gradient_loss) | -6.12         |
|    loss                     | 0.773         |
|    n_updates                | 736           |
|    policy_gradient_loss     | 0.00221       |
|    std                      | 1.08          |
|    value_loss               | 3.64          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22775352] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 49            |
|    time_elapsed             | 642           |
|    total_timesteps          | 1404928       |
| train/                      |               |
|    approx_kl                | 0.004338033   |
|    approx_ln(kl)            | -5.4403343    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.94         |
|    ln(policy_gradient_loss) | -6.9          |
|    loss                     | 0.143         |
|    n_updates                | 737           |
|    policy_gradient_loss     | 0.00101       |
|    std                      | 1.08          |
|    value_loss               | 0.186         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------
| reward             | [-0.5364899] |
| time/              |              |
|    fps             | 163          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 1406976      |
-------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.449772] |
| time/                       |             |
|    fps                      | 160         |
|    iterations               | 2           |
|    time_elapsed             | 25          |
|    total_timesteps          | 1409024     |
| train/                      |             |
|    approx_kl                | 0.006448496 |
|    approx_ln(kl)            | -5.043908   |
|    clip_range               | 0.2         |
|    entropy_loss             | -3          |
|    explained_variance       | 0.999       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -2.89       |
|    ln(policy_gradient_loss) | -5.57       |
|    loss                     | 0.0558      |
|    n_updates                | 739         |
|    policy_gradient_loss     | 0.0038      |
|    std                      | 1.08        |
|    value_loss               | 0.108       |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42420828] |
| time/                       |               |
|    fps                      | 158           |
|    iterations               | 3             |
|    time_elapsed             | 38            |
|    total_timesteps          | 1411072       |
| train/                      |               |
|    approx_kl                | 0.0060958555  |
|    approx_ln(kl)            | -5.1001463    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.6          |
|    ln(policy_gradient_loss) | -5.7          |
|    loss                     | 0.0273        |
|    n_updates                | 740           |
|    policy_gradient_loss     | 0.00334       |
|    std                      | 1.09          |
|    value_loss               | 0.0357        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32210767] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 4             |
|    time_elapsed             | 52            |
|    total_timesteps          | 1413120       |
| train/                      |               |
|    approx_kl                | 0.0061663887  |
|    approx_ln(kl)            | -5.088642     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.14         |
|    ln(policy_gradient_loss) | -5.84         |
|    loss                     | 0.32          |
|    n_updates                | 741           |
|    policy_gradient_loss     | 0.00289       |
|    std                      | 1.09          |
|    value_loss               | 2.23          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.48566753] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 5             |
|    time_elapsed             | 66            |
|    total_timesteps          | 1415168       |
| train/                      |               |
|    approx_kl                | 0.0067363014  |
|    approx_ln(kl)            | -5.000244     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.12         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.328         |
|    n_updates                | 742           |
|    policy_gradient_loss     | -0.00713      |
|    std                      | 1.09          |
|    value_loss               | 1.95          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47864383] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 6             |
|    time_elapsed             | 79            |
|    total_timesteps          | 1417216       |
| train/                      |               |
|    approx_kl                | 0.005827846   |
|    approx_ln(kl)            | -5.1451077    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.13         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.323         |
|    n_updates                | 743           |
|    policy_gradient_loss     | -0.0229       |
|    std                      | 1.09          |
|    value_loss               | 2.05          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28160623] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 7             |
|    time_elapsed             | 93            |
|    total_timesteps          | 1419264       |
| train/                      |               |
|    approx_kl                | 0.0052724066  |
|    approx_ln(kl)            | -5.2452683    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.13         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.118         |
|    n_updates                | 744           |
|    policy_gradient_loss     | -0.014        |
|    std                      | 1.09          |
|    value_loss               | 2.01          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48301008] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 8             |
|    time_elapsed             | 106           |
|    total_timesteps          | 1421312       |
| train/                      |               |
|    approx_kl                | 0.0056795957  |
|    approx_ln(kl)            | -5.170875     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3.01         |
|    explained_variance       | 0.968         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.521         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.68          |
|    n_updates                | 745           |
|    policy_gradient_loss     | -0.00165      |
|    std                      | 1.09          |
|    value_loss               | 2.72          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32578862] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 9             |
|    time_elapsed             | 120           |
|    total_timesteps          | 1423360       |
| train/                      |               |
|    approx_kl                | 0.0023207131  |
|    approx_ln(kl)            | -6.065881     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.957        |
|    ln(policy_gradient_loss) | -8.73         |
|    loss                     | 0.384         |
|    n_updates                | 746           |
|    policy_gradient_loss     | 0.000162      |
|    std                      | 1.09          |
|    value_loss               | 1.52          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.20674674] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 10            |
|    time_elapsed             | 133           |
|    total_timesteps          | 1425408       |
| train/                      |               |
|    approx_kl                | 0.0043357103  |
|    approx_ln(kl)            | -5.44087      |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.36         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0346        |
|    n_updates                | 747           |
|    policy_gradient_loss     | -0.0034       |
|    std                      | 1.08          |
|    value_loss               | 0.263         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.5851045] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 11           |
|    time_elapsed             | 146          |
|    total_timesteps          | 1427456      |
| train/                      |              |
|    approx_kl                | 0.006874138  |
|    approx_ln(kl)            | -4.979989    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.988        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.382       |
|    ln(policy_gradient_loss) | -5.19        |
|    loss                     | 0.682        |
|    n_updates                | 748          |
|    policy_gradient_loss     | 0.00556      |
|    std                      | 1.08         |
|    value_loss               | 2.05         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37009108] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 12            |
|    time_elapsed             | 159           |
|    total_timesteps          | 1429504       |
| train/                      |               |
|    approx_kl                | 0.0035397834  |
|    approx_ln(kl)            | -5.6436896    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.546        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.579         |
|    n_updates                | 749           |
|    policy_gradient_loss     | -0.00194      |
|    std                      | 1.08          |
|    value_loss               | 1.31          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3837972] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 13           |
|    time_elapsed             | 173          |
|    total_timesteps          | 1431552      |
| train/                      |              |
|    approx_kl                | 0.004879854  |
|    approx_ln(kl)            | -5.32264     |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.72        |
|    ln(policy_gradient_loss) | -6.12        |
|    loss                     | 0.0656       |
|    n_updates                | 750          |
|    policy_gradient_loss     | 0.0022       |
|    std                      | 1.09         |
|    value_loss               | 0.187        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43302697] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 14            |
|    time_elapsed             | 186           |
|    total_timesteps          | 1433600       |
| train/                      |               |
|    approx_kl                | 0.0046595363  |
|    approx_ln(kl)            | -5.3688393    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.33          |
|    ln(policy_gradient_loss) | -7.35         |
|    loss                     | 1.39          |
|    n_updates                | 751           |
|    policy_gradient_loss     | 0.00064       |
|    std                      | 1.08          |
|    value_loss               | 1.63          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5222644] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 15           |
|    time_elapsed             | 199          |
|    total_timesteps          | 1435648      |
| train/                      |              |
|    approx_kl                | 0.006988749  |
|    approx_ln(kl)            | -4.963454    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.979        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.0348      |
|    ln(policy_gradient_loss) | -6.11        |
|    loss                     | 0.966        |
|    n_updates                | 752          |
|    policy_gradient_loss     | 0.00221      |
|    std                      | 1.08         |
|    value_loss               | 2.93         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4901569] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 16           |
|    time_elapsed             | 212          |
|    total_timesteps          | 1437696      |
| train/                      |              |
|    approx_kl                | 0.0046898685 |
|    approx_ln(kl)            | -5.362351    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.63        |
|    ln(policy_gradient_loss) | -6.66        |
|    loss                     | 0.072        |
|    n_updates                | 753          |
|    policy_gradient_loss     | 0.00129      |
|    std                      | 1.08         |
|    value_loss               | 0.0804       |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44977978] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 17            |
|    time_elapsed             | 225           |
|    total_timesteps          | 1439744       |
| train/                      |               |
|    approx_kl                | 0.007169714   |
|    approx_ln(kl)            | -4.9378896    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.36         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0346        |
|    n_updates                | 754           |
|    policy_gradient_loss     | -0.00672      |
|    std                      | 1.08          |
|    value_loss               | 0.134         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.23154481] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 18            |
|    time_elapsed             | 238           |
|    total_timesteps          | 1441792       |
| train/                      |               |
|    approx_kl                | 0.00735071    |
|    approx_ln(kl)            | -4.912958     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.8          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.00821       |
|    n_updates                | 755           |
|    policy_gradient_loss     | -0.00201      |
|    std                      | 1.08          |
|    value_loss               | 0.157         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.51738054] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 19            |
|    time_elapsed             | 251           |
|    total_timesteps          | 1443840       |
| train/                      |               |
|    approx_kl                | 0.0057279794  |
|    approx_ln(kl)            | -5.1623926    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.84         |
|    ln(policy_gradient_loss) | -5.58         |
|    loss                     | 0.0585        |
|    n_updates                | 756           |
|    policy_gradient_loss     | 0.00376       |
|    std                      | 1.08          |
|    value_loss               | 0.189         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4773708] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 20           |
|    time_elapsed             | 264          |
|    total_timesteps          | 1445888      |
| train/                      |              |
|    approx_kl                | 0.0066726916 |
|    approx_ln(kl)            | -5.009732    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.99        |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.352        |
|    ln(policy_gradient_loss) | -4.77        |
|    loss                     | 1.42         |
|    n_updates                | 757          |
|    policy_gradient_loss     | 0.00849      |
|    std                      | 1.08         |
|    value_loss               | 1.05         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30322927] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 21            |
|    time_elapsed             | 277           |
|    total_timesteps          | 1447936       |
| train/                      |               |
|    approx_kl                | 0.004271867   |
|    approx_ln(kl)            | -5.455704     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.74         |
|    ln(policy_gradient_loss) | -4.93         |
|    loss                     | 0.0647        |
|    n_updates                | 758           |
|    policy_gradient_loss     | 0.00724       |
|    std                      | 1.08          |
|    value_loss               | 0.0567        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44857836] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 22            |
|    time_elapsed             | 290           |
|    total_timesteps          | 1449984       |
| train/                      |               |
|    approx_kl                | 0.0072294488  |
|    approx_ln(kl)            | -4.9295926    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.55         |
|    ln(policy_gradient_loss) | -4.62         |
|    loss                     | 0.0286        |
|    n_updates                | 759           |
|    policy_gradient_loss     | 0.00981       |
|    std                      | 1.08          |
|    value_loss               | 0.0221        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5316397] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 23           |
|    time_elapsed             | 303          |
|    total_timesteps          | 1452032      |
| train/                      |              |
|    approx_kl                | 0.0054654214 |
|    approx_ln(kl)            | -5.2093143   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.98        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.99        |
|    ln(policy_gradient_loss) | -9.45        |
|    loss                     | 0.0186       |
|    n_updates                | 760          |
|    policy_gradient_loss     | 7.86e-05     |
|    std                      | 1.08         |
|    value_loss               | 0.06         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2917107] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 24           |
|    time_elapsed             | 316          |
|    total_timesteps          | 1454080      |
| train/                      |              |
|    approx_kl                | 0.008993982  |
|    approx_ln(kl)            | -4.7111998   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.98        |
|    explained_variance       | 0.993        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.75        |
|    ln(policy_gradient_loss) | -4.47        |
|    loss                     | 0.0641       |
|    n_updates                | 761          |
|    policy_gradient_loss     | 0.0114       |
|    std                      | 1.07         |
|    value_loss               | 0.239        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3164838] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 25           |
|    time_elapsed             | 330          |
|    total_timesteps          | 1456128      |
| train/                      |              |
|    approx_kl                | 0.0056610364 |
|    approx_ln(kl)            | -5.174148    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.98        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.88        |
|    ln(policy_gradient_loss) | -4.58        |
|    loss                     | 0.152        |
|    n_updates                | 762          |
|    policy_gradient_loss     | 0.0103       |
|    std                      | 1.07         |
|    value_loss               | 0.201        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.50997144] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 26            |
|    time_elapsed             | 343           |
|    total_timesteps          | 1458176       |
| train/                      |               |
|    approx_kl                | 0.008428724   |
|    approx_ln(kl)            | -4.77611      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.98         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | nan           |
|    ln(policy_gradient_loss) | -8.16         |
|    loss                     | -0.00674      |
|    n_updates                | 763           |
|    policy_gradient_loss     | 0.000287      |
|    std                      | 1.07          |
|    value_loss               | 0.043         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4697492] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 27           |
|    time_elapsed             | 357          |
|    total_timesteps          | 1460224      |
| train/                      |              |
|    approx_kl                | 0.0077633215 |
|    approx_ln(kl)            | -4.858345    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.98        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -4.27        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.014        |
|    n_updates                | 764          |
|    policy_gradient_loss     | -0.00667     |
|    std                      | 1.08         |
|    value_loss               | 0.0941       |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.518535] |
| time/                       |             |
|    fps                      | 154         |
|    iterations               | 28          |
|    time_elapsed             | 370         |
|    total_timesteps          | 1462272     |
| train/                      |             |
|    approx_kl                | 0.006671481 |
|    approx_ln(kl)            | -5.0099134  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.98       |
|    explained_variance       | 0.999       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -3.61       |
|    ln(policy_gradient_loss) | -4.57       |
|    loss                     | 0.0269      |
|    n_updates                | 765         |
|    policy_gradient_loss     | 0.0103      |
|    std                      | 1.08        |
|    value_loss               | 0.0575      |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49967393] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 29            |
|    time_elapsed             | 383           |
|    total_timesteps          | 1464320       |
| train/                      |               |
|    approx_kl                | 0.006527771   |
|    approx_ln(kl)            | -5.0316896    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.98         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.63         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0265        |
|    n_updates                | 766           |
|    policy_gradient_loss     | -0.000549     |
|    std                      | 1.08          |
|    value_loss               | 0.11          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.51220924] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 30            |
|    time_elapsed             | 397           |
|    total_timesteps          | 1466368       |
| train/                      |               |
|    approx_kl                | 0.0067706895  |
|    approx_ln(kl)            | -4.9951525    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.06         |
|    ln(policy_gradient_loss) | -5.65         |
|    loss                     | 0.047         |
|    n_updates                | 767           |
|    policy_gradient_loss     | 0.0035        |
|    std                      | 1.08          |
|    value_loss               | 0.0312        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.38984025] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 31            |
|    time_elapsed             | 413           |
|    total_timesteps          | 1468416       |
| train/                      |               |
|    approx_kl                | 0.00835577    |
|    approx_ln(kl)            | -4.784803     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.753         |
|    ln(policy_gradient_loss) | -4.46         |
|    loss                     | 2.12          |
|    n_updates                | 768           |
|    policy_gradient_loss     | 0.0115        |
|    std                      | 1.08          |
|    value_loss               | 2.65          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47660848] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 32            |
|    time_elapsed             | 427           |
|    total_timesteps          | 1470464       |
| train/                      |               |
|    approx_kl                | 0.0061384346  |
|    approx_ln(kl)            | -5.0931854    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.88         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0559        |
|    n_updates                | 769           |
|    policy_gradient_loss     | -0.00401      |
|    std                      | 1.08          |
|    value_loss               | 0.172         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43268147] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 33            |
|    time_elapsed             | 440           |
|    total_timesteps          | 1472512       |
| train/                      |               |
|    approx_kl                | 0.00445355    |
|    approx_ln(kl)            | -5.414054     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.03         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.131         |
|    n_updates                | 770           |
|    policy_gradient_loss     | -0.000646     |
|    std                      | 1.08          |
|    value_loss               | 0.513         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45237735] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 34            |
|    time_elapsed             | 452           |
|    total_timesteps          | 1474560       |
| train/                      |               |
|    approx_kl                | 0.006207606   |
|    approx_ln(kl)            | -5.08198      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.46         |
|    ln(policy_gradient_loss) | -4.53         |
|    loss                     | 0.0851        |
|    n_updates                | 771           |
|    policy_gradient_loss     | 0.0108        |
|    std                      | 1.08          |
|    value_loss               | 0.185         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5312447] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 35           |
|    time_elapsed             | 465          |
|    total_timesteps          | 1476608      |
| train/                      |              |
|    approx_kl                | 0.0058451877 |
|    approx_ln(kl)            | -5.1421366   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.99        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.94        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.053        |
|    n_updates                | 772          |
|    policy_gradient_loss     | -0.0044      |
|    std                      | 1.08         |
|    value_loss               | 0.231        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25611755] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 36            |
|    time_elapsed             | 479           |
|    total_timesteps          | 1478656       |
| train/                      |               |
|    approx_kl                | 0.0060454984  |
|    approx_ln(kl)            | -5.1084414    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.83          |
|    ln(policy_gradient_loss) | -6.41         |
|    loss                     | 2.29          |
|    n_updates                | 773           |
|    policy_gradient_loss     | 0.00164       |
|    std                      | 1.08          |
|    value_loss               | 2.1           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.46466458] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 37            |
|    time_elapsed             | 492           |
|    total_timesteps          | 1480704       |
| train/                      |               |
|    approx_kl                | 0.0037285388  |
|    approx_ln(kl)            | -5.5917387    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.973         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.31         |
|    ln(policy_gradient_loss) | -5.05         |
|    loss                     | 0.27          |
|    n_updates                | 774           |
|    policy_gradient_loss     | 0.00643       |
|    std                      | 1.08          |
|    value_loss               | 0.724         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4090953] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 38           |
|    time_elapsed             | 505          |
|    total_timesteps          | 1482752      |
| train/                      |              |
|    approx_kl                | 0.0042311554 |
|    approx_ln(kl)            | -5.46528     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.99        |
|    explained_variance       | 0.987        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.713       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.49         |
|    n_updates                | 775          |
|    policy_gradient_loss     | -0.00217     |
|    std                      | 1.08         |
|    value_loss               | 1.53         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28795448] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 39            |
|    time_elapsed             | 518           |
|    total_timesteps          | 1484800       |
| train/                      |               |
|    approx_kl                | 0.0059135137  |
|    approx_ln(kl)            | -5.130515     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.706         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 2.03          |
|    n_updates                | 776           |
|    policy_gradient_loss     | -0.000592     |
|    std                      | 1.08          |
|    value_loss               | 2.1           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48582253] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 40            |
|    time_elapsed             | 531           |
|    total_timesteps          | 1486848       |
| train/                      |               |
|    approx_kl                | 0.0053328807  |
|    approx_ln(kl)            | -5.233864     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.53         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0294        |
|    n_updates                | 777           |
|    policy_gradient_loss     | -0.00904      |
|    std                      | 1.08          |
|    value_loss               | 0.129         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41097003] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 41            |
|    time_elapsed             | 545           |
|    total_timesteps          | 1488896       |
| train/                      |               |
|    approx_kl                | 0.0063007097  |
|    approx_ln(kl)            | -5.067093     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.76         |
|    ln(policy_gradient_loss) | -5.3          |
|    loss                     | 0.063         |
|    n_updates                | 778           |
|    policy_gradient_loss     | 0.00498       |
|    std                      | 1.08          |
|    value_loss               | 0.154         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.25687876] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 42            |
|    time_elapsed             | 558           |
|    total_timesteps          | 1490944       |
| train/                      |               |
|    approx_kl                | 0.0085050315  |
|    approx_ln(kl)            | -4.7670975    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.84         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.159         |
|    n_updates                | 779           |
|    policy_gradient_loss     | -0.00381      |
|    std                      | 1.08          |
|    value_loss               | 2.03          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5666452] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 43           |
|    time_elapsed             | 572          |
|    total_timesteps          | 1492992      |
| train/                      |              |
|    approx_kl                | 0.005095804  |
|    approx_ln(kl)            | -5.279338    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.33        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0971       |
|    n_updates                | 780          |
|    policy_gradient_loss     | -0.00609     |
|    std                      | 1.08         |
|    value_loss               | 0.201        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23867013] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 44            |
|    time_elapsed             | 585           |
|    total_timesteps          | 1495040       |
| train/                      |               |
|    approx_kl                | 0.0056852777  |
|    approx_ln(kl)            | -5.169875     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | nan           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | -0.0152       |
|    n_updates                | 781           |
|    policy_gradient_loss     | -0.01         |
|    std                      | 1.08          |
|    value_loss               | 0.093         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45420155] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 45            |
|    time_elapsed             | 598           |
|    total_timesteps          | 1497088       |
| train/                      |               |
|    approx_kl                | 0.006789566   |
|    approx_ln(kl)            | -4.992368     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.37         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0934        |
|    n_updates                | 782           |
|    policy_gradient_loss     | -0.00268      |
|    std                      | 1.08          |
|    value_loss               | 0.127         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4214735] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 46           |
|    time_elapsed             | 612          |
|    total_timesteps          | 1499136      |
| train/                      |              |
|    approx_kl                | 0.0041646278 |
|    approx_ln(kl)            | -5.481128    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.99        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -4.01        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0181       |
|    n_updates                | 783          |
|    policy_gradient_loss     | -0.000271    |
|    std                      | 1.08         |
|    value_loss               | 0.14         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.46327472] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 47            |
|    time_elapsed             | 625           |
|    total_timesteps          | 1501184       |
| train/                      |               |
|    approx_kl                | 0.005769333   |
|    approx_ln(kl)            | -5.155199     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.91         |
|    ln(policy_gradient_loss) | -5.32         |
|    loss                     | 0.02          |
|    n_updates                | 784           |
|    policy_gradient_loss     | 0.00491       |
|    std                      | 1.08          |
|    value_loss               | 0.0714        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44102338] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 48            |
|    time_elapsed             | 637           |
|    total_timesteps          | 1503232       |
| train/                      |               |
|    approx_kl                | 0.0058010174  |
|    approx_ln(kl)            | -5.149722     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.24         |
|    ln(policy_gradient_loss) | -6.02         |
|    loss                     | 0.0391        |
|    n_updates                | 785           |
|    policy_gradient_loss     | 0.00242       |
|    std                      | 1.08          |
|    value_loss               | 0.0896        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.16872469] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 49            |
|    time_elapsed             | 651           |
|    total_timesteps          | 1505280       |
| train/                      |               |
|    approx_kl                | 0.0054981816  |
|    approx_ln(kl)            | -5.2033377    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.78         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0228        |
|    n_updates                | 786           |
|    policy_gradient_loss     | -0.00377      |
|    std                      | 1.08          |
|    value_loss               | 0.0778        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
--------------------------------------
| reward             | [-0.29466465] |
| time/              |               |
|    fps             | 145           |
|    iterations      | 1             |
|    time_elapsed    | 14            |
|    total_timesteps | 1507328       |
--------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35414886] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 2             |
|    time_elapsed             | 27            |
|    total_timesteps          | 1509376       |
| train/                      |               |
|    approx_kl                | 0.0050851307  |
|    approx_ln(kl)            | -5.2814345    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.609        |
|    ln(policy_gradient_loss) | -6.34         |
|    loss                     | 0.544         |
|    n_updates                | 788           |
|    policy_gradient_loss     | 0.00177       |
|    std                      | 1.08          |
|    value_loss               | 1.39          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24038921] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 3             |
|    time_elapsed             | 40            |
|    total_timesteps          | 1511424       |
| train/                      |               |
|    approx_kl                | 0.004339953   |
|    approx_ln(kl)            | -5.439892     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.969         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.7          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.183         |
|    n_updates                | 789           |
|    policy_gradient_loss     | -0.00542      |
|    std                      | 1.08          |
|    value_loss               | 0.599         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42603204] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 4             |
|    time_elapsed             | 53            |
|    total_timesteps          | 1513472       |
| train/                      |               |
|    approx_kl                | 0.0050913864  |
|    approx_ln(kl)            | -5.2802052    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.28         |
|    ln(policy_gradient_loss) | -5.1          |
|    loss                     | 0.0378        |
|    n_updates                | 790           |
|    policy_gradient_loss     | 0.00611       |
|    std                      | 1.08          |
|    value_loss               | 0.07          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.245401] |
| time/                       |             |
|    fps                      | 153         |
|    iterations               | 5           |
|    time_elapsed             | 66          |
|    total_timesteps          | 1515520     |
| train/                      |             |
|    approx_kl                | 0.006520961 |
|    approx_ln(kl)            | -5.0327334  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.99       |
|    explained_variance       | 0.979       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -2.39       |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.0914      |
|    n_updates                | 791         |
|    policy_gradient_loss     | -0.00394    |
|    std                      | 1.08        |
|    value_loss               | 1.33        |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48909318] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 6             |
|    time_elapsed             | 80            |
|    total_timesteps          | 1517568       |
| train/                      |               |
|    approx_kl                | 0.0040295874  |
|    approx_ln(kl)            | -5.5140915    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.3          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0371        |
|    n_updates                | 792           |
|    policy_gradient_loss     | -0.000954     |
|    std                      | 1.08          |
|    value_loss               | 0.0692        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4127518] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 7            |
|    time_elapsed             | 93           |
|    total_timesteps          | 1519616      |
| train/                      |              |
|    approx_kl                | 0.006085034  |
|    approx_ln(kl)            | -5.101923    |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.36        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0947       |
|    n_updates                | 793          |
|    policy_gradient_loss     | -0.0114      |
|    std                      | 1.08         |
|    value_loss               | 0.199        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4975306] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 8            |
|    time_elapsed             | 107          |
|    total_timesteps          | 1521664      |
| train/                      |              |
|    approx_kl                | 0.0055476236 |
|    approx_ln(kl)            | -5.1943855   |
|    clip_range               | 0.2          |
|    entropy_loss             | -3           |
|    explained_variance       | 0.988        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.414       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.661        |
|    n_updates                | 794          |
|    policy_gradient_loss     | -0.00248     |
|    std                      | 1.08         |
|    value_loss               | 1.07         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44672856] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 9             |
|    time_elapsed             | 120           |
|    total_timesteps          | 1523712       |
| train/                      |               |
|    approx_kl                | 0.005655432   |
|    approx_ln(kl)            | -5.175139     |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.54         |
|    ln(policy_gradient_loss) | -5.9          |
|    loss                     | 0.583         |
|    n_updates                | 795           |
|    policy_gradient_loss     | 0.00274       |
|    std                      | 1.09          |
|    value_loss               | 2.13          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49075484] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 10            |
|    time_elapsed             | 133           |
|    total_timesteps          | 1525760       |
| train/                      |               |
|    approx_kl                | 0.006463311   |
|    approx_ln(kl)            | -5.0416136    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.88         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0563        |
|    n_updates                | 796           |
|    policy_gradient_loss     | -0.00141      |
|    std                      | 1.08          |
|    value_loss               | 0.226         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32334125] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 11            |
|    time_elapsed             | 146           |
|    total_timesteps          | 1527808       |
| train/                      |               |
|    approx_kl                | 0.0065886327  |
|    approx_ln(kl)            | -5.0224094    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.61         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0738        |
|    n_updates                | 797           |
|    policy_gradient_loss     | -0.00662      |
|    std                      | 1.08          |
|    value_loss               | 0.158         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39885712] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 12            |
|    time_elapsed             | 159           |
|    total_timesteps          | 1529856       |
| train/                      |               |
|    approx_kl                | 0.007039746   |
|    approx_ln(kl)            | -4.9561834    |
|    clip_range               | 0.2           |
|    entropy_loss             | -3            |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | nan           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | -0.018        |
|    n_updates                | 798           |
|    policy_gradient_loss     | -0.00949      |
|    std                      | 1.08          |
|    value_loss               | 0.0852        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40958646] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 13            |
|    time_elapsed             | 173           |
|    total_timesteps          | 1531904       |
| train/                      |               |
|    approx_kl                | 0.0053900843  |
|    approx_ln(kl)            | -5.223194     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.14         |
|    ln(policy_gradient_loss) | -5.14         |
|    loss                     | 0.117         |
|    n_updates                | 799           |
|    policy_gradient_loss     | 0.00587       |
|    std                      | 1.08          |
|    value_loss               | 0.451         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49083516] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 14            |
|    time_elapsed             | 186           |
|    total_timesteps          | 1533952       |
| train/                      |               |
|    approx_kl                | 0.005032863   |
|    approx_ln(kl)            | -5.291766     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.15         |
|    ln(policy_gradient_loss) | -5.19         |
|    loss                     | 0.116         |
|    n_updates                | 800           |
|    policy_gradient_loss     | 0.00555       |
|    std                      | 1.08          |
|    value_loss               | 0.216         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39323843] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 15            |
|    time_elapsed             | 201           |
|    total_timesteps          | 1536000       |
| train/                      |               |
|    approx_kl                | 0.0039720815  |
|    approx_ln(kl)            | -5.5284653    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.53         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0797        |
|    n_updates                | 801           |
|    policy_gradient_loss     | -0.00303      |
|    std                      | 1.08          |
|    value_loss               | 0.217         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5180342] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 16           |
|    time_elapsed             | 214          |
|    total_timesteps          | 1538048      |
| train/                      |              |
|    approx_kl                | 0.006327829  |
|    approx_ln(kl)            | -5.062798    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.99        |
|    explained_variance       | 0.986        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.39        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0918       |
|    n_updates                | 802          |
|    policy_gradient_loss     | -0.0119      |
|    std                      | 1.08         |
|    value_loss               | 0.505        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.485483]  |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 17           |
|    time_elapsed             | 227          |
|    total_timesteps          | 1540096      |
| train/                      |              |
|    approx_kl                | 0.0053131683 |
|    approx_ln(kl)            | -5.237567    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.99        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.27        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.28         |
|    n_updates                | 803          |
|    policy_gradient_loss     | -0.00103     |
|    std                      | 1.08         |
|    value_loss               | 0.491        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37713766] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 18            |
|    time_elapsed             | 240           |
|    total_timesteps          | 1542144       |
| train/                      |               |
|    approx_kl                | 0.004791668   |
|    approx_ln(kl)            | -5.3408766    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.99         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.67         |
|    ln(policy_gradient_loss) | -4.99         |
|    loss                     | 0.188         |
|    n_updates                | 804           |
|    policy_gradient_loss     | 0.00677       |
|    std                      | 1.08          |
|    value_loss               | 0.319         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2462829] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 19           |
|    time_elapsed             | 254          |
|    total_timesteps          | 1544192      |
| train/                      |              |
|    approx_kl                | 0.0059346505 |
|    approx_ln(kl)            | -5.126947    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.98        |
|    explained_variance       | 0.953        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.32        |
|    ln(policy_gradient_loss) | -5.82        |
|    loss                     | 0.098        |
|    n_updates                | 805          |
|    policy_gradient_loss     | 0.00298      |
|    std                      | 1.07         |
|    value_loss               | 0.388        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36009562] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 20            |
|    time_elapsed             | 267           |
|    total_timesteps          | 1546240       |
| train/                      |               |
|    approx_kl                | 0.0041523483  |
|    approx_ln(kl)            | -5.4840813    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.98         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.61         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.2           |
|    n_updates                | 806           |
|    policy_gradient_loss     | -0.000923     |
|    std                      | 1.07          |
|    value_loss               | 0.273         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.14902663] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 21            |
|    time_elapsed             | 280           |
|    total_timesteps          | 1548288       |
| train/                      |               |
|    approx_kl                | 0.006272817   |
|    approx_ln(kl)            | -5.07153      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.98         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.87         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.154         |
|    n_updates                | 807           |
|    policy_gradient_loss     | -0.00392      |
|    std                      | 1.07          |
|    value_loss               | 1.07          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44005895] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 22            |
|    time_elapsed             | 292           |
|    total_timesteps          | 1550336       |
| train/                      |               |
|    approx_kl                | 0.0069227414  |
|    approx_ln(kl)            | -4.9729433    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.98         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.23         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0394        |
|    n_updates                | 808           |
|    policy_gradient_loss     | -0.00708      |
|    std                      | 1.07          |
|    value_loss               | 0.598         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30550662] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 23            |
|    time_elapsed             | 306           |
|    total_timesteps          | 1552384       |
| train/                      |               |
|    approx_kl                | 0.0069417497  |
|    approx_ln(kl)            | -4.9702015    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.98         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.396         |
|    ln(policy_gradient_loss) | -6.31         |
|    loss                     | 1.49          |
|    n_updates                | 809           |
|    policy_gradient_loss     | 0.00182       |
|    std                      | 1.07          |
|    value_loss               | 1.47          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40380025] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 24            |
|    time_elapsed             | 319           |
|    total_timesteps          | 1554432       |
| train/                      |               |
|    approx_kl                | 0.0050429436  |
|    approx_ln(kl)            | -5.2897654    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.97         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.95         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0524        |
|    n_updates                | 810           |
|    policy_gradient_loss     | -0.000893     |
|    std                      | 1.07          |
|    value_loss               | 0.158         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.17239282] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 25            |
|    time_elapsed             | 333           |
|    total_timesteps          | 1556480       |
| train/                      |               |
|    approx_kl                | 0.008096479   |
|    approx_ln(kl)            | -4.816326     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.97         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.96         |
|    ln(policy_gradient_loss) | -5.6          |
|    loss                     | 0.141         |
|    n_updates                | 811           |
|    policy_gradient_loss     | 0.0037        |
|    std                      | 1.07          |
|    value_loss               | 0.182         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44334486] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 26            |
|    time_elapsed             | 347           |
|    total_timesteps          | 1558528       |
| train/                      |               |
|    approx_kl                | 0.003918988   |
|    approx_ln(kl)            | -5.5419216    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.97         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.445         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.56          |
|    n_updates                | 812           |
|    policy_gradient_loss     | -0.00119      |
|    std                      | 1.07          |
|    value_loss               | 1.05          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42045802] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 27            |
|    time_elapsed             | 360           |
|    total_timesteps          | 1560576       |
| train/                      |               |
|    approx_kl                | 0.0050212536  |
|    approx_ln(kl)            | -5.2940755    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.97         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.28         |
|    ln(policy_gradient_loss) | -6.59         |
|    loss                     | 0.0377        |
|    n_updates                | 813           |
|    policy_gradient_loss     | 0.00138       |
|    std                      | 1.07          |
|    value_loss               | 0.0782        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.3387391] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 28           |
|    time_elapsed             | 374          |
|    total_timesteps          | 1562624      |
| train/                      |              |
|    approx_kl                | 0.007674865  |
|    approx_ln(kl)            | -4.8698044   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.96        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.84        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0214       |
|    n_updates                | 814          |
|    policy_gradient_loss     | -0.00552     |
|    std                      | 1.06         |
|    value_loss               | 0.145        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33346546] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 29            |
|    time_elapsed             | 387           |
|    total_timesteps          | 1564672       |
| train/                      |               |
|    approx_kl                | 0.0045251483  |
|    approx_ln(kl)            | -5.398105     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.96         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.47         |
|    ln(policy_gradient_loss) | -4.87         |
|    loss                     | 0.0846        |
|    n_updates                | 815           |
|    policy_gradient_loss     | 0.00769       |
|    std                      | 1.06          |
|    value_loss               | 0.0776        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34900826] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 30            |
|    time_elapsed             | 400           |
|    total_timesteps          | 1566720       |
| train/                      |               |
|    approx_kl                | 0.0062247333  |
|    approx_ln(kl)            | -5.0792246    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.18         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.113         |
|    n_updates                | 816           |
|    policy_gradient_loss     | -0.00235      |
|    std                      | 1.06          |
|    value_loss               | 0.175         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.14391896] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 31            |
|    time_elapsed             | 414           |
|    total_timesteps          | 1568768       |
| train/                      |               |
|    approx_kl                | 0.00785743    |
|    approx_ln(kl)            | -4.846296     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | 0.963         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.0504        |
|    ln(policy_gradient_loss) | -5.83         |
|    loss                     | 1.05          |
|    n_updates                | 817           |
|    policy_gradient_loss     | 0.00294       |
|    std                      | 1.06          |
|    value_loss               | 0.71          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35233045] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 32            |
|    time_elapsed             | 427           |
|    total_timesteps          | 1570816       |
| train/                      |               |
|    approx_kl                | 0.007805114   |
|    approx_ln(kl)            | -4.8529763    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.95         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -5.29         |
|    ln(policy_gradient_loss) | -6.07         |
|    loss                     | 0.00506       |
|    n_updates                | 818           |
|    policy_gradient_loss     | 0.0023        |
|    std                      | 1.05          |
|    value_loss               | 0.146         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27782124] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 33            |
|    time_elapsed             | 440           |
|    total_timesteps          | 1572864       |
| train/                      |               |
|    approx_kl                | 0.0070264414  |
|    approx_ln(kl)            | -4.958075     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.94         |
|    explained_variance       | 0.974         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.667         |
|    ln(policy_gradient_loss) | -4.79         |
|    loss                     | 1.95          |
|    n_updates                | 819           |
|    policy_gradient_loss     | 0.00831       |
|    std                      | 1.05          |
|    value_loss               | 3.94          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3081279] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 34           |
|    time_elapsed             | 454          |
|    total_timesteps          | 1574912      |
| train/                      |              |
|    approx_kl                | 0.0057776673 |
|    approx_ln(kl)            | -5.153755    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | 0.99         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.41        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.244        |
|    n_updates                | 820          |
|    policy_gradient_loss     | -0.000214    |
|    std                      | 1.05         |
|    value_loss               | 0.78         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3510587] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 35           |
|    time_elapsed             | 467          |
|    total_timesteps          | 1576960      |
| train/                      |              |
|    approx_kl                | 0.004668244  |
|    approx_ln(kl)            | -5.3669724   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.94        |
|    explained_variance       | 0.993        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.583       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.558        |
|    n_updates                | 821          |
|    policy_gradient_loss     | -0.00553     |
|    std                      | 1.05         |
|    value_loss               | 0.859        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3228234] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 36           |
|    time_elapsed             | 481          |
|    total_timesteps          | 1579008      |
| train/                      |              |
|    approx_kl                | 0.0054614237 |
|    approx_ln(kl)            | -5.210046    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.976        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.545       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.58         |
|    n_updates                | 822          |
|    policy_gradient_loss     | -0.000864    |
|    std                      | 1.05         |
|    value_loss               | 1.94         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40465584] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 37            |
|    time_elapsed             | 495           |
|    total_timesteps          | 1581056       |
| train/                      |               |
|    approx_kl                | 0.0050259274  |
|    approx_ln(kl)            | -5.293145     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.93         |
|    explained_variance       | 0.975         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.18         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.113         |
|    n_updates                | 823           |
|    policy_gradient_loss     | -0.00208      |
|    std                      | 1.05          |
|    value_loss               | 0.873         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23378368] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 38            |
|    time_elapsed             | 509           |
|    total_timesteps          | 1583104       |
| train/                      |               |
|    approx_kl                | 0.004748461   |
|    approx_ln(kl)            | -5.3499346    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.93         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.894        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.409         |
|    n_updates                | 824           |
|    policy_gradient_loss     | -3.94e-05     |
|    std                      | 1.05          |
|    value_loss               | 0.9           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4627352] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 39           |
|    time_elapsed             | 522          |
|    total_timesteps          | 1585152      |
| train/                      |              |
|    approx_kl                | 0.0063984673 |
|    approx_ln(kl)            | -5.051697    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.963        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.07        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.342        |
|    n_updates                | 825          |
|    policy_gradient_loss     | -0.00196     |
|    std                      | 1.05         |
|    value_loss               | 1.08         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3941473] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 40           |
|    time_elapsed             | 535          |
|    total_timesteps          | 1587200      |
| train/                      |              |
|    approx_kl                | 0.004377359  |
|    approx_ln(kl)            | -5.4313097   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.68        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0689       |
|    n_updates                | 826          |
|    policy_gradient_loss     | -0.00285     |
|    std                      | 1.05         |
|    value_loss               | 0.165        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31567279] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 41            |
|    time_elapsed             | 548           |
|    total_timesteps          | 1589248       |
| train/                      |               |
|    approx_kl                | 0.005420499   |
|    approx_ln(kl)            | -5.2175674    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.93         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.08         |
|    ln(policy_gradient_loss) | -6.52         |
|    loss                     | 0.124         |
|    n_updates                | 827           |
|    policy_gradient_loss     | 0.00147       |
|    std                      | 1.05          |
|    value_loss               | 0.668         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4387863] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 42           |
|    time_elapsed             | 561          |
|    total_timesteps          | 1591296      |
| train/                      |              |
|    approx_kl                | 0.0051409756 |
|    approx_ln(kl)            | -5.2705126   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.93        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.71        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0666       |
|    n_updates                | 828          |
|    policy_gradient_loss     | -0.00118     |
|    std                      | 1.04         |
|    value_loss               | 0.161        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3091801] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 43           |
|    time_elapsed             | 575          |
|    total_timesteps          | 1593344      |
| train/                      |              |
|    approx_kl                | 0.006437582  |
|    approx_ln(kl)            | -5.0456023   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.987        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.919       |
|    ln(policy_gradient_loss) | -5.68        |
|    loss                     | 0.399        |
|    n_updates                | 829          |
|    policy_gradient_loss     | 0.00343      |
|    std                      | 1.04         |
|    value_loss               | 1.1          |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.50542766] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 44            |
|    time_elapsed             | 587           |
|    total_timesteps          | 1595392       |
| train/                      |               |
|    approx_kl                | 0.0059743393  |
|    approx_ln(kl)            | -5.1202817    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.75         |
|    ln(policy_gradient_loss) | -10.6         |
|    loss                     | 0.173         |
|    n_updates                | 830           |
|    policy_gradient_loss     | 2.42e-05      |
|    std                      | 1.04          |
|    value_loss               | 1.33          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31809866] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 45            |
|    time_elapsed             | 600           |
|    total_timesteps          | 1597440       |
| train/                      |               |
|    approx_kl                | 0.0025724298  |
|    approx_ln(kl)            | -5.9629045    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.26         |
|    ln(policy_gradient_loss) | -6.81         |
|    loss                     | 0.283         |
|    n_updates                | 831           |
|    policy_gradient_loss     | 0.0011        |
|    std                      | 1.04          |
|    value_loss               | 0.929         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30659896] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 46            |
|    time_elapsed             | 614           |
|    total_timesteps          | 1599488       |
| train/                      |               |
|    approx_kl                | 0.004987642   |
|    approx_ln(kl)            | -5.300792     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.21         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0403        |
|    n_updates                | 832           |
|    policy_gradient_loss     | -0.00455      |
|    std                      | 1.04          |
|    value_loss               | 0.119         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40318805] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 47            |
|    time_elapsed             | 627           |
|    total_timesteps          | 1601536       |
| train/                      |               |
|    approx_kl                | 0.0070863995  |
|    approx_ln(kl)            | -4.949578     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.42         |
|    ln(policy_gradient_loss) | -5.76         |
|    loss                     | 0.0891        |
|    n_updates                | 833           |
|    policy_gradient_loss     | 0.00316       |
|    std                      | 1.04          |
|    value_loss               | 0.209         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23777132] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 48            |
|    time_elapsed             | 641           |
|    total_timesteps          | 1603584       |
| train/                      |               |
|    approx_kl                | 0.0066186464  |
|    approx_ln(kl)            | -5.017864     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.37         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.255         |
|    n_updates                | 834           |
|    policy_gradient_loss     | -0.0013       |
|    std                      | 1.04          |
|    value_loss               | 0.933         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25757945] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 49            |
|    time_elapsed             | 654           |
|    total_timesteps          | 1605632       |
| train/                      |               |
|    approx_kl                | 0.006272584   |
|    approx_ln(kl)            | -5.071567     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.53         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.08          |
|    n_updates                | 835           |
|    policy_gradient_loss     | -0.0074       |
|    std                      | 1.04          |
|    value_loss               | 0.191         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
--------------------------------------
| reward             | [-0.34227312] |
| time/              |               |
|    fps             | 162           |
|    iterations      | 1             |
|    time_elapsed    | 12            |
|    total_timesteps | 1607680       |
--------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2256937] |
| time/                       |              |
|    fps                      | 159          |
|    iterations               | 2            |
|    time_elapsed             | 25           |
|    total_timesteps          | 1609728      |
| train/                      |              |
|    approx_kl                | 0.0065022353 |
|    approx_ln(kl)            | -5.0356092   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.15        |
|    ln(policy_gradient_loss) | -6.67        |
|    loss                     | 0.317        |
|    n_updates                | 837          |
|    policy_gradient_loss     | 0.00126      |
|    std                      | 1.04         |
|    value_loss               | 1.1          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28731433] |
| time/                       |               |
|    fps                      | 157           |
|    iterations               | 3             |
|    time_elapsed             | 38            |
|    total_timesteps          | 1611776       |
| train/                      |               |
|    approx_kl                | 0.005913176   |
|    approx_ln(kl)            | -5.1305723    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.1          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.122         |
|    n_updates                | 838           |
|    policy_gradient_loss     | -0.000589     |
|    std                      | 1.04          |
|    value_loss               | 0.231         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.30126935] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 4             |
|    time_elapsed             | 52            |
|    total_timesteps          | 1613824       |
| train/                      |               |
|    approx_kl                | 0.008187234   |
|    approx_ln(kl)            | -4.805179     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.87         |
|    ln(policy_gradient_loss) | -6.53         |
|    loss                     | 0.155         |
|    n_updates                | 839           |
|    policy_gradient_loss     | 0.00146       |
|    std                      | 1.04          |
|    value_loss               | 1.03          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.19068423] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 5             |
|    time_elapsed             | 65            |
|    total_timesteps          | 1615872       |
| train/                      |               |
|    approx_kl                | 0.0048170863  |
|    approx_ln(kl)            | -5.335586     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.5          |
|    ln(policy_gradient_loss) | -8.48         |
|    loss                     | 0.223         |
|    n_updates                | 840           |
|    policy_gradient_loss     | 0.000207      |
|    std                      | 1.04          |
|    value_loss               | 0.853         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34842518] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 6             |
|    time_elapsed             | 78            |
|    total_timesteps          | 1617920       |
| train/                      |               |
|    approx_kl                | 0.0063946494  |
|    approx_ln(kl)            | -5.052294     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.84         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0586        |
|    n_updates                | 841           |
|    policy_gradient_loss     | -0.0148       |
|    std                      | 1.04          |
|    value_loss               | 0.208         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23529577] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 7             |
|    time_elapsed             | 91            |
|    total_timesteps          | 1619968       |
| train/                      |               |
|    approx_kl                | 0.0058475756  |
|    approx_ln(kl)            | -5.141728     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.62         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0727        |
|    n_updates                | 842           |
|    policy_gradient_loss     | -4.42e-05     |
|    std                      | 1.04          |
|    value_loss               | 0.383         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.57988685] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 8             |
|    time_elapsed             | 104           |
|    total_timesteps          | 1622016       |
| train/                      |               |
|    approx_kl                | 0.0056101424  |
|    approx_ln(kl)            | -5.1831794    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.31         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0989        |
|    n_updates                | 843           |
|    policy_gradient_loss     | -0.00445      |
|    std                      | 1.04          |
|    value_loss               | 0.302         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.57114017] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 9             |
|    time_elapsed             | 117           |
|    total_timesteps          | 1624064       |
| train/                      |               |
|    approx_kl                | 0.0049679484  |
|    approx_ln(kl)            | -5.3047485    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.6          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0744        |
|    n_updates                | 844           |
|    policy_gradient_loss     | -0.00159      |
|    std                      | 1.04          |
|    value_loss               | 0.0766        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44091272] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 10            |
|    time_elapsed             | 131           |
|    total_timesteps          | 1626112       |
| train/                      |               |
|    approx_kl                | 0.007188835   |
|    approx_ln(kl)            | -4.935226     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.28         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.102         |
|    n_updates                | 845           |
|    policy_gradient_loss     | -0.00521      |
|    std                      | 1.04          |
|    value_loss               | 0.19          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38684237] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 11            |
|    time_elapsed             | 144           |
|    total_timesteps          | 1628160       |
| train/                      |               |
|    approx_kl                | 0.0049273316  |
|    approx_ln(kl)            | -5.312958     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.83         |
|    ln(policy_gradient_loss) | -8.35         |
|    loss                     | 0.16          |
|    n_updates                | 846           |
|    policy_gradient_loss     | 0.000236      |
|    std                      | 1.04          |
|    value_loss               | 0.293         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.5482097] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 12           |
|    time_elapsed             | 157          |
|    total_timesteps          | 1630208      |
| train/                      |              |
|    approx_kl                | 0.009532824  |
|    approx_ln(kl)            | -4.653014    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.27        |
|    ln(policy_gradient_loss) | -5.41        |
|    loss                     | 0.103        |
|    n_updates                | 847          |
|    policy_gradient_loss     | 0.00447      |
|    std                      | 1.04         |
|    value_loss               | 0.223        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4731993] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 13           |
|    time_elapsed             | 170          |
|    total_timesteps          | 1632256      |
| train/                      |              |
|    approx_kl                | 0.005514369  |
|    approx_ln(kl)            | -5.200398    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.92        |
|    explained_variance       | 0.995        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.54        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0789       |
|    n_updates                | 848          |
|    policy_gradient_loss     | -0.0137      |
|    std                      | 1.04         |
|    value_loss               | 0.281        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.50227785] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 14            |
|    time_elapsed             | 183           |
|    total_timesteps          | 1634304       |
| train/                      |               |
|    approx_kl                | 0.006717438   |
|    approx_ln(kl)            | -5.0030484    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.72         |
|    ln(policy_gradient_loss) | -5.7          |
|    loss                     | 0.18          |
|    n_updates                | 849           |
|    policy_gradient_loss     | 0.00334       |
|    std                      | 1.04          |
|    value_loss               | 0.275         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39972752] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 15            |
|    time_elapsed             | 196           |
|    total_timesteps          | 1636352       |
| train/                      |               |
|    approx_kl                | 0.007359076   |
|    approx_ln(kl)            | -4.911821     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.08         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.125         |
|    n_updates                | 850           |
|    policy_gradient_loss     | -0.00483      |
|    std                      | 1.04          |
|    value_loss               | 1.37          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2549264] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 16           |
|    time_elapsed             | 209          |
|    total_timesteps          | 1638400      |
| train/                      |              |
|    approx_kl                | 0.005335616  |
|    approx_ln(kl)            | -5.2333508   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.91        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.89        |
|    ln(policy_gradient_loss) | -6.96        |
|    loss                     | 0.151        |
|    n_updates                | 851          |
|    policy_gradient_loss     | 0.000945     |
|    std                      | 1.04         |
|    value_loss               | 0.13         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.20254813] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 17            |
|    time_elapsed             | 222           |
|    total_timesteps          | 1640448       |
| train/                      |               |
|    approx_kl                | 0.0069826115  |
|    approx_ln(kl)            | -4.964332     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.35         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0352        |
|    n_updates                | 852           |
|    policy_gradient_loss     | -0.0027       |
|    std                      | 1.04          |
|    value_loss               | 0.141         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36623183] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 18            |
|    time_elapsed             | 235           |
|    total_timesteps          | 1642496       |
| train/                      |               |
|    approx_kl                | 0.004999403   |
|    approx_ln(kl)            | -5.2984366    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.23         |
|    ln(policy_gradient_loss) | -7.5          |
|    loss                     | 0.108         |
|    n_updates                | 853           |
|    policy_gradient_loss     | 0.000551      |
|    std                      | 1.04          |
|    value_loss               | 0.141         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5029392] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 19           |
|    time_elapsed             | 248          |
|    total_timesteps          | 1644544      |
| train/                      |              |
|    approx_kl                | 0.0062535363 |
|    approx_ln(kl)            | -5.0746083   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.91        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.76        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0631       |
|    n_updates                | 854          |
|    policy_gradient_loss     | -0.00208     |
|    std                      | 1.04         |
|    value_loss               | 0.166        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3923271] |
| time/                       |              |
|    fps                      | 156          |
|    iterations               | 20           |
|    time_elapsed             | 261          |
|    total_timesteps          | 1646592      |
| train/                      |              |
|    approx_kl                | 0.0077123777 |
|    approx_ln(kl)            | -4.8649287   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.91        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.13        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0437       |
|    n_updates                | 855          |
|    policy_gradient_loss     | -0.000978    |
|    std                      | 1.04         |
|    value_loss               | 0.106        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37173763] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 21            |
|    time_elapsed             | 274           |
|    total_timesteps          | 1648640       |
| train/                      |               |
|    approx_kl                | 0.0050027897  |
|    approx_ln(kl)            | -5.2977595    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.92         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.66         |
|    ln(policy_gradient_loss) | -5.14         |
|    loss                     | 0.0701        |
|    n_updates                | 856           |
|    policy_gradient_loss     | 0.00584       |
|    std                      | 1.04          |
|    value_loss               | 0.0915        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.30413777] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 22            |
|    time_elapsed             | 287           |
|    total_timesteps          | 1650688       |
| train/                      |               |
|    approx_kl                | 0.0077904034  |
|    approx_ln(kl)            | -4.8548627    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.85         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0578        |
|    n_updates                | 857           |
|    policy_gradient_loss     | -0.00251      |
|    std                      | 1.04          |
|    value_loss               | 0.0891        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.51152414] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 23            |
|    time_elapsed             | 300           |
|    total_timesteps          | 1652736       |
| train/                      |               |
|    approx_kl                | 0.008796998   |
|    approx_ln(kl)            | -4.7333446    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.54         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0791        |
|    n_updates                | 858           |
|    policy_gradient_loss     | -0.00138      |
|    std                      | 1.04          |
|    value_loss               | 0.168         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35854274] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 24            |
|    time_elapsed             | 314           |
|    total_timesteps          | 1654784       |
| train/                      |               |
|    approx_kl                | 0.0057640495  |
|    approx_ln(kl)            | -5.156115     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.51         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.011         |
|    n_updates                | 859           |
|    policy_gradient_loss     | -0.0181       |
|    std                      | 1.04          |
|    value_loss               | 0.142         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38932553] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 25            |
|    time_elapsed             | 330           |
|    total_timesteps          | 1656832       |
| train/                      |               |
|    approx_kl                | 0.006430375   |
|    approx_ln(kl)            | -5.0467224    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.47         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0843        |
|    n_updates                | 860           |
|    policy_gradient_loss     | -0.0136       |
|    std                      | 1.04          |
|    value_loss               | 0.249         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.17681007] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 26            |
|    time_elapsed             | 347           |
|    total_timesteps          | 1658880       |
| train/                      |               |
|    approx_kl                | 0.007556924   |
|    approx_ln(kl)            | -4.885291     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.401         |
|    ln(policy_gradient_loss) | -4.86         |
|    loss                     | 1.49          |
|    n_updates                | 861           |
|    policy_gradient_loss     | 0.00774       |
|    std                      | 1.04          |
|    value_loss               | 1.94          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49463716] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 27            |
|    time_elapsed             | 364           |
|    total_timesteps          | 1660928       |
| train/                      |               |
|    approx_kl                | 0.0061795465  |
|    approx_ln(kl)            | -5.0865107    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.07         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.126         |
|    n_updates                | 862           |
|    policy_gradient_loss     | -0.00484      |
|    std                      | 1.04          |
|    value_loss               | 0.234         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48657158] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 28            |
|    time_elapsed             | 381           |
|    total_timesteps          | 1662976       |
| train/                      |               |
|    approx_kl                | 0.0051426147  |
|    approx_ln(kl)            | -5.2701936    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.91         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.82         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.163         |
|    n_updates                | 863           |
|    policy_gradient_loss     | -0.0057       |
|    std                      | 1.04          |
|    value_loss               | 1.52          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3690939] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 29           |
|    time_elapsed             | 398          |
|    total_timesteps          | 1665024      |
| train/                      |              |
|    approx_kl                | 0.006761491  |
|    approx_ln(kl)            | -4.996512    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.9         |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -6.95        |
|    ln(policy_gradient_loss) | -5.98        |
|    loss                     | 0.000954     |
|    n_updates                | 864          |
|    policy_gradient_loss     | 0.00253      |
|    std                      | 1.03         |
|    value_loss               | 0.196        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3006176] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 30           |
|    time_elapsed             | 415          |
|    total_timesteps          | 1667072      |
| train/                      |              |
|    approx_kl                | 0.007930041  |
|    approx_ln(kl)            | -4.837097    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.9         |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.64        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0263       |
|    n_updates                | 865          |
|    policy_gradient_loss     | -0.00141     |
|    std                      | 1.03         |
|    value_loss               | 0.0739       |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48611784] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 31            |
|    time_elapsed             | 431           |
|    total_timesteps          | 1669120       |
| train/                      |               |
|    approx_kl                | 0.006864584   |
|    approx_ln(kl)            | -4.98138      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.89         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.43         |
|    ln(policy_gradient_loss) | -5.21         |
|    loss                     | 0.0878        |
|    n_updates                | 866           |
|    policy_gradient_loss     | 0.00547       |
|    std                      | 1.03          |
|    value_loss               | 0.0892        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.46712878] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 32            |
|    time_elapsed             | 447           |
|    total_timesteps          | 1671168       |
| train/                      |               |
|    approx_kl                | 0.004151004   |
|    approx_ln(kl)            | -5.484405     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.89         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.82         |
|    ln(policy_gradient_loss) | -5.39         |
|    loss                     | 0.163         |
|    n_updates                | 867           |
|    policy_gradient_loss     | 0.00456       |
|    std                      | 1.03          |
|    value_loss               | 1.82          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.30601463] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 33            |
|    time_elapsed             | 464           |
|    total_timesteps          | 1673216       |
| train/                      |               |
|    approx_kl                | 0.006128142   |
|    approx_ln(kl)            | -5.094864     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.9          |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.91         |
|    ln(policy_gradient_loss) | -6.52         |
|    loss                     | 0.0201        |
|    n_updates                | 868           |
|    policy_gradient_loss     | 0.00147       |
|    std                      | 1.03          |
|    value_loss               | 0.0752        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30454853] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 34            |
|    time_elapsed             | 480           |
|    total_timesteps          | 1675264       |
| train/                      |               |
|    approx_kl                | 0.005663164   |
|    approx_ln(kl)            | -5.173773     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.9          |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.42         |
|    ln(policy_gradient_loss) | -7.35         |
|    loss                     | 0.0891        |
|    n_updates                | 869           |
|    policy_gradient_loss     | 0.000639      |
|    std                      | 1.03          |
|    value_loss               | 0.148         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31414223] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 35            |
|    time_elapsed             | 496           |
|    total_timesteps          | 1677312       |
| train/                      |               |
|    approx_kl                | 0.005464504   |
|    approx_ln(kl)            | -5.2094817    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.9          |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.16         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.315         |
|    n_updates                | 870           |
|    policy_gradient_loss     | -0.00246      |
|    std                      | 1.03          |
|    value_loss               | 0.71          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37552974] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 36            |
|    time_elapsed             | 513           |
|    total_timesteps          | 1679360       |
| train/                      |               |
|    approx_kl                | 0.0035511863  |
|    approx_ln(kl)            | -5.6404734    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.9          |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.98         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0508        |
|    n_updates                | 871           |
|    policy_gradient_loss     | -0.00123      |
|    std                      | 1.03          |
|    value_loss               | 0.0766        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.2522606] |
| time/                       |              |
|    fps                      | 142          |
|    iterations               | 37           |
|    time_elapsed             | 530          |
|    total_timesteps          | 1681408      |
| train/                      |              |
|    approx_kl                | 0.012628853  |
|    approx_ln(kl)            | -4.3717713   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.9         |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.88        |
|    ln(policy_gradient_loss) | -5.57        |
|    loss                     | 0.056        |
|    n_updates                | 872          |
|    policy_gradient_loss     | 0.00381      |
|    std                      | 1.03         |
|    value_loss               | 0.0542       |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.55973715] |
| time/                       |               |
|    fps                      | 142           |
|    iterations               | 38            |
|    time_elapsed             | 547           |
|    total_timesteps          | 1683456       |
| train/                      |               |
|    approx_kl                | 0.009360129   |
|    approx_ln(kl)            | -4.671296     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.89         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | nan           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | -0.0328       |
|    n_updates                | 873           |
|    policy_gradient_loss     | -0.0144       |
|    std                      | 1.03          |
|    value_loss               | 0.069         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33558255] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 39            |
|    time_elapsed             | 564           |
|    total_timesteps          | 1685504       |
| train/                      |               |
|    approx_kl                | 0.006623726   |
|    approx_ln(kl)            | -5.0170975    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.89         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.8           |
|    ln(policy_gradient_loss) | -5.55         |
|    loss                     | 2.22          |
|    n_updates                | 874           |
|    policy_gradient_loss     | 0.00387       |
|    std                      | 1.02          |
|    value_loss               | 1.72          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32140765] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 40            |
|    time_elapsed             | 581           |
|    total_timesteps          | 1687552       |
| train/                      |               |
|    approx_kl                | 0.0059168935  |
|    approx_ln(kl)            | -5.129944     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.88         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.75         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0642        |
|    n_updates                | 875           |
|    policy_gradient_loss     | -0.00106      |
|    std                      | 1.02          |
|    value_loss               | 0.231         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.454071] |
| time/                       |             |
|    fps                      | 140         |
|    iterations               | 41          |
|    time_elapsed             | 598         |
|    total_timesteps          | 1689600     |
| train/                      |             |
|    approx_kl                | 0.006551085 |
|    approx_ln(kl)            | -5.028125   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.88       |
|    explained_variance       | 0.993       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -1.97       |
|    ln(policy_gradient_loss) | -6.06       |
|    loss                     | 0.14        |
|    n_updates                | 876         |
|    policy_gradient_loss     | 0.00235     |
|    std                      | 1.02        |
|    value_loss               | 0.349       |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.46969274] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 42            |
|    time_elapsed             | 615           |
|    total_timesteps          | 1691648       |
| train/                      |               |
|    approx_kl                | 0.008522579   |
|    approx_ln(kl)            | -4.765036     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.86         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.01         |
|    ln(policy_gradient_loss) | -6            |
|    loss                     | 0.135         |
|    n_updates                | 877           |
|    policy_gradient_loss     | 0.00248       |
|    std                      | 1.01          |
|    value_loss               | 0.185         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45662424] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 43            |
|    time_elapsed             | 629           |
|    total_timesteps          | 1693696       |
| train/                      |               |
|    approx_kl                | 0.006068523   |
|    approx_ln(kl)            | -5.10464      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.85         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.8          |
|    ln(policy_gradient_loss) | -5.25         |
|    loss                     | 0.00821       |
|    n_updates                | 878           |
|    policy_gradient_loss     | 0.00526       |
|    std                      | 1             |
|    value_loss               | 0.0419        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2083616] |
| time/                       |              |
|    fps                      | 140          |
|    iterations               | 44           |
|    time_elapsed             | 642          |
|    total_timesteps          | 1695744      |
| train/                      |              |
|    approx_kl                | 0.004924538  |
|    approx_ln(kl)            | -5.3135247   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.84        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.18        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0414       |
|    n_updates                | 879          |
|    policy_gradient_loss     | -0.00245     |
|    std                      | 0.998        |
|    value_loss               | 0.119        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4827407] |
| time/                       |              |
|    fps                      | 140          |
|    iterations               | 45           |
|    time_elapsed             | 655          |
|    total_timesteps          | 1697792      |
| train/                      |              |
|    approx_kl                | 0.006240123  |
|    approx_ln(kl)            | -5.0767555   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.83        |
|    explained_variance       | 0.993        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.4         |
|    ln(policy_gradient_loss) | -4.92        |
|    loss                     | 0.246        |
|    n_updates                | 880          |
|    policy_gradient_loss     | 0.00727      |
|    std                      | 0.997        |
|    value_loss               | 0.507        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.47718883] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 46            |
|    time_elapsed             | 669           |
|    total_timesteps          | 1699840       |
| train/                      |               |
|    approx_kl                | 0.0071890894  |
|    approx_ln(kl)            | -4.9351907    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.91         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0543        |
|    n_updates                | 881           |
|    policy_gradient_loss     | -0.00637      |
|    std                      | 0.997         |
|    value_loss               | 0.54          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29399702] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 47            |
|    time_elapsed             | 682           |
|    total_timesteps          | 1701888       |
| train/                      |               |
|    approx_kl                | 0.0051336717  |
|    approx_ln(kl)            | -5.271934     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.21         |
|    ln(policy_gradient_loss) | -6.78         |
|    loss                     | 0.297         |
|    n_updates                | 882           |
|    policy_gradient_loss     | 0.00113       |
|    std                      | 0.997         |
|    value_loss               | 1.58          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31784567] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 48            |
|    time_elapsed             | 695           |
|    total_timesteps          | 1703936       |
| train/                      |               |
|    approx_kl                | 0.00503319    |
|    approx_ln(kl)            | -5.2917013    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.02         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0489        |
|    n_updates                | 883           |
|    policy_gradient_loss     | -0.0029       |
|    std                      | 0.997         |
|    value_loss               | 0.143         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.46707425] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 49            |
|    time_elapsed             | 708           |
|    total_timesteps          | 1705984       |
| train/                      |               |
|    approx_kl                | 0.007993501   |
|    approx_ln(kl)            | -4.8291264    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.968         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.932        |
|    ln(policy_gradient_loss) | -5.11         |
|    loss                     | 0.394         |
|    n_updates                | 884           |
|    policy_gradient_loss     | 0.00603       |
|    std                      | 0.996         |
|    value_loss               | 1.95          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------
| reward             | [-0.3095988] |
| time/              |              |
|    fps             | 158          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 1708032      |
-------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.20416582] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 2             |
|    time_elapsed             | 26            |
|    total_timesteps          | 1710080       |
| train/                      |               |
|    approx_kl                | 0.0076532676  |
|    approx_ln(kl)            | -4.8726225    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.34         |
|    ln(policy_gradient_loss) | -7.6          |
|    loss                     | 0.263         |
|    n_updates                | 886           |
|    policy_gradient_loss     | 0.000502      |
|    std                      | 0.996         |
|    value_loss               | 0.545         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5609271] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 3            |
|    time_elapsed             | 39           |
|    total_timesteps          | 1712128      |
| train/                      |              |
|    approx_kl                | 0.006020916  |
|    approx_ln(kl)            | -5.112516    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.83        |
|    explained_variance       | 0.99         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.921       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.398        |
|    n_updates                | 887          |
|    policy_gradient_loss     | -0.00395     |
|    std                      | 0.996        |
|    value_loss               | 0.646        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35920888] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 4             |
|    time_elapsed             | 52            |
|    total_timesteps          | 1714176       |
| train/                      |               |
|    approx_kl                | 0.0059269546  |
|    approx_ln(kl)            | -5.128245     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.46         |
|    ln(policy_gradient_loss) | -10.8         |
|    loss                     | 0.0852        |
|    n_updates                | 888           |
|    policy_gradient_loss     | 2.12e-05      |
|    std                      | 0.997         |
|    value_loss               | 0.175         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33549196] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 5             |
|    time_elapsed             | 67            |
|    total_timesteps          | 1716224       |
| train/                      |               |
|    approx_kl                | 0.004919782   |
|    approx_ln(kl)            | -5.3144913    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.79         |
|    ln(policy_gradient_loss) | -5.94         |
|    loss                     | 0.167         |
|    n_updates                | 889           |
|    policy_gradient_loss     | 0.00263       |
|    std                      | 0.998         |
|    value_loss               | 0.868         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31785697] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 6             |
|    time_elapsed             | 81            |
|    total_timesteps          | 1718272       |
| train/                      |               |
|    approx_kl                | 0.0061577563  |
|    approx_ln(kl)            | -5.0900426    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.82         |
|    ln(policy_gradient_loss) | -6.83         |
|    loss                     | 0.161         |
|    n_updates                | 890           |
|    policy_gradient_loss     | 0.00108       |
|    std                      | 0.998         |
|    value_loss               | 0.813         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34669214] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 7             |
|    time_elapsed             | 94            |
|    total_timesteps          | 1720320       |
| train/                      |               |
|    approx_kl                | 0.0033407025  |
|    approx_ln(kl)            | -5.7015743    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | nan           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | -0.0609       |
|    n_updates                | 891           |
|    policy_gradient_loss     | -0.00954      |
|    std                      | 0.997         |
|    value_loss               | 0.135         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38051257] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 8             |
|    time_elapsed             | 107           |
|    total_timesteps          | 1722368       |
| train/                      |               |
|    approx_kl                | 0.00802118    |
|    approx_ln(kl)            | -4.82567      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | nan           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | -0.00696      |
|    n_updates                | 892           |
|    policy_gradient_loss     | -0.00195      |
|    std                      | 0.995         |
|    value_loss               | 0.17          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24122916] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 9             |
|    time_elapsed             | 120           |
|    total_timesteps          | 1724416       |
| train/                      |               |
|    approx_kl                | 0.005142674   |
|    approx_ln(kl)            | -5.270182     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.977         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.788        |
|    ln(policy_gradient_loss) | -5.52         |
|    loss                     | 0.455         |
|    n_updates                | 893           |
|    policy_gradient_loss     | 0.00399       |
|    std                      | 0.995         |
|    value_loss               | 0.403         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49885663] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 10            |
|    time_elapsed             | 133           |
|    total_timesteps          | 1726464       |
| train/                      |               |
|    approx_kl                | 0.0063163876  |
|    approx_ln(kl)            | -5.0646076    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.43         |
|    ln(policy_gradient_loss) | -4.78         |
|    loss                     | 0.0878        |
|    n_updates                | 894           |
|    policy_gradient_loss     | 0.00839       |
|    std                      | 0.995         |
|    value_loss               | 0.642         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2626446] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 11           |
|    time_elapsed             | 146          |
|    total_timesteps          | 1728512      |
| train/                      |              |
|    approx_kl                | 0.0036590456 |
|    approx_ln(kl)            | -5.610553    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.83        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.01        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.134        |
|    n_updates                | 895          |
|    policy_gradient_loss     | -0.00602     |
|    std                      | 0.995        |
|    value_loss               | 0.408        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31886607] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 12            |
|    time_elapsed             | 159           |
|    total_timesteps          | 1730560       |
| train/                      |               |
|    approx_kl                | 0.006614114   |
|    approx_ln(kl)            | -5.0185494    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.73         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.177         |
|    n_updates                | 896           |
|    policy_gradient_loss     | -0.0035       |
|    std                      | 0.995         |
|    value_loss               | 0.436         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.46601963] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 13            |
|    time_elapsed             | 175           |
|    total_timesteps          | 1732608       |
| train/                      |               |
|    approx_kl                | 0.0061651464  |
|    approx_ln(kl)            | -5.0888433    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.72         |
|    ln(policy_gradient_loss) | -5.24         |
|    loss                     | 0.0662        |
|    n_updates                | 897           |
|    policy_gradient_loss     | 0.00531       |
|    std                      | 0.995         |
|    value_loss               | 0.108         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28431532] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 14            |
|    time_elapsed             | 190           |
|    total_timesteps          | 1734656       |
| train/                      |               |
|    approx_kl                | 0.0060026166  |
|    approx_ln(kl)            | -5.1155596    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.63         |
|    ln(policy_gradient_loss) | -6.09         |
|    loss                     | 0.0722        |
|    n_updates                | 898           |
|    policy_gradient_loss     | 0.00227       |
|    std                      | 0.995         |
|    value_loss               | 0.137         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5811892] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 15           |
|    time_elapsed             | 204          |
|    total_timesteps          | 1736704      |
| train/                      |              |
|    approx_kl                | 0.007184282  |
|    approx_ln(kl)            | -4.9358597   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.5         |
|    ln(policy_gradient_loss) | -5.42        |
|    loss                     | 0.0822       |
|    n_updates                | 899          |
|    policy_gradient_loss     | 0.00442      |
|    std                      | 0.993        |
|    value_loss               | 0.133        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33586153] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 16            |
|    time_elapsed             | 221           |
|    total_timesteps          | 1738752       |
| train/                      |               |
|    approx_kl                | 0.008356985   |
|    approx_ln(kl)            | -4.7846575    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.12         |
|    ln(policy_gradient_loss) | -6.59         |
|    loss                     | 0.12          |
|    n_updates                | 900           |
|    policy_gradient_loss     | 0.00138       |
|    std                      | 0.993         |
|    value_loss               | 0.141         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.4504183] |
| time/                       |              |
|    fps                      | 146          |
|    iterations               | 17           |
|    time_elapsed             | 237          |
|    total_timesteps          | 1740800      |
| train/                      |              |
|    approx_kl                | 0.008786698  |
|    approx_ln(kl)            | -4.734516    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -4.25        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0142       |
|    n_updates                | 901          |
|    policy_gradient_loss     | -0.00722     |
|    std                      | 0.993        |
|    value_loss               | 0.097        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25171143] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 18            |
|    time_elapsed             | 255           |
|    total_timesteps          | 1742848       |
| train/                      |               |
|    approx_kl                | 0.007102421   |
|    approx_ln(kl)            | -4.9473195    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.01         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0495        |
|    n_updates                | 902           |
|    policy_gradient_loss     | -0.000589     |
|    std                      | 0.995         |
|    value_loss               | 0.102         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.12988988] |
| time/                       |               |
|    fps                      | 142           |
|    iterations               | 19            |
|    time_elapsed             | 272           |
|    total_timesteps          | 1744896       |
| train/                      |               |
|    approx_kl                | 0.007229675   |
|    approx_ln(kl)            | -4.929561     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.13         |
|    ln(policy_gradient_loss) | -7.22         |
|    loss                     | 0.119         |
|    n_updates                | 903           |
|    policy_gradient_loss     | 0.000729      |
|    std                      | 0.996         |
|    value_loss               | 0.24          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5014429] |
| time/                       |              |
|    fps                      | 140          |
|    iterations               | 20           |
|    time_elapsed             | 292          |
|    total_timesteps          | 1746944      |
| train/                      |              |
|    approx_kl                | 0.0049598115 |
|    approx_ln(kl)            | -5.3063874   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.83        |
|    explained_variance       | 0.988        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.04        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0477       |
|    n_updates                | 904          |
|    policy_gradient_loss     | -0.00433     |
|    std                      | 0.993        |
|    value_loss               | 0.168        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42184827] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 21            |
|    time_elapsed             | 308           |
|    total_timesteps          | 1748992       |
| train/                      |               |
|    approx_kl                | 0.0077652894  |
|    approx_ln(kl)            | -4.8580914    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.971         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0443       |
|    ln(policy_gradient_loss) | -6.09         |
|    loss                     | 0.957         |
|    n_updates                | 905           |
|    policy_gradient_loss     | 0.00226       |
|    std                      | 0.992         |
|    value_loss               | 3.81          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
---------------------------------------------
| reward                      | [-0.350399] |
| time/                       |             |
|    fps                      | 139         |
|    iterations               | 22          |
|    time_elapsed             | 322         |
|    total_timesteps          | 1751040     |
| train/                      |             |
|    approx_kl                | 0.00621467  |
|    approx_ln(kl)            | -5.0808425  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.82       |
|    explained_variance       | 0.999       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -3.04       |
|    ln(policy_gradient_loss) | -6.41       |
|    loss                     | 0.048       |
|    n_updates                | 906         |
|    policy_gradient_loss     | 0.00165     |
|    std                      | 0.992       |
|    value_loss               | 0.0689      |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41932365] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 23            |
|    time_elapsed             | 337           |
|    total_timesteps          | 1753088       |
| train/                      |               |
|    approx_kl                | 0.007495839   |
|    approx_ln(kl)            | -4.8934073    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.961        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.383         |
|    n_updates                | 907           |
|    policy_gradient_loss     | -0.00331      |
|    std                      | 0.992         |
|    value_loss               | 1.05          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35593015] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 24            |
|    time_elapsed             | 350           |
|    total_timesteps          | 1755136       |
| train/                      |               |
|    approx_kl                | 0.0049347687  |
|    approx_ln(kl)            | -5.3114495    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.77         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.17          |
|    n_updates                | 908           |
|    policy_gradient_loss     | -0.00393      |
|    std                      | 0.992         |
|    value_loss               | 0.375         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28602043] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 25            |
|    time_elapsed             | 363           |
|    total_timesteps          | 1757184       |
| train/                      |               |
|    approx_kl                | 0.0054538567  |
|    approx_ln(kl)            | -5.2114325    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.7          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.00909       |
|    n_updates                | 909           |
|    policy_gradient_loss     | -0.00506      |
|    std                      | 0.993         |
|    value_loss               | 0.0848        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34374014] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 26            |
|    time_elapsed             | 376           |
|    total_timesteps          | 1759232       |
| train/                      |               |
|    approx_kl                | 0.0065585473  |
|    approx_ln(kl)            | -5.026986     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.294         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.34          |
|    n_updates                | 910           |
|    policy_gradient_loss     | -0.00206      |
|    std                      | 0.993         |
|    value_loss               | 1.93          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2599595] |
| time/                       |              |
|    fps                      | 141          |
|    iterations               | 27           |
|    time_elapsed             | 390          |
|    total_timesteps          | 1761280      |
| train/                      |              |
|    approx_kl                | 0.006809123  |
|    approx_ln(kl)            | -4.989492    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.972        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.77        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0624       |
|    n_updates                | 911          |
|    policy_gradient_loss     | -0.0027      |
|    std                      | 0.993        |
|    value_loss               | 0.603        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.33663213] |
| time/                       |               |
|    fps                      | 138           |
|    iterations               | 28            |
|    time_elapsed             | 412           |
|    total_timesteps          | 1763328       |
| train/                      |               |
|    approx_kl                | 0.0056019067  |
|    approx_ln(kl)            | -5.1846485    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.59         |
|    ln(policy_gradient_loss) | -5.58         |
|    loss                     | 0.205         |
|    n_updates                | 912           |
|    policy_gradient_loss     | 0.00379       |
|    std                      | 0.993         |
|    value_loss               | 0.605         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36444575] |
| time/                       |               |
|    fps                      | 138           |
|    iterations               | 29            |
|    time_elapsed             | 430           |
|    total_timesteps          | 1765376       |
| train/                      |               |
|    approx_kl                | 0.006522747   |
|    approx_ln(kl)            | -5.0324597    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.16         |
|    ln(policy_gradient_loss) | -6.23         |
|    loss                     | 0.0424        |
|    n_updates                | 913           |
|    policy_gradient_loss     | 0.00196       |
|    std                      | 0.993         |
|    value_loss               | 0.158         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3539498] |
| time/                       |              |
|    fps                      | 137          |
|    iterations               | 30           |
|    time_elapsed             | 448          |
|    total_timesteps          | 1767424      |
| train/                      |              |
|    approx_kl                | 0.006432313  |
|    approx_ln(kl)            | -5.046421    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.976        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.929       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.395        |
|    n_updates                | 914          |
|    policy_gradient_loss     | -0.000133    |
|    std                      | 0.994        |
|    value_loss               | 2.2          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.537919] |
| time/                       |             |
|    fps                      | 137         |
|    iterations               | 31          |
|    time_elapsed             | 463         |
|    total_timesteps          | 1769472     |
| train/                      |             |
|    approx_kl                | 0.004626488 |
|    approx_ln(kl)            | -5.3759575  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.82       |
|    explained_variance       | 0.995       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -2.61       |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.0733      |
|    n_updates                | 915         |
|    policy_gradient_loss     | -0.00333    |
|    std                      | 0.993       |
|    value_loss               | 0.139       |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23373808] |
| time/                       |               |
|    fps                      | 137           |
|    iterations               | 32            |
|    time_elapsed             | 476           |
|    total_timesteps          | 1771520       |
| train/                      |               |
|    approx_kl                | 0.007277873   |
|    approx_ln(kl)            | -4.922917     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.18         |
|    ln(policy_gradient_loss) | -5.72         |
|    loss                     | 0.306         |
|    n_updates                | 916           |
|    policy_gradient_loss     | 0.00327       |
|    std                      | 0.993         |
|    value_loss               | 2.66          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31599838] |
| time/                       |               |
|    fps                      | 138           |
|    iterations               | 33            |
|    time_elapsed             | 489           |
|    total_timesteps          | 1773568       |
| train/                      |               |
|    approx_kl                | 0.0041886014  |
|    approx_ln(kl)            | -5.4753885    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.00443       |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1             |
|    n_updates                | 917           |
|    policy_gradient_loss     | -0.00283      |
|    std                      | 0.993         |
|    value_loss               | 1.86          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3323414] |
| time/                       |              |
|    fps                      | 138          |
|    iterations               | 34           |
|    time_elapsed             | 502          |
|    total_timesteps          | 1775616      |
| train/                      |              |
|    approx_kl                | 0.0075480384 |
|    approx_ln(kl)            | -4.8864675   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.985        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.14        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.321        |
|    n_updates                | 918          |
|    policy_gradient_loss     | -0.000794    |
|    std                      | 0.993        |
|    value_loss               | 0.853        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27843088] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 35            |
|    time_elapsed             | 515           |
|    total_timesteps          | 1777664       |
| train/                      |               |
|    approx_kl                | 0.0029962587  |
|    approx_ln(kl)            | -5.810391     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.133        |
|    ln(policy_gradient_loss) | -6.28         |
|    loss                     | 0.875         |
|    n_updates                | 919           |
|    policy_gradient_loss     | 0.00187       |
|    std                      | 0.993         |
|    value_loss               | 2.44          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40619296] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 36            |
|    time_elapsed             | 528           |
|    total_timesteps          | 1779712       |
| train/                      |               |
|    approx_kl                | 0.006659057   |
|    approx_ln(kl)            | -5.0117774    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.09         |
|    ln(policy_gradient_loss) | -6.12         |
|    loss                     | 0.336         |
|    n_updates                | 920           |
|    policy_gradient_loss     | 0.0022        |
|    std                      | 0.993         |
|    value_loss               | 1.01          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3353211] |
| time/                       |              |
|    fps                      | 139          |
|    iterations               | 37           |
|    time_elapsed             | 541          |
|    total_timesteps          | 1781760      |
| train/                      |              |
|    approx_kl                | 0.003945533  |
|    approx_ln(kl)            | -5.535171    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.993        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.64        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.194        |
|    n_updates                | 921          |
|    policy_gradient_loss     | -0.0066      |
|    std                      | 0.994        |
|    value_loss               | 0.827        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.57797724] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 38            |
|    time_elapsed             | 554           |
|    total_timesteps          | 1783808       |
| train/                      |               |
|    approx_kl                | 0.006236572   |
|    approx_ln(kl)            | -5.077325     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.268         |
|    ln(policy_gradient_loss) | -5.91         |
|    loss                     | 1.31          |
|    n_updates                | 922           |
|    policy_gradient_loss     | 0.0027        |
|    std                      | 0.994         |
|    value_loss               | 1.28          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38953644] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 39            |
|    time_elapsed             | 568           |
|    total_timesteps          | 1785856       |
| train/                      |               |
|    approx_kl                | 0.008451358   |
|    approx_ln(kl)            | -4.773428     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.52         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0801        |
|    n_updates                | 923           |
|    policy_gradient_loss     | -0.0164       |
|    std                      | 0.994         |
|    value_loss               | 0.271         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32598257] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 40            |
|    time_elapsed             | 581           |
|    total_timesteps          | 1787904       |
| train/                      |               |
|    approx_kl                | 0.008194277   |
|    approx_ln(kl)            | -4.8043194    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.5          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.223         |
|    n_updates                | 924           |
|    policy_gradient_loss     | -0.00405      |
|    std                      | 0.994         |
|    value_loss               | 2.8           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3259831] |
| time/                       |              |
|    fps                      | 141          |
|    iterations               | 41           |
|    time_elapsed             | 594          |
|    total_timesteps          | 1789952      |
| train/                      |              |
|    approx_kl                | 0.007845523  |
|    approx_ln(kl)            | -4.847812    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.991        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.53        |
|    ln(policy_gradient_loss) | -5.92        |
|    loss                     | 0.217        |
|    n_updates                | 925          |
|    policy_gradient_loss     | 0.00269      |
|    std                      | 0.993        |
|    value_loss               | 1.52         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5037812] |
| time/                       |              |
|    fps                      | 141          |
|    iterations               | 42           |
|    time_elapsed             | 608          |
|    total_timesteps          | 1792000      |
| train/                      |              |
|    approx_kl                | 0.0042854073 |
|    approx_ln(kl)            | -5.45254     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.34        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0355       |
|    n_updates                | 926          |
|    policy_gradient_loss     | -0.000175    |
|    std                      | 0.992        |
|    value_loss               | 0.315        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4271815] |
| time/                       |              |
|    fps                      | 141          |
|    iterations               | 43           |
|    time_elapsed             | 621          |
|    total_timesteps          | 1794048      |
| train/                      |              |
|    approx_kl                | 0.0068488303 |
|    approx_ln(kl)            | -4.9836774   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.982        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.619       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.538        |
|    n_updates                | 927          |
|    policy_gradient_loss     | -0.0058      |
|    std                      | 0.993        |
|    value_loss               | 1.38         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43939635] |
| time/                       |               |
|    fps                      | 142           |
|    iterations               | 44            |
|    time_elapsed             | 634           |
|    total_timesteps          | 1796096       |
| train/                      |               |
|    approx_kl                | 0.005723215   |
|    approx_ln(kl)            | -5.1632247    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.609        |
|    ln(policy_gradient_loss) | -6.27         |
|    loss                     | 0.544         |
|    n_updates                | 928           |
|    policy_gradient_loss     | 0.00189       |
|    std                      | 0.993         |
|    value_loss               | 1.74          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.50384027] |
| time/                       |               |
|    fps                      | 142           |
|    iterations               | 45            |
|    time_elapsed             | 647           |
|    total_timesteps          | 1798144       |
| train/                      |               |
|    approx_kl                | 0.0036897203  |
|    approx_ln(kl)            | -5.602205     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.686        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.503         |
|    n_updates                | 929           |
|    policy_gradient_loss     | -0.000411     |
|    std                      | 0.994         |
|    value_loss               | 1.45          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29417354] |
| time/                       |               |
|    fps                      | 142           |
|    iterations               | 46            |
|    time_elapsed             | 660           |
|    total_timesteps          | 1800192       |
| train/                      |               |
|    approx_kl                | 0.0065774983  |
|    approx_ln(kl)            | -5.024101     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.51         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.22          |
|    n_updates                | 930           |
|    policy_gradient_loss     | -0.00066      |
|    std                      | 0.994         |
|    value_loss               | 0.84          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30072054] |
| time/                       |               |
|    fps                      | 142           |
|    iterations               | 47            |
|    time_elapsed             | 674           |
|    total_timesteps          | 1802240       |
| train/                      |               |
|    approx_kl                | 0.0061320835  |
|    approx_ln(kl)            | -5.0942206    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.26         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0142        |
|    n_updates                | 931           |
|    policy_gradient_loss     | -0.00965      |
|    std                      | 0.994         |
|    value_loss               | 0.144         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33810824] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 48            |
|    time_elapsed             | 687           |
|    total_timesteps          | 1804288       |
| train/                      |               |
|    approx_kl                | 0.007956242   |
|    approx_ln(kl)            | -4.8337984    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.54         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0787        |
|    n_updates                | 932           |
|    policy_gradient_loss     | -0.0036       |
|    std                      | 0.995         |
|    value_loss               | 0.177         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3658039] |
| time/                       |              |
|    fps                      | 143          |
|    iterations               | 49           |
|    time_elapsed             | 700          |
|    total_timesteps          | 1806336      |
| train/                      |              |
|    approx_kl                | 0.0038789639 |
|    approx_ln(kl)            | -5.5521874   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.28        |
|    ln(policy_gradient_loss) | -5.75        |
|    loss                     | 0.0374       |
|    n_updates                | 933          |
|    policy_gradient_loss     | 0.00319      |
|    std                      | 0.992        |
|    value_loss               | 0.0785       |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
--------------------------------------
| reward             | [-0.22109732] |
| time/              |               |
|    fps             | 149           |
|    iterations      | 1             |
|    time_elapsed    | 13            |
|    total_timesteps | 1808384       |
--------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4733463] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 2            |
|    time_elapsed             | 27           |
|    total_timesteps          | 1810432      |
| train/                      |              |
|    approx_kl                | 0.005052599  |
|    approx_ln(kl)            | -5.2878523   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.977        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.53        |
|    ln(policy_gradient_loss) | -6.49        |
|    loss                     | 0.0294       |
|    n_updates                | 935          |
|    policy_gradient_loss     | 0.00152      |
|    std                      | 0.992        |
|    value_loss               | 0.985        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34923992] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 3             |
|    time_elapsed             | 40            |
|    total_timesteps          | 1812480       |
| train/                      |               |
|    approx_kl                | 0.004858926   |
|    approx_ln(kl)            | -5.3269377    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.74         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0643        |
|    n_updates                | 936           |
|    policy_gradient_loss     | -0.0055       |
|    std                      | 0.993         |
|    value_loss               | 0.875         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.21916625] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 4             |
|    time_elapsed             | 53            |
|    total_timesteps          | 1814528       |
| train/                      |               |
|    approx_kl                | 0.0057382938  |
|    approx_ln(kl)            | -5.1605935    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.883        |
|    ln(policy_gradient_loss) | -6.06         |
|    loss                     | 0.414         |
|    n_updates                | 937           |
|    policy_gradient_loss     | 0.00234       |
|    std                      | 0.993         |
|    value_loss               | 1.39          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5175903] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 5            |
|    time_elapsed             | 66           |
|    total_timesteps          | 1816576      |
| train/                      |              |
|    approx_kl                | 0.005739524  |
|    approx_ln(kl)            | -5.160379    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.06        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.128        |
|    n_updates                | 938          |
|    policy_gradient_loss     | -0.0127      |
|    std                      | 0.992        |
|    value_loss               | 0.828        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.38283598] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 6             |
|    time_elapsed             | 79            |
|    total_timesteps          | 1818624       |
| train/                      |               |
|    approx_kl                | 0.008323495   |
|    approx_ln(kl)            | -4.788673     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.382         |
|    ln(policy_gradient_loss) | -4.68         |
|    loss                     | 1.47          |
|    n_updates                | 939           |
|    policy_gradient_loss     | 0.00927       |
|    std                      | 0.992         |
|    value_loss               | 1.23          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4988179] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 7            |
|    time_elapsed             | 93           |
|    total_timesteps          | 1820672      |
| train/                      |              |
|    approx_kl                | 0.006412552  |
|    approx_ln(kl)            | -5.049498    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.24        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.106        |
|    n_updates                | 940          |
|    policy_gradient_loss     | -0.00322     |
|    std                      | 0.992        |
|    value_loss               | 0.24         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5114067] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 8            |
|    time_elapsed             | 106          |
|    total_timesteps          | 1822720      |
| train/                      |              |
|    approx_kl                | 0.0054973816 |
|    approx_ln(kl)            | -5.2034836   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.991        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | nan          |
|    ln(policy_gradient_loss) | -5.15        |
|    loss                     | -0.00922     |
|    n_updates                | 941          |
|    policy_gradient_loss     | 0.00582      |
|    std                      | 0.991        |
|    value_loss               | 0.621        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34674796] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 9             |
|    time_elapsed             | 119           |
|    total_timesteps          | 1824768       |
| train/                      |               |
|    approx_kl                | 0.0065943985  |
|    approx_ln(kl)            | -5.021535     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.39         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.248         |
|    n_updates                | 942           |
|    policy_gradient_loss     | -0.00134      |
|    std                      | 0.992         |
|    value_loss               | 0.955         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23592404] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 10            |
|    time_elapsed             | 132           |
|    total_timesteps          | 1826816       |
| train/                      |               |
|    approx_kl                | 0.0057840417  |
|    approx_ln(kl)            | -5.1526527    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.863        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.422         |
|    n_updates                | 943           |
|    policy_gradient_loss     | -0.00669      |
|    std                      | 0.993         |
|    value_loss               | 0.311         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5907374] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 11           |
|    time_elapsed             | 145          |
|    total_timesteps          | 1828864      |
| train/                      |              |
|    approx_kl                | 0.008119033  |
|    approx_ln(kl)            | -4.8135443   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.61        |
|    ln(policy_gradient_loss) | -5.29        |
|    loss                     | 0.0734       |
|    n_updates                | 944          |
|    policy_gradient_loss     | 0.00504      |
|    std                      | 0.994        |
|    value_loss               | 0.176        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.17655045] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 12            |
|    time_elapsed             | 159           |
|    total_timesteps          | 1830912       |
| train/                      |               |
|    approx_kl                | 0.0074878084  |
|    approx_ln(kl)            | -4.8944793    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.504        |
|    ln(policy_gradient_loss) | -6.32         |
|    loss                     | 0.604         |
|    n_updates                | 945           |
|    policy_gradient_loss     | 0.0018        |
|    std                      | 0.993         |
|    value_loss               | 1.88          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47049335] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 13            |
|    time_elapsed             | 173           |
|    total_timesteps          | 1832960       |
| train/                      |               |
|    approx_kl                | 0.0048956624  |
|    approx_ln(kl)            | -5.3194056    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.85         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.158         |
|    n_updates                | 946           |
|    policy_gradient_loss     | -0.00233      |
|    std                      | 0.993         |
|    value_loss               | 0.835         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3623424] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 14           |
|    time_elapsed             | 186          |
|    total_timesteps          | 1835008      |
| train/                      |              |
|    approx_kl                | 0.0058267266 |
|    approx_ln(kl)            | -5.1453      |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.87        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.419        |
|    n_updates                | 947          |
|    policy_gradient_loss     | -0.0101      |
|    std                      | 0.993        |
|    value_loss               | 0.967        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37521467] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 15            |
|    time_elapsed             | 199           |
|    total_timesteps          | 1837056       |
| train/                      |               |
|    approx_kl                | 0.0057409704  |
|    approx_ln(kl)            | -5.160127     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.7          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0248        |
|    n_updates                | 948           |
|    policy_gradient_loss     | -0.00693      |
|    std                      | 0.992         |
|    value_loss               | 0.114         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36033237] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 16            |
|    time_elapsed             | 212           |
|    total_timesteps          | 1839104       |
| train/                      |               |
|    approx_kl                | 0.005602248   |
|    approx_ln(kl)            | -5.1845875    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.13         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.118         |
|    n_updates                | 949           |
|    policy_gradient_loss     | -0.00423      |
|    std                      | 0.991         |
|    value_loss               | 0.386         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.464956]  |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 17           |
|    time_elapsed             | 225          |
|    total_timesteps          | 1841152      |
| train/                      |              |
|    approx_kl                | 0.0057674143 |
|    approx_ln(kl)            | -5.1555314   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.99         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.262       |
|    ln(policy_gradient_loss) | -4.91        |
|    loss                     | 0.769        |
|    n_updates                | 950          |
|    policy_gradient_loss     | 0.00741      |
|    std                      | 0.994        |
|    value_loss               | 1.22         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5855579] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 18           |
|    time_elapsed             | 239          |
|    total_timesteps          | 1843200      |
| train/                      |              |
|    approx_kl                | 0.005665418  |
|    approx_ln(kl)            | -5.1733747   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.87        |
|    ln(policy_gradient_loss) | -6.21        |
|    loss                     | 0.0569       |
|    n_updates                | 951          |
|    policy_gradient_loss     | 0.00202      |
|    std                      | 0.995        |
|    value_loss               | 0.0908       |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47194576] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 19            |
|    time_elapsed             | 252           |
|    total_timesteps          | 1845248       |
| train/                      |               |
|    approx_kl                | 0.008025485   |
|    approx_ln(kl)            | -4.8251333    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.02         |
|    ln(policy_gradient_loss) | -5.19         |
|    loss                     | 0.132         |
|    n_updates                | 952           |
|    policy_gradient_loss     | 0.00556       |
|    std                      | 0.996         |
|    value_loss               | 0.363         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41678247] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 20            |
|    time_elapsed             | 266           |
|    total_timesteps          | 1847296       |
| train/                      |               |
|    approx_kl                | 0.005695531   |
|    approx_ln(kl)            | -5.1680737    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.207        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.813         |
|    n_updates                | 953           |
|    policy_gradient_loss     | -0.0186       |
|    std                      | 0.997         |
|    value_loss               | 0.634         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38853356] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 21            |
|    time_elapsed             | 279           |
|    total_timesteps          | 1849344       |
| train/                      |               |
|    approx_kl                | 0.006629604   |
|    approx_ln(kl)            | -5.01621      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.37         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0935        |
|    n_updates                | 954           |
|    policy_gradient_loss     | -0.00111      |
|    std                      | 0.997         |
|    value_loss               | 0.45          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.359055]  |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 22           |
|    time_elapsed             | 292          |
|    total_timesteps          | 1851392      |
| train/                      |              |
|    approx_kl                | 0.0071960366 |
|    approx_ln(kl)            | -4.934225    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.83        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.741       |
|    ln(policy_gradient_loss) | -7.22        |
|    loss                     | 0.477        |
|    n_updates                | 955          |
|    policy_gradient_loss     | 0.000734     |
|    std                      | 0.998        |
|    value_loss               | 0.611        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.26335433] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 23            |
|    time_elapsed             | 305           |
|    total_timesteps          | 1853440       |
| train/                      |               |
|    approx_kl                | 0.00886371    |
|    approx_ln(kl)            | -4.72579      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.09         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.335         |
|    n_updates                | 956           |
|    policy_gradient_loss     | -0.00852      |
|    std                      | 0.999         |
|    value_loss               | 0.848         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27703652] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 24            |
|    time_elapsed             | 318           |
|    total_timesteps          | 1855488       |
| train/                      |               |
|    approx_kl                | 0.0049445922  |
|    approx_ln(kl)            | -5.3094606    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.967         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.46         |
|    ln(policy_gradient_loss) | -6.22         |
|    loss                     | 0.232         |
|    n_updates                | 957           |
|    policy_gradient_loss     | 0.00199       |
|    std                      | 1             |
|    value_loss               | 0.578         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.54697716] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 25            |
|    time_elapsed             | 332           |
|    total_timesteps          | 1857536       |
| train/                      |               |
|    approx_kl                | 0.00279238    |
|    approx_ln(kl)            | -5.880861     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2            |
|    ln(policy_gradient_loss) | -5.22         |
|    loss                     | 0.136         |
|    n_updates                | 958           |
|    policy_gradient_loss     | 0.00542       |
|    std                      | 1             |
|    value_loss               | 0.733         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47827792] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 26            |
|    time_elapsed             | 345           |
|    total_timesteps          | 1859584       |
| train/                      |               |
|    approx_kl                | 0.004421674   |
|    approx_ln(kl)            | -5.421237     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.84         |
|    explained_variance       | 0.966         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.46         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0313        |
|    n_updates                | 959           |
|    policy_gradient_loss     | -0.0108       |
|    std                      | 1             |
|    value_loss               | 0.698         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3298951] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 27           |
|    time_elapsed             | 358          |
|    total_timesteps          | 1861632      |
| train/                      |              |
|    approx_kl                | 0.0067121913 |
|    approx_ln(kl)            | -5.00383     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.83        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.52        |
|    ln(policy_gradient_loss) | -4.45        |
|    loss                     | 0.218        |
|    n_updates                | 960          |
|    policy_gradient_loss     | 0.0117       |
|    std                      | 1            |
|    value_loss               | 0.603        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.46881452] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 28            |
|    time_elapsed             | 371           |
|    total_timesteps          | 1863680       |
| train/                      |               |
|    approx_kl                | 0.0061394046  |
|    approx_ln(kl)            | -5.0930276    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.976         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.316        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.729         |
|    n_updates                | 961           |
|    policy_gradient_loss     | -0.00384      |
|    std                      | 0.999         |
|    value_loss               | 1.85          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30983463] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 29            |
|    time_elapsed             | 384           |
|    total_timesteps          | 1865728       |
| train/                      |               |
|    approx_kl                | 0.0048861853  |
|    approx_ln(kl)            | -5.3213434    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.98         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.139         |
|    n_updates                | 962           |
|    policy_gradient_loss     | -0.000462     |
|    std                      | 0.998         |
|    value_loss               | 0.741         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24969667] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 30            |
|    time_elapsed             | 398           |
|    total_timesteps          | 1867776       |
| train/                      |               |
|    approx_kl                | 0.007825972   |
|    approx_ln(kl)            | -4.8503075    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.974         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.247        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.781         |
|    n_updates                | 963           |
|    policy_gradient_loss     | -0.00799      |
|    std                      | 0.997         |
|    value_loss               | 2.18          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4692219] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 31           |
|    time_elapsed             | 411          |
|    total_timesteps          | 1869824      |
| train/                      |              |
|    approx_kl                | 0.0047309734 |
|    approx_ln(kl)            | -5.3536243   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.83        |
|    explained_variance       | 0.987        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.55        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0778       |
|    n_updates                | 964          |
|    policy_gradient_loss     | -0.00495     |
|    std                      | 0.998        |
|    value_loss               | 0.546        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.24814644] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 32            |
|    time_elapsed             | 425           |
|    total_timesteps          | 1871872       |
| train/                      |               |
|    approx_kl                | 0.00882928    |
|    approx_ln(kl)            | -4.729682     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.8          |
|    ln(policy_gradient_loss) | -5.02         |
|    loss                     | 0.0605        |
|    n_updates                | 965           |
|    policy_gradient_loss     | 0.00663       |
|    std                      | 0.998         |
|    value_loss               | 0.0698        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42214045] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 33            |
|    time_elapsed             | 439           |
|    total_timesteps          | 1873920       |
| train/                      |               |
|    approx_kl                | 0.007520502   |
|    approx_ln(kl)            | -4.8901224    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.91         |
|    ln(policy_gradient_loss) | -5.64         |
|    loss                     | 0.0547        |
|    n_updates                | 966           |
|    policy_gradient_loss     | 0.00354       |
|    std                      | 0.997         |
|    value_loss               | 0.103         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33111823] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 34            |
|    time_elapsed             | 452           |
|    total_timesteps          | 1875968       |
| train/                      |               |
|    approx_kl                | 0.008618586   |
|    approx_ln(kl)            | -4.7538342    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.37         |
|    ln(policy_gradient_loss) | -4.77         |
|    loss                     | 0.0937        |
|    n_updates                | 967           |
|    policy_gradient_loss     | 0.00849       |
|    std                      | 0.996         |
|    value_loss               | 0.109         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.18452977] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 35            |
|    time_elapsed             | 465           |
|    total_timesteps          | 1878016       |
| train/                      |               |
|    approx_kl                | 0.0062512574  |
|    approx_ln(kl)            | -5.0749726    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.18         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.307         |
|    n_updates                | 968           |
|    policy_gradient_loss     | -0.00997      |
|    std                      | 0.995         |
|    value_loss               | 1.07          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.31421298] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 36            |
|    time_elapsed             | 479           |
|    total_timesteps          | 1880064       |
| train/                      |               |
|    approx_kl                | 0.007813319   |
|    approx_ln(kl)            | -4.8519254    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.971         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.327        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.721         |
|    n_updates                | 969           |
|    policy_gradient_loss     | -0.000274     |
|    std                      | 0.994         |
|    value_loss               | 1.58          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40626395] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 37            |
|    time_elapsed             | 492           |
|    total_timesteps          | 1882112       |
| train/                      |               |
|    approx_kl                | 0.0050352137  |
|    approx_ln(kl)            | -5.2912993    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.96          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.06         |
|    ln(policy_gradient_loss) | -6.31         |
|    loss                     | 0.348         |
|    n_updates                | 970           |
|    policy_gradient_loss     | 0.00182       |
|    std                      | 0.994         |
|    value_loss               | 0.887         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49466434] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 38            |
|    time_elapsed             | 506           |
|    total_timesteps          | 1884160       |
| train/                      |               |
|    approx_kl                | 0.006734484   |
|    approx_ln(kl)            | -5.000514     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.24         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.107         |
|    n_updates                | 971           |
|    policy_gradient_loss     | -0.00416      |
|    std                      | 0.994         |
|    value_loss               | 1.43          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27953374] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 39            |
|    time_elapsed             | 520           |
|    total_timesteps          | 1886208       |
| train/                      |               |
|    approx_kl                | 0.0060951444  |
|    approx_ln(kl)            | -5.1002626    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.47         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0849        |
|    n_updates                | 972           |
|    policy_gradient_loss     | -0.00377      |
|    std                      | 0.994         |
|    value_loss               | 0.162         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3443606] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 40           |
|    time_elapsed             | 533          |
|    total_timesteps          | 1888256      |
| train/                      |              |
|    approx_kl                | 0.0061395094 |
|    approx_ln(kl)            | -5.0930104   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.69        |
|    ln(policy_gradient_loss) | -6.68        |
|    loss                     | 0.0679       |
|    n_updates                | 973          |
|    policy_gradient_loss     | 0.00125      |
|    std                      | 0.994        |
|    value_loss               | 0.219        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.21874103] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 41            |
|    time_elapsed             | 546           |
|    total_timesteps          | 1890304       |
| train/                      |               |
|    approx_kl                | 0.00569997    |
|    approx_ln(kl)            | -5.1672945    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.98         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0508        |
|    n_updates                | 974           |
|    policy_gradient_loss     | -0.00424      |
|    std                      | 0.994         |
|    value_loss               | 0.143         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2725315] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 42           |
|    time_elapsed             | 560          |
|    total_timesteps          | 1892352      |
| train/                      |              |
|    approx_kl                | 0.004683587  |
|    approx_ln(kl)            | -5.363691    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.24        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0393       |
|    n_updates                | 975          |
|    policy_gradient_loss     | -0.000398    |
|    std                      | 0.994        |
|    value_loss               | 0.0982       |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28164312] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 43            |
|    time_elapsed             | 573           |
|    total_timesteps          | 1894400       |
| train/                      |               |
|    approx_kl                | 0.0063048783  |
|    approx_ln(kl)            | -5.0664315    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.88         |
|    ln(policy_gradient_loss) | -5.4          |
|    loss                     | 0.152         |
|    n_updates                | 976           |
|    policy_gradient_loss     | 0.00451       |
|    std                      | 0.996         |
|    value_loss               | 1.34          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.21057749] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 44            |
|    time_elapsed             | 587           |
|    total_timesteps          | 1896448       |
| train/                      |               |
|    approx_kl                | 0.0057309344  |
|    approx_ln(kl)            | -5.1618767    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.66         |
|    ln(policy_gradient_loss) | -5.21         |
|    loss                     | 0.0696        |
|    n_updates                | 977           |
|    policy_gradient_loss     | 0.00546       |
|    std                      | 0.997         |
|    value_loss               | 0.0999        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49653614] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 45            |
|    time_elapsed             | 600           |
|    total_timesteps          | 1898496       |
| train/                      |               |
|    approx_kl                | 0.0065382444  |
|    approx_ln(kl)            | -5.0300865    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.97          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.557         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.74          |
|    n_updates                | 978           |
|    policy_gradient_loss     | -0.00505      |
|    std                      | 0.997         |
|    value_loss               | 1.27          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3257905] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 46           |
|    time_elapsed             | 613          |
|    total_timesteps          | 1900544      |
| train/                      |              |
|    approx_kl                | 0.0055001806 |
|    approx_ln(kl)            | -5.2029743   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.83        |
|    explained_variance       | 0.972        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.793        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 2.21         |
|    n_updates                | 979          |
|    policy_gradient_loss     | -0.00185     |
|    std                      | 0.997        |
|    value_loss               | 3.94         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3450729] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 47           |
|    time_elapsed             | 627          |
|    total_timesteps          | 1902592      |
| train/                      |              |
|    approx_kl                | 0.0062065353 |
|    approx_ln(kl)            | -5.0821524   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.83        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.89        |
|    ln(policy_gradient_loss) | -4.86        |
|    loss                     | 0.0557       |
|    n_updates                | 980          |
|    policy_gradient_loss     | 0.00774      |
|    std                      | 0.996        |
|    value_loss               | 0.154        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.50750995] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 48            |
|    time_elapsed             | 640           |
|    total_timesteps          | 1904640       |
| train/                      |               |
|    approx_kl                | 0.0045479024  |
|    approx_ln(kl)            | -5.3930893    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.4          |
|    ln(policy_gradient_loss) | -6.41         |
|    loss                     | 0.0907        |
|    n_updates                | 981           |
|    policy_gradient_loss     | 0.00164       |
|    std                      | 0.994         |
|    value_loss               | 0.196         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.22317688] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 49            |
|    time_elapsed             | 653           |
|    total_timesteps          | 1906688       |
| train/                      |               |
|    approx_kl                | 0.0055053737  |
|    approx_ln(kl)            | -5.2020307    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | nan           |
|    ln(policy_gradient_loss) | -8.88         |
|    loss                     | -0.00477      |
|    n_updates                | 982           |
|    policy_gradient_loss     | 0.00014       |
|    std                      | 0.992         |
|    value_loss               | 0.0933        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
--------------------------------------
| reward             | [-0.39913785] |
| time/              |               |
|    fps             | 156           |
|    iterations      | 1             |
|    time_elapsed    | 13            |
|    total_timesteps | 1908736       |
--------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30623305] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 2             |
|    time_elapsed             | 26            |
|    total_timesteps          | 1910784       |
| train/                      |               |
|    approx_kl                | 0.008684648   |
|    approx_ln(kl)            | -4.746198     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.514        |
|    ln(policy_gradient_loss) | -5.33         |
|    loss                     | 0.598         |
|    n_updates                | 984           |
|    policy_gradient_loss     | 0.00484       |
|    std                      | 0.986         |
|    value_loss               | 1.14          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2666019] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 3            |
|    time_elapsed             | 40           |
|    total_timesteps          | 1912832      |
| train/                      |              |
|    approx_kl                | 0.007908415  |
|    approx_ln(kl)            | -4.839828    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.83        |
|    ln(policy_gradient_loss) | -4.29        |
|    loss                     | 0.0592       |
|    n_updates                | 985          |
|    policy_gradient_loss     | 0.0138       |
|    std                      | 0.981        |
|    value_loss               | 0.115        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27711394] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 4             |
|    time_elapsed             | 53            |
|    total_timesteps          | 1914880       |
| train/                      |               |
|    approx_kl                | 0.0071280613  |
|    approx_ln(kl)            | -4.943716     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.552        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.576         |
|    n_updates                | 986           |
|    policy_gradient_loss     | -0.0121       |
|    std                      | 0.98          |
|    value_loss               | 1.02          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33461717] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 5             |
|    time_elapsed             | 67            |
|    total_timesteps          | 1916928       |
| train/                      |               |
|    approx_kl                | 0.0068678996  |
|    approx_ln(kl)            | -4.980897     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.18         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.307         |
|    n_updates                | 987           |
|    policy_gradient_loss     | -0.0141       |
|    std                      | 0.979         |
|    value_loss               | 0.564         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26760805] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 6             |
|    time_elapsed             | 80            |
|    total_timesteps          | 1918976       |
| train/                      |               |
|    approx_kl                | 0.0037182828  |
|    approx_ln(kl)            | -5.5944934    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.53         |
|    ln(policy_gradient_loss) | -5.85         |
|    loss                     | 0.0293        |
|    n_updates                | 988           |
|    policy_gradient_loss     | 0.00289       |
|    std                      | 0.98          |
|    value_loss               | 0.0511        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.21541828] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 7             |
|    time_elapsed             | 93            |
|    total_timesteps          | 1921024       |
| train/                      |               |
|    approx_kl                | 0.006109428   |
|    approx_ln(kl)            | -5.0979223    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.964         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.0838        |
|    ln(policy_gradient_loss) | -5.92         |
|    loss                     | 1.09          |
|    n_updates                | 989           |
|    policy_gradient_loss     | 0.00268       |
|    std                      | 0.98          |
|    value_loss               | 1.75          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3381723] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 8            |
|    time_elapsed             | 107          |
|    total_timesteps          | 1923072      |
| train/                      |              |
|    approx_kl                | 0.0062112235 |
|    approx_ln(kl)            | -5.0813975   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.957        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.09        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.337        |
|    n_updates                | 990          |
|    policy_gradient_loss     | -0.00126     |
|    std                      | 0.98         |
|    value_loss               | 2.9          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.400519]  |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 9            |
|    time_elapsed             | 120          |
|    total_timesteps          | 1925120      |
| train/                      |              |
|    approx_kl                | 0.0046525174 |
|    approx_ln(kl)            | -5.370347    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.96        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.383        |
|    n_updates                | 991          |
|    policy_gradient_loss     | -0.00482     |
|    std                      | 0.981        |
|    value_loss               | 0.907        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2160858] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 10           |
|    time_elapsed             | 133          |
|    total_timesteps          | 1927168      |
| train/                      |              |
|    approx_kl                | 0.0036887948 |
|    approx_ln(kl)            | -5.6024556   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.976        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.63        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0723       |
|    n_updates                | 992          |
|    policy_gradient_loss     | -0.00507     |
|    std                      | 0.981        |
|    value_loss               | 0.939        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28848752] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 11            |
|    time_elapsed             | 147           |
|    total_timesteps          | 1929216       |
| train/                      |               |
|    approx_kl                | 0.005817132   |
|    approx_ln(kl)            | -5.146948     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.45         |
|    ln(policy_gradient_loss) | -5.13         |
|    loss                     | 0.0859        |
|    n_updates                | 993           |
|    policy_gradient_loss     | 0.00594       |
|    std                      | 0.981         |
|    value_loss               | 0.134         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38824368] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 12            |
|    time_elapsed             | 159           |
|    total_timesteps          | 1931264       |
| train/                      |               |
|    approx_kl                | 0.0073008514  |
|    approx_ln(kl)            | -4.9197645    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.41         |
|    ln(policy_gradient_loss) | -5.39         |
|    loss                     | 0.09          |
|    n_updates                | 994           |
|    policy_gradient_loss     | 0.00457       |
|    std                      | 0.982         |
|    value_loss               | 0.936         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26570153] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 13            |
|    time_elapsed             | 173           |
|    total_timesteps          | 1933312       |
| train/                      |               |
|    approx_kl                | 0.005561235   |
|    approx_ln(kl)            | -5.191935     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.9          |
|    ln(policy_gradient_loss) | -5.6          |
|    loss                     | 0.15          |
|    n_updates                | 995           |
|    policy_gradient_loss     | 0.00369       |
|    std                      | 0.982         |
|    value_loss               | 0.916         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.55151445] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 14            |
|    time_elapsed             | 187           |
|    total_timesteps          | 1935360       |
| train/                      |               |
|    approx_kl                | 0.0035839295  |
|    approx_ln(kl)            | -5.6312957    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.87         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.155         |
|    n_updates                | 996           |
|    policy_gradient_loss     | -0.00802      |
|    std                      | 0.981         |
|    value_loss               | 0.436         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3880193] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 15           |
|    time_elapsed             | 200          |
|    total_timesteps          | 1937408      |
| train/                      |              |
|    approx_kl                | 0.0045755156 |
|    approx_ln(kl)            | -5.387036    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -5.15        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.00579      |
|    n_updates                | 997          |
|    policy_gradient_loss     | -0.00521     |
|    std                      | 0.981        |
|    value_loss               | 0.0872       |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3200348] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 16           |
|    time_elapsed             | 213          |
|    total_timesteps          | 1939456      |
| train/                      |              |
|    approx_kl                | 0.0069855405 |
|    approx_ln(kl)            | -4.963913    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.995        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.69        |
|    ln(policy_gradient_loss) | -4.83        |
|    loss                     | 0.184        |
|    n_updates                | 998          |
|    policy_gradient_loss     | 0.00797      |
|    std                      | 0.981        |
|    value_loss               | 0.465        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49644157] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 17            |
|    time_elapsed             | 227           |
|    total_timesteps          | 1941504       |
| train/                      |               |
|    approx_kl                | 0.008523199   |
|    approx_ln(kl)            | -4.7649636    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | nan           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | -0.0161       |
|    n_updates                | 999           |
|    policy_gradient_loss     | -0.00354      |
|    std                      | 0.983         |
|    value_loss               | 0.121         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26798892] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 18            |
|    time_elapsed             | 240           |
|    total_timesteps          | 1943552       |
| train/                      |               |
|    approx_kl                | 0.00681609    |
|    approx_ln(kl)            | -4.988469     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.07         |
|    ln(policy_gradient_loss) | -5.25         |
|    loss                     | 0.0466        |
|    n_updates                | 1000          |
|    policy_gradient_loss     | 0.00526       |
|    std                      | 0.983         |
|    value_loss               | 0.0944        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41891587] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 19            |
|    time_elapsed             | 253           |
|    total_timesteps          | 1945600       |
| train/                      |               |
|    approx_kl                | 0.0055233743  |
|    approx_ln(kl)            | -5.198766     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -5.61         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.00368       |
|    n_updates                | 1001          |
|    policy_gradient_loss     | -3.86e-05     |
|    std                      | 0.984         |
|    value_loss               | 0.065         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.54275435] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 20            |
|    time_elapsed             | 267           |
|    total_timesteps          | 1947648       |
| train/                      |               |
|    approx_kl                | 0.005384139   |
|    approx_ln(kl)            | -5.224298     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.8          |
|    ln(policy_gradient_loss) | -7.37         |
|    loss                     | 0.0224        |
|    n_updates                | 1002          |
|    policy_gradient_loss     | 0.000628      |
|    std                      | 0.985         |
|    value_loss               | 0.382         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.3278994] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 21           |
|    time_elapsed             | 280          |
|    total_timesteps          | 1949696      |
| train/                      |              |
|    approx_kl                | 0.01015436   |
|    approx_ln(kl)            | -4.5898523   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.01        |
|    ln(policy_gradient_loss) | -4.58        |
|    loss                     | 0.134        |
|    n_updates                | 1003         |
|    policy_gradient_loss     | 0.0103       |
|    std                      | 0.984        |
|    value_loss               | 0.115        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26914355] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 22            |
|    time_elapsed             | 294           |
|    total_timesteps          | 1951744       |
| train/                      |               |
|    approx_kl                | 0.0074887993  |
|    approx_ln(kl)            | -4.8943467    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.54         |
|    ln(policy_gradient_loss) | -6.32         |
|    loss                     | 0.0787        |
|    n_updates                | 1004          |
|    policy_gradient_loss     | 0.00181       |
|    std                      | 0.983         |
|    value_loss               | 0.549         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29758596] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 23            |
|    time_elapsed             | 311           |
|    total_timesteps          | 1953792       |
| train/                      |               |
|    approx_kl                | 0.006554415   |
|    approx_ln(kl)            | -5.0276165    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.14         |
|    ln(policy_gradient_loss) | -7.7          |
|    loss                     | 0.118         |
|    n_updates                | 1005          |
|    policy_gradient_loss     | 0.000453      |
|    std                      | 0.982         |
|    value_loss               | 1             |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3542682] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 24           |
|    time_elapsed             | 328          |
|    total_timesteps          | 1955840      |
| train/                      |              |
|    approx_kl                | 0.005417449  |
|    approx_ln(kl)            | -5.21813     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.995        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.5         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0822       |
|    n_updates                | 1006         |
|    policy_gradient_loss     | -0.00257     |
|    std                      | 0.982        |
|    value_loss               | 0.199        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3775355] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 25           |
|    time_elapsed             | 345          |
|    total_timesteps          | 1957888      |
| train/                      |              |
|    approx_kl                | 0.0059386413 |
|    approx_ln(kl)            | -5.126275    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.02        |
|    ln(policy_gradient_loss) | -6.32        |
|    loss                     | 0.0489       |
|    n_updates                | 1007         |
|    policy_gradient_loss     | 0.00181      |
|    std                      | 0.984        |
|    value_loss               | 0.573        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47393367] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 26            |
|    time_elapsed             | 361           |
|    total_timesteps          | 1959936       |
| train/                      |               |
|    approx_kl                | 0.005817525   |
|    approx_ln(kl)            | -5.14688      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.1          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.123         |
|    n_updates                | 1008          |
|    policy_gradient_loss     | -0.00164      |
|    std                      | 0.987         |
|    value_loss               | 0.514         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29880527] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 27            |
|    time_elapsed             | 377           |
|    total_timesteps          | 1961984       |
| train/                      |               |
|    approx_kl                | 0.005528153   |
|    approx_ln(kl)            | -5.1979017    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.62         |
|    ln(policy_gradient_loss) | -4.26         |
|    loss                     | 0.197         |
|    n_updates                | 1009          |
|    policy_gradient_loss     | 0.0142        |
|    std                      | 0.987         |
|    value_loss               | 0.144         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29225546] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 28            |
|    time_elapsed             | 393           |
|    total_timesteps          | 1964032       |
| train/                      |               |
|    approx_kl                | 0.0071264817  |
|    approx_ln(kl)            | -4.943938     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.24         |
|    ln(policy_gradient_loss) | -6.05         |
|    loss                     | 0.291         |
|    n_updates                | 1010          |
|    policy_gradient_loss     | 0.00236       |
|    std                      | 0.987         |
|    value_loss               | 1.11          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4447188] |
| time/                       |              |
|    fps                      | 144          |
|    iterations               | 29           |
|    time_elapsed             | 410          |
|    total_timesteps          | 1966080      |
| train/                      |              |
|    approx_kl                | 0.007363388  |
|    approx_ln(kl)            | -4.9112353   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.81        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.96        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0192       |
|    n_updates                | 1011         |
|    policy_gradient_loss     | -0.0124      |
|    std                      | 0.987        |
|    value_loss               | 0.177        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3372339] |
| time/                       |              |
|    fps                      | 144          |
|    iterations               | 30           |
|    time_elapsed             | 426          |
|    total_timesteps          | 1968128      |
| train/                      |              |
|    approx_kl                | 0.007861256  |
|    approx_ln(kl)            | -4.845809    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.81        |
|    explained_variance       | 0.988        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.13        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.324        |
|    n_updates                | 1012         |
|    policy_gradient_loss     | -0.0102      |
|    std                      | 0.988        |
|    value_loss               | 0.623        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24231273] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 31            |
|    time_elapsed             | 441           |
|    total_timesteps          | 1970176       |
| train/                      |               |
|    approx_kl                | 0.006101354   |
|    approx_ln(kl)            | -5.0992446    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.966         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.566         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.76          |
|    n_updates                | 1013          |
|    policy_gradient_loss     | -0.000767     |
|    std                      | 0.988         |
|    value_loss               | 2.32          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3180322] |
| time/                       |              |
|    fps                      | 143          |
|    iterations               | 32           |
|    time_elapsed             | 457          |
|    total_timesteps          | 1972224      |
| train/                      |              |
|    approx_kl                | 0.00540386   |
|    approx_ln(kl)            | -5.2206416   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.81        |
|    explained_variance       | 0.993        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.14        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.118        |
|    n_updates                | 1014         |
|    policy_gradient_loss     | -0.000433    |
|    std                      | 0.989        |
|    value_loss               | 1.13         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.259089]  |
| time/                       |              |
|    fps                      | 142          |
|    iterations               | 33           |
|    time_elapsed             | 475          |
|    total_timesteps          | 1974272      |
| train/                      |              |
|    approx_kl                | 0.0049272375 |
|    approx_ln(kl)            | -5.312977    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.81        |
|    explained_variance       | 0.981        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.82        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0596       |
|    n_updates                | 1015         |
|    policy_gradient_loss     | -0.0121      |
|    std                      | 0.99         |
|    value_loss               | 0.887        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22112684] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 34            |
|    time_elapsed             | 492           |
|    total_timesteps          | 1976320       |
| train/                      |               |
|    approx_kl                | 0.0053974288  |
|    approx_ln(kl)            | -5.2218328    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.1          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0449        |
|    n_updates                | 1016          |
|    policy_gradient_loss     | -0.00459      |
|    std                      | 0.991         |
|    value_loss               | 0.141         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.24502485] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 35            |
|    time_elapsed             | 507           |
|    total_timesteps          | 1978368       |
| train/                      |               |
|    approx_kl                | 0.009318077   |
|    approx_ln(kl)            | -4.675799     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.974         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.14         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.32          |
|    n_updates                | 1017          |
|    policy_gradient_loss     | -0.0084       |
|    std                      | 0.992         |
|    value_loss               | 1.04          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.20731255] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 36            |
|    time_elapsed             | 522           |
|    total_timesteps          | 1980416       |
| train/                      |               |
|    approx_kl                | 0.0050911503  |
|    approx_ln(kl)            | -5.2802515    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.54         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0788        |
|    n_updates                | 1018          |
|    policy_gradient_loss     | -0.00911      |
|    std                      | 0.994         |
|    value_loss               | 0.148         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31851307] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 37            |
|    time_elapsed             | 535           |
|    total_timesteps          | 1982464       |
| train/                      |               |
|    approx_kl                | 0.0058105825  |
|    approx_ln(kl)            | -5.1480746    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.943        |
|    ln(policy_gradient_loss) | -5.71         |
|    loss                     | 0.389         |
|    n_updates                | 1019          |
|    policy_gradient_loss     | 0.00332       |
|    std                      | 0.995         |
|    value_loss               | 1.27          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5082956] |
| time/                       |              |
|    fps                      | 141          |
|    iterations               | 38           |
|    time_elapsed             | 548          |
|    total_timesteps          | 1984512      |
| train/                      |              |
|    approx_kl                | 0.0052094823 |
|    approx_ln(kl)            | -5.2572746   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.82        |
|    explained_variance       | 0.968        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.245        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.28         |
|    n_updates                | 1020         |
|    policy_gradient_loss     | -0.00245     |
|    std                      | 0.993        |
|    value_loss               | 1.57         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.32674572] |
| time/                       |               |
|    fps                      | 142           |
|    iterations               | 39            |
|    time_elapsed             | 561           |
|    total_timesteps          | 1986560       |
| train/                      |               |
|    approx_kl                | 0.007172323   |
|    approx_ln(kl)            | -4.9375257    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.82         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.615        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.541         |
|    n_updates                | 1021          |
|    policy_gradient_loss     | -0.00569      |
|    std                      | 0.991         |
|    value_loss               | 1.09          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5594212] |
| time/                       |              |
|    fps                      | 142          |
|    iterations               | 40           |
|    time_elapsed             | 575          |
|    total_timesteps          | 1988608      |
| train/                      |              |
|    approx_kl                | 0.0079010725 |
|    approx_ln(kl)            | -4.840757    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.81        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.92        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0541       |
|    n_updates                | 1022         |
|    policy_gradient_loss     | -0.00673     |
|    std                      | 0.99         |
|    value_loss               | 0.147        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.27897134] |
| time/                       |               |
|    fps                      | 142           |
|    iterations               | 41            |
|    time_elapsed             | 589           |
|    total_timesteps          | 1990656       |
| train/                      |               |
|    approx_kl                | 0.007584474   |
|    approx_ln(kl)            | -4.881652     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.879        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.415         |
|    n_updates                | 1023          |
|    policy_gradient_loss     | -0.00303      |
|    std                      | 0.989         |
|    value_loss               | 0.287         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38155603] |
| time/                       |               |
|    fps                      | 142           |
|    iterations               | 42            |
|    time_elapsed             | 602           |
|    total_timesteps          | 1992704       |
| train/                      |               |
|    approx_kl                | 0.006885519   |
|    approx_ln(kl)            | -4.978335     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.58         |
|    ln(policy_gradient_loss) | -5.66         |
|    loss                     | 0.0756        |
|    n_updates                | 1024          |
|    policy_gradient_loss     | 0.00347       |
|    std                      | 0.989         |
|    value_loss               | 0.108         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39149332] |
| time/                       |               |
|    fps                      | 142           |
|    iterations               | 43            |
|    time_elapsed             | 616           |
|    total_timesteps          | 1994752       |
| train/                      |               |
|    approx_kl                | 0.0066674994  |
|    approx_ln(kl)            | -5.0105104    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 1             |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -5            |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.00672       |
|    n_updates                | 1025          |
|    policy_gradient_loss     | -0.00281      |
|    std                      | 0.988         |
|    value_loss               | 0.0727        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41084146] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 44            |
|    time_elapsed             | 629           |
|    total_timesteps          | 1996800       |
| train/                      |               |
|    approx_kl                | 0.0071024066  |
|    approx_ln(kl)            | -4.9473214    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.68         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0686        |
|    n_updates                | 1026          |
|    policy_gradient_loss     | -0.00501      |
|    std                      | 0.986         |
|    value_loss               | 0.472         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29248074] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 45            |
|    time_elapsed             | 643           |
|    total_timesteps          | 1998848       |
| train/                      |               |
|    approx_kl                | 0.0068659224  |
|    approx_ln(kl)            | -4.981185     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.57         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.207         |
|    n_updates                | 1027          |
|    policy_gradient_loss     | -0.0105       |
|    std                      | 0.985         |
|    value_loss               | 1.53          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39651433] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 46            |
|    time_elapsed             | 656           |
|    total_timesteps          | 2000896       |
| train/                      |               |
|    approx_kl                | 0.005966508   |
|    approx_ln(kl)            | -5.1215935    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | nan           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | -0.00586      |
|    n_updates                | 1028          |
|    policy_gradient_loss     | -0.0156       |
|    std                      | 0.984         |
|    value_loss               | 0.118         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23724043] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 47            |
|    time_elapsed             | 669           |
|    total_timesteps          | 2002944       |
| train/                      |               |
|    approx_kl                | 0.008280288   |
|    approx_ln(kl)            | -4.7938776    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.82         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0596        |
|    n_updates                | 1029          |
|    policy_gradient_loss     | -0.00643      |
|    std                      | 0.983         |
|    value_loss               | 0.184         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.30589285] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 48            |
|    time_elapsed             | 682           |
|    total_timesteps          | 2004992       |
| train/                      |               |
|    approx_kl                | 0.011911656   |
|    approx_ln(kl)            | -4.430238     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.65         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0708        |
|    n_updates                | 1030          |
|    policy_gradient_loss     | -0.00963      |
|    std                      | 0.981         |
|    value_loss               | 0.12          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5478266] |
| time/                       |              |
|    fps                      | 144          |
|    iterations               | 49           |
|    time_elapsed             | 695          |
|    total_timesteps          | 2007040      |
| train/                      |              |
|    approx_kl                | 0.00708572   |
|    approx_ln(kl)            | -4.9496737   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.49        |
|    ln(policy_gradient_loss) | -8.01        |
|    loss                     | 0.0304       |
|    n_updates                | 1031         |
|    policy_gradient_loss     | 0.000332     |
|    std                      | 0.98         |
|    value_loss               | 0.0698       |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
--------------------------------------
| reward             | [-0.45443377] |
| time/              |               |
|    fps             | 159           |
|    iterations      | 1             |
|    time_elapsed    | 12            |
|    total_timesteps | 2009088       |
--------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4226741] |
| time/                       |              |
|    fps                      | 157          |
|    iterations               | 2            |
|    time_elapsed             | 26           |
|    total_timesteps          | 2011136      |
| train/                      |              |
|    approx_kl                | 0.0054558236 |
|    approx_ln(kl)            | -5.2110715   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.23        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.291        |
|    n_updates                | 1033         |
|    policy_gradient_loss     | -0.0029      |
|    std                      | 0.978        |
|    value_loss               | 0.686        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36462966] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 3             |
|    time_elapsed             | 39            |
|    total_timesteps          | 2013184       |
| train/                      |               |
|    approx_kl                | 0.0046962937  |
|    approx_ln(kl)            | -5.3609815    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.86         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.156         |
|    n_updates                | 1034          |
|    policy_gradient_loss     | -0.00204      |
|    std                      | 0.978         |
|    value_loss               | 0.59          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36582687] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 4             |
|    time_elapsed             | 53            |
|    total_timesteps          | 2015232       |
| train/                      |               |
|    approx_kl                | 0.0056406036  |
|    approx_ln(kl)            | -5.1777644    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.3          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.273         |
|    n_updates                | 1035          |
|    policy_gradient_loss     | -0.00297      |
|    std                      | 0.978         |
|    value_loss               | 0.734         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2520158] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 5            |
|    time_elapsed             | 67           |
|    total_timesteps          | 2017280      |
| train/                      |              |
|    approx_kl                | 0.0043645203 |
|    approx_ln(kl)            | -5.434247    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.984        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.43        |
|    ln(policy_gradient_loss) | -6.28        |
|    loss                     | 0.24         |
|    n_updates                | 1036         |
|    policy_gradient_loss     | 0.00188      |
|    std                      | 0.978        |
|    value_loss               | 0.49         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4354534] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 6            |
|    time_elapsed             | 80           |
|    total_timesteps          | 2019328      |
| train/                      |              |
|    approx_kl                | 0.007837941  |
|    approx_ln(kl)            | -4.848779    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.979        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.836        |
|    ln(policy_gradient_loss) | -5.01        |
|    loss                     | 2.31         |
|    n_updates                | 1037         |
|    policy_gradient_loss     | 0.0067       |
|    std                      | 0.978        |
|    value_loss               | 4.37         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24119745] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 7             |
|    time_elapsed             | 93            |
|    total_timesteps          | 2021376       |
| train/                      |               |
|    approx_kl                | 0.004617684   |
|    approx_ln(kl)            | -5.377862     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.12         |
|    ln(policy_gradient_loss) | -5.02         |
|    loss                     | 0.12          |
|    n_updates                | 1038          |
|    policy_gradient_loss     | 0.00659       |
|    std                      | 0.979         |
|    value_loss               | 0.524         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.41110802] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 8             |
|    time_elapsed             | 106           |
|    total_timesteps          | 2023424       |
| train/                      |               |
|    approx_kl                | 0.007082138   |
|    approx_ln(kl)            | -4.9501796    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.374         |
|    ln(policy_gradient_loss) | -6.68         |
|    loss                     | 1.45          |
|    n_updates                | 1039          |
|    policy_gradient_loss     | 0.00126       |
|    std                      | 0.979         |
|    value_loss               | 1.47          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25749436] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 9             |
|    time_elapsed             | 119           |
|    total_timesteps          | 2025472       |
| train/                      |               |
|    approx_kl                | 0.004259324   |
|    approx_ln(kl)            | -5.458645     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.28         |
|    ln(policy_gradient_loss) | -5.04         |
|    loss                     | 0.756         |
|    n_updates                | 1040          |
|    policy_gradient_loss     | 0.00645       |
|    std                      | 0.979         |
|    value_loss               | 0.719         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.51374054] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 10            |
|    time_elapsed             | 132           |
|    total_timesteps          | 2027520       |
| train/                      |               |
|    approx_kl                | 0.008730264   |
|    approx_ln(kl)            | -4.7409596    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.55         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0782        |
|    n_updates                | 1041          |
|    policy_gradient_loss     | -0.0121       |
|    std                      | 0.979         |
|    value_loss               | 0.382         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48105025] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 11            |
|    time_elapsed             | 145           |
|    total_timesteps          | 2029568       |
| train/                      |               |
|    approx_kl                | 0.0057767555  |
|    approx_ln(kl)            | -5.153913     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.434        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.648         |
|    n_updates                | 1042          |
|    policy_gradient_loss     | -0.00269      |
|    std                      | 0.979         |
|    value_loss               | 1.22          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.30447719] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 12            |
|    time_elapsed             | 159           |
|    total_timesteps          | 2031616       |
| train/                      |               |
|    approx_kl                | 0.008732952   |
|    approx_ln(kl)            | -4.7406516    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.36         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0349        |
|    n_updates                | 1043          |
|    policy_gradient_loss     | -0.00699      |
|    std                      | 0.979         |
|    value_loss               | 0.103         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4217599] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 13           |
|    time_elapsed             | 172          |
|    total_timesteps          | 2033664      |
| train/                      |              |
|    approx_kl                | 0.005749728  |
|    approx_ln(kl)            | -5.1586027   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.9         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.149        |
|    n_updates                | 1044         |
|    policy_gradient_loss     | -0.000126    |
|    std                      | 0.978        |
|    value_loss               | 0.339        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40592286] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 14            |
|    time_elapsed             | 185           |
|    total_timesteps          | 2035712       |
| train/                      |               |
|    approx_kl                | 0.0071692797  |
|    approx_ln(kl)            | -4.93795      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.81         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.163         |
|    n_updates                | 1045          |
|    policy_gradient_loss     | -0.00203      |
|    std                      | 0.979         |
|    value_loss               | 0.504         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40504727] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 15            |
|    time_elapsed             | 198           |
|    total_timesteps          | 2037760       |
| train/                      |               |
|    approx_kl                | 0.0064578503  |
|    approx_ln(kl)            | -5.042459     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.25         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0387        |
|    n_updates                | 1046          |
|    policy_gradient_loss     | -0.00188      |
|    std                      | 0.979         |
|    value_loss               | 0.157         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.31960905] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 16            |
|    time_elapsed             | 212           |
|    total_timesteps          | 2039808       |
| train/                      |               |
|    approx_kl                | 0.009382854   |
|    approx_ln(kl)            | -4.6688714    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.82         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0595        |
|    n_updates                | 1047          |
|    policy_gradient_loss     | -0.00601      |
|    std                      | 0.979         |
|    value_loss               | 0.0884        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39300916] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 17            |
|    time_elapsed             | 225           |
|    total_timesteps          | 2041856       |
| train/                      |               |
|    approx_kl                | 0.008658808   |
|    approx_ln(kl)            | -4.7491784    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.776        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.46          |
|    n_updates                | 1048          |
|    policy_gradient_loss     | -0.0108       |
|    std                      | 0.98          |
|    value_loss               | 2.16          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2143445] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 18           |
|    time_elapsed             | 239          |
|    total_timesteps          | 2043904      |
| train/                      |              |
|    approx_kl                | 0.0059190127 |
|    approx_ln(kl)            | -5.1295857   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.9         |
|    ln(policy_gradient_loss) | -4.9         |
|    loss                     | 0.055        |
|    n_updates                | 1049         |
|    policy_gradient_loss     | 0.00748      |
|    std                      | 0.98         |
|    value_loss               | 0.0752       |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3142336] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 19           |
|    time_elapsed             | 253          |
|    total_timesteps          | 2045952      |
| train/                      |              |
|    approx_kl                | 0.005836592  |
|    approx_ln(kl)            | -5.143608    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.69        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.184        |
|    n_updates                | 1050         |
|    policy_gradient_loss     | -0.0101      |
|    std                      | 0.98         |
|    value_loss               | 0.248        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4993057] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 20           |
|    time_elapsed             | 267          |
|    total_timesteps          | 2048000      |
| train/                      |              |
|    approx_kl                | 0.0073471777 |
|    approx_ln(kl)            | -4.913439    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.995        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.28        |
|    ln(policy_gradient_loss) | -5.52        |
|    loss                     | 0.102        |
|    n_updates                | 1051         |
|    policy_gradient_loss     | 0.004        |
|    std                      | 0.98         |
|    value_loss               | 0.359        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37584263] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 21            |
|    time_elapsed             | 280           |
|    total_timesteps          | 2050048       |
| train/                      |               |
|    approx_kl                | 0.0053411433  |
|    approx_ln(kl)            | -5.2323155    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.18         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.835         |
|    n_updates                | 1052          |
|    policy_gradient_loss     | -0.0185       |
|    std                      | 0.981         |
|    value_loss               | 0.952         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45891872] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 22            |
|    time_elapsed             | 293           |
|    total_timesteps          | 2052096       |
| train/                      |               |
|    approx_kl                | 0.008742429   |
|    approx_ln(kl)            | -4.7395673    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.512        |
|    ln(policy_gradient_loss) | -5.17         |
|    loss                     | 0.599         |
|    n_updates                | 1053          |
|    policy_gradient_loss     | 0.00566       |
|    std                      | 0.98          |
|    value_loss               | 0.976         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28546706] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 23            |
|    time_elapsed             | 306           |
|    total_timesteps          | 2054144       |
| train/                      |               |
|    approx_kl                | 0.0065825772  |
|    approx_ln(kl)            | -5.023329     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.02         |
|    ln(policy_gradient_loss) | -8.99         |
|    loss                     | 0.132         |
|    n_updates                | 1054          |
|    policy_gradient_loss     | 0.000125      |
|    std                      | 0.979         |
|    value_loss               | 1.16          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4453418] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 24           |
|    time_elapsed             | 319          |
|    total_timesteps          | 2056192      |
| train/                      |              |
|    approx_kl                | 0.005117367  |
|    approx_ln(kl)            | -5.275115    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.82        |
|    ln(policy_gradient_loss) | -5.26        |
|    loss                     | 0.0219       |
|    n_updates                | 1055         |
|    policy_gradient_loss     | 0.00519      |
|    std                      | 0.979        |
|    value_loss               | 0.102        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4831916] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 25           |
|    time_elapsed             | 331          |
|    total_timesteps          | 2058240      |
| train/                      |              |
|    approx_kl                | 0.008257327  |
|    approx_ln(kl)            | -4.796654    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.985        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.978       |
|    ln(policy_gradient_loss) | -5.63        |
|    loss                     | 0.376        |
|    n_updates                | 1056         |
|    policy_gradient_loss     | 0.0036       |
|    std                      | 0.98         |
|    value_loss               | 1.49         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28816068] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 26            |
|    time_elapsed             | 344           |
|    total_timesteps          | 2060288       |
| train/                      |               |
|    approx_kl                | 0.008085601   |
|    approx_ln(kl)            | -4.8176703    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.774        |
|    ln(policy_gradient_loss) | -6.84         |
|    loss                     | 0.461         |
|    n_updates                | 1057          |
|    policy_gradient_loss     | 0.00107       |
|    std                      | 0.98          |
|    value_loss               | 0.677         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4097352] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 27           |
|    time_elapsed             | 358          |
|    total_timesteps          | 2062336      |
| train/                      |              |
|    approx_kl                | 0.0036375937 |
|    approx_ln(kl)            | -5.6164327   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.991        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.09        |
|    ln(policy_gradient_loss) | -5.4         |
|    loss                     | 0.337        |
|    n_updates                | 1058         |
|    policy_gradient_loss     | 0.0045       |
|    std                      | 0.98         |
|    value_loss               | 0.671        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25529793] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 28            |
|    time_elapsed             | 371           |
|    total_timesteps          | 2064384       |
| train/                      |               |
|    approx_kl                | 0.007857273   |
|    approx_ln(kl)            | -4.846316     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.37         |
|    ln(policy_gradient_loss) | -5.52         |
|    loss                     | 0.254         |
|    n_updates                | 1059          |
|    policy_gradient_loss     | 0.00401       |
|    std                      | 0.98          |
|    value_loss               | 0.815         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4408074] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 29           |
|    time_elapsed             | 385          |
|    total_timesteps          | 2066432      |
| train/                      |              |
|    approx_kl                | 0.007313864  |
|    approx_ln(kl)            | -4.9179835   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.97        |
|    ln(policy_gradient_loss) | -5.98        |
|    loss                     | 0.0512       |
|    n_updates                | 1060         |
|    policy_gradient_loss     | 0.00253      |
|    std                      | 0.981        |
|    value_loss               | 0.148        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43338916] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 30            |
|    time_elapsed             | 398           |
|    total_timesteps          | 2068480       |
| train/                      |               |
|    approx_kl                | 0.005390381   |
|    approx_ln(kl)            | -5.2231393    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.969         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.14         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.32          |
|    n_updates                | 1061          |
|    policy_gradient_loss     | -0.00114      |
|    std                      | 0.98          |
|    value_loss               | 0.672         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24998851] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 31            |
|    time_elapsed             | 411           |
|    total_timesteps          | 2070528       |
| train/                      |               |
|    approx_kl                | 0.0054356293  |
|    approx_ln(kl)            | -5.21478      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.03         |
|    ln(policy_gradient_loss) | -8.13         |
|    loss                     | 0.0485        |
|    n_updates                | 1062          |
|    policy_gradient_loss     | 0.000294      |
|    std                      | 0.977         |
|    value_loss               | 0.212         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30529782] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 32            |
|    time_elapsed             | 424           |
|    total_timesteps          | 2072576       |
| train/                      |               |
|    approx_kl                | 0.007134178   |
|    approx_ln(kl)            | -4.942858     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.33         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0131        |
|    n_updates                | 1063          |
|    policy_gradient_loss     | -0.0045       |
|    std                      | 0.978         |
|    value_loss               | 0.077         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26734486] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 33            |
|    time_elapsed             | 437           |
|    total_timesteps          | 2074624       |
| train/                      |               |
|    approx_kl                | 0.006088059   |
|    approx_ln(kl)            | -5.101426     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.41         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0895        |
|    n_updates                | 1064          |
|    policy_gradient_loss     | -0.00882      |
|    std                      | 0.977         |
|    value_loss               | 0.505         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29262763] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 34            |
|    time_elapsed             | 451           |
|    total_timesteps          | 2076672       |
| train/                      |               |
|    approx_kl                | 0.0059007616  |
|    approx_ln(kl)            | -5.1326737    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.753        |
|    ln(policy_gradient_loss) | -5.18         |
|    loss                     | 0.471         |
|    n_updates                | 1065          |
|    policy_gradient_loss     | 0.00563       |
|    std                      | 0.977         |
|    value_loss               | 1.31          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.17996268] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 35            |
|    time_elapsed             | 464           |
|    total_timesteps          | 2078720       |
| train/                      |               |
|    approx_kl                | 0.009173106   |
|    approx_ln(kl)            | -4.691479     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.965         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.9          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.149         |
|    n_updates                | 1066          |
|    policy_gradient_loss     | -0.0136       |
|    std                      | 0.977         |
|    value_loss               | 0.871         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28417772] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 36            |
|    time_elapsed             | 478           |
|    total_timesteps          | 2080768       |
| train/                      |               |
|    approx_kl                | 0.0058996193  |
|    approx_ln(kl)            | -5.1328673    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.971         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.00836       |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.01          |
|    n_updates                | 1067          |
|    policy_gradient_loss     | -0.00478      |
|    std                      | 0.977         |
|    value_loss               | 1.8           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38593325] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 37            |
|    time_elapsed             | 491           |
|    total_timesteps          | 2082816       |
| train/                      |               |
|    approx_kl                | 0.0070868493  |
|    approx_ln(kl)            | -4.9495144    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.957         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.565        |
|    ln(policy_gradient_loss) | -4.59         |
|    loss                     | 0.568         |
|    n_updates                | 1068          |
|    policy_gradient_loss     | 0.0102        |
|    std                      | 0.977         |
|    value_loss               | 1.57          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41661784] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 38            |
|    time_elapsed             | 504           |
|    total_timesteps          | 2084864       |
| train/                      |               |
|    approx_kl                | 0.0049007186  |
|    approx_ln(kl)            | -5.3183737    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.62         |
|    ln(policy_gradient_loss) | -4.54         |
|    loss                     | 0.0726        |
|    n_updates                | 1069          |
|    policy_gradient_loss     | 0.0107        |
|    std                      | 0.977         |
|    value_loss               | 0.0713        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41986012] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 39            |
|    time_elapsed             | 519           |
|    total_timesteps          | 2086912       |
| train/                      |               |
|    approx_kl                | 0.00826788    |
|    approx_ln(kl)            | -4.7953773    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.28         |
|    ln(policy_gradient_loss) | -4.65         |
|    loss                     | 0.103         |
|    n_updates                | 1070          |
|    policy_gradient_loss     | 0.00954       |
|    std                      | 0.977         |
|    value_loss               | 0.108         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5539783] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 40           |
|    time_elapsed             | 532          |
|    total_timesteps          | 2088960      |
| train/                      |              |
|    approx_kl                | 0.004819843  |
|    approx_ln(kl)            | -5.335014    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.999        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.95        |
|    ln(policy_gradient_loss) | -5.62        |
|    loss                     | 0.0521       |
|    n_updates                | 1071         |
|    policy_gradient_loss     | 0.00363      |
|    std                      | 0.978        |
|    value_loss               | 0.0859       |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.16590807] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 41            |
|    time_elapsed             | 546           |
|    total_timesteps          | 2091008       |
| train/                      |               |
|    approx_kl                | 0.0049132397  |
|    approx_ln(kl)            | -5.3158216    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.846        |
|    ln(policy_gradient_loss) | -5.23         |
|    loss                     | 0.429         |
|    n_updates                | 1072          |
|    policy_gradient_loss     | 0.00534       |
|    std                      | 0.979         |
|    value_loss               | 0.589         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.48224223] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 42            |
|    time_elapsed             | 560           |
|    total_timesteps          | 2093056       |
| train/                      |               |
|    approx_kl                | 0.009125148   |
|    approx_ln(kl)            | -4.696721     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | nan           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | -0.111        |
|    n_updates                | 1073          |
|    policy_gradient_loss     | -0.0214       |
|    std                      | 0.979         |
|    value_loss               | 0.0776        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34450293] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 43            |
|    time_elapsed             | 573           |
|    total_timesteps          | 2095104       |
| train/                      |               |
|    approx_kl                | 0.006604628   |
|    approx_ln(kl)            | -5.0199847    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.95         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.142         |
|    n_updates                | 1074          |
|    policy_gradient_loss     | -0.0177       |
|    std                      | 0.98          |
|    value_loss               | 0.806         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.12697046] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 44            |
|    time_elapsed             | 586           |
|    total_timesteps          | 2097152       |
| train/                      |               |
|    approx_kl                | 0.008242192   |
|    approx_ln(kl)            | -4.798489     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.434        |
|    ln(policy_gradient_loss) | -4.83         |
|    loss                     | 0.648         |
|    n_updates                | 1075          |
|    policy_gradient_loss     | 0.00796       |
|    std                      | 0.98          |
|    value_loss               | 0.861         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36864075] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 45            |
|    time_elapsed             | 600           |
|    total_timesteps          | 2099200       |
| train/                      |               |
|    approx_kl                | 0.007871996   |
|    approx_ln(kl)            | -4.844444     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.04         |
|    ln(policy_gradient_loss) | -5.45         |
|    loss                     | 0.353         |
|    n_updates                | 1076          |
|    policy_gradient_loss     | 0.00429       |
|    std                      | 0.979         |
|    value_loss               | 0.58          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4539554] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 46           |
|    time_elapsed             | 613          |
|    total_timesteps          | 2101248      |
| train/                      |              |
|    approx_kl                | 0.0060206624 |
|    approx_ln(kl)            | -5.112558    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.967        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.405        |
|    ln(policy_gradient_loss) | -6.95        |
|    loss                     | 1.5          |
|    n_updates                | 1077         |
|    policy_gradient_loss     | 0.000958     |
|    std                      | 0.979        |
|    value_loss               | 2.57         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.50941557] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 47            |
|    time_elapsed             | 626           |
|    total_timesteps          | 2103296       |
| train/                      |               |
|    approx_kl                | 0.00704535    |
|    approx_ln(kl)            | -4.9553876    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.858        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.424         |
|    n_updates                | 1078          |
|    policy_gradient_loss     | -0.000984     |
|    std                      | 0.979         |
|    value_loss               | 0.55          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.194228] |
| time/                       |             |
|    fps                      | 153         |
|    iterations               | 48          |
|    time_elapsed             | 639         |
|    total_timesteps          | 2105344     |
| train/                      |             |
|    approx_kl                | 0.005069453 |
|    approx_ln(kl)            | -5.2845225  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.79       |
|    explained_variance       | 0.978       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -0.858      |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.424       |
|    n_updates                | 1079        |
|    policy_gradient_loss     | -0.00339    |
|    std                      | 0.98        |
|    value_loss               | 0.943       |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.26975068] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 49            |
|    time_elapsed             | 653           |
|    total_timesteps          | 2107392       |
| train/                      |               |
|    approx_kl                | 0.009340634   |
|    approx_ln(kl)            | -4.6733813    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.09         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.338         |
|    n_updates                | 1080          |
|    policy_gradient_loss     | -0.0119       |
|    std                      | 0.98          |
|    value_loss               | 0.768         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
--------------------------------------
| reward             | [-0.21992369] |
| time/              |               |
|    fps             | 161           |
|    iterations      | 1             |
|    time_elapsed    | 12            |
|    total_timesteps | 2109440       |
--------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.49130705] |
| time/                       |               |
|    fps                      | 158           |
|    iterations               | 2             |
|    time_elapsed             | 25            |
|    total_timesteps          | 2111488       |
| train/                      |               |
|    approx_kl                | 0.008229625   |
|    approx_ln(kl)            | -4.800015     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.576        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.562         |
|    n_updates                | 1082          |
|    policy_gradient_loss     | -0.0017       |
|    std                      | 0.98          |
|    value_loss               | 1.1           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29553872] |
| time/                       |               |
|    fps                      | 158           |
|    iterations               | 3             |
|    time_elapsed             | 38            |
|    total_timesteps          | 2113536       |
| train/                      |               |
|    approx_kl                | 0.0053035575  |
|    approx_ln(kl)            | -5.2393775    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.64         |
|    ln(policy_gradient_loss) | -6.42         |
|    loss                     | 0.0712        |
|    n_updates                | 1083          |
|    policy_gradient_loss     | 0.00164       |
|    std                      | 0.98          |
|    value_loss               | 0.711         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.30279702] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 4             |
|    time_elapsed             | 52            |
|    total_timesteps          | 2115584       |
| train/                      |               |
|    approx_kl                | 0.008799573   |
|    approx_ln(kl)            | -4.7330523    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.81         |
|    ln(policy_gradient_loss) | -4.64         |
|    loss                     | 0.163         |
|    n_updates                | 1084          |
|    policy_gradient_loss     | 0.0097        |
|    std                      | 0.98          |
|    value_loss               | 0.361         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31437173] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 5             |
|    time_elapsed             | 65            |
|    total_timesteps          | 2117632       |
| train/                      |               |
|    approx_kl                | 0.005718913   |
|    approx_ln(kl)            | -5.1639767    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.74         |
|    ln(policy_gradient_loss) | -5.6          |
|    loss                     | 0.176         |
|    n_updates                | 1085          |
|    policy_gradient_loss     | 0.00369       |
|    std                      | 0.98          |
|    value_loss               | 0.273         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28566056] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 6             |
|    time_elapsed             | 80            |
|    total_timesteps          | 2119680       |
| train/                      |               |
|    approx_kl                | 0.009044613   |
|    approx_ln(kl)            | -4.705586     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.976         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.985        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.373         |
|    n_updates                | 1086          |
|    policy_gradient_loss     | -0.00391      |
|    std                      | 0.98          |
|    value_loss               | 0.432         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25307345] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 7             |
|    time_elapsed             | 97            |
|    total_timesteps          | 2121728       |
| train/                      |               |
|    approx_kl                | 0.009300148   |
|    approx_ln(kl)            | -4.677725     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.851         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 2.34          |
|    n_updates                | 1087          |
|    policy_gradient_loss     | -0.00185      |
|    std                      | 0.981         |
|    value_loss               | 1.95          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.37702256] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 8             |
|    time_elapsed             | 112           |
|    total_timesteps          | 2123776       |
| train/                      |               |
|    approx_kl                | 0.010941785   |
|    approx_ln(kl)            | -4.5151663    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.06         |
|    ln(policy_gradient_loss) | -6.67         |
|    loss                     | 0.127         |
|    n_updates                | 1088          |
|    policy_gradient_loss     | 0.00127       |
|    std                      | 0.981         |
|    value_loss               | 0.358         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.37191686] |
| time/                       |               |
|    fps                      | 142           |
|    iterations               | 9             |
|    time_elapsed             | 129           |
|    total_timesteps          | 2125824       |
| train/                      |               |
|    approx_kl                | 0.009180345   |
|    approx_ln(kl)            | -4.6906905    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.02         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.36          |
|    n_updates                | 1089          |
|    policy_gradient_loss     | -0.00331      |
|    std                      | 0.982         |
|    value_loss               | 1.09          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4128263] |
| time/                       |              |
|    fps                      | 144          |
|    iterations               | 10           |
|    time_elapsed             | 142          |
|    total_timesteps          | 2127872      |
| train/                      |              |
|    approx_kl                | 0.0065856283 |
|    approx_ln(kl)            | -5.022866    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.945        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.0459       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.05         |
|    n_updates                | 1090         |
|    policy_gradient_loss     | -0.00274     |
|    std                      | 0.982        |
|    value_loss               | 3.88         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3941503] |
| time/                       |              |
|    fps                      | 145          |
|    iterations               | 11           |
|    time_elapsed             | 155          |
|    total_timesteps          | 2129920      |
| train/                      |              |
|    approx_kl                | 0.006726958  |
|    approx_ln(kl)            | -5.001632    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.973        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.31        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.269        |
|    n_updates                | 1091         |
|    policy_gradient_loss     | -0.00183     |
|    std                      | 0.983        |
|    value_loss               | 2.18         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3629614] |
| time/                       |              |
|    fps                      | 146          |
|    iterations               | 12           |
|    time_elapsed             | 168          |
|    total_timesteps          | 2131968      |
| train/                      |              |
|    approx_kl                | 0.005298067  |
|    approx_ln(kl)            | -5.240413    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.982        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.254        |
|    ln(policy_gradient_loss) | -5.99        |
|    loss                     | 1.29         |
|    n_updates                | 1092         |
|    policy_gradient_loss     | 0.0025       |
|    std                      | 0.984        |
|    value_loss               | 1.99         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38490692] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 13            |
|    time_elapsed             | 181           |
|    total_timesteps          | 2134016       |
| train/                      |               |
|    approx_kl                | 0.004594895   |
|    approx_ln(kl)            | -5.382809     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.977         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.1          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.332         |
|    n_updates                | 1093          |
|    policy_gradient_loss     | -0.00306      |
|    std                      | 0.983         |
|    value_loss               | 1.17          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3656878] |
| time/                       |              |
|    fps                      | 147          |
|    iterations               | 14           |
|    time_elapsed             | 194          |
|    total_timesteps          | 2136064      |
| train/                      |              |
|    approx_kl                | 0.0053202217 |
|    approx_ln(kl)            | -5.2362404   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.995        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.34        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0961       |
|    n_updates                | 1094         |
|    policy_gradient_loss     | -0.00833     |
|    std                      | 0.982        |
|    value_loss               | 0.214        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4019773] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 15           |
|    time_elapsed             | 207          |
|    total_timesteps          | 2138112      |
| train/                      |              |
|    approx_kl                | 0.009277334  |
|    approx_ln(kl)            | -4.680181    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.964        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.541       |
|    ln(policy_gradient_loss) | -5.58        |
|    loss                     | 0.582        |
|    n_updates                | 1095         |
|    policy_gradient_loss     | 0.00376      |
|    std                      | 0.981        |
|    value_loss               | 2.5          |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37824538] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 16            |
|    time_elapsed             | 220           |
|    total_timesteps          | 2140160       |
| train/                      |               |
|    approx_kl                | 0.008742741   |
|    approx_ln(kl)            | -4.7395315    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.969        |
|    ln(policy_gradient_loss) | -4.46         |
|    loss                     | 0.379         |
|    n_updates                | 1096          |
|    policy_gradient_loss     | 0.0116        |
|    std                      | 0.981         |
|    value_loss               | 1.08          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.46396756] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 17            |
|    time_elapsed             | 233           |
|    total_timesteps          | 2142208       |
| train/                      |               |
|    approx_kl                | 0.0054228944  |
|    approx_ln(kl)            | -5.2171254    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.37         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.253         |
|    n_updates                | 1097          |
|    policy_gradient_loss     | -4.75e-05     |
|    std                      | 0.98          |
|    value_loss               | 0.53          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.20518646] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 18            |
|    time_elapsed             | 247           |
|    total_timesteps          | 2144256       |
| train/                      |               |
|    approx_kl                | 0.0046133595  |
|    approx_ln(kl)            | -5.378799     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.966         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.242        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.785         |
|    n_updates                | 1098          |
|    policy_gradient_loss     | -0.00305      |
|    std                      | 0.981         |
|    value_loss               | 2.1           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36051014] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 19            |
|    time_elapsed             | 260           |
|    total_timesteps          | 2146304       |
| train/                      |               |
|    approx_kl                | 0.005695031   |
|    approx_ln(kl)            | -5.1681614    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.253        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.776         |
|    n_updates                | 1099          |
|    policy_gradient_loss     | -0.0052       |
|    std                      | 0.982         |
|    value_loss               | 2.31          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33452317] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 20            |
|    time_elapsed             | 272           |
|    total_timesteps          | 2148352       |
| train/                      |               |
|    approx_kl                | 0.0073699984  |
|    approx_ln(kl)            | -4.910338     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.94         |
|    ln(policy_gradient_loss) | -5.66         |
|    loss                     | 0.143         |
|    n_updates                | 1100          |
|    policy_gradient_loss     | 0.00349       |
|    std                      | 0.983         |
|    value_loss               | 0.264         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36307684] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 21            |
|    time_elapsed             | 285           |
|    total_timesteps          | 2150400       |
| train/                      |               |
|    approx_kl                | 0.008328822   |
|    approx_ln(kl)            | -4.788033     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | nan           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | -0.00753      |
|    n_updates                | 1101          |
|    policy_gradient_loss     | -0.0168       |
|    std                      | 0.982         |
|    value_loss               | 0.225         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35756165] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 22            |
|    time_elapsed             | 300           |
|    total_timesteps          | 2152448       |
| train/                      |               |
|    approx_kl                | 0.006941008   |
|    approx_ln(kl)            | -4.9703083    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.27         |
|    ln(policy_gradient_loss) | -6.01         |
|    loss                     | 0.104         |
|    n_updates                | 1102          |
|    policy_gradient_loss     | 0.00246       |
|    std                      | 0.982         |
|    value_loss               | 0.152         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35293698] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 23            |
|    time_elapsed             | 315           |
|    total_timesteps          | 2154496       |
| train/                      |               |
|    approx_kl                | 0.006213109   |
|    approx_ln(kl)            | -5.081094     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.41         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0329        |
|    n_updates                | 1103          |
|    policy_gradient_loss     | -0.00187      |
|    std                      | 0.982         |
|    value_loss               | 0.215         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33237475] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 24            |
|    time_elapsed             | 328           |
|    total_timesteps          | 2156544       |
| train/                      |               |
|    approx_kl                | 0.0045296657  |
|    approx_ln(kl)            | -5.397107     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.91         |
|    ln(policy_gradient_loss) | -9.91         |
|    loss                     | 0.02          |
|    n_updates                | 1104          |
|    policy_gradient_loss     | 4.97e-05      |
|    std                      | 0.983         |
|    value_loss               | 0.108         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5499947] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 25           |
|    time_elapsed             | 342          |
|    total_timesteps          | 2158592      |
| train/                      |              |
|    approx_kl                | 0.0043196473 |
|    approx_ln(kl)            | -5.4445815   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.983        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.75        |
|    ln(policy_gradient_loss) | -6.86        |
|    loss                     | 0.472        |
|    n_updates                | 1105         |
|    policy_gradient_loss     | 0.00105      |
|    std                      | 0.983        |
|    value_loss               | 1.41         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5036316] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 26           |
|    time_elapsed             | 355          |
|    total_timesteps          | 2160640      |
| train/                      |              |
|    approx_kl                | 0.0047226786 |
|    approx_ln(kl)            | -5.355379    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.32        |
|    ln(policy_gradient_loss) | -5.38        |
|    loss                     | 0.098        |
|    n_updates                | 1106         |
|    policy_gradient_loss     | 0.00462      |
|    std                      | 0.981        |
|    value_loss               | 0.22         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.2793448] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 27           |
|    time_elapsed             | 368          |
|    total_timesteps          | 2162688      |
| train/                      |              |
|    approx_kl                | 0.010308264  |
|    approx_ln(kl)            | -4.5748096   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.995        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.83        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.059        |
|    n_updates                | 1107         |
|    policy_gradient_loss     | -0.00393     |
|    std                      | 0.981        |
|    value_loss               | 0.396        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2778936] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 28           |
|    time_elapsed             | 381          |
|    total_timesteps          | 2164736      |
| train/                      |              |
|    approx_kl                | 0.007269901  |
|    approx_ln(kl)            | -4.9240127   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.978        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.35        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.26         |
|    n_updates                | 1108         |
|    policy_gradient_loss     | -0.00132     |
|    std                      | 0.982        |
|    value_loss               | 0.652        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24998933] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 29            |
|    time_elapsed             | 395           |
|    total_timesteps          | 2166784       |
| train/                      |               |
|    approx_kl                | 0.0041393577  |
|    approx_ln(kl)            | -5.4872146    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.46         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0854        |
|    n_updates                | 1109          |
|    policy_gradient_loss     | -0.00218      |
|    std                      | 0.982         |
|    value_loss               | 0.208         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5417481] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 30           |
|    time_elapsed             | 408          |
|    total_timesteps          | 2168832      |
| train/                      |              |
|    approx_kl                | 0.006585394  |
|    approx_ln(kl)            | -5.022901    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.964        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.408        |
|    ln(policy_gradient_loss) | -6.35        |
|    loss                     | 1.5          |
|    n_updates                | 1110         |
|    policy_gradient_loss     | 0.00175      |
|    std                      | 0.982        |
|    value_loss               | 1.61         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26247075] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 31            |
|    time_elapsed             | 421           |
|    total_timesteps          | 2170880       |
| train/                      |               |
|    approx_kl                | 0.0051054284  |
|    approx_ln(kl)            | -5.277451     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.29         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.276         |
|    n_updates                | 1111          |
|    policy_gradient_loss     | -0.0217       |
|    std                      | 0.983         |
|    value_loss               | 0.58          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32531863] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 32            |
|    time_elapsed             | 434           |
|    total_timesteps          | 2172928       |
| train/                      |               |
|    approx_kl                | 0.0065437523  |
|    approx_ln(kl)            | -5.0292444    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.974         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.97         |
|    ln(policy_gradient_loss) | -7.72         |
|    loss                     | 0.139         |
|    n_updates                | 1112          |
|    policy_gradient_loss     | 0.000443      |
|    std                      | 0.983         |
|    value_loss               | 0.537         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2955608] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 33           |
|    time_elapsed             | 448          |
|    total_timesteps          | 2174976      |
| train/                      |              |
|    approx_kl                | 0.0059451633 |
|    approx_ln(kl)            | -5.1251774   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.973        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.1         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.333        |
|    n_updates                | 1113         |
|    policy_gradient_loss     | -0.00128     |
|    std                      | 0.984        |
|    value_loss               | 0.729        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24068932] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 34            |
|    time_elapsed             | 462           |
|    total_timesteps          | 2177024       |
| train/                      |               |
|    approx_kl                | 0.004264896   |
|    approx_ln(kl)            | -5.4573374    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -5.15         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.00583       |
|    n_updates                | 1114          |
|    policy_gradient_loss     | -0.0045       |
|    std                      | 0.983         |
|    value_loss               | 0.0837        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25180942] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 35            |
|    time_elapsed             | 475           |
|    total_timesteps          | 2179072       |
| train/                      |               |
|    approx_kl                | 0.0055061793  |
|    approx_ln(kl)            | -5.2018843    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.487        |
|    ln(policy_gradient_loss) | -6.37         |
|    loss                     | 0.615         |
|    n_updates                | 1115          |
|    policy_gradient_loss     | 0.00171       |
|    std                      | 0.981         |
|    value_loss               | 0.937         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39077684] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 36            |
|    time_elapsed             | 489           |
|    total_timesteps          | 2181120       |
| train/                      |               |
|    approx_kl                | 0.006564004   |
|    approx_ln(kl)            | -5.0261545    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.95         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.142         |
|    n_updates                | 1116          |
|    policy_gradient_loss     | -7.86e-06     |
|    std                      | 0.98          |
|    value_loss               | 0.401         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28372592] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 37            |
|    time_elapsed             | 502           |
|    total_timesteps          | 2183168       |
| train/                      |               |
|    approx_kl                | 0.004844201   |
|    approx_ln(kl)            | -5.329973     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.77         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0628        |
|    n_updates                | 1117          |
|    policy_gradient_loss     | -0.00602      |
|    std                      | 0.979         |
|    value_loss               | 0.171         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.22596306] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 38            |
|    time_elapsed             | 515           |
|    total_timesteps          | 2185216       |
| train/                      |               |
|    approx_kl                | 0.0062713088  |
|    approx_ln(kl)            | -5.07177      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.79         |
|    ln(policy_gradient_loss) | -6.94         |
|    loss                     | 0.0614        |
|    n_updates                | 1118          |
|    policy_gradient_loss     | 0.000972      |
|    std                      | 0.98          |
|    value_loss               | 0.109         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45226488] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 39            |
|    time_elapsed             | 529           |
|    total_timesteps          | 2187264       |
| train/                      |               |
|    approx_kl                | 0.0066063926  |
|    approx_ln(kl)            | -5.0197177    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.963         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.156         |
|    ln(policy_gradient_loss) | -5.69         |
|    loss                     | 1.17          |
|    n_updates                | 1119          |
|    policy_gradient_loss     | 0.00338       |
|    std                      | 0.981         |
|    value_loss               | 1.32          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.41703066] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 40            |
|    time_elapsed             | 542           |
|    total_timesteps          | 2189312       |
| train/                      |               |
|    approx_kl                | 0.0065902206  |
|    approx_ln(kl)            | -5.0221686    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.67         |
|    ln(policy_gradient_loss) | -6.49         |
|    loss                     | 0.189         |
|    n_updates                | 1120          |
|    policy_gradient_loss     | 0.00151       |
|    std                      | 0.982         |
|    value_loss               | 0.41          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22820114] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 41            |
|    time_elapsed             | 560           |
|    total_timesteps          | 2191360       |
| train/                      |               |
|    approx_kl                | 0.0059514954  |
|    approx_ln(kl)            | -5.1241126    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.5          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.222         |
|    n_updates                | 1121          |
|    policy_gradient_loss     | -0.00181      |
|    std                      | 0.982         |
|    value_loss               | 0.436         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3076452] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 42           |
|    time_elapsed             | 576          |
|    total_timesteps          | 2193408      |
| train/                      |              |
|    approx_kl                | 0.006011412  |
|    approx_ln(kl)            | -5.1140957   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.968        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.23        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.291        |
|    n_updates                | 1122         |
|    policy_gradient_loss     | -0.00137     |
|    std                      | 0.983        |
|    value_loss               | 1.5          |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2493313] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 43           |
|    time_elapsed             | 592          |
|    total_timesteps          | 2195456      |
| train/                      |              |
|    approx_kl                | 0.005789218  |
|    approx_ln(kl)            | -5.151758    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.912        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.142       |
|    ln(policy_gradient_loss) | -8.48        |
|    loss                     | 0.868        |
|    n_updates                | 1123         |
|    policy_gradient_loss     | 0.000208     |
|    std                      | 0.983        |
|    value_loss               | 4.61         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39320588] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 44            |
|    time_elapsed             | 609           |
|    total_timesteps          | 2197504       |
| train/                      |               |
|    approx_kl                | 0.006481191   |
|    approx_ln(kl)            | -5.038851     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.927         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.824        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.439         |
|    n_updates                | 1124          |
|    policy_gradient_loss     | -0.0024       |
|    std                      | 0.983         |
|    value_loss               | 1.48          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31783202] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 45            |
|    time_elapsed             | 626           |
|    total_timesteps          | 2199552       |
| train/                      |               |
|    approx_kl                | 0.0075940792  |
|    approx_ln(kl)            | -4.8803864    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.63         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.196         |
|    n_updates                | 1125          |
|    policy_gradient_loss     | -0.00473      |
|    std                      | 0.984         |
|    value_loss               | 0.376         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4881505] |
| time/                       |              |
|    fps                      | 146          |
|    iterations               | 46           |
|    time_elapsed             | 642          |
|    total_timesteps          | 2201600      |
| train/                      |              |
|    approx_kl                | 0.0049875397 |
|    approx_ln(kl)            | -5.3008127   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.963        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.362        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.44         |
|    n_updates                | 1126         |
|    policy_gradient_loss     | -0.00368     |
|    std                      | 0.985        |
|    value_loss               | 0.837        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27946034] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 47            |
|    time_elapsed             | 659           |
|    total_timesteps          | 2203648       |
| train/                      |               |
|    approx_kl                | 0.0059916857  |
|    approx_ln(kl)            | -5.1173825    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.999         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.25         |
|    ln(policy_gradient_loss) | -9.37         |
|    loss                     | 0.0388        |
|    n_updates                | 1127          |
|    policy_gradient_loss     | 8.55e-05      |
|    std                      | 0.986         |
|    value_loss               | 0.132         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.34641293] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 48            |
|    time_elapsed             | 676           |
|    total_timesteps          | 2205696       |
| train/                      |               |
|    approx_kl                | 0.014133598   |
|    approx_ln(kl)            | -4.2592006    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.955         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.24         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.106         |
|    n_updates                | 1128          |
|    policy_gradient_loss     | -0.0134       |
|    std                      | 0.986         |
|    value_loss               | 0.535         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37379462] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 49            |
|    time_elapsed             | 696           |
|    total_timesteps          | 2207744       |
| train/                      |               |
|    approx_kl                | 0.008148058   |
|    approx_ln(kl)            | -4.8099756    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.52         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0802        |
|    n_updates                | 1129          |
|    policy_gradient_loss     | -0.00156      |
|    std                      | 0.985         |
|    value_loss               | 0.14          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
--------------------------------------
| reward             | [-0.21056469] |
| time/              |               |
|    fps             | 131           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2209792       |
--------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4986127] |
| time/                       |              |
|    fps                      | 134          |
|    iterations               | 2            |
|    time_elapsed             | 30           |
|    total_timesteps          | 2211840      |
| train/                      |              |
|    approx_kl                | 0.0050403792 |
|    approx_ln(kl)            | -5.290274    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.946        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.27        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.763        |
|    n_updates                | 1131         |
|    policy_gradient_loss     | -0.0018      |
|    std                      | 0.982        |
|    value_loss               | 2.84         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33646932] |
| time/                       |               |
|    fps                      | 137           |
|    iterations               | 3             |
|    time_elapsed             | 44            |
|    total_timesteps          | 2213888       |
| train/                      |               |
|    approx_kl                | 0.008426478   |
|    approx_ln(kl)            | -4.7763762    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.955         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.627        |
|    ln(policy_gradient_loss) | -4.39         |
|    loss                     | 0.534         |
|    n_updates                | 1132          |
|    policy_gradient_loss     | 0.0124        |
|    std                      | 0.981         |
|    value_loss               | 1.34          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.37321368] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 4             |
|    time_elapsed             | 57            |
|    total_timesteps          | 2215936       |
| train/                      |               |
|    approx_kl                | 0.009608624   |
|    approx_ln(kl)            | -4.6450944    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.442         |
|    ln(policy_gradient_loss) | -5.41         |
|    loss                     | 1.56          |
|    n_updates                | 1133          |
|    policy_gradient_loss     | 0.00449       |
|    std                      | 0.98          |
|    value_loss               | 1.22          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36371562] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 5             |
|    time_elapsed             | 70            |
|    total_timesteps          | 2217984       |
| train/                      |               |
|    approx_kl                | 0.0039695716  |
|    approx_ln(kl)            | -5.529097     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.45         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.234         |
|    n_updates                | 1134          |
|    policy_gradient_loss     | -0.00132      |
|    std                      | 0.978         |
|    value_loss               | 0.735         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.31230438] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 6             |
|    time_elapsed             | 83            |
|    total_timesteps          | 2220032       |
| train/                      |               |
|    approx_kl                | 0.007549754   |
|    approx_ln(kl)            | -4.8862405    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.946         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.187        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.83          |
|    n_updates                | 1135          |
|    policy_gradient_loss     | -0.00565      |
|    std                      | 0.979         |
|    value_loss               | 2.35          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.32536325] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 7             |
|    time_elapsed             | 96            |
|    total_timesteps          | 2222080       |
| train/                      |               |
|    approx_kl                | 0.0082404865  |
|    approx_ln(kl)            | -4.798696     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.974         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.631         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.88          |
|    n_updates                | 1136          |
|    policy_gradient_loss     | -0.00127      |
|    std                      | 0.978         |
|    value_loss               | 3.05          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25407416] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 8             |
|    time_elapsed             | 109           |
|    total_timesteps          | 2224128       |
| train/                      |               |
|    approx_kl                | 0.0054715946  |
|    approx_ln(kl)            | -5.208185     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.0905        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.09          |
|    n_updates                | 1137          |
|    policy_gradient_loss     | -0.00426      |
|    std                      | 0.979         |
|    value_loss               | 2.5           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22061561] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 9             |
|    time_elapsed             | 123           |
|    total_timesteps          | 2226176       |
| train/                      |               |
|    approx_kl                | 0.0048901928  |
|    approx_ln(kl)            | -5.3205237    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.55         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0782        |
|    n_updates                | 1138          |
|    policy_gradient_loss     | -0.00433      |
|    std                      | 0.979         |
|    value_loss               | 0.245         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.2153731] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 10           |
|    time_elapsed             | 137          |
|    total_timesteps          | 2228224      |
| train/                      |              |
|    approx_kl                | 0.010444385  |
|    approx_ln(kl)            | -4.561691    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.962        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.58        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0755       |
|    n_updates                | 1139         |
|    policy_gradient_loss     | -0.0088      |
|    std                      | 0.979        |
|    value_loss               | 0.365        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22865577] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 11            |
|    time_elapsed             | 150           |
|    total_timesteps          | 2230272       |
| train/                      |               |
|    approx_kl                | 0.008887018   |
|    approx_ln(kl)            | -4.7231636    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.01          |
|    ln(policy_gradient_loss) | -5.38         |
|    loss                     | 2.76          |
|    n_updates                | 1140          |
|    policy_gradient_loss     | 0.00461       |
|    std                      | 0.979         |
|    value_loss               | 2.24          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22556476] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 12            |
|    time_elapsed             | 164           |
|    total_timesteps          | 2232320       |
| train/                      |               |
|    approx_kl                | 0.0064702407  |
|    approx_ln(kl)            | -5.040542     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0533       |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.948         |
|    n_updates                | 1141          |
|    policy_gradient_loss     | -0.0007       |
|    std                      | 0.979         |
|    value_loss               | 0.561         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22454104] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 13            |
|    time_elapsed             | 178           |
|    total_timesteps          | 2234368       |
| train/                      |               |
|    approx_kl                | 0.0058184597  |
|    approx_ln(kl)            | -5.14672      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.62         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0731        |
|    n_updates                | 1142          |
|    policy_gradient_loss     | -0.00298      |
|    std                      | 0.979         |
|    value_loss               | 0.6           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30508485] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 14            |
|    time_elapsed             | 191           |
|    total_timesteps          | 2236416       |
| train/                      |               |
|    approx_kl                | 0.0050000967  |
|    approx_ln(kl)            | -5.298298     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.959         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.06         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.346         |
|    n_updates                | 1143          |
|    policy_gradient_loss     | -0.000119     |
|    std                      | 0.978         |
|    value_loss               | 1.29          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23978522] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 15            |
|    time_elapsed             | 204           |
|    total_timesteps          | 2238464       |
| train/                      |               |
|    approx_kl                | 0.007000569   |
|    approx_ln(kl)            | -4.961764     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.886        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.412         |
|    n_updates                | 1144          |
|    policy_gradient_loss     | -0.00615      |
|    std                      | 0.977         |
|    value_loss               | 0.567         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2717176] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 16           |
|    time_elapsed             | 217          |
|    total_timesteps          | 2240512      |
| train/                      |              |
|    approx_kl                | 0.0067759748 |
|    approx_ln(kl)            | -4.994372    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.78        |
|    explained_variance       | 0.984        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.204       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.815        |
|    n_updates                | 1145         |
|    policy_gradient_loss     | -0.00349     |
|    std                      | 0.976        |
|    value_loss               | 0.698        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23832792] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 17            |
|    time_elapsed             | 231           |
|    total_timesteps          | 2242560       |
| train/                      |               |
|    approx_kl                | 0.009955036   |
|    approx_ln(kl)            | -4.609677     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.49         |
|    ln(policy_gradient_loss) | -4.97         |
|    loss                     | 0.0832        |
|    n_updates                | 1146          |
|    policy_gradient_loss     | 0.00695       |
|    std                      | 0.977         |
|    value_loss               | 0.689         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4391621] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 18           |
|    time_elapsed             | 245          |
|    total_timesteps          | 2244608      |
| train/                      |              |
|    approx_kl                | 0.005141083  |
|    approx_ln(kl)            | -5.2704916   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.78        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.64        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0712       |
|    n_updates                | 1147         |
|    policy_gradient_loss     | -0.00488     |
|    std                      | 0.977        |
|    value_loss               | 0.197        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
-----------------------------------------------
| reward                      | [-0.31924215] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 19            |
|    time_elapsed             | 259           |
|    total_timesteps          | 2246656       |
| train/                      |               |
|    approx_kl                | 0.01576382    |
|    approx_ln(kl)            | -4.150038     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.13         |
|    ln(policy_gradient_loss) | -4.3          |
|    loss                     | 0.119         |
|    n_updates                | 1148          |
|    policy_gradient_loss     | 0.0135        |
|    std                      | 0.978         |
|    value_loss               | 0.137         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23455879] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 20            |
|    time_elapsed             | 272           |
|    total_timesteps          | 2248704       |
| train/                      |               |
|    approx_kl                | 0.0059610098  |
|    approx_ln(kl)            | -5.122515     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.58         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0279        |
|    n_updates                | 1149          |
|    policy_gradient_loss     | -0.000548     |
|    std                      | 0.98          |
|    value_loss               | 0.164         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.17523678] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 21            |
|    time_elapsed             | 285           |
|    total_timesteps          | 2250752       |
| train/                      |               |
|    approx_kl                | 0.0077325692  |
|    approx_ln(kl)            | -4.862314     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.977         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.16         |
|    ln(policy_gradient_loss) | -6.68         |
|    loss                     | 0.313         |
|    n_updates                | 1150          |
|    policy_gradient_loss     | 0.00125       |
|    std                      | 0.981         |
|    value_loss               | 0.78          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.20121174] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 22            |
|    time_elapsed             | 298           |
|    total_timesteps          | 2252800       |
| train/                      |               |
|    approx_kl                | 0.009082101   |
|    approx_ln(kl)            | -4.70145      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.4          |
|    ln(policy_gradient_loss) | -6.04         |
|    loss                     | 0.247         |
|    n_updates                | 1151          |
|    policy_gradient_loss     | 0.00239       |
|    std                      | 0.982         |
|    value_loss               | 0.513         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28135636] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 23            |
|    time_elapsed             | 312           |
|    total_timesteps          | 2254848       |
| train/                      |               |
|    approx_kl                | 0.006387074   |
|    approx_ln(kl)            | -5.053479     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.955         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.373         |
|    ln(policy_gradient_loss) | -5.33         |
|    loss                     | 1.45          |
|    n_updates                | 1152          |
|    policy_gradient_loss     | 0.00484       |
|    std                      | 0.984         |
|    value_loss               | 1.71          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5536997] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 24           |
|    time_elapsed             | 325          |
|    total_timesteps          | 2256896      |
| train/                      |              |
|    approx_kl                | 0.004918673  |
|    approx_ln(kl)            | -5.3147163   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.966        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.05        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.351        |
|    n_updates                | 1153         |
|    policy_gradient_loss     | -0.00466     |
|    std                      | 0.985        |
|    value_loss               | 0.649        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.38865355] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 25            |
|    time_elapsed             | 338           |
|    total_timesteps          | 2258944       |
| train/                      |               |
|    approx_kl                | 0.010677593   |
|    approx_ln(kl)            | -4.539608     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.977         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.469        |
|    ln(policy_gradient_loss) | -3.83         |
|    loss                     | 0.626         |
|    n_updates                | 1154          |
|    policy_gradient_loss     | 0.0216        |
|    std                      | 0.986         |
|    value_loss               | 0.988         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3014145] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 26           |
|    time_elapsed             | 351          |
|    total_timesteps          | 2260992      |
| train/                      |              |
|    approx_kl                | 0.0072559444 |
|    approx_ln(kl)            | -4.9259343   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.952        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.839       |
|    ln(policy_gradient_loss) | -5.63        |
|    loss                     | 0.432        |
|    n_updates                | 1155         |
|    policy_gradient_loss     | 0.0036       |
|    std                      | 0.987        |
|    value_loss               | 2.03         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3169853] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 27           |
|    time_elapsed             | 364          |
|    total_timesteps          | 2263040      |
| train/                      |              |
|    approx_kl                | 0.0053189015 |
|    approx_ln(kl)            | -5.2364883   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.924        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.83         |
|    ln(policy_gradient_loss) | -5.77        |
|    loss                     | 6.22         |
|    n_updates                | 1156         |
|    policy_gradient_loss     | 0.00311      |
|    std                      | 0.988        |
|    value_loss               | 4.84         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.26243448] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 28            |
|    time_elapsed             | 377           |
|    total_timesteps          | 2265088       |
| train/                      |               |
|    approx_kl                | 0.008482443   |
|    approx_ln(kl)            | -4.769757     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.745        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.475         |
|    n_updates                | 1157          |
|    policy_gradient_loss     | -0.00968      |
|    std                      | 0.989         |
|    value_loss               | 0.501         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27071807] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 29            |
|    time_elapsed             | 390           |
|    total_timesteps          | 2267136       |
| train/                      |               |
|    approx_kl                | 0.00731963    |
|    approx_ln(kl)            | -4.9171953    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.977         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.59         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.205         |
|    n_updates                | 1158          |
|    policy_gradient_loss     | -0.00158      |
|    std                      | 0.989         |
|    value_loss               | 1.56          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32307956] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 30            |
|    time_elapsed             | 403           |
|    total_timesteps          | 2269184       |
| train/                      |               |
|    approx_kl                | 0.0047316104  |
|    approx_ln(kl)            | -5.35349      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1            |
|    ln(policy_gradient_loss) | -6.3          |
|    loss                     | 0.366         |
|    n_updates                | 1159          |
|    policy_gradient_loss     | 0.00184       |
|    std                      | 0.989         |
|    value_loss               | 2.1           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26166117] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 31            |
|    time_elapsed             | 417           |
|    total_timesteps          | 2271232       |
| train/                      |               |
|    approx_kl                | 0.0043779053  |
|    approx_ln(kl)            | -5.431185     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.964         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.265        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.767         |
|    n_updates                | 1160          |
|    policy_gradient_loss     | -0.00629      |
|    std                      | 0.989         |
|    value_loss               | 2.02          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23795442] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 32            |
|    time_elapsed             | 431           |
|    total_timesteps          | 2273280       |
| train/                      |               |
|    approx_kl                | 0.0057390286  |
|    approx_ln(kl)            | -5.1604652    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.973         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.541        |
|    ln(policy_gradient_loss) | -5.76         |
|    loss                     | 0.582         |
|    n_updates                | 1161          |
|    policy_gradient_loss     | 0.00314       |
|    std                      | 0.988         |
|    value_loss               | 1.26          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25332108] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 33            |
|    time_elapsed             | 449           |
|    total_timesteps          | 2275328       |
| train/                      |               |
|    approx_kl                | 0.0060576242  |
|    approx_ln(kl)            | -5.1064377    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.203        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.816         |
|    n_updates                | 1162          |
|    policy_gradient_loss     | -0.000521     |
|    std                      | 0.988         |
|    value_loss               | 1.81          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22242686] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 34            |
|    time_elapsed             | 465           |
|    total_timesteps          | 2277376       |
| train/                      |               |
|    approx_kl                | 0.005650866   |
|    approx_ln(kl)            | -5.1759467    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.917         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.92         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.146         |
|    n_updates                | 1163          |
|    policy_gradient_loss     | -0.00907      |
|    std                      | 0.989         |
|    value_loss               | 0.268         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23216937] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 35            |
|    time_elapsed             | 482           |
|    total_timesteps          | 2279424       |
| train/                      |               |
|    approx_kl                | 0.007313182   |
|    approx_ln(kl)            | -4.918077     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.0921        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.1           |
|    n_updates                | 1164          |
|    policy_gradient_loss     | -0.00747      |
|    std                      | 0.989         |
|    value_loss               | 1.19          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24375866] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 36            |
|    time_elapsed             | 499           |
|    total_timesteps          | 2281472       |
| train/                      |               |
|    approx_kl                | 0.006711386   |
|    approx_ln(kl)            | -5.0039496    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.965         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.44         |
|    ln(policy_gradient_loss) | -5.29         |
|    loss                     | 0.237         |
|    n_updates                | 1165          |
|    policy_gradient_loss     | 0.00503       |
|    std                      | 0.989         |
|    value_loss               | 0.907         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.25794503] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 37            |
|    time_elapsed             | 516           |
|    total_timesteps          | 2283520       |
| train/                      |               |
|    approx_kl                | 0.008480259   |
|    approx_ln(kl)            | -4.7700143    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.967         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.233        |
|    ln(policy_gradient_loss) | -4.73         |
|    loss                     | 0.792         |
|    n_updates                | 1166          |
|    policy_gradient_loss     | 0.00884       |
|    std                      | 0.989         |
|    value_loss               | 1.52          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.35667267] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 38            |
|    time_elapsed             | 534           |
|    total_timesteps          | 2285568       |
| train/                      |               |
|    approx_kl                | 0.009997506   |
|    approx_ln(kl)            | -4.6054196    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.21         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0402        |
|    n_updates                | 1167          |
|    policy_gradient_loss     | -0.00616      |
|    std                      | 0.989         |
|    value_loss               | 0.0821        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.22909182] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 39            |
|    time_elapsed             | 552           |
|    total_timesteps          | 2287616       |
| train/                      |               |
|    approx_kl                | 0.011009596   |
|    approx_ln(kl)            | -4.508988     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.83         |
|    ln(policy_gradient_loss) | -6.48         |
|    loss                     | 0.161         |
|    n_updates                | 1168          |
|    policy_gradient_loss     | 0.00153       |
|    std                      | 0.99          |
|    value_loss               | 0.84          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.46091008] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 40            |
|    time_elapsed             | 569           |
|    total_timesteps          | 2289664       |
| train/                      |               |
|    approx_kl                | 0.007511317   |
|    approx_ln(kl)            | -4.8913445    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.964         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.06         |
|    ln(policy_gradient_loss) | -4.99         |
|    loss                     | 0.128         |
|    n_updates                | 1169          |
|    policy_gradient_loss     | 0.0068        |
|    std                      | 0.99          |
|    value_loss               | 0.615         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23277035] |
| time/                       |               |
|    fps                      | 142           |
|    iterations               | 41            |
|    time_elapsed             | 587           |
|    total_timesteps          | 2291712       |
| train/                      |               |
|    approx_kl                | 0.0063249106  |
|    approx_ln(kl)            | -5.0632596    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.17         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.114         |
|    n_updates                | 1170          |
|    policy_gradient_loss     | -0.00937      |
|    std                      | 0.99          |
|    value_loss               | 1.16          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2279961] |
| time/                       |              |
|    fps                      | 142          |
|    iterations               | 42           |
|    time_elapsed             | 605          |
|    total_timesteps          | 2293760      |
| train/                      |              |
|    approx_kl                | 0.0067160362 |
|    approx_ln(kl)            | -5.0032573   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.81        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.89        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.151        |
|    n_updates                | 1171         |
|    policy_gradient_loss     | -0.00589     |
|    std                      | 0.989        |
|    value_loss               | 0.315        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31423843] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 43            |
|    time_elapsed             | 622           |
|    total_timesteps          | 2295808       |
| train/                      |               |
|    approx_kl                | 0.003728326   |
|    approx_ln(kl)            | -5.591796     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.61         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0735        |
|    n_updates                | 1172          |
|    policy_gradient_loss     | -0.00235      |
|    std                      | 0.99          |
|    value_loss               | 0.357         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22590518] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 44            |
|    time_elapsed             | 637           |
|    total_timesteps          | 2297856       |
| train/                      |               |
|    approx_kl                | 0.008361938   |
|    approx_ln(kl)            | -4.7840652    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.73         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0652        |
|    n_updates                | 1173          |
|    policy_gradient_loss     | -0.00107      |
|    std                      | 0.991         |
|    value_loss               | 1.08          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24162424] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 45            |
|    time_elapsed             | 654           |
|    total_timesteps          | 2299904       |
| train/                      |               |
|    approx_kl                | 0.006875093   |
|    approx_ln(kl)            | -4.9798503    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.72         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0241        |
|    n_updates                | 1174          |
|    policy_gradient_loss     | -0.0106       |
|    std                      | 0.991         |
|    value_loss               | 0.595         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41346815] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 46            |
|    time_elapsed             | 670           |
|    total_timesteps          | 2301952       |
| train/                      |               |
|    approx_kl                | 0.0049601668  |
|    approx_ln(kl)            | -5.306316     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.77         |
|    ln(policy_gradient_loss) | -6.06         |
|    loss                     | 0.17          |
|    n_updates                | 1175          |
|    policy_gradient_loss     | 0.00233       |
|    std                      | 0.991         |
|    value_loss               | 0.581         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.227542] |
| time/                       |             |
|    fps                      | 140         |
|    iterations               | 47          |
|    time_elapsed             | 686         |
|    total_timesteps          | 2304000     |
| train/                      |             |
|    approx_kl                | 0.00732681  |
|    approx_ln(kl)            | -4.916215   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.81       |
|    explained_variance       | 0.98        |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -0.95       |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.387       |
|    n_updates                | 1176        |
|    policy_gradient_loss     | -0.0119     |
|    std                      | 0.992       |
|    value_loss               | 0.96        |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23169951] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 48            |
|    time_elapsed             | 701           |
|    total_timesteps          | 2306048       |
| train/                      |               |
|    approx_kl                | 0.008072289   |
|    approx_ln(kl)            | -4.8193183    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.1          |
|    ln(policy_gradient_loss) | -4.66         |
|    loss                     | 0.332         |
|    n_updates                | 1177          |
|    policy_gradient_loss     | 0.00951       |
|    std                      | 0.993         |
|    value_loss               | 0.436         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.31039068] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 49            |
|    time_elapsed             | 716           |
|    total_timesteps          | 2308096       |
| train/                      |               |
|    approx_kl                | 0.008652193   |
|    approx_ln(kl)            | -4.7499423    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.11         |
|    ln(policy_gradient_loss) | -5.46         |
|    loss                     | 0.329         |
|    n_updates                | 1178          |
|    policy_gradient_loss     | 0.00424       |
|    std                      | 0.993         |
|    value_loss               | 1.56          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------
| reward             | [-0.4166747] |
| time/              |              |
|    fps             | 145          |
|    iterations      | 1            |
|    time_elapsed    | 14           |
|    total_timesteps | 2310144      |
-------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42064002] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 2             |
|    time_elapsed             | 28            |
|    total_timesteps          | 2312192       |
| train/                      |               |
|    approx_kl                | 0.0076096538  |
|    approx_ln(kl)            | -4.8783374    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.68         |
|    ln(policy_gradient_loss) | -4.57         |
|    loss                     | 0.187         |
|    n_updates                | 1180          |
|    policy_gradient_loss     | 0.0103        |
|    std                      | 0.993         |
|    value_loss               | 0.288         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.53553736] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 3             |
|    time_elapsed             | 42            |
|    total_timesteps          | 2314240       |
| train/                      |               |
|    approx_kl                | 0.0067308675  |
|    approx_ln(kl)            | -5.0010514    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.66         |
|    ln(policy_gradient_loss) | -7.11         |
|    loss                     | 0.19          |
|    n_updates                | 1181          |
|    policy_gradient_loss     | 0.000817      |
|    std                      | 0.993         |
|    value_loss               | 0.501         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4811317] |
| time/                       |              |
|    fps                      | 145          |
|    iterations               | 4            |
|    time_elapsed             | 56           |
|    total_timesteps          | 2316288      |
| train/                      |              |
|    approx_kl                | 0.006646243  |
|    approx_ln(kl)            | -5.0137033   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.81        |
|    explained_variance       | 0.99         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.233       |
|    ln(policy_gradient_loss) | -4.46        |
|    loss                     | 0.792        |
|    n_updates                | 1182         |
|    policy_gradient_loss     | 0.0116       |
|    std                      | 0.991        |
|    value_loss               | 0.953        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30128318] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 5             |
|    time_elapsed             | 70            |
|    total_timesteps          | 2318336       |
| train/                      |               |
|    approx_kl                | 0.00895487    |
|    approx_ln(kl)            | -4.715558     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.63         |
|    ln(policy_gradient_loss) | -5.68         |
|    loss                     | 0.0719        |
|    n_updates                | 1183          |
|    policy_gradient_loss     | 0.0034        |
|    std                      | 0.99          |
|    value_loss               | 0.163         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3526986] |
| time/                       |              |
|    fps                      | 145          |
|    iterations               | 6            |
|    time_elapsed             | 84           |
|    total_timesteps          | 2320384      |
| train/                      |              |
|    approx_kl                | 0.007039235  |
|    approx_ln(kl)            | -4.956256    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.81        |
|    explained_variance       | 0.991        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.4         |
|    ln(policy_gradient_loss) | -4.4         |
|    loss                     | 0.245        |
|    n_updates                | 1184         |
|    policy_gradient_loss     | 0.0123       |
|    std                      | 0.988        |
|    value_loss               | 0.732        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2954855] |
| time/                       |              |
|    fps                      | 144          |
|    iterations               | 7            |
|    time_elapsed             | 98           |
|    total_timesteps          | 2322432      |
| train/                      |              |
|    approx_kl                | 0.0068811625 |
|    approx_ln(kl)            | -4.9789677   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.979        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.738       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.478        |
|    n_updates                | 1185         |
|    policy_gradient_loss     | -0.00123     |
|    std                      | 0.987        |
|    value_loss               | 1.6          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28107333] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 8             |
|    time_elapsed             | 113           |
|    total_timesteps          | 2324480       |
| train/                      |               |
|    approx_kl                | 0.0050397115  |
|    approx_ln(kl)            | -5.290406     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.194         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.21          |
|    n_updates                | 1186          |
|    policy_gradient_loss     | -0.00942      |
|    std                      | 0.987         |
|    value_loss               | 2.25          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2609326] |
| time/                       |              |
|    fps                      | 144          |
|    iterations               | 9            |
|    time_elapsed             | 127          |
|    total_timesteps          | 2326528      |
| train/                      |              |
|    approx_kl                | 0.007292339  |
|    approx_ln(kl)            | -4.920931    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.987        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.77        |
|    ln(policy_gradient_loss) | -5.82        |
|    loss                     | 0.0629       |
|    n_updates                | 1187         |
|    policy_gradient_loss     | 0.00298      |
|    std                      | 0.986        |
|    value_loss               | 0.16         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30395606] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 10            |
|    time_elapsed             | 141           |
|    total_timesteps          | 2328576       |
| train/                      |               |
|    approx_kl                | 0.005908829   |
|    approx_ln(kl)            | -5.1313076    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0708       |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.932         |
|    n_updates                | 1188          |
|    policy_gradient_loss     | -0.00415      |
|    std                      | 0.986         |
|    value_loss               | 1.13          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.340102] |
| time/                       |             |
|    fps                      | 144         |
|    iterations               | 11          |
|    time_elapsed             | 155         |
|    total_timesteps          | 2330624     |
| train/                      |             |
|    approx_kl                | 0.004005218 |
|    approx_ln(kl)            | -5.5201573  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.8        |
|    explained_variance       | 0.997       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -2.72       |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.066       |
|    n_updates                | 1189        |
|    policy_gradient_loss     | -0.000894   |
|    std                      | 0.986       |
|    value_loss               | 0.19        |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25178522] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 12            |
|    time_elapsed             | 169           |
|    total_timesteps          | 2332672       |
| train/                      |               |
|    approx_kl                | 0.0061580506  |
|    approx_ln(kl)            | -5.089995     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.974         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.253        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.776         |
|    n_updates                | 1190          |
|    policy_gradient_loss     | -0.00108      |
|    std                      | 0.986         |
|    value_loss               | 1.3           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22969861] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 13            |
|    time_elapsed             | 183           |
|    total_timesteps          | 2334720       |
| train/                      |               |
|    approx_kl                | 0.006520649   |
|    approx_ln(kl)            | -5.0327816    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.53         |
|    ln(policy_gradient_loss) | -4.56         |
|    loss                     | 0.217         |
|    n_updates                | 1191          |
|    policy_gradient_loss     | 0.0104        |
|    std                      | 0.987         |
|    value_loss               | 0.46          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42709085] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 14            |
|    time_elapsed             | 196           |
|    total_timesteps          | 2336768       |
| train/                      |               |
|    approx_kl                | 0.005664773   |
|    approx_ln(kl)            | -5.1734886    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.3          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0999        |
|    n_updates                | 1192          |
|    policy_gradient_loss     | -0.0119       |
|    std                      | 0.987         |
|    value_loss               | 0.276         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26708898] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 15            |
|    time_elapsed             | 209           |
|    total_timesteps          | 2338816       |
| train/                      |               |
|    approx_kl                | 0.008715472   |
|    approx_ln(kl)            | -4.7426553    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.61         |
|    ln(policy_gradient_loss) | -4.97         |
|    loss                     | 0.2           |
|    n_updates                | 1193          |
|    policy_gradient_loss     | 0.00697       |
|    std                      | 0.987         |
|    value_loss               | 0.932         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.25281247] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 16            |
|    time_elapsed             | 222           |
|    total_timesteps          | 2340864       |
| train/                      |               |
|    approx_kl                | 0.010452303   |
|    approx_ln(kl)            | -4.560933     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.763        |
|    ln(policy_gradient_loss) | -6.61         |
|    loss                     | 0.466         |
|    n_updates                | 1194          |
|    policy_gradient_loss     | 0.00135       |
|    std                      | 0.988         |
|    value_loss               | 1.09          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25857195] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 17            |
|    time_elapsed             | 236           |
|    total_timesteps          | 2342912       |
| train/                      |               |
|    approx_kl                | 0.0063469014  |
|    approx_ln(kl)            | -5.0597887    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.956         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.465         |
|    ln(policy_gradient_loss) | -5.81         |
|    loss                     | 1.59          |
|    n_updates                | 1195          |
|    policy_gradient_loss     | 0.00301       |
|    std                      | 0.988         |
|    value_loss               | 2.71          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30799103] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 18            |
|    time_elapsed             | 249           |
|    total_timesteps          | 2344960       |
| train/                      |               |
|    approx_kl                | 0.0062309774  |
|    approx_ln(kl)            | -5.0782223    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.955         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.07         |
|    ln(policy_gradient_loss) | -5.64         |
|    loss                     | 0.344         |
|    n_updates                | 1196          |
|    policy_gradient_loss     | 0.00356       |
|    std                      | 0.988         |
|    value_loss               | 0.948         |
-----------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33251655] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 19            |
|    time_elapsed             | 262           |
|    total_timesteps          | 2347008       |
| train/                      |               |
|    approx_kl                | 0.012617481   |
|    approx_ln(kl)            | -4.372672     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.959         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.187        |
|    ln(policy_gradient_loss) | -5.94         |
|    loss                     | 0.829         |
|    n_updates                | 1198          |
|    policy_gradient_loss     | 0.00263       |
|    std                      | 0.987         |
|    value_loss               | 2.2           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.15077826] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 20            |
|    time_elapsed             | 276           |
|    total_timesteps          | 2349056       |
| train/                      |               |
|    approx_kl                | 0.0063299104  |
|    approx_ln(kl)            | -5.062469     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.95          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.921        |
|    ln(policy_gradient_loss) | -5.27         |
|    loss                     | 0.398         |
|    n_updates                | 1199          |
|    policy_gradient_loss     | 0.00512       |
|    std                      | 0.986         |
|    value_loss               | 1.37          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.32271042] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 21            |
|    time_elapsed             | 289           |
|    total_timesteps          | 2351104       |
| train/                      |               |
|    approx_kl                | 0.008355997   |
|    approx_ln(kl)            | -4.7847757    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.975         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.208        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.812         |
|    n_updates                | 1200          |
|    policy_gradient_loss     | -0.00578      |
|    std                      | 0.984         |
|    value_loss               | 1.92          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22219788] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 22            |
|    time_elapsed             | 304           |
|    total_timesteps          | 2353152       |
| train/                      |               |
|    approx_kl                | 0.0067302287  |
|    approx_ln(kl)            | -5.0011463    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.967         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.792        |
|    ln(policy_gradient_loss) | -6.72         |
|    loss                     | 0.453         |
|    n_updates                | 1201          |
|    policy_gradient_loss     | 0.00121       |
|    std                      | 0.983         |
|    value_loss               | 1.19          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48870495] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 23            |
|    time_elapsed             | 324           |
|    total_timesteps          | 2355200       |
| train/                      |               |
|    approx_kl                | 0.0062225414  |
|    approx_ln(kl)            | -5.079577     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.958         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.964        |
|    ln(policy_gradient_loss) | -7.09         |
|    loss                     | 0.381         |
|    n_updates                | 1202          |
|    policy_gradient_loss     | 0.000833      |
|    std                      | 0.983         |
|    value_loss               | 1.84          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44405222] |
| time/                       |               |
|    fps                      | 142           |
|    iterations               | 24            |
|    time_elapsed             | 344           |
|    total_timesteps          | 2357248       |
| train/                      |               |
|    approx_kl                | 0.004379319   |
|    approx_ln(kl)            | -5.430862     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.961         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.751        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.472         |
|    n_updates                | 1203          |
|    policy_gradient_loss     | -0.0049       |
|    std                      | 0.983         |
|    value_loss               | 1.4           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28927368] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 25            |
|    time_elapsed             | 361           |
|    total_timesteps          | 2359296       |
| train/                      |               |
|    approx_kl                | 0.007161917   |
|    approx_ln(kl)            | -4.9389777    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.91          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.857        |
|    ln(policy_gradient_loss) | -4.9          |
|    loss                     | 0.424         |
|    n_updates                | 1204          |
|    policy_gradient_loss     | 0.00741       |
|    std                      | 0.981         |
|    value_loss               | 2.59          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.46129483] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 26            |
|    time_elapsed             | 377           |
|    total_timesteps          | 2361344       |
| train/                      |               |
|    approx_kl                | 0.006137285   |
|    approx_ln(kl)            | -5.093373     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.919         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.115        |
|    ln(policy_gradient_loss) | -5.18         |
|    loss                     | 0.891         |
|    n_updates                | 1205          |
|    policy_gradient_loss     | 0.00563       |
|    std                      | 0.98          |
|    value_loss               | 2.66          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.2702993] |
| time/                       |              |
|    fps                      | 140          |
|    iterations               | 27           |
|    time_elapsed             | 394          |
|    total_timesteps          | 2363392      |
| train/                      |              |
|    approx_kl                | 0.007218611  |
|    approx_ln(kl)            | -4.9310927   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.979        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.74        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.175        |
|    n_updates                | 1206         |
|    policy_gradient_loss     | -0.00569     |
|    std                      | 0.979        |
|    value_loss               | 1.01         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24099903] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 28            |
|    time_elapsed             | 410           |
|    total_timesteps          | 2365440       |
| train/                      |               |
|    approx_kl                | 0.008314694   |
|    approx_ln(kl)            | -4.789731     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0342       |
|    ln(policy_gradient_loss) | -4.93         |
|    loss                     | 0.966         |
|    n_updates                | 1207          |
|    policy_gradient_loss     | 0.00724       |
|    std                      | 0.98          |
|    value_loss               | 1.28          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29716927] |
| time/                       |               |
|    fps                      | 138           |
|    iterations               | 29            |
|    time_elapsed             | 427           |
|    total_timesteps          | 2367488       |
| train/                      |               |
|    approx_kl                | 0.0070931264  |
|    approx_ln(kl)            | -4.948629     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.967         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.89         |
|    ln(policy_gradient_loss) | -6.16         |
|    loss                     | 0.15          |
|    n_updates                | 1208          |
|    policy_gradient_loss     | 0.00211       |
|    std                      | 0.98          |
|    value_loss               | 0.499         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3929401] |
| time/                       |              |
|    fps                      | 138          |
|    iterations               | 30           |
|    time_elapsed             | 444          |
|    total_timesteps          | 2369536      |
| train/                      |              |
|    approx_kl                | 0.009345998  |
|    approx_ln(kl)            | -4.672807    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.986        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.23        |
|    ln(policy_gradient_loss) | -6.21        |
|    loss                     | 0.293        |
|    n_updates                | 1209         |
|    policy_gradient_loss     | 0.00201      |
|    std                      | 0.98         |
|    value_loss               | 0.488        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.32228905] |
| time/                       |               |
|    fps                      | 137           |
|    iterations               | 31            |
|    time_elapsed             | 460           |
|    total_timesteps          | 2371584       |
| train/                      |               |
|    approx_kl                | 0.007223686   |
|    approx_ln(kl)            | -4.93039      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.971         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.931        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.394         |
|    n_updates                | 1210          |
|    policy_gradient_loss     | -0.000435     |
|    std                      | 0.98          |
|    value_loss               | 1.26          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.50047463] |
| time/                       |               |
|    fps                      | 137           |
|    iterations               | 32            |
|    time_elapsed             | 478           |
|    total_timesteps          | 2373632       |
| train/                      |               |
|    approx_kl                | 0.0073567545  |
|    approx_ln(kl)            | -4.9121366    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.96          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.09         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.124         |
|    n_updates                | 1211          |
|    policy_gradient_loss     | -0.0042       |
|    std                      | 0.98          |
|    value_loss               | 0.881         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47363135] |
| time/                       |               |
|    fps                      | 136           |
|    iterations               | 33            |
|    time_elapsed             | 494           |
|    total_timesteps          | 2375680       |
| train/                      |               |
|    approx_kl                | 0.0054182196  |
|    approx_ln(kl)            | -5.217988     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.51         |
|    ln(policy_gradient_loss) | -6.32         |
|    loss                     | 0.222         |
|    n_updates                | 1212          |
|    policy_gradient_loss     | 0.0018        |
|    std                      | 0.98          |
|    value_loss               | 0.873         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4087191] |
| time/                       |              |
|    fps                      | 137          |
|    iterations               | 34           |
|    time_elapsed             | 507          |
|    total_timesteps          | 2377728      |
| train/                      |              |
|    approx_kl                | 0.0044802283 |
|    approx_ln(kl)            | -5.408081    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.982        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.64        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.194        |
|    n_updates                | 1213         |
|    policy_gradient_loss     | -0.00306     |
|    std                      | 0.98         |
|    value_loss               | 0.779        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33481717] |
| time/                       |               |
|    fps                      | 137           |
|    iterations               | 35            |
|    time_elapsed             | 520           |
|    total_timesteps          | 2379776       |
| train/                      |               |
|    approx_kl                | 0.006996321   |
|    approx_ln(kl)            | -4.962371     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.48         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.227         |
|    n_updates                | 1214          |
|    policy_gradient_loss     | -0.00872      |
|    std                      | 0.98          |
|    value_loss               | 0.62          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.43813396] |
| time/                       |               |
|    fps                      | 137           |
|    iterations               | 36            |
|    time_elapsed             | 536           |
|    total_timesteps          | 2381824       |
| train/                      |               |
|    approx_kl                | 0.008907352   |
|    approx_ln(kl)            | -4.720878     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.925         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.5          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.223         |
|    n_updates                | 1215          |
|    policy_gradient_loss     | -0.00576      |
|    std                      | 0.98          |
|    value_loss               | 0.961         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33361745] |
| time/                       |               |
|    fps                      | 136           |
|    iterations               | 37            |
|    time_elapsed             | 554           |
|    total_timesteps          | 2383872       |
| train/                      |               |
|    approx_kl                | 0.0060852715  |
|    approx_ln(kl)            | -5.101884     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.938         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.913         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 2.49          |
|    n_updates                | 1216          |
|    policy_gradient_loss     | -0.00131      |
|    std                      | 0.979         |
|    value_loss               | 4.29          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4808559] |
| time/                       |              |
|    fps                      | 136          |
|    iterations               | 38           |
|    time_elapsed             | 571          |
|    total_timesteps          | 2385920      |
| train/                      |              |
|    approx_kl                | 0.007325271  |
|    approx_ln(kl)            | -4.916425    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.78        |
|    explained_variance       | 0.924        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.815       |
|    ln(policy_gradient_loss) | -4.96        |
|    loss                     | 0.442        |
|    n_updates                | 1217         |
|    policy_gradient_loss     | 0.00698      |
|    std                      | 0.977        |
|    value_loss               | 1.98         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.54162604] |
| time/                       |               |
|    fps                      | 135           |
|    iterations               | 39            |
|    time_elapsed             | 588           |
|    total_timesteps          | 2387968       |
| train/                      |               |
|    approx_kl                | 0.006292452   |
|    approx_ln(kl)            | -5.0684047    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.969         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.612        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.542         |
|    n_updates                | 1218          |
|    policy_gradient_loss     | -0.00457      |
|    std                      | 0.977         |
|    value_loss               | 2.31          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.38963994] |
| time/                       |               |
|    fps                      | 135           |
|    iterations               | 40            |
|    time_elapsed             | 605           |
|    total_timesteps          | 2390016       |
| train/                      |               |
|    approx_kl                | 0.007898039   |
|    approx_ln(kl)            | -4.8411407    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0628       |
|    ln(policy_gradient_loss) | -5.85         |
|    loss                     | 0.939         |
|    n_updates                | 1219          |
|    policy_gradient_loss     | 0.00289       |
|    std                      | 0.978         |
|    value_loss               | 2.17          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4091092] |
| time/                       |              |
|    fps                      | 135          |
|    iterations               | 41           |
|    time_elapsed             | 621          |
|    total_timesteps          | 2392064      |
| train/                      |              |
|    approx_kl                | 0.0045162234 |
|    approx_ln(kl)            | -5.4000793   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.78        |
|    explained_variance       | 0.965        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.35        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.705        |
|    n_updates                | 1220         |
|    policy_gradient_loss     | -0.00213     |
|    std                      | 0.977        |
|    value_loss               | 1.5          |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47940516] |
| time/                       |               |
|    fps                      | 134           |
|    iterations               | 42            |
|    time_elapsed             | 637           |
|    total_timesteps          | 2394112       |
| train/                      |               |
|    approx_kl                | 0.0055795857  |
|    approx_ln(kl)            | -5.1886406    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.946         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.427        |
|    ln(policy_gradient_loss) | -5.48         |
|    loss                     | 0.653         |
|    n_updates                | 1221          |
|    policy_gradient_loss     | 0.00417       |
|    std                      | 0.976         |
|    value_loss               | 2.38          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3430541] |
| time/                       |              |
|    fps                      | 134          |
|    iterations               | 43           |
|    time_elapsed             | 653          |
|    total_timesteps          | 2396160      |
| train/                      |              |
|    approx_kl                | 0.0056798193 |
|    approx_ln(kl)            | -5.170836    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.78        |
|    explained_variance       | 0.962        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.616        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.85         |
|    n_updates                | 1222         |
|    policy_gradient_loss     | -0.0114      |
|    std                      | 0.975        |
|    value_loss               | 2.7          |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32652235] |
| time/                       |               |
|    fps                      | 134           |
|    iterations               | 44            |
|    time_elapsed             | 669           |
|    total_timesteps          | 2398208       |
| train/                      |               |
|    approx_kl                | 0.008101573   |
|    approx_ln(kl)            | -4.815697     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.939         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.437        |
|    ln(policy_gradient_loss) | -5            |
|    loss                     | 0.646         |
|    n_updates                | 1223          |
|    policy_gradient_loss     | 0.00672       |
|    std                      | 0.973         |
|    value_loss               | 3.36          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2857506] |
| time/                       |              |
|    fps                      | 134          |
|    iterations               | 45           |
|    time_elapsed             | 686          |
|    total_timesteps          | 2400256      |
| train/                      |              |
|    approx_kl                | 0.006634837  |
|    approx_ln(kl)            | -5.0154214   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.948        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.849       |
|    ln(policy_gradient_loss) | -5.52        |
|    loss                     | 0.428        |
|    n_updates                | 1224         |
|    policy_gradient_loss     | 0.00401      |
|    std                      | 0.971        |
|    value_loss               | 2.38         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.44034413] |
| time/                       |               |
|    fps                      | 134           |
|    iterations               | 46            |
|    time_elapsed             | 701           |
|    total_timesteps          | 2402304       |
| train/                      |               |
|    approx_kl                | 0.010932533   |
|    approx_ln(kl)            | -4.516012     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.976         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.197        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.821         |
|    n_updates                | 1225          |
|    policy_gradient_loss     | -0.00493      |
|    std                      | 0.971         |
|    value_loss               | 1.71          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43535396] |
| time/                       |               |
|    fps                      | 134           |
|    iterations               | 47            |
|    time_elapsed             | 717           |
|    total_timesteps          | 2404352       |
| train/                      |               |
|    approx_kl                | 0.0064738346  |
|    approx_ln(kl)            | -5.0399866    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.968         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.893        |
|    ln(policy_gradient_loss) | -5.68         |
|    loss                     | 0.409         |
|    n_updates                | 1226          |
|    policy_gradient_loss     | 0.00343       |
|    std                      | 0.971         |
|    value_loss               | 1.35          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33580446] |
| time/                       |               |
|    fps                      | 134           |
|    iterations               | 48            |
|    time_elapsed             | 733           |
|    total_timesteps          | 2406400       |
| train/                      |               |
|    approx_kl                | 0.004179106   |
|    approx_ln(kl)            | -5.477658     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.27         |
|    ln(policy_gradient_loss) | -7.23         |
|    loss                     | 0.28          |
|    n_updates                | 1227          |
|    policy_gradient_loss     | 0.000728      |
|    std                      | 0.973         |
|    value_loss               | 0.446         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24290538] |
| time/                       |               |
|    fps                      | 133           |
|    iterations               | 49            |
|    time_elapsed             | 750           |
|    total_timesteps          | 2408448       |
| train/                      |               |
|    approx_kl                | 0.006796093   |
|    approx_ln(kl)            | -4.9914074    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.962         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.05         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.349         |
|    n_updates                | 1228          |
|    policy_gradient_loss     | -0.00254      |
|    std                      | 0.973         |
|    value_loss               | 1.94          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-------------------------------------
| reward             | [-0.5367171] |
| time/              |              |
|    fps             | 124          |
|    iterations      | 1            |
|    time_elapsed    | 16           |
|    total_timesteps | 2410496      |
-------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.32379434] |
| time/                       |               |
|    fps                      | 125           |
|    iterations               | 2             |
|    time_elapsed             | 32            |
|    total_timesteps          | 2412544       |
| train/                      |               |
|    approx_kl                | 0.011577637   |
|    approx_ln(kl)            | -4.4586797    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.47         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0844        |
|    n_updates                | 1230          |
|    policy_gradient_loss     | -9.79e-05     |
|    std                      | 0.97          |
|    value_loss               | 0.228         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.20881264] |
| time/                       |               |
|    fps                      | 126           |
|    iterations               | 3             |
|    time_elapsed             | 48            |
|    total_timesteps          | 2414592       |
| train/                      |               |
|    approx_kl                | 0.005403136   |
|    approx_ln(kl)            | -5.2207756    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.84         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.158         |
|    n_updates                | 1231          |
|    policy_gradient_loss     | -0.00171      |
|    std                      | 0.968         |
|    value_loss               | 0.496         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23942842] |
| time/                       |               |
|    fps                      | 126           |
|    iterations               | 4             |
|    time_elapsed             | 64            |
|    total_timesteps          | 2416640       |
| train/                      |               |
|    approx_kl                | 0.006059578   |
|    approx_ln(kl)            | -5.1061153    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.03         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.132         |
|    n_updates                | 1232          |
|    policy_gradient_loss     | -0.000375     |
|    std                      | 0.967         |
|    value_loss               | 0.435         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29755607] |
| time/                       |               |
|    fps                      | 123           |
|    iterations               | 5             |
|    time_elapsed             | 83            |
|    total_timesteps          | 2418688       |
| train/                      |               |
|    approx_kl                | 0.0064437217  |
|    approx_ln(kl)            | -5.044649     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.966         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.05          |
|    ln(policy_gradient_loss) | -4.91         |
|    loss                     | 2.86          |
|    n_updates                | 1233          |
|    policy_gradient_loss     | 0.00738       |
|    std                      | 0.966         |
|    value_loss               | 4.13          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30179483] |
| time/                       |               |
|    fps                      | 125           |
|    iterations               | 6             |
|    time_elapsed             | 98            |
|    total_timesteps          | 2420736       |
| train/                      |               |
|    approx_kl                | 0.00683757    |
|    approx_ln(kl)            | -4.985323     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.67         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.188         |
|    n_updates                | 1234          |
|    policy_gradient_loss     | -0.000855     |
|    std                      | 0.965         |
|    value_loss               | 0.464         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5082204] |
| time/                       |              |
|    fps                      | 126          |
|    iterations               | 7            |
|    time_elapsed             | 113          |
|    total_timesteps          | 2422784      |
| train/                      |              |
|    approx_kl                | 0.006632965  |
|    approx_ln(kl)            | -5.015703    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.76        |
|    explained_variance       | 0.934        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.569        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.77         |
|    n_updates                | 1235         |
|    policy_gradient_loss     | -0.000353    |
|    std                      | 0.964        |
|    value_loss               | 2.85         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22752917] |
| time/                       |               |
|    fps                      | 129           |
|    iterations               | 8             |
|    time_elapsed             | 126           |
|    total_timesteps          | 2424832       |
| train/                      |               |
|    approx_kl                | 0.006377706   |
|    approx_ln(kl)            | -5.054947     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.46         |
|    ln(policy_gradient_loss) | -5.21         |
|    loss                     | 0.231         |
|    n_updates                | 1236          |
|    policy_gradient_loss     | 0.00545       |
|    std                      | 0.965         |
|    value_loss               | 0.514         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2882449] |
| time/                       |              |
|    fps                      | 132          |
|    iterations               | 9            |
|    time_elapsed             | 139          |
|    total_timesteps          | 2426880      |
| train/                      |              |
|    approx_kl                | 0.0059225946 |
|    approx_ln(kl)            | -5.1289806   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.76        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.49        |
|    ln(policy_gradient_loss) | -5.61        |
|    loss                     | 0.0825       |
|    n_updates                | 1237         |
|    policy_gradient_loss     | 0.00366      |
|    std                      | 0.964        |
|    value_loss               | 0.148        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26324177] |
| time/                       |               |
|    fps                      | 134           |
|    iterations               | 10            |
|    time_elapsed             | 152           |
|    total_timesteps          | 2428928       |
| train/                      |               |
|    approx_kl                | 0.007769794   |
|    approx_ln(kl)            | -4.8575115    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.683        |
|    ln(policy_gradient_loss) | -4.99         |
|    loss                     | 0.505         |
|    n_updates                | 1238          |
|    policy_gradient_loss     | 0.0068        |
|    std                      | 0.961         |
|    value_loss               | 2.38          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37563732] |
| time/                       |               |
|    fps                      | 135           |
|    iterations               | 11            |
|    time_elapsed             | 165           |
|    total_timesteps          | 2430976       |
| train/                      |               |
|    approx_kl                | 0.006001003   |
|    approx_ln(kl)            | -5.1158285    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.96          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.461        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.63          |
|    n_updates                | 1239          |
|    policy_gradient_loss     | -0.0054       |
|    std                      | 0.96          |
|    value_loss               | 2.61          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3850153] |
| time/                       |              |
|    fps                      | 136          |
|    iterations               | 12           |
|    time_elapsed             | 179          |
|    total_timesteps          | 2433024      |
| train/                      |              |
|    approx_kl                | 0.006124856  |
|    approx_ln(kl)            | -5.0954      |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.74        |
|    explained_variance       | 0.879        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.53         |
|    ln(policy_gradient_loss) | -7.35        |
|    loss                     | 4.6          |
|    n_updates                | 1240         |
|    policy_gradient_loss     | 0.00064      |
|    std                      | 0.958        |
|    value_loss               | 5.44         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44036108] |
| time/                       |               |
|    fps                      | 137           |
|    iterations               | 13            |
|    time_elapsed             | 193           |
|    total_timesteps          | 2435072       |
| train/                      |               |
|    approx_kl                | 0.0034979421  |
|    approx_ln(kl)            | -5.6555805    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.913         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.657         |
|    ln(policy_gradient_loss) | -8.25         |
|    loss                     | 1.93          |
|    n_updates                | 1241          |
|    policy_gradient_loss     | 0.000262      |
|    std                      | 0.956         |
|    value_loss               | 3.32          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.18722863] |
| time/                       |               |
|    fps                      | 138           |
|    iterations               | 14            |
|    time_elapsed             | 206           |
|    total_timesteps          | 2437120       |
| train/                      |               |
|    approx_kl                | 0.005787387   |
|    approx_ln(kl)            | -5.1520743    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.968         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.996        |
|    ln(policy_gradient_loss) | -5.85         |
|    loss                     | 0.369         |
|    n_updates                | 1242          |
|    policy_gradient_loss     | 0.00287       |
|    std                      | 0.957         |
|    value_loss               | 1.65          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.51243705] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 15            |
|    time_elapsed             | 220           |
|    total_timesteps          | 2439168       |
| train/                      |               |
|    approx_kl                | 0.0067941346  |
|    approx_ln(kl)            | -4.9916954    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.965         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.332        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.718         |
|    n_updates                | 1243          |
|    policy_gradient_loss     | -0.000823     |
|    std                      | 0.956         |
|    value_loss               | 2.34          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25335968] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 16            |
|    time_elapsed             | 234           |
|    total_timesteps          | 2441216       |
| train/                      |               |
|    approx_kl                | 0.007865316   |
|    approx_ln(kl)            | -4.8452926    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.971         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.552         |
|    ln(policy_gradient_loss) | -4.09         |
|    loss                     | 1.74          |
|    n_updates                | 1244          |
|    policy_gradient_loss     | 0.0167        |
|    std                      | 0.956         |
|    value_loss               | 1.78          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38828188] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 17            |
|    time_elapsed             | 247           |
|    total_timesteps          | 2443264       |
| train/                      |               |
|    approx_kl                | 0.0069343555  |
|    approx_ln(kl)            | -4.971267     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.954         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.503         |
|    ln(policy_gradient_loss) | -6.21         |
|    loss                     | 1.65          |
|    n_updates                | 1245          |
|    policy_gradient_loss     | 0.00201       |
|    std                      | 0.957         |
|    value_loss               | 3.29          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.20792277] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 18            |
|    time_elapsed             | 262           |
|    total_timesteps          | 2445312       |
| train/                      |               |
|    approx_kl                | 0.004219042   |
|    approx_ln(kl)            | -5.4681473    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.522         |
|    ln(policy_gradient_loss) | -7.71         |
|    loss                     | 1.68          |
|    n_updates                | 1246          |
|    policy_gradient_loss     | 0.000449      |
|    std                      | 0.958         |
|    value_loss               | 1.82          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.46658128] |
| time/                       |               |
|    fps                      | 135           |
|    iterations               | 19            |
|    time_elapsed             | 287           |
|    total_timesteps          | 2447360       |
| train/                      |               |
|    approx_kl                | 0.006871488   |
|    approx_ln(kl)            | -4.980375     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.957         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.85         |
|    ln(policy_gradient_loss) | -7.23         |
|    loss                     | 0.428         |
|    n_updates                | 1247          |
|    policy_gradient_loss     | 0.000728      |
|    std                      | 0.958         |
|    value_loss               | 2.42          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3123383] |
| time/                       |              |
|    fps                      | 134          |
|    iterations               | 20           |
|    time_elapsed             | 305          |
|    total_timesteps          | 2449408      |
| train/                      |              |
|    approx_kl                | 0.0057840124 |
|    approx_ln(kl)            | -5.1526575   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.74        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.45        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0863       |
|    n_updates                | 1248         |
|    policy_gradient_loss     | -0.0011      |
|    std                      | 0.956        |
|    value_loss               | 0.614        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.19671927] |
| time/                       |               |
|    fps                      | 133           |
|    iterations               | 21            |
|    time_elapsed             | 323           |
|    total_timesteps          | 2451456       |
| train/                      |               |
|    approx_kl                | 0.0064710393  |
|    approx_ln(kl)            | -5.0404186    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.105        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.9           |
|    n_updates                | 1249          |
|    policy_gradient_loss     | -0.00766      |
|    std                      | 0.954         |
|    value_loss               | 0.941         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2835572] |
| time/                       |              |
|    fps                      | 132          |
|    iterations               | 22           |
|    time_elapsed             | 340          |
|    total_timesteps          | 2453504      |
| train/                      |              |
|    approx_kl                | 0.0070201145 |
|    approx_ln(kl)            | -4.958976    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.73        |
|    explained_variance       | 0.905        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.68        |
|    ln(policy_gradient_loss) | -6.84        |
|    loss                     | 0.507        |
|    n_updates                | 1250         |
|    policy_gradient_loss     | 0.00107      |
|    std                      | 0.954        |
|    value_loss               | 0.969        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.15761954] |
| time/                       |               |
|    fps                      | 131           |
|    iterations               | 23            |
|    time_elapsed             | 357           |
|    total_timesteps          | 2455552       |
| train/                      |               |
|    approx_kl                | 0.005321721   |
|    approx_ln(kl)            | -5.2359586    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.02         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.361         |
|    n_updates                | 1251          |
|    policy_gradient_loss     | -0.00504      |
|    std                      | 0.953         |
|    value_loss               | 0.465         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3224153] |
| time/                       |              |
|    fps                      | 131          |
|    iterations               | 24           |
|    time_elapsed             | 374          |
|    total_timesteps          | 2457600      |
| train/                      |              |
|    approx_kl                | 0.00543491   |
|    approx_ln(kl)            | -5.2149124   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.73        |
|    explained_variance       | 0.98         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.277        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.32         |
|    n_updates                | 1252         |
|    policy_gradient_loss     | -0.00178     |
|    std                      | 0.953        |
|    value_loss               | 1.5          |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
---------------------------------------------
| reward                      | [-0.38541]  |
| time/                       |             |
|    fps                      | 131         |
|    iterations               | 25          |
|    time_elapsed             | 388         |
|    total_timesteps          | 2459648     |
| train/                      |             |
|    approx_kl                | 0.008706604 |
|    approx_ln(kl)            | -4.7436733  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.73       |
|    explained_variance       | 0.985       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -0.887      |
|    ln(policy_gradient_loss) | -3.58       |
|    loss                     | 0.412       |
|    n_updates                | 1253        |
|    policy_gradient_loss     | 0.0279      |
|    std                      | 0.952       |
|    value_loss               | 0.846       |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.42598858] |
| time/                       |               |
|    fps                      | 131           |
|    iterations               | 26            |
|    time_elapsed             | 404           |
|    total_timesteps          | 2461696       |
| train/                      |               |
|    approx_kl                | 0.0054501295  |
|    approx_ln(kl)            | -5.212116     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.954         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.42         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.241         |
|    n_updates                | 1254          |
|    policy_gradient_loss     | -0.00593      |
|    std                      | 0.95          |
|    value_loss               | 0.535         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.503949]  |
| time/                       |              |
|    fps                      | 131          |
|    iterations               | 27           |
|    time_elapsed             | 420          |
|    total_timesteps          | 2463744      |
| train/                      |              |
|    approx_kl                | 0.0068205255 |
|    approx_ln(kl)            | -4.9878187   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.73        |
|    explained_variance       | 0.932        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.77        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.171        |
|    n_updates                | 1255         |
|    policy_gradient_loss     | -0.0012      |
|    std                      | 0.95         |
|    value_loss               | 1.08         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28301495] |
| time/                       |               |
|    fps                      | 131           |
|    iterations               | 28            |
|    time_elapsed             | 435           |
|    total_timesteps          | 2465792       |
| train/                      |               |
|    approx_kl                | 0.006573839   |
|    approx_ln(kl)            | -5.0246572    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.977         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.829        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.437         |
|    n_updates                | 1256          |
|    policy_gradient_loss     | -0.00364      |
|    std                      | 0.95          |
|    value_loss               | 1.22          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2720254] |
| time/                       |              |
|    fps                      | 132          |
|    iterations               | 29           |
|    time_elapsed             | 449          |
|    total_timesteps          | 2467840      |
| train/                      |              |
|    approx_kl                | 0.00949289   |
|    approx_ln(kl)            | -4.6572123   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.73        |
|    explained_variance       | 0.979        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.25        |
|    ln(policy_gradient_loss) | -4.92        |
|    loss                     | 0.286        |
|    n_updates                | 1257         |
|    policy_gradient_loss     | 0.0073       |
|    std                      | 0.949        |
|    value_loss               | 0.857        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.14621738] |
| time/                       |               |
|    fps                      | 132           |
|    iterations               | 30            |
|    time_elapsed             | 462           |
|    total_timesteps          | 2469888       |
| train/                      |               |
|    approx_kl                | 0.007682153   |
|    approx_ln(kl)            | -4.8688555    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.952         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.442        |
|    ln(policy_gradient_loss) | -4.99         |
|    loss                     | 0.643         |
|    n_updates                | 1258          |
|    policy_gradient_loss     | 0.00678       |
|    std                      | 0.949         |
|    value_loss               | 0.942         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.18522808] |
| time/                       |               |
|    fps                      | 133           |
|    iterations               | 31            |
|    time_elapsed             | 475           |
|    total_timesteps          | 2471936       |
| train/                      |               |
|    approx_kl                | 0.0090628015  |
|    approx_ln(kl)            | -4.703577     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.628        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.534         |
|    n_updates                | 1259          |
|    policy_gradient_loss     | -0.000398     |
|    std                      | 0.948         |
|    value_loss               | 1.67          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25399208] |
| time/                       |               |
|    fps                      | 134           |
|    iterations               | 32            |
|    time_elapsed             | 488           |
|    total_timesteps          | 2473984       |
| train/                      |               |
|    approx_kl                | 0.0067049074  |
|    approx_ln(kl)            | -5.0049157    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.47         |
|    ln(policy_gradient_loss) | -4.72         |
|    loss                     | 0.229         |
|    n_updates                | 1260          |
|    policy_gradient_loss     | 0.00891       |
|    std                      | 0.947         |
|    value_loss               | 1.26          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47424212] |
| time/                       |               |
|    fps                      | 134           |
|    iterations               | 33            |
|    time_elapsed             | 501           |
|    total_timesteps          | 2476032       |
| train/                      |               |
|    approx_kl                | 0.0073888386  |
|    approx_ln(kl)            | -4.907785     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.125        |
|    ln(policy_gradient_loss) | -6.79         |
|    loss                     | 0.882         |
|    n_updates                | 1261          |
|    policy_gradient_loss     | 0.00113       |
|    std                      | 0.946         |
|    value_loss               | 0.593         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3621314] |
| time/                       |              |
|    fps                      | 135          |
|    iterations               | 34           |
|    time_elapsed             | 515          |
|    total_timesteps          | 2478080      |
| train/                      |              |
|    approx_kl                | 0.0077066924 |
|    approx_ln(kl)            | -4.8656664   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.71        |
|    explained_variance       | 0.995        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.82        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.162        |
|    n_updates                | 1262         |
|    policy_gradient_loss     | -0.00275     |
|    std                      | 0.944        |
|    value_loss               | 0.27         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.38794148] |
| time/                       |               |
|    fps                      | 135           |
|    iterations               | 35            |
|    time_elapsed             | 528           |
|    total_timesteps          | 2480128       |
| train/                      |               |
|    approx_kl                | 0.010132058   |
|    approx_ln(kl)            | -4.592051     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.74         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.175         |
|    n_updates                | 1263          |
|    policy_gradient_loss     | -0.0022       |
|    std                      | 0.943         |
|    value_loss               | 1.38          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.40215874] |
| time/                       |               |
|    fps                      | 136           |
|    iterations               | 36            |
|    time_elapsed             | 542           |
|    total_timesteps          | 2482176       |
| train/                      |               |
|    approx_kl                | 0.0088271145  |
|    approx_ln(kl)            | -4.729927     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.06         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.347         |
|    n_updates                | 1264          |
|    policy_gradient_loss     | -0.00675      |
|    std                      | 0.942         |
|    value_loss               | 0.399         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.494288] |
| time/                       |             |
|    fps                      | 136         |
|    iterations               | 37          |
|    time_elapsed             | 555         |
|    total_timesteps          | 2484224     |
| train/                      |             |
|    approx_kl                | 0.008958797 |
|    approx_ln(kl)            | -4.7151194  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.71       |
|    explained_variance       | 0.977       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -0.533      |
|    ln(policy_gradient_loss) | -4.18       |
|    loss                     | 0.587       |
|    n_updates                | 1265        |
|    policy_gradient_loss     | 0.0153      |
|    std                      | 0.942       |
|    value_loss               | 1.31        |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28604317] |
| time/                       |               |
|    fps                      | 137           |
|    iterations               | 38            |
|    time_elapsed             | 568           |
|    total_timesteps          | 2486272       |
| train/                      |               |
|    approx_kl                | 0.0065460256  |
|    approx_ln(kl)            | -5.0288973    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.972         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.668        |
|    ln(policy_gradient_loss) | -4.9          |
|    loss                     | 0.513         |
|    n_updates                | 1266          |
|    policy_gradient_loss     | 0.00745       |
|    std                      | 0.94          |
|    value_loss               | 2.12          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34214577] |
| time/                       |               |
|    fps                      | 137           |
|    iterations               | 39            |
|    time_elapsed             | 581           |
|    total_timesteps          | 2488320       |
| train/                      |               |
|    approx_kl                | 0.006036178   |
|    approx_ln(kl)            | -5.1099844    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.97          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0265       |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.974         |
|    n_updates                | 1267          |
|    policy_gradient_loss     | -0.00136      |
|    std                      | 0.94          |
|    value_loss               | 2.71          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4041476] |
| time/                       |              |
|    fps                      | 137          |
|    iterations               | 40           |
|    time_elapsed             | 594          |
|    total_timesteps          | 2490368      |
| train/                      |              |
|    approx_kl                | 0.0082839    |
|    approx_ln(kl)            | -4.7934413   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.961        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.719       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.487        |
|    n_updates                | 1268         |
|    policy_gradient_loss     | -4.68e-05    |
|    std                      | 0.939        |
|    value_loss               | 1.15         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3716701] |
| time/                       |              |
|    fps                      | 138          |
|    iterations               | 41           |
|    time_elapsed             | 607          |
|    total_timesteps          | 2492416      |
| train/                      |              |
|    approx_kl                | 0.0058542094 |
|    approx_ln(kl)            | -5.1405945   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.935        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.648        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.91         |
|    n_updates                | 1269         |
|    policy_gradient_loss     | -0.00698     |
|    std                      | 0.939        |
|    value_loss               | 2.04         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.276632] |
| time/                       |             |
|    fps                      | 138         |
|    iterations               | 42          |
|    time_elapsed             | 621         |
|    total_timesteps          | 2494464     |
| train/                      |             |
|    approx_kl                | 0.006631614 |
|    approx_ln(kl)            | -5.0159073  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.7        |
|    explained_variance       | 0.973       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -1.01       |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.363       |
|    n_updates                | 1270        |
|    policy_gradient_loss     | -0.00527    |
|    std                      | 0.939       |
|    value_loss               | 1.91        |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3338009] |
| time/                       |              |
|    fps                      | 138          |
|    iterations               | 43           |
|    time_elapsed             | 634          |
|    total_timesteps          | 2496512      |
| train/                      |              |
|    approx_kl                | 0.005990977  |
|    approx_ln(kl)            | -5.117501    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.959        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.999        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 2.72         |
|    n_updates                | 1271         |
|    policy_gradient_loss     | -0.00114     |
|    std                      | 0.939        |
|    value_loss               | 3.46         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.26093152] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 44            |
|    time_elapsed             | 647           |
|    total_timesteps          | 2498560       |
| train/                      |               |
|    approx_kl                | 0.009466626   |
|    approx_ln(kl)            | -4.6599827    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.08         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.125         |
|    n_updates                | 1272          |
|    policy_gradient_loss     | -0.0176       |
|    std                      | 0.939         |
|    value_loss               | 1.03          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.29654458] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 45            |
|    time_elapsed             | 660           |
|    total_timesteps          | 2500608       |
| train/                      |               |
|    approx_kl                | 0.012746739   |
|    approx_ln(kl)            | -4.3624797    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.324        |
|    ln(policy_gradient_loss) | -5.55         |
|    loss                     | 0.723         |
|    n_updates                | 1273          |
|    policy_gradient_loss     | 0.0039        |
|    std                      | 0.939         |
|    value_loss               | 2.36          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.26221523] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 46            |
|    time_elapsed             | 674           |
|    total_timesteps          | 2502656       |
| train/                      |               |
|    approx_kl                | 0.011266122   |
|    approx_ln(kl)            | -4.485955     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.677        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.508         |
|    n_updates                | 1274          |
|    policy_gradient_loss     | -0.007        |
|    std                      | 0.939         |
|    value_loss               | 1.28          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.05
-----------------------------------------------
| reward                      | [-0.25573468] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 47            |
|    time_elapsed             | 688           |
|    total_timesteps          | 2504704       |
| train/                      |               |
|    approx_kl                | 0.012954808   |
|    approx_ln(kl)            | -4.346288     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.28         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.102         |
|    n_updates                | 1275          |
|    policy_gradient_loss     | -0.00358      |
|    std                      | 0.94          |
|    value_loss               | 0.35          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28137097] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 48            |
|    time_elapsed             | 701           |
|    total_timesteps          | 2506752       |
| train/                      |               |
|    approx_kl                | 0.007681717   |
|    approx_ln(kl)            | -4.868912     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.975         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.872        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.418         |
|    n_updates                | 1276          |
|    policy_gradient_loss     | -0.00482      |
|    std                      | 0.94          |
|    value_loss               | 2.02          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.21390638] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 49            |
|    time_elapsed             | 715           |
|    total_timesteps          | 2508800       |
| train/                      |               |
|    approx_kl                | 0.006554749   |
|    approx_ln(kl)            | -5.0275655    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.798        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.45          |
|    n_updates                | 1277          |
|    policy_gradient_loss     | -0.00888      |
|    std                      | 0.94          |
|    value_loss               | 1.14          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.07
--------------------------------------
| reward             | [-0.49958774] |
| time/              |               |
|    fps             | 152           |
|    iterations      | 1             |
|    time_elapsed    | 13            |
|    total_timesteps | 2510848       |
--------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26261556] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 2             |
|    time_elapsed             | 26            |
|    total_timesteps          | 2512896       |
| train/                      |               |
|    approx_kl                | 0.0054595377  |
|    approx_ln(kl)            | -5.210391     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.975         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.161         |
|    ln(policy_gradient_loss) | -5.03         |
|    loss                     | 1.17          |
|    n_updates                | 1279          |
|    policy_gradient_loss     | 0.00657       |
|    std                      | 0.94          |
|    value_loss               | 2.12          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3914795] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 3            |
|    time_elapsed             | 40           |
|    total_timesteps          | 2514944      |
| train/                      |              |
|    approx_kl                | 0.007156474  |
|    approx_ln(kl)            | -4.939738    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.993        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.168       |
|    ln(policy_gradient_loss) | -7.11        |
|    loss                     | 0.845        |
|    n_updates                | 1280         |
|    policy_gradient_loss     | 0.000816     |
|    std                      | 0.94         |
|    value_loss               | 1.11         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29025865] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 4             |
|    time_elapsed             | 54            |
|    total_timesteps          | 2516992       |
| train/                      |               |
|    approx_kl                | 0.008708571   |
|    approx_ln(kl)            | -4.743448     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.03         |
|    ln(policy_gradient_loss) | -9            |
|    loss                     | 0.131         |
|    n_updates                | 1281          |
|    policy_gradient_loss     | 0.000123      |
|    std                      | 0.941         |
|    value_loss               | 0.237         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35907868] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 5             |
|    time_elapsed             | 67            |
|    total_timesteps          | 2519040       |
| train/                      |               |
|    approx_kl                | 0.006838206   |
|    approx_ln(kl)            | -4.98523      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.494        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.61          |
|    n_updates                | 1282          |
|    policy_gradient_loss     | -0.00165      |
|    std                      | 0.942         |
|    value_loss               | 0.53          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2714262] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 6            |
|    time_elapsed             | 80           |
|    total_timesteps          | 2521088      |
| train/                      |              |
|    approx_kl                | 0.0058067627 |
|    approx_ln(kl)            | -5.148732    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.71        |
|    explained_variance       | 0.966        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.141        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.15         |
|    n_updates                | 1283         |
|    policy_gradient_loss     | -0.000419    |
|    std                      | 0.943        |
|    value_loss               | 1.55         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49699622] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 7             |
|    time_elapsed             | 94            |
|    total_timesteps          | 2523136       |
| train/                      |               |
|    approx_kl                | 0.007296138   |
|    approx_ln(kl)            | -4.92041      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.435        |
|    ln(policy_gradient_loss) | -5.19         |
|    loss                     | 0.647         |
|    n_updates                | 1284          |
|    policy_gradient_loss     | 0.00555       |
|    std                      | 0.943         |
|    value_loss               | 0.746         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35037914] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 8             |
|    time_elapsed             | 108           |
|    total_timesteps          | 2525184       |
| train/                      |               |
|    approx_kl                | 0.0067255707  |
|    approx_ln(kl)            | -5.0018387    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.881        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.414         |
|    n_updates                | 1285          |
|    policy_gradient_loss     | -0.00149      |
|    std                      | 0.942         |
|    value_loss               | 1.02          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4897898] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 9            |
|    time_elapsed             | 121          |
|    total_timesteps          | 2527232      |
| train/                      |              |
|    approx_kl                | 0.006378031  |
|    approx_ln(kl)            | -5.054896    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.71        |
|    explained_variance       | 0.983        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.385       |
|    ln(policy_gradient_loss) | -5.2         |
|    loss                     | 0.681        |
|    n_updates                | 1286         |
|    policy_gradient_loss     | 0.00549      |
|    std                      | 0.942        |
|    value_loss               | 1.11         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.29692653] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 10            |
|    time_elapsed             | 134           |
|    total_timesteps          | 2529280       |
| train/                      |               |
|    approx_kl                | 0.008501154   |
|    approx_ln(kl)            | -4.7675533    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.86         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.156         |
|    n_updates                | 1287          |
|    policy_gradient_loss     | -0.00322      |
|    std                      | 0.941         |
|    value_loss               | 0.455         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38137665] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 11            |
|    time_elapsed             | 148           |
|    total_timesteps          | 2531328       |
| train/                      |               |
|    approx_kl                | 0.008198377   |
|    approx_ln(kl)            | -4.803819     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.66         |
|    ln(policy_gradient_loss) | -5.46         |
|    loss                     | 0.19          |
|    n_updates                | 1288          |
|    policy_gradient_loss     | 0.00427       |
|    std                      | 0.941         |
|    value_loss               | 0.62          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.28738266] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 12            |
|    time_elapsed             | 162           |
|    total_timesteps          | 2533376       |
| train/                      |               |
|    approx_kl                | 0.010182211   |
|    approx_ln(kl)            | -4.587113     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.977         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.29         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.274         |
|    n_updates                | 1289          |
|    policy_gradient_loss     | -0.0101       |
|    std                      | 0.94          |
|    value_loss               | 0.562         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4718257] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 13           |
|    time_elapsed             | 175          |
|    total_timesteps          | 2535424      |
| train/                      |              |
|    approx_kl                | 0.008835242  |
|    approx_ln(kl)            | -4.729007    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.71        |
|    explained_variance       | 0.948        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.391       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.676        |
|    n_updates                | 1290         |
|    policy_gradient_loss     | -0.00159     |
|    std                      | 0.94         |
|    value_loss               | 2.22         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28710908] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 14            |
|    time_elapsed             | 188           |
|    total_timesteps          | 2537472       |
| train/                      |               |
|    approx_kl                | 0.0057955775  |
|    approx_ln(kl)            | -5.15066      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.89         |
|    ln(policy_gradient_loss) | -4.95         |
|    loss                     | 0.151         |
|    n_updates                | 1291          |
|    policy_gradient_loss     | 0.00711       |
|    std                      | 0.939         |
|    value_loss               | 0.435         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3008925] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 15           |
|    time_elapsed             | 201          |
|    total_timesteps          | 2539520      |
| train/                      |              |
|    approx_kl                | 0.00781492   |
|    approx_ln(kl)            | -4.851721    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.971        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.219        |
|    ln(policy_gradient_loss) | -5.58        |
|    loss                     | 1.24         |
|    n_updates                | 1292         |
|    policy_gradient_loss     | 0.00376      |
|    std                      | 0.939        |
|    value_loss               | 3.16         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27020237] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 16            |
|    time_elapsed             | 215           |
|    total_timesteps          | 2541568       |
| train/                      |               |
|    approx_kl                | 0.0043320833  |
|    approx_ln(kl)            | -5.4417067    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.1          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.334         |
|    n_updates                | 1293          |
|    policy_gradient_loss     | -0.00432      |
|    std                      | 0.939         |
|    value_loss               | 1.03          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28853667] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 17            |
|    time_elapsed             | 228           |
|    total_timesteps          | 2543616       |
| train/                      |               |
|    approx_kl                | 0.0046872087  |
|    approx_ln(kl)            | -5.362918     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.967         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.174        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.84          |
|    n_updates                | 1294          |
|    policy_gradient_loss     | -0.000429     |
|    std                      | 0.938         |
|    value_loss               | 1.85          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28983042] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 18            |
|    time_elapsed             | 241           |
|    total_timesteps          | 2545664       |
| train/                      |               |
|    approx_kl                | 0.0057897917  |
|    approx_ln(kl)            | -5.151659     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.48         |
|    ln(policy_gradient_loss) | -4.49         |
|    loss                     | 0.228         |
|    n_updates                | 1295          |
|    policy_gradient_loss     | 0.0113        |
|    std                      | 0.938         |
|    value_loss               | 0.245         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27848628] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 19            |
|    time_elapsed             | 255           |
|    total_timesteps          | 2547712       |
| train/                      |               |
|    approx_kl                | 0.006949281   |
|    approx_ln(kl)            | -4.969117     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.962         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.2          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.302         |
|    n_updates                | 1296          |
|    policy_gradient_loss     | -0.0111       |
|    std                      | 0.937         |
|    value_loss               | 1.34          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24722634] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 20            |
|    time_elapsed             | 268           |
|    total_timesteps          | 2549760       |
| train/                      |               |
|    approx_kl                | 0.0068163676  |
|    approx_ln(kl)            | -4.9884286    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.88         |
|    ln(policy_gradient_loss) | -7.79         |
|    loss                     | 0.152         |
|    n_updates                | 1297          |
|    policy_gradient_loss     | 0.000415      |
|    std                      | 0.938         |
|    value_loss               | 0.442         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35336298] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 21            |
|    time_elapsed             | 284           |
|    total_timesteps          | 2551808       |
| train/                      |               |
|    approx_kl                | 0.0067058844  |
|    approx_ln(kl)            | -5.00477      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.37         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.253         |
|    n_updates                | 1298          |
|    policy_gradient_loss     | -0.00878      |
|    std                      | 0.938         |
|    value_loss               | 0.943         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.33848783] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 22            |
|    time_elapsed             | 302           |
|    total_timesteps          | 2553856       |
| train/                      |               |
|    approx_kl                | 0.010120081   |
|    approx_ln(kl)            | -4.5932336    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.32         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0361        |
|    n_updates                | 1299          |
|    policy_gradient_loss     | -0.0213       |
|    std                      | 0.938         |
|    value_loss               | 0.206         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.24173404] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 23            |
|    time_elapsed             | 320           |
|    total_timesteps          | 2555904       |
| train/                      |               |
|    approx_kl                | 0.012308203   |
|    approx_ln(kl)            | -4.3974895    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2            |
|    ln(policy_gradient_loss) | -5.51         |
|    loss                     | 0.135         |
|    n_updates                | 1300          |
|    policy_gradient_loss     | 0.00406       |
|    std                      | 0.938         |
|    value_loss               | 0.252         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49374166] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 24            |
|    time_elapsed             | 338           |
|    total_timesteps          | 2557952       |
| train/                      |               |
|    approx_kl                | 0.0072322013  |
|    approx_ln(kl)            | -4.9292116    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.51         |
|    ln(policy_gradient_loss) | -5.18         |
|    loss                     | 0.221         |
|    n_updates                | 1301          |
|    policy_gradient_loss     | 0.00566       |
|    std                      | 0.938         |
|    value_loss               | 0.4           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32108504] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 25            |
|    time_elapsed             | 355           |
|    total_timesteps          | 2560000       |
| train/                      |               |
|    approx_kl                | 0.0074864845  |
|    approx_ln(kl)            | -4.894656     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.997        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.369         |
|    n_updates                | 1302          |
|    policy_gradient_loss     | -0.0106       |
|    std                      | 0.938         |
|    value_loss               | 1.74          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25723597] |
| time/                       |               |
|    fps                      | 142           |
|    iterations               | 26            |
|    time_elapsed             | 372           |
|    total_timesteps          | 2562048       |
| train/                      |               |
|    approx_kl                | 0.0057541244  |
|    approx_ln(kl)            | -5.1578383    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.09         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.123         |
|    n_updates                | 1303          |
|    policy_gradient_loss     | -0.00321      |
|    std                      | 0.941         |
|    value_loss               | 0.756         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4416586] |
| time/                       |              |
|    fps                      | 141          |
|    iterations               | 27           |
|    time_elapsed             | 390          |
|    total_timesteps          | 2564096      |
| train/                      |              |
|    approx_kl                | 0.0051908935 |
|    approx_ln(kl)            | -5.2608495   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.71        |
|    explained_variance       | 0.982        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.165        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.18         |
|    n_updates                | 1304         |
|    policy_gradient_loss     | -0.00324     |
|    std                      | 0.941        |
|    value_loss               | 1.78         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.44540915] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 28            |
|    time_elapsed             | 407           |
|    total_timesteps          | 2566144       |
| train/                      |               |
|    approx_kl                | 0.0072025266  |
|    approx_ln(kl)            | -4.9333234    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.469        |
|    ln(policy_gradient_loss) | -5.45         |
|    loss                     | 0.626         |
|    n_updates                | 1305          |
|    policy_gradient_loss     | 0.0043        |
|    std                      | 0.941         |
|    value_loss               | 1.13          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.27394843] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 29            |
|    time_elapsed             | 424           |
|    total_timesteps          | 2568192       |
| train/                      |               |
|    approx_kl                | 0.008751816   |
|    approx_ln(kl)            | -4.738494     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.0825        |
|    ln(policy_gradient_loss) | -4.99         |
|    loss                     | 1.09          |
|    n_updates                | 1306          |
|    policy_gradient_loss     | 0.00682       |
|    std                      | 0.94          |
|    value_loss               | 1.73          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2571146] |
| time/                       |              |
|    fps                      | 139          |
|    iterations               | 30           |
|    time_elapsed             | 442          |
|    total_timesteps          | 2570240      |
| train/                      |              |
|    approx_kl                | 0.007227409  |
|    approx_ln(kl)            | -4.9298744   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.979        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.238        |
|    ln(policy_gradient_loss) | -7.9         |
|    loss                     | 1.27         |
|    n_updates                | 1307         |
|    policy_gradient_loss     | 0.000371     |
|    std                      | 0.939        |
|    value_loss               | 1.8          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27003768] |
| time/                       |               |
|    fps                      | 137           |
|    iterations               | 31            |
|    time_elapsed             | 460           |
|    total_timesteps          | 2572288       |
| train/                      |               |
|    approx_kl                | 0.0037934706  |
|    approx_ln(kl)            | -5.574474     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.669        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.512         |
|    n_updates                | 1308          |
|    policy_gradient_loss     | -0.0049       |
|    std                      | 0.939         |
|    value_loss               | 1.34          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2738361] |
| time/                       |              |
|    fps                      | 137          |
|    iterations               | 32           |
|    time_elapsed             | 477          |
|    total_timesteps          | 2574336      |
| train/                      |              |
|    approx_kl                | 0.0062395954 |
|    approx_ln(kl)            | -5.07684     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.983        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.369       |
|    ln(policy_gradient_loss) | -5.6         |
|    loss                     | 0.692        |
|    n_updates                | 1309         |
|    policy_gradient_loss     | 0.00371      |
|    std                      | 0.939        |
|    value_loss               | 1.49         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3515309] |
| time/                       |              |
|    fps                      | 136          |
|    iterations               | 33           |
|    time_elapsed             | 496          |
|    total_timesteps          | 2576384      |
| train/                      |              |
|    approx_kl                | 0.0075697    |
|    approx_ln(kl)            | -4.8836017   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.991        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.0289       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.03         |
|    n_updates                | 1310         |
|    policy_gradient_loss     | -0.00751     |
|    std                      | 0.938        |
|    value_loss               | 1.5          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2993277] |
| time/                       |              |
|    fps                      | 136          |
|    iterations               | 34           |
|    time_elapsed             | 511          |
|    total_timesteps          | 2578432      |
| train/                      |              |
|    approx_kl                | 0.0075532338 |
|    approx_ln(kl)            | -4.8857794   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.891       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.41         |
|    n_updates                | 1311         |
|    policy_gradient_loss     | -0.00446     |
|    std                      | 0.938        |
|    value_loss               | 0.911        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.19324118] |
| time/                       |               |
|    fps                      | 136           |
|    iterations               | 35            |
|    time_elapsed             | 524           |
|    total_timesteps          | 2580480       |
| train/                      |               |
|    approx_kl                | 0.0057375133  |
|    approx_ln(kl)            | -5.1607294    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.597        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.551         |
|    n_updates                | 1312          |
|    policy_gradient_loss     | -0.00286      |
|    std                      | 0.938         |
|    value_loss               | 0.972         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2486031] |
| time/                       |              |
|    fps                      | 136          |
|    iterations               | 36           |
|    time_elapsed             | 538          |
|    total_timesteps          | 2582528      |
| train/                      |              |
|    approx_kl                | 0.0067623993 |
|    approx_ln(kl)            | -4.9963775   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.936        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.477        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.61         |
|    n_updates                | 1313         |
|    policy_gradient_loss     | -0.00116     |
|    std                      | 0.938        |
|    value_loss               | 3.23         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2667073] |
| time/                       |              |
|    fps                      | 137          |
|    iterations               | 37           |
|    time_elapsed             | 552          |
|    total_timesteps          | 2584576      |
| train/                      |              |
|    approx_kl                | 0.006137944  |
|    approx_ln(kl)            | -5.0932655   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.976        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.538        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.71         |
|    n_updates                | 1314         |
|    policy_gradient_loss     | -0.00352     |
|    std                      | 0.938        |
|    value_loss               | 2.44         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22410306] |
| time/                       |               |
|    fps                      | 137           |
|    iterations               | 38            |
|    time_elapsed             | 566           |
|    total_timesteps          | 2586624       |
| train/                      |               |
|    approx_kl                | 0.0068324106  |
|    approx_ln(kl)            | -4.986078     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.18         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.113         |
|    n_updates                | 1315          |
|    policy_gradient_loss     | -0.00332      |
|    std                      | 0.938         |
|    value_loss               | 0.319         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32797685] |
| time/                       |               |
|    fps                      | 137           |
|    iterations               | 39            |
|    time_elapsed             | 580           |
|    total_timesteps          | 2588672       |
| train/                      |               |
|    approx_kl                | 0.0056551085  |
|    approx_ln(kl)            | -5.175196     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.153        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.858         |
|    n_updates                | 1316          |
|    policy_gradient_loss     | -0.00805      |
|    std                      | 0.938         |
|    value_loss               | 1.13          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3077596] |
| time/                       |              |
|    fps                      | 137          |
|    iterations               | 40           |
|    time_elapsed             | 593          |
|    total_timesteps          | 2590720      |
| train/                      |              |
|    approx_kl                | 0.006984074  |
|    approx_ln(kl)            | -4.964123    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.985        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.67        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0692       |
|    n_updates                | 1317         |
|    policy_gradient_loss     | -0.0228      |
|    std                      | 0.938        |
|    value_loss               | 0.547        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.14553769] |
| time/                       |               |
|    fps                      | 138           |
|    iterations               | 41            |
|    time_elapsed             | 606           |
|    total_timesteps          | 2592768       |
| train/                      |               |
|    approx_kl                | 0.009159925   |
|    approx_ln(kl)            | -4.6929173    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.132        |
|    ln(policy_gradient_loss) | -9.51         |
|    loss                     | 0.877         |
|    n_updates                | 1318          |
|    policy_gradient_loss     | 7.41e-05      |
|    std                      | 0.938         |
|    value_loss               | 2.34          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29464293] |
| time/                       |               |
|    fps                      | 138           |
|    iterations               | 42            |
|    time_elapsed             | 620           |
|    total_timesteps          | 2594816       |
| train/                      |               |
|    approx_kl                | 0.0071347947  |
|    approx_ln(kl)            | -4.942772     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.352        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.703         |
|    n_updates                | 1319          |
|    policy_gradient_loss     | -0.00557      |
|    std                      | 0.938         |
|    value_loss               | 1.25          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.5800256] |
| time/                       |              |
|    fps                      | 138          |
|    iterations               | 43           |
|    time_elapsed             | 633          |
|    total_timesteps          | 2596864      |
| train/                      |              |
|    approx_kl                | 0.010075355  |
|    approx_ln(kl)            | -4.597663    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.977        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.316       |
|    ln(policy_gradient_loss) | -4.78        |
|    loss                     | 0.729        |
|    n_updates                | 1320         |
|    policy_gradient_loss     | 0.00838      |
|    std                      | 0.938        |
|    value_loss               | 1.89         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.33584675] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 44            |
|    time_elapsed             | 646           |
|    total_timesteps          | 2598912       |
| train/                      |               |
|    approx_kl                | 0.007938683   |
|    approx_ln(kl)            | -4.836008     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.475        |
|    ln(policy_gradient_loss) | -5.04         |
|    loss                     | 0.622         |
|    n_updates                | 1321          |
|    policy_gradient_loss     | 0.00651       |
|    std                      | 0.938         |
|    value_loss               | 1.36          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.21682234] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 45            |
|    time_elapsed             | 660           |
|    total_timesteps          | 2600960       |
| train/                      |               |
|    approx_kl                | 0.007375102   |
|    approx_ln(kl)            | -4.9096456    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.895        |
|    ln(policy_gradient_loss) | -4.2          |
|    loss                     | 0.408         |
|    n_updates                | 1322          |
|    policy_gradient_loss     | 0.0149        |
|    std                      | 0.939         |
|    value_loss               | 1.4           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2639363] |
| time/                       |              |
|    fps                      | 139          |
|    iterations               | 46           |
|    time_elapsed             | 674          |
|    total_timesteps          | 2603008      |
| train/                      |              |
|    approx_kl                | 0.005674226  |
|    approx_ln(kl)            | -5.171821    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.987        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.12        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.326        |
|    n_updates                | 1323         |
|    policy_gradient_loss     | -0.0078      |
|    std                      | 0.94         |
|    value_loss               | 1.14         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.19484389] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 47            |
|    time_elapsed             | 687           |
|    total_timesteps          | 2605056       |
| train/                      |               |
|    approx_kl                | 0.0078737745  |
|    approx_ln(kl)            | -4.844218     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.971         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.495         |
|    ln(policy_gradient_loss) | -4.86         |
|    loss                     | 1.64          |
|    n_updates                | 1324          |
|    policy_gradient_loss     | 0.00774       |
|    std                      | 0.94          |
|    value_loss               | 2.49          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45320213] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 48            |
|    time_elapsed             | 700           |
|    total_timesteps          | 2607104       |
| train/                      |               |
|    approx_kl                | 0.006263186   |
|    approx_ln(kl)            | -5.073066     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.975         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.113         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.12          |
|    n_updates                | 1325          |
|    policy_gradient_loss     | -0.00233      |
|    std                      | 0.94          |
|    value_loss               | 1.82          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39146423] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 49            |
|    time_elapsed             | 713           |
|    total_timesteps          | 2609152       |
| train/                      |               |
|    approx_kl                | 0.0074381176  |
|    approx_ln(kl)            | -4.9011374    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.924         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.523        |
|    ln(policy_gradient_loss) | -6.18         |
|    loss                     | 0.593         |
|    n_updates                | 1326          |
|    policy_gradient_loss     | 0.00206       |
|    std                      | 0.94          |
|    value_loss               | 2.8           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------
| reward             | [-0.2408825] |
| time/              |              |
|    fps             | 158          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 2611200      |
-------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.19053112] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 2             |
|    time_elapsed             | 26            |
|    total_timesteps          | 2613248       |
| train/                      |               |
|    approx_kl                | 0.005830647   |
|    approx_ln(kl)            | -5.1446276    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.04         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.13          |
|    n_updates                | 1328          |
|    policy_gradient_loss     | -0.0108       |
|    std                      | 0.939         |
|    value_loss               | 0.52          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31857488] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 3             |
|    time_elapsed             | 40            |
|    total_timesteps          | 2615296       |
| train/                      |               |
|    approx_kl                | 0.006499055   |
|    approx_ln(kl)            | -5.0360985    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.963         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.0225        |
|    ln(policy_gradient_loss) | -8.43         |
|    loss                     | 1.02          |
|    n_updates                | 1329          |
|    policy_gradient_loss     | 0.000218      |
|    std                      | 0.94          |
|    value_loss               | 1.82          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30664966] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 4             |
|    time_elapsed             | 53            |
|    total_timesteps          | 2617344       |
| train/                      |               |
|    approx_kl                | 0.008274419   |
|    approx_ln(kl)            | -4.7945867    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.945         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.465        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.628         |
|    n_updates                | 1330          |
|    policy_gradient_loss     | -0.00212      |
|    std                      | 0.94          |
|    value_loss               | 1.57          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43925974] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 5             |
|    time_elapsed             | 67            |
|    total_timesteps          | 2619392       |
| train/                      |               |
|    approx_kl                | 0.007799175   |
|    approx_ln(kl)            | -4.8537374    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.933         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.987         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 2.68          |
|    n_updates                | 1331          |
|    policy_gradient_loss     | -0.00178      |
|    std                      | 0.94          |
|    value_loss               | 1.45          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27339646] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 6             |
|    time_elapsed             | 80            |
|    total_timesteps          | 2621440       |
| train/                      |               |
|    approx_kl                | 0.007853881   |
|    approx_ln(kl)            | -4.8467474    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.909         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.54          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 4.66          |
|    n_updates                | 1332          |
|    policy_gradient_loss     | -0.0017       |
|    std                      | 0.94          |
|    value_loss               | 5.84          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.19970074] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 7             |
|    time_elapsed             | 94            |
|    total_timesteps          | 2623488       |
| train/                      |               |
|    approx_kl                | 0.0034328355  |
|    approx_ln(kl)            | -5.674369     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.94          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.214        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.808         |
|    n_updates                | 1333          |
|    policy_gradient_loss     | -0.0043       |
|    std                      | 0.939         |
|    value_loss               | 1.88          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.39232722] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 8             |
|    time_elapsed             | 112           |
|    total_timesteps          | 2625536       |
| train/                      |               |
|    approx_kl                | 0.00682275    |
|    approx_ln(kl)            | -4.9874926    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.933         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.166         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.18          |
|    n_updates                | 1334          |
|    policy_gradient_loss     | -0.008        |
|    std                      | 0.939         |
|    value_loss               | 2.36          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24010085] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 9             |
|    time_elapsed             | 128           |
|    total_timesteps          | 2627584       |
| train/                      |               |
|    approx_kl                | 0.0057114162  |
|    approx_ln(kl)            | -5.1652884    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.962         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.834         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 2.3           |
|    n_updates                | 1335          |
|    policy_gradient_loss     | -0.00208      |
|    std                      | 0.939         |
|    value_loss               | 2.23          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3523596] |
| time/                       |              |
|    fps                      | 141          |
|    iterations               | 10           |
|    time_elapsed             | 144          |
|    total_timesteps          | 2629632      |
| train/                      |              |
|    approx_kl                | 0.0074861622 |
|    approx_ln(kl)            | -4.894699    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.979        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.615       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.541        |
|    n_updates                | 1336         |
|    policy_gradient_loss     | -0.00292     |
|    std                      | 0.939        |
|    value_loss               | 2.09         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.4703492] |
| time/                       |              |
|    fps                      | 141          |
|    iterations               | 11           |
|    time_elapsed             | 159          |
|    total_timesteps          | 2631680      |
| train/                      |              |
|    approx_kl                | 0.0104554    |
|    approx_ln(kl)            | -4.5606365   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.804        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.638       |
|    ln(policy_gradient_loss) | -4.43        |
|    loss                     | 0.528        |
|    n_updates                | 1337         |
|    policy_gradient_loss     | 0.0119       |
|    std                      | 0.937        |
|    value_loss               | 2.78         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2535664] |
| time/                       |              |
|    fps                      | 138          |
|    iterations               | 12           |
|    time_elapsed             | 178          |
|    total_timesteps          | 2633728      |
| train/                      |              |
|    approx_kl                | 0.008200433  |
|    approx_ln(kl)            | -4.8035684   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.936        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.332       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.717        |
|    n_updates                | 1338         |
|    policy_gradient_loss     | -0.000575    |
|    std                      | 0.937        |
|    value_loss               | 1.85         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.15688662] |
| time/                       |               |
|    fps                      | 138           |
|    iterations               | 13            |
|    time_elapsed             | 192           |
|    total_timesteps          | 2635776       |
| train/                      |               |
|    approx_kl                | 0.00505429    |
|    approx_ln(kl)            | -5.287518     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.93          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.897        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.408         |
|    n_updates                | 1339          |
|    policy_gradient_loss     | -0.0063       |
|    std                      | 0.938         |
|    value_loss               | 2.76          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3228839] |
| time/                       |              |
|    fps                      | 139          |
|    iterations               | 14           |
|    time_elapsed             | 205          |
|    total_timesteps          | 2637824      |
| train/                      |              |
|    approx_kl                | 0.004909429  |
|    approx_ln(kl)            | -5.316598    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.972        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.15         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 3.15         |
|    n_updates                | 1340         |
|    policy_gradient_loss     | -0.0043      |
|    std                      | 0.939        |
|    value_loss               | 2.44         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.37663692] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 15            |
|    time_elapsed             | 219           |
|    total_timesteps          | 2639872       |
| train/                      |               |
|    approx_kl                | 0.010113475   |
|    approx_ln(kl)            | -4.5938864    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.954         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.166         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.18          |
|    n_updates                | 1341          |
|    policy_gradient_loss     | -0.0197       |
|    std                      | 0.941         |
|    value_loss               | 7.2           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27671576] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 16            |
|    time_elapsed             | 232           |
|    total_timesteps          | 2641920       |
| train/                      |               |
|    approx_kl                | 0.0074610105  |
|    approx_ln(kl)            | -4.8980646    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.954         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.206        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.814         |
|    n_updates                | 1342          |
|    policy_gradient_loss     | -0.000289     |
|    std                      | 0.942         |
|    value_loss               | 2.37          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32572854] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 17            |
|    time_elapsed             | 246           |
|    total_timesteps          | 2643968       |
| train/                      |               |
|    approx_kl                | 0.006924184   |
|    approx_ln(kl)            | -4.972735     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.923         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.195        |
|    ln(policy_gradient_loss) | -8.59         |
|    loss                     | 0.823         |
|    n_updates                | 1343          |
|    policy_gradient_loss     | 0.000185      |
|    std                      | 0.943         |
|    value_loss               | 2.16          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25730377] |
| time/                       |               |
|    fps                      | 142           |
|    iterations               | 18            |
|    time_elapsed             | 259           |
|    total_timesteps          | 2646016       |
| train/                      |               |
|    approx_kl                | 0.004444019   |
|    approx_ln(kl)            | -5.4161963    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.872         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.852         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 2.34          |
|    n_updates                | 1344          |
|    policy_gradient_loss     | -0.00419      |
|    std                      | 0.944         |
|    value_loss               | 5.06          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27254725] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 19            |
|    time_elapsed             | 275           |
|    total_timesteps          | 2648064       |
| train/                      |               |
|    approx_kl                | 0.004823456   |
|    approx_ln(kl)            | -5.3342648    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.954         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.173        |
|    ln(policy_gradient_loss) | -5.24         |
|    loss                     | 0.841         |
|    n_updates                | 1345          |
|    policy_gradient_loss     | 0.00533       |
|    std                      | 0.945         |
|    value_loss               | 2.61          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.34303874] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 20            |
|    time_elapsed             | 291           |
|    total_timesteps          | 2650112       |
| train/                      |               |
|    approx_kl                | 0.0059616244  |
|    approx_ln(kl)            | -5.122412     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.891         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.555         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.74          |
|    n_updates                | 1346          |
|    policy_gradient_loss     | -0.00409      |
|    std                      | 0.945         |
|    value_loss               | 8.13          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34113783] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 21            |
|    time_elapsed             | 306           |
|    total_timesteps          | 2652160       |
| train/                      |               |
|    approx_kl                | 0.0056008464  |
|    approx_ln(kl)            | -5.1848373    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.941         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.295        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.745         |
|    n_updates                | 1347          |
|    policy_gradient_loss     | -0.00498      |
|    std                      | 0.944         |
|    value_loss               | 3.5           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.31035388] |
| time/                       |               |
|    fps                      | 138           |
|    iterations               | 22            |
|    time_elapsed             | 324           |
|    total_timesteps          | 2654208       |
| train/                      |               |
|    approx_kl                | 0.0068831304  |
|    approx_ln(kl)            | -4.9786816    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.974         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.688        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.502         |
|    n_updates                | 1348          |
|    policy_gradient_loss     | -0.00964      |
|    std                      | 0.944         |
|    value_loss               | 0.876         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27362278] |
| time/                       |               |
|    fps                      | 138           |
|    iterations               | 23            |
|    time_elapsed             | 340           |
|    total_timesteps          | 2656256       |
| train/                      |               |
|    approx_kl                | 0.0063169347  |
|    approx_ln(kl)            | -5.0645213    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.956         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.21         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.299         |
|    n_updates                | 1349          |
|    policy_gradient_loss     | -0.00438      |
|    std                      | 0.944         |
|    value_loss               | 1.62          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3304846] |
| time/                       |              |
|    fps                      | 138          |
|    iterations               | 24           |
|    time_elapsed             | 355          |
|    total_timesteps          | 2658304      |
| train/                      |              |
|    approx_kl                | 0.010356235  |
|    approx_ln(kl)            | -4.5701666   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.71        |
|    explained_variance       | 0.955        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.38        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.251        |
|    n_updates                | 1350         |
|    policy_gradient_loss     | -0.0055      |
|    std                      | 0.944        |
|    value_loss               | 0.319        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.31546843] |
| time/                       |               |
|    fps                      | 138           |
|    iterations               | 25            |
|    time_elapsed             | 369           |
|    total_timesteps          | 2660352       |
| train/                      |               |
|    approx_kl                | 0.01191582    |
|    approx_ln(kl)            | -4.4298882    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.953         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.799         |
|    ln(policy_gradient_loss) | -4.62         |
|    loss                     | 2.22          |
|    n_updates                | 1351          |
|    policy_gradient_loss     | 0.00982       |
|    std                      | 0.944         |
|    value_loss               | 3.51          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.45969442] |
| time/                       |               |
|    fps                      | 138           |
|    iterations               | 26            |
|    time_elapsed             | 384           |
|    total_timesteps          | 2662400       |
| train/                      |               |
|    approx_kl                | 0.0106897475  |
|    approx_ln(kl)            | -4.5384703    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.884         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.123        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.884         |
|    n_updates                | 1352          |
|    policy_gradient_loss     | -0.00608      |
|    std                      | 0.945         |
|    value_loss               | 6.37          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24009153] |
| time/                       |               |
|    fps                      | 138           |
|    iterations               | 27            |
|    time_elapsed             | 398           |
|    total_timesteps          | 2664448       |
| train/                      |               |
|    approx_kl                | 0.0070948284  |
|    approx_ln(kl)            | -4.948389     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.352        |
|    ln(policy_gradient_loss) | -6.16         |
|    loss                     | 0.703         |
|    n_updates                | 1353          |
|    policy_gradient_loss     | 0.00212       |
|    std                      | 0.945         |
|    value_loss               | 1.82          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.26627654] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 28            |
|    time_elapsed             | 411           |
|    total_timesteps          | 2666496       |
| train/                      |               |
|    approx_kl                | 0.00892665    |
|    approx_ln(kl)            | -4.718714     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.957         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0497       |
|    ln(policy_gradient_loss) | -8.14         |
|    loss                     | 0.951         |
|    n_updates                | 1354          |
|    policy_gradient_loss     | 0.00029       |
|    std                      | 0.947         |
|    value_loss               | 3.05          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37760952] |
| time/                       |               |
|    fps                      | 139           |
|    iterations               | 29            |
|    time_elapsed             | 425           |
|    total_timesteps          | 2668544       |
| train/                      |               |
|    approx_kl                | 0.007312423   |
|    approx_ln(kl)            | -4.9181805    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.945         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0879       |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.916         |
|    n_updates                | 1355          |
|    policy_gradient_loss     | -0.00286      |
|    std                      | 0.949         |
|    value_loss               | 2.74          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33661735] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 30            |
|    time_elapsed             | 438           |
|    total_timesteps          | 2670592       |
| train/                      |               |
|    approx_kl                | 0.0080930535  |
|    approx_ln(kl)            | -4.816749     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.2          |
|    ln(policy_gradient_loss) | -4.12         |
|    loss                     | 0.302         |
|    n_updates                | 1356          |
|    policy_gradient_loss     | 0.0162        |
|    std                      | 0.95          |
|    value_loss               | 0.596         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33144057] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 31            |
|    time_elapsed             | 451           |
|    total_timesteps          | 2672640       |
| train/                      |               |
|    approx_kl                | 0.009282014   |
|    approx_ln(kl)            | -4.6796765    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.965         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.689         |
|    ln(policy_gradient_loss) | -5.74         |
|    loss                     | 1.99          |
|    n_updates                | 1357          |
|    policy_gradient_loss     | 0.0032        |
|    std                      | 0.95          |
|    value_loss               | 1.86          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29554588] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 32            |
|    time_elapsed             | 465           |
|    total_timesteps          | 2674688       |
| train/                      |               |
|    approx_kl                | 0.005340992   |
|    approx_ln(kl)            | -5.2323437    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.43         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.24          |
|    n_updates                | 1358          |
|    policy_gradient_loss     | -0.00307      |
|    std                      | 0.951         |
|    value_loss               | 0.336         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.24756724] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 33            |
|    time_elapsed             | 479           |
|    total_timesteps          | 2676736       |
| train/                      |               |
|    approx_kl                | 0.008711419   |
|    approx_ln(kl)            | -4.7431207    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.215         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.24          |
|    n_updates                | 1359          |
|    policy_gradient_loss     | -0.00911      |
|    std                      | 0.951         |
|    value_loss               | 1.88          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39060912] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 34            |
|    time_elapsed             | 492           |
|    total_timesteps          | 2678784       |
| train/                      |               |
|    approx_kl                | 0.0069948747  |
|    approx_ln(kl)            | -4.962578     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.92         |
|    ln(policy_gradient_loss) | -5.79         |
|    loss                     | 0.146         |
|    n_updates                | 1360          |
|    policy_gradient_loss     | 0.00305       |
|    std                      | 0.953         |
|    value_loss               | 0.547         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32810324] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 35            |
|    time_elapsed             | 505           |
|    total_timesteps          | 2680832       |
| train/                      |               |
|    approx_kl                | 0.0057150396  |
|    approx_ln(kl)            | -5.1646543    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.948         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.243         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.27          |
|    n_updates                | 1361          |
|    policy_gradient_loss     | -0.00566      |
|    std                      | 0.954         |
|    value_loss               | 2.12          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3778355] |
| time/                       |              |
|    fps                      | 142          |
|    iterations               | 36           |
|    time_elapsed             | 519          |
|    total_timesteps          | 2682880      |
| train/                      |              |
|    approx_kl                | 0.010008296  |
|    approx_ln(kl)            | -4.604341    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.74        |
|    explained_variance       | 0.905        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.0952      |
|    ln(policy_gradient_loss) | -5.28        |
|    loss                     | 0.909        |
|    n_updates                | 1362         |
|    policy_gradient_loss     | 0.00511      |
|    std                      | 0.956        |
|    value_loss               | 4.15         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-28.171251] |
| time/                       |              |
|    fps                      | 142          |
|    iterations               | 37           |
|    time_elapsed             | 532          |
|    total_timesteps          | 2684928      |
| train/                      |              |
|    approx_kl                | 0.0073270183 |
|    approx_ln(kl)            | -4.916187    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.74        |
|    explained_variance       | 0.969        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.523        |
|    ln(policy_gradient_loss) | -7.98        |
|    loss                     | 1.69         |
|    n_updates                | 1363         |
|    policy_gradient_loss     | 0.000341     |
|    std                      | 0.956        |
|    value_loss               | 2.57         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.2582828] |
| time/                       |              |
|    fps                      | 142          |
|    iterations               | 38           |
|    time_elapsed             | 545          |
|    total_timesteps          | 2686976      |
| train/                      |              |
|    approx_kl                | 0.010542862  |
|    approx_ln(kl)            | -4.552306    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.74        |
|    explained_variance       | 0.935        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.01        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.363        |
|    n_updates                | 1364         |
|    policy_gradient_loss     | -0.000793    |
|    std                      | 0.956        |
|    value_loss               | 1.5          |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2120776] |
| time/                       |              |
|    fps                      | 142          |
|    iterations               | 39           |
|    time_elapsed             | 558          |
|    total_timesteps          | 2689024      |
| train/                      |              |
|    approx_kl                | 0.00740358   |
|    approx_ln(kl)            | -4.9057918   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.74        |
|    explained_variance       | 0.975        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.138       |
|    ln(policy_gradient_loss) | -9.58        |
|    loss                     | 0.871        |
|    n_updates                | 1365         |
|    policy_gradient_loss     | 6.88e-05     |
|    std                      | 0.956        |
|    value_loss               | 1.22         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3137462] |
| time/                       |              |
|    fps                      | 143          |
|    iterations               | 40           |
|    time_elapsed             | 572          |
|    total_timesteps          | 2691072      |
| train/                      |              |
|    approx_kl                | 0.0057482803 |
|    approx_ln(kl)            | -5.1588545   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.74        |
|    explained_variance       | 0.964        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.0526       |
|    ln(policy_gradient_loss) | -6.83        |
|    loss                     | 1.05         |
|    n_updates                | 1366         |
|    policy_gradient_loss     | 0.00108      |
|    std                      | 0.957        |
|    value_loss               | 0.967        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.34149277] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 41            |
|    time_elapsed             | 585           |
|    total_timesteps          | 2693120       |
| train/                      |               |
|    approx_kl                | 0.008915389   |
|    approx_ln(kl)            | -4.7199764    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.943         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.157         |
|    ln(policy_gradient_loss) | -3.95         |
|    loss                     | 1.17          |
|    n_updates                | 1367          |
|    policy_gradient_loss     | 0.0193        |
|    std                      | 0.957         |
|    value_loss               | 6.21          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.342657] |
| time/                       |             |
|    fps                      | 143         |
|    iterations               | 42          |
|    time_elapsed             | 599         |
|    total_timesteps          | 2695168     |
| train/                      |             |
|    approx_kl                | 0.005502548 |
|    approx_ln(kl)            | -5.202544   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.74       |
|    explained_variance       | 0.97        |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -0.709      |
|    ln(policy_gradient_loss) | -4.24       |
|    loss                     | 0.492       |
|    n_updates                | 1368        |
|    policy_gradient_loss     | 0.0144      |
|    std                      | 0.957       |
|    value_loss               | 0.786       |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32796997] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 43            |
|    time_elapsed             | 613           |
|    total_timesteps          | 2697216       |
| train/                      |               |
|    approx_kl                | 0.0058380454  |
|    approx_ln(kl)            | -5.143359     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.529        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.589         |
|    n_updates                | 1369          |
|    policy_gradient_loss     | -0.00426      |
|    std                      | 0.958         |
|    value_loss               | 1.33          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
---------------------------------------------
| reward                      | [-0.304285] |
| time/                       |             |
|    fps                      | 143         |
|    iterations               | 44          |
|    time_elapsed             | 627         |
|    total_timesteps          | 2699264     |
| train/                      |             |
|    approx_kl                | 0.007909561 |
|    approx_ln(kl)            | -4.839683   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.74       |
|    explained_variance       | 0.976       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -1.36       |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.258       |
|    n_updates                | 1370        |
|    policy_gradient_loss     | -0.00324    |
|    std                      | 0.957       |
|    value_loss               | 0.985       |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.28396326] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 45            |
|    time_elapsed             | 640           |
|    total_timesteps          | 2701312       |
| train/                      |               |
|    approx_kl                | 0.008469351   |
|    approx_ln(kl)            | -4.7713013    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.943         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.749        |
|    ln(policy_gradient_loss) | -9.71         |
|    loss                     | 0.473         |
|    n_updates                | 1371          |
|    policy_gradient_loss     | 6.09e-05      |
|    std                      | 0.957         |
|    value_loss               | 1.24          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32326367] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 46            |
|    time_elapsed             | 653           |
|    total_timesteps          | 2703360       |
| train/                      |               |
|    approx_kl                | 0.005190306   |
|    approx_ln(kl)            | -5.2609625    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.968         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.12         |
|    ln(policy_gradient_loss) | -7.11         |
|    loss                     | 0.328         |
|    n_updates                | 1372          |
|    policy_gradient_loss     | 0.000818      |
|    std                      | 0.957         |
|    value_loss               | 1.07          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.36875668] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 47            |
|    time_elapsed             | 666           |
|    total_timesteps          | 2705408       |
| train/                      |               |
|    approx_kl                | 0.0061719576  |
|    approx_ln(kl)            | -5.0877395    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.956         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.56         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.209         |
|    n_updates                | 1373          |
|    policy_gradient_loss     | -0.00306      |
|    std                      | 0.957         |
|    value_loss               | 0.347         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42591143] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 48            |
|    time_elapsed             | 679           |
|    total_timesteps          | 2707456       |
| train/                      |               |
|    approx_kl                | 0.006694231   |
|    approx_ln(kl)            | -5.0065093    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.878         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.1           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 3.01          |
|    n_updates                | 1374          |
|    policy_gradient_loss     | -0.00429      |
|    std                      | 0.957         |
|    value_loss               | 2.34          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3220421] |
| time/                       |              |
|    fps                      | 144          |
|    iterations               | 49           |
|    time_elapsed             | 693          |
|    total_timesteps          | 2709504      |
| train/                      |              |
|    approx_kl                | 0.0065968884 |
|    approx_ln(kl)            | -5.0211573   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.74        |
|    explained_variance       | 0.933        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.161        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.18         |
|    n_updates                | 1375         |
|    policy_gradient_loss     | -0.00426     |
|    std                      | 0.957        |
|    value_loss               | 2.76         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
--------------------------------------
| reward             | [-0.25836352] |
| time/              |               |
|    fps             | 156           |
|    iterations      | 1             |
|    time_elapsed    | 13            |
|    total_timesteps | 2711552       |
--------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.20018233] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 2             |
|    time_elapsed             | 26            |
|    total_timesteps          | 2713600       |
| train/                      |               |
|    approx_kl                | 0.006728734   |
|    approx_ln(kl)            | -5.0013685    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.974         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.15         |
|    ln(policy_gradient_loss) | -4.78         |
|    loss                     | 0.117         |
|    n_updates                | 1377          |
|    policy_gradient_loss     | 0.00841       |
|    std                      | 0.957         |
|    value_loss               | 0.391         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35747993] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 3             |
|    time_elapsed             | 39            |
|    total_timesteps          | 2715648       |
| train/                      |               |
|    approx_kl                | 0.004777469   |
|    approx_ln(kl)            | -5.3438444    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.969         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.62         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.198         |
|    n_updates                | 1378          |
|    policy_gradient_loss     | -0.00171      |
|    std                      | 0.957         |
|    value_loss               | 0.556         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25768602] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 4             |
|    time_elapsed             | 53            |
|    total_timesteps          | 2717696       |
| train/                      |               |
|    approx_kl                | 0.0095038535  |
|    approx_ln(kl)            | -4.656058     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.953         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.44         |
|    ln(policy_gradient_loss) | -4.87         |
|    loss                     | 0.238         |
|    n_updates                | 1379          |
|    policy_gradient_loss     | 0.00769       |
|    std                      | 0.958         |
|    value_loss               | 1.26          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32341564] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 5             |
|    time_elapsed             | 66            |
|    total_timesteps          | 2719744       |
| train/                      |               |
|    approx_kl                | 0.005487941   |
|    approx_ln(kl)            | -5.205202     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.52         |
|    ln(policy_gradient_loss) | -5.24         |
|    loss                     | 0.0804        |
|    n_updates                | 1380          |
|    policy_gradient_loss     | 0.00529       |
|    std                      | 0.959         |
|    value_loss               | 0.166         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.2431335] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 6            |
|    time_elapsed             | 80           |
|    total_timesteps          | 2721792      |
| train/                      |              |
|    approx_kl                | 0.010066104  |
|    approx_ln(kl)            | -4.5985813   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.74        |
|    explained_variance       | 0.956        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.42        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.241        |
|    n_updates                | 1381         |
|    policy_gradient_loss     | -0.00303     |
|    std                      | 0.961        |
|    value_loss               | 0.541        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3171931] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 7            |
|    time_elapsed             | 93           |
|    total_timesteps          | 2723840      |
| train/                      |              |
|    approx_kl                | 0.007700186  |
|    approx_ln(kl)            | -4.866511    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.75        |
|    explained_variance       | 0.947        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.299        |
|    ln(policy_gradient_loss) | -6.43        |
|    loss                     | 1.35         |
|    n_updates                | 1382         |
|    policy_gradient_loss     | 0.00161      |
|    std                      | 0.961        |
|    value_loss               | 3.46         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.23010021] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 8             |
|    time_elapsed             | 106           |
|    total_timesteps          | 2725888       |
| train/                      |               |
|    approx_kl                | 0.008900899   |
|    approx_ln(kl)            | -4.721603     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.939         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.643         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.9           |
|    n_updates                | 1383          |
|    policy_gradient_loss     | -0.00011      |
|    std                      | 0.962         |
|    value_loss               | 5.11          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3209631] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 9            |
|    time_elapsed             | 120          |
|    total_timesteps          | 2727936      |
| train/                      |              |
|    approx_kl                | 0.007026394  |
|    approx_ln(kl)            | -4.9580817   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.75        |
|    explained_variance       | 0.916        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.527        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.69         |
|    n_updates                | 1384         |
|    policy_gradient_loss     | -0.00046     |
|    std                      | 0.963        |
|    value_loss               | 3.85         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35815206] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 10            |
|    time_elapsed             | 133           |
|    total_timesteps          | 2729984       |
| train/                      |               |
|    approx_kl                | 0.005691504   |
|    approx_ln(kl)            | -5.168781     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.962         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.18          |
|    ln(policy_gradient_loss) | -4.91         |
|    loss                     | 3.26          |
|    n_updates                | 1385          |
|    policy_gradient_loss     | 0.00737       |
|    std                      | 0.963         |
|    value_loss               | 1.72          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.05
-----------------------------------------------
| reward                      | [-0.24072094] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 11            |
|    time_elapsed             | 146           |
|    total_timesteps          | 2732032       |
| train/                      |               |
|    approx_kl                | 0.014490813   |
|    approx_ln(kl)            | -4.2342405    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.963         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.47         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0845        |
|    n_updates                | 1386          |
|    policy_gradient_loss     | -0.0222       |
|    std                      | 0.964         |
|    value_loss               | 0.621         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26391855] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 12            |
|    time_elapsed             | 160           |
|    total_timesteps          | 2734080       |
| train/                      |               |
|    approx_kl                | 0.009843383   |
|    approx_ln(kl)            | -4.620956     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.939         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.48         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.228         |
|    n_updates                | 1387          |
|    policy_gradient_loss     | -0.0027       |
|    std                      | 0.964         |
|    value_loss               | 1.56          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.29590514] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 13            |
|    time_elapsed             | 173           |
|    total_timesteps          | 2736128       |
| train/                      |               |
|    approx_kl                | 0.011879474   |
|    approx_ln(kl)            | -4.4329433    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.14         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.117         |
|    n_updates                | 1388          |
|    policy_gradient_loss     | -0.00154      |
|    std                      | 0.964         |
|    value_loss               | 0.259         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38950527] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 14            |
|    time_elapsed             | 187           |
|    total_timesteps          | 2738176       |
| train/                      |               |
|    approx_kl                | 0.008902232   |
|    approx_ln(kl)            | -4.721453     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.963         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.232         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.26          |
|    n_updates                | 1389          |
|    policy_gradient_loss     | -0.00217      |
|    std                      | 0.964         |
|    value_loss               | 2.1           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40793785] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 15            |
|    time_elapsed             | 201           |
|    total_timesteps          | 2740224       |
| train/                      |               |
|    approx_kl                | 0.00826475    |
|    approx_ln(kl)            | -4.795756     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.12         |
|    ln(policy_gradient_loss) | -4.8          |
|    loss                     | 0.328         |
|    n_updates                | 1390          |
|    policy_gradient_loss     | 0.0082        |
|    std                      | 0.965         |
|    value_loss               | 1.08          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2649991] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 16           |
|    time_elapsed             | 214          |
|    total_timesteps          | 2742272      |
| train/                      |              |
|    approx_kl                | 0.004887476  |
|    approx_ln(kl)            | -5.3210793   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.75        |
|    explained_variance       | 0.976        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.21        |
|    ln(policy_gradient_loss) | -7.17        |
|    loss                     | 0.298        |
|    n_updates                | 1391         |
|    policy_gradient_loss     | 0.000766     |
|    std                      | 0.965        |
|    value_loss               | 1.15         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4547912] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 17           |
|    time_elapsed             | 229          |
|    total_timesteps          | 2744320      |
| train/                      |              |
|    approx_kl                | 0.00909224   |
|    approx_ln(kl)            | -4.700334    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.75        |
|    explained_variance       | 0.966        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.736       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.479        |
|    n_updates                | 1392         |
|    policy_gradient_loss     | -0.0126      |
|    std                      | 0.964        |
|    value_loss               | 0.947        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29355112] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 18            |
|    time_elapsed             | 243           |
|    total_timesteps          | 2746368       |
| train/                      |               |
|    approx_kl                | 0.00614997    |
|    approx_ln(kl)            | -5.091308     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.388         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.47          |
|    n_updates                | 1393          |
|    policy_gradient_loss     | -0.02         |
|    std                      | 0.965         |
|    value_loss               | 1.11          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.20498623] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 19            |
|    time_elapsed             | 256           |
|    total_timesteps          | 2748416       |
| train/                      |               |
|    approx_kl                | 0.004977767   |
|    approx_ln(kl)            | -5.302774     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.965         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.205         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.23          |
|    n_updates                | 1394          |
|    policy_gradient_loss     | -0.0112       |
|    std                      | 0.965         |
|    value_loss               | 2.25          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.30867073] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 20            |
|    time_elapsed             | 269           |
|    total_timesteps          | 2750464       |
| train/                      |               |
|    approx_kl                | 0.00919724    |
|    approx_ln(kl)            | -4.688852     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.972         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.45         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0864        |
|    n_updates                | 1395          |
|    policy_gradient_loss     | -0.0205       |
|    std                      | 0.965         |
|    value_loss               | 0.331         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2302054] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 21           |
|    time_elapsed             | 283          |
|    total_timesteps          | 2752512      |
| train/                      |              |
|    approx_kl                | 0.0075116064 |
|    approx_ln(kl)            | -4.891306    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.75        |
|    explained_variance       | 0.974        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.527       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.591        |
|    n_updates                | 1396         |
|    policy_gradient_loss     | -0.000596    |
|    std                      | 0.964        |
|    value_loss               | 2.91         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2771888] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 22           |
|    time_elapsed             | 296          |
|    total_timesteps          | 2754560      |
| train/                      |              |
|    approx_kl                | 0.006053971  |
|    approx_ln(kl)            | -5.107041    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.75        |
|    explained_variance       | 0.962        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.486        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.63         |
|    n_updates                | 1397         |
|    policy_gradient_loss     | -0.00386     |
|    std                      | 0.963        |
|    value_loss               | 2.82         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2601408] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 23           |
|    time_elapsed             | 310          |
|    total_timesteps          | 2756608      |
| train/                      |              |
|    approx_kl                | 0.005466352  |
|    approx_ln(kl)            | -5.2091436   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.75        |
|    explained_variance       | 0.972        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.08        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0459       |
|    n_updates                | 1398         |
|    policy_gradient_loss     | -0.0113      |
|    std                      | 0.962        |
|    value_loss               | 0.202        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.19840986] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 24            |
|    time_elapsed             | 324           |
|    total_timesteps          | 2758656       |
| train/                      |               |
|    approx_kl                | 0.012228645   |
|    approx_ln(kl)            | -4.403974     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.974         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.979        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.376         |
|    n_updates                | 1399          |
|    policy_gradient_loss     | -0.00463      |
|    std                      | 0.961         |
|    value_loss               | 1.24          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23755117] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 25            |
|    time_elapsed             | 337           |
|    total_timesteps          | 2760704       |
| train/                      |               |
|    approx_kl                | 0.0069416654  |
|    approx_ln(kl)            | -4.9702134    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.968         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.17         |
|    ln(policy_gradient_loss) | -6.33         |
|    loss                     | 0.844         |
|    n_updates                | 1400          |
|    policy_gradient_loss     | 0.00179       |
|    std                      | 0.96          |
|    value_loss               | 1.68          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32284755] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 26            |
|    time_elapsed             | 351           |
|    total_timesteps          | 2762752       |
| train/                      |               |
|    approx_kl                | 0.008695629   |
|    approx_ln(kl)            | -4.7449346    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.965         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.595        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.551         |
|    n_updates                | 1401          |
|    policy_gradient_loss     | -0.00243      |
|    std                      | 0.958         |
|    value_loss               | 2.15          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.14472845] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 27            |
|    time_elapsed             | 366           |
|    total_timesteps          | 2764800       |
| train/                      |               |
|    approx_kl                | 0.0053833365  |
|    approx_ln(kl)            | -5.224447     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.299        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.742         |
|    n_updates                | 1402          |
|    policy_gradient_loss     | -0.00678      |
|    std                      | 0.957         |
|    value_loss               | 1.35          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.18995565] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 28            |
|    time_elapsed             | 379           |
|    total_timesteps          | 2766848       |
| train/                      |               |
|    approx_kl                | 0.008357876   |
|    approx_ln(kl)            | -4.784551     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.966         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.411        |
|    ln(policy_gradient_loss) | -6.86         |
|    loss                     | 0.663         |
|    n_updates                | 1403          |
|    policy_gradient_loss     | 0.00105       |
|    std                      | 0.956         |
|    value_loss               | 1.75          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26218778] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 29            |
|    time_elapsed             | 393           |
|    total_timesteps          | 2768896       |
| train/                      |               |
|    approx_kl                | 0.010027023   |
|    approx_ln(kl)            | -4.6024714    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.523         |
|    ln(policy_gradient_loss) | -4.21         |
|    loss                     | 1.69          |
|    n_updates                | 1404          |
|    policy_gradient_loss     | 0.0148        |
|    std                      | 0.956         |
|    value_loss               | 1.55          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.13913052] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 30            |
|    time_elapsed             | 406           |
|    total_timesteps          | 2770944       |
| train/                      |               |
|    approx_kl                | 0.0065565887  |
|    approx_ln(kl)            | -5.0272846    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.97          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.27         |
|    ln(policy_gradient_loss) | -5.4          |
|    loss                     | 0.281         |
|    n_updates                | 1405          |
|    policy_gradient_loss     | 0.00451       |
|    std                      | 0.955         |
|    value_loss               | 1.62          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5243827] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 31           |
|    time_elapsed             | 420          |
|    total_timesteps          | 2772992      |
| train/                      |              |
|    approx_kl                | 0.0076474026 |
|    approx_ln(kl)            | -4.8733892   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.73        |
|    explained_variance       | 0.975        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.0724       |
|    ln(policy_gradient_loss) | -6.77        |
|    loss                     | 1.08         |
|    n_updates                | 1406         |
|    policy_gradient_loss     | 0.00115      |
|    std                      | 0.954        |
|    value_loss               | 1.27         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38276625] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 32            |
|    time_elapsed             | 434           |
|    total_timesteps          | 2775040       |
| train/                      |               |
|    approx_kl                | 0.0072844587  |
|    approx_ln(kl)            | -4.9220123    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.41         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.243         |
|    n_updates                | 1407          |
|    policy_gradient_loss     | -0.00706      |
|    std                      | 0.954         |
|    value_loss               | 0.824         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27492067] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 33            |
|    time_elapsed             | 448           |
|    total_timesteps          | 2777088       |
| train/                      |               |
|    approx_kl                | 0.006251335   |
|    approx_ln(kl)            | -5.07496      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.27         |
|    ln(policy_gradient_loss) | -6.48         |
|    loss                     | 0.103         |
|    n_updates                | 1408          |
|    policy_gradient_loss     | 0.00153       |
|    std                      | 0.953         |
|    value_loss               | 0.219         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26688138] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 34            |
|    time_elapsed             | 462           |
|    total_timesteps          | 2779136       |
| train/                      |               |
|    approx_kl                | 0.0071459087  |
|    approx_ln(kl)            | -4.9412155    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.973         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.307         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.36          |
|    n_updates                | 1409          |
|    policy_gradient_loss     | -0.000716     |
|    std                      | 0.954         |
|    value_loss               | 0.968         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33938965] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 35            |
|    time_elapsed             | 475           |
|    total_timesteps          | 2781184       |
| train/                      |               |
|    approx_kl                | 0.006381357   |
|    approx_ln(kl)            | -5.0543747    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.482        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.618         |
|    n_updates                | 1410          |
|    policy_gradient_loss     | -0.000276     |
|    std                      | 0.955         |
|    value_loss               | 0.563         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.23936312] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 36            |
|    time_elapsed             | 489           |
|    total_timesteps          | 2783232       |
| train/                      |               |
|    approx_kl                | 0.007983089   |
|    approx_ln(kl)            | -4.83043      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.6          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0745        |
|    n_updates                | 1411          |
|    policy_gradient_loss     | -0.00671      |
|    std                      | 0.956         |
|    value_loss               | 0.162         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34736848] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 37            |
|    time_elapsed             | 502           |
|    total_timesteps          | 2785280       |
| train/                      |               |
|    approx_kl                | 0.008875659   |
|    approx_ln(kl)            | -4.724443     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.11         |
|    ln(policy_gradient_loss) | -4.16         |
|    loss                     | 0.33          |
|    n_updates                | 1412          |
|    policy_gradient_loss     | 0.0156        |
|    std                      | 0.957         |
|    value_loss               | 0.713         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25487205] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 38            |
|    time_elapsed             | 515           |
|    total_timesteps          | 2787328       |
| train/                      |               |
|    approx_kl                | 0.0059804753  |
|    approx_ln(kl)            | -5.119255     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.957         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.53          |
|    ln(policy_gradient_loss) | -5.56         |
|    loss                     | 4.61          |
|    n_updates                | 1413          |
|    policy_gradient_loss     | 0.00384       |
|    std                      | 0.958         |
|    value_loss               | 3.59          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.21170016] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 39            |
|    time_elapsed             | 529           |
|    total_timesteps          | 2789376       |
| train/                      |               |
|    approx_kl                | 0.0060437354  |
|    approx_ln(kl)            | -5.108733     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.953         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.31         |
|    ln(policy_gradient_loss) | -7.68         |
|    loss                     | 0.271         |
|    n_updates                | 1414          |
|    policy_gradient_loss     | 0.000462      |
|    std                      | 0.959         |
|    value_loss               | 1.56          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.46837112] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 40            |
|    time_elapsed             | 543           |
|    total_timesteps          | 2791424       |
| train/                      |               |
|    approx_kl                | 0.0055473996  |
|    approx_ln(kl)            | -5.194426     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.953         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.826        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.438         |
|    n_updates                | 1415          |
|    policy_gradient_loss     | -0.00499      |
|    std                      | 0.959         |
|    value_loss               | 0.972         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25992438] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 41            |
|    time_elapsed             | 557           |
|    total_timesteps          | 2793472       |
| train/                      |               |
|    approx_kl                | 0.006274188   |
|    approx_ln(kl)            | -5.071311     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.962         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.02         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.359         |
|    n_updates                | 1416          |
|    policy_gradient_loss     | -0.000817     |
|    std                      | 0.959         |
|    value_loss               | 1.51          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24276535] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 42            |
|    time_elapsed             | 571           |
|    total_timesteps          | 2795520       |
| train/                      |               |
|    approx_kl                | 0.0068560573  |
|    approx_ln(kl)            | -4.9826226    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.945         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.143        |
|    ln(policy_gradient_loss) | -5.45         |
|    loss                     | 0.867         |
|    n_updates                | 1417          |
|    policy_gradient_loss     | 0.00431       |
|    std                      | 0.959         |
|    value_loss               | 2.46          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.2277237] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 43           |
|    time_elapsed             | 584          |
|    total_timesteps          | 2797568      |
| train/                      |              |
|    approx_kl                | 0.007933784  |
|    approx_ln(kl)            | -4.836625    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.74        |
|    explained_variance       | 0.93         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.901        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 2.46         |
|    n_updates                | 1418         |
|    policy_gradient_loss     | -0.000232    |
|    std                      | 0.961        |
|    value_loss               | 3.55         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.23429708] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 44            |
|    time_elapsed             | 598           |
|    total_timesteps          | 2799616       |
| train/                      |               |
|    approx_kl                | 0.010954939   |
|    approx_ln(kl)            | -4.5139647    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.939         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.497        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.608         |
|    n_updates                | 1419          |
|    policy_gradient_loss     | -0.00258      |
|    std                      | 0.962         |
|    value_loss               | 1.31          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.53091896] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 45            |
|    time_elapsed             | 612           |
|    total_timesteps          | 2801664       |
| train/                      |               |
|    approx_kl                | 0.00597724    |
|    approx_ln(kl)            | -5.1197963    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.97          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.93         |
|    ln(policy_gradient_loss) | -5.44         |
|    loss                     | 0.146         |
|    n_updates                | 1420          |
|    policy_gradient_loss     | 0.00433       |
|    std                      | 0.962         |
|    value_loss               | 0.839         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.1473107] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 46           |
|    time_elapsed             | 626          |
|    total_timesteps          | 2803712      |
| train/                      |              |
|    approx_kl                | 0.005921682  |
|    approx_ln(kl)            | -5.1291347   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.75        |
|    explained_variance       | 0.981        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.54        |
|    ln(policy_gradient_loss) | -5.51        |
|    loss                     | 0.215        |
|    n_updates                | 1421         |
|    policy_gradient_loss     | 0.00403      |
|    std                      | 0.962        |
|    value_loss               | 1.39         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34051436] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 47            |
|    time_elapsed             | 640           |
|    total_timesteps          | 2805760       |
| train/                      |               |
|    approx_kl                | 0.006493913   |
|    approx_ln(kl)            | -5.03689      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.28         |
|    ln(policy_gradient_loss) | -7.48         |
|    loss                     | 0.277         |
|    n_updates                | 1422          |
|    policy_gradient_loss     | 0.000563      |
|    std                      | 0.959         |
|    value_loss               | 1.07          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
-----------------------------------------------
| reward                      | [-0.41526154] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 48            |
|    time_elapsed             | 654           |
|    total_timesteps          | 2807808       |
| train/                      |               |
|    approx_kl                | 0.017851895   |
|    approx_ln(kl)            | -4.0256457    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.904        |
|    ln(policy_gradient_loss) | -5.42         |
|    loss                     | 0.405         |
|    n_updates                | 1423          |
|    policy_gradient_loss     | 0.00441       |
|    std                      | 0.959         |
|    value_loss               | 0.412         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.31890097] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 49            |
|    time_elapsed             | 667           |
|    total_timesteps          | 2809856       |
| train/                      |               |
|    approx_kl                | 0.010094074   |
|    approx_ln(kl)            | -4.5958066    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.952         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.327        |
|    ln(policy_gradient_loss) | -4.86         |
|    loss                     | 0.721         |
|    n_updates                | 1424          |
|    policy_gradient_loss     | 0.00773       |
|    std                      | 0.959         |
|    value_loss               | 2.31          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-------------------------------------
| reward             | [-0.5172874] |
| time/              |              |
|    fps             | 156          |
|    iterations      | 1            |
|    time_elapsed    | 13           |
|    total_timesteps | 2811904      |
-------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24579664] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 2             |
|    time_elapsed             | 26            |
|    total_timesteps          | 2813952       |
| train/                      |               |
|    approx_kl                | 0.006834939   |
|    approx_ln(kl)            | -4.9857078    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1            |
|    ln(policy_gradient_loss) | -5.95         |
|    loss                     | 0.367         |
|    n_updates                | 1426          |
|    policy_gradient_loss     | 0.0026        |
|    std                      | 0.959         |
|    value_loss               | 1.17          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3078341] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 3            |
|    time_elapsed             | 39           |
|    total_timesteps          | 2816000      |
| train/                      |              |
|    approx_kl                | 0.007297925  |
|    approx_ln(kl)            | -4.920165    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.74        |
|    explained_variance       | 0.966        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.564        |
|    ln(policy_gradient_loss) | -4.18        |
|    loss                     | 1.76         |
|    n_updates                | 1427         |
|    policy_gradient_loss     | 0.0153       |
|    std                      | 0.959        |
|    value_loss               | 2.33         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.18215227] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 4             |
|    time_elapsed             | 52            |
|    total_timesteps          | 2818048       |
| train/                      |               |
|    approx_kl                | 0.008867836   |
|    approx_ln(kl)            | -4.7253246    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.948         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.228        |
|    ln(policy_gradient_loss) | -5.28         |
|    loss                     | 0.796         |
|    n_updates                | 1428          |
|    policy_gradient_loss     | 0.00507       |
|    std                      | 0.959         |
|    value_loss               | 2.41          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26348332] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 5             |
|    time_elapsed             | 66            |
|    total_timesteps          | 2820096       |
| train/                      |               |
|    approx_kl                | 0.0068121976  |
|    approx_ln(kl)            | -4.9890404    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.0732        |
|    ln(policy_gradient_loss) | -5.4          |
|    loss                     | 1.08          |
|    n_updates                | 1429          |
|    policy_gradient_loss     | 0.00451       |
|    std                      | 0.96          |
|    value_loss               | 1.68          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24355572] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 6             |
|    time_elapsed             | 79            |
|    total_timesteps          | 2822144       |
| train/                      |               |
|    approx_kl                | 0.006663198   |
|    approx_ln(kl)            | -5.0111556    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.966         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.356         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.43          |
|    n_updates                | 1430          |
|    policy_gradient_loss     | -0.000966     |
|    std                      | 0.961         |
|    value_loss               | 2.78          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.20029]   |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 7            |
|    time_elapsed             | 93           |
|    total_timesteps          | 2824192      |
| train/                      |              |
|    approx_kl                | 0.0071262354 |
|    approx_ln(kl)            | -4.943972    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.75        |
|    explained_variance       | 0.982        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.244       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.784        |
|    n_updates                | 1431         |
|    policy_gradient_loss     | -0.0106      |
|    std                      | 0.962        |
|    value_loss               | 2.04         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.05
-----------------------------------------------
| reward                      | [-0.26321548] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 8             |
|    time_elapsed             | 106           |
|    total_timesteps          | 2826240       |
| train/                      |               |
|    approx_kl                | 0.0123932315  |
|    approx_ln(kl)            | -4.390605     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.97          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.000411     |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1             |
|    n_updates                | 1432          |
|    policy_gradient_loss     | -0.0102       |
|    std                      | 0.963         |
|    value_loss               | 2.52          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24135685] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 9             |
|    time_elapsed             | 120           |
|    total_timesteps          | 2828288       |
| train/                      |               |
|    approx_kl                | 0.0061254683  |
|    approx_ln(kl)            | -5.0953       |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.971         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.248        |
|    ln(policy_gradient_loss) | -6.23         |
|    loss                     | 0.78          |
|    n_updates                | 1433          |
|    policy_gradient_loss     | 0.00197       |
|    std                      | 0.964         |
|    value_loss               | 2.51          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2876412] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 10           |
|    time_elapsed             | 133          |
|    total_timesteps          | 2830336      |
| train/                      |              |
|    approx_kl                | 0.0059986995 |
|    approx_ln(kl)            | -5.116213    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.75        |
|    explained_variance       | 0.98         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.48         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.62         |
|    n_updates                | 1434         |
|    policy_gradient_loss     | -0.00893     |
|    std                      | 0.963        |
|    value_loss               | 1.99         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.19739227] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 11            |
|    time_elapsed             | 147           |
|    total_timesteps          | 2832384       |
| train/                      |               |
|    approx_kl                | 0.0082353335  |
|    approx_ln(kl)            | -4.7993217    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.574        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.563         |
|    n_updates                | 1435          |
|    policy_gradient_loss     | -0.0156       |
|    std                      | 0.963         |
|    value_loss               | 1.02          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.23443057] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 12            |
|    time_elapsed             | 161           |
|    total_timesteps          | 2834432       |
| train/                      |               |
|    approx_kl                | 0.009627925   |
|    approx_ln(kl)            | -4.6430874    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.753        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.471         |
|    n_updates                | 1436          |
|    policy_gradient_loss     | -0.0205       |
|    std                      | 0.964         |
|    value_loss               | 1.07          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2897685] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 13           |
|    time_elapsed             | 175          |
|    total_timesteps          | 2836480      |
| train/                      |              |
|    approx_kl                | 0.006972939  |
|    approx_ln(kl)            | -4.9657187   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.75        |
|    explained_variance       | 0.985        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.753       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.471        |
|    n_updates                | 1437         |
|    policy_gradient_loss     | -0.00703     |
|    std                      | 0.965        |
|    value_loss               | 1.11         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2591275] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 14           |
|    time_elapsed             | 189          |
|    total_timesteps          | 2838528      |
| train/                      |              |
|    approx_kl                | 0.005463132  |
|    approx_ln(kl)            | -5.209733    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.75        |
|    explained_variance       | 0.987        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.87        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.155        |
|    n_updates                | 1438         |
|    policy_gradient_loss     | -0.0124      |
|    std                      | 0.965        |
|    value_loss               | 0.665        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.18809211] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 15            |
|    time_elapsed             | 202           |
|    total_timesteps          | 2840576       |
| train/                      |               |
|    approx_kl                | 0.009710444   |
|    approx_ln(kl)            | -4.6345534    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.58         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.206         |
|    n_updates                | 1439          |
|    policy_gradient_loss     | -0.0083       |
|    std                      | 0.966         |
|    value_loss               | 1.54          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.53733]   |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 16           |
|    time_elapsed             | 216          |
|    total_timesteps          | 2842624      |
| train/                      |              |
|    approx_kl                | 0.0065978337 |
|    approx_ln(kl)            | -5.0210137   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.76        |
|    explained_variance       | 0.983        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.334        |
|    ln(policy_gradient_loss) | -5.47        |
|    loss                     | 1.4          |
|    n_updates                | 1440         |
|    policy_gradient_loss     | 0.00422      |
|    std                      | 0.967        |
|    value_loss               | 2.12         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3001231] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 17           |
|    time_elapsed             | 230          |
|    total_timesteps          | 2844672      |
| train/                      |              |
|    approx_kl                | 0.0039389296 |
|    approx_ln(kl)            | -5.536846    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.76        |
|    explained_variance       | 0.985        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.689       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.502        |
|    n_updates                | 1441         |
|    policy_gradient_loss     | -0.00786     |
|    std                      | 0.968        |
|    value_loss               | 0.89         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30854306] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 18            |
|    time_elapsed             | 243           |
|    total_timesteps          | 2846720       |
| train/                      |               |
|    approx_kl                | 0.0036289766  |
|    approx_ln(kl)            | -5.6188045    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.453        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.636         |
|    n_updates                | 1442          |
|    policy_gradient_loss     | -0.00121      |
|    std                      | 0.968         |
|    value_loss               | 0.83          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24989836] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 19            |
|    time_elapsed             | 256           |
|    total_timesteps          | 2848768       |
| train/                      |               |
|    approx_kl                | 0.0068387766  |
|    approx_ln(kl)            | -4.9851465    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.751        |
|    ln(policy_gradient_loss) | -4.99         |
|    loss                     | 0.472         |
|    n_updates                | 1443          |
|    policy_gradient_loss     | 0.00683       |
|    std                      | 0.968         |
|    value_loss               | 1.72          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.14343835] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 20            |
|    time_elapsed             | 270           |
|    total_timesteps          | 2850816       |
| train/                      |               |
|    approx_kl                | 0.009778617   |
|    approx_ln(kl)            | -4.6275573    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.608        |
|    ln(policy_gradient_loss) | -4.99         |
|    loss                     | 0.544         |
|    n_updates                | 1444          |
|    policy_gradient_loss     | 0.00684       |
|    std                      | 0.969         |
|    value_loss               | 1.53          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.20083198] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 21            |
|    time_elapsed             | 283           |
|    total_timesteps          | 2852864       |
| train/                      |               |
|    approx_kl                | 0.009981213   |
|    approx_ln(kl)            | -4.607051     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.13         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.324         |
|    n_updates                | 1445          |
|    policy_gradient_loss     | -0.00645      |
|    std                      | 0.969         |
|    value_loss               | 0.808         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49872172] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 22            |
|    time_elapsed             | 297           |
|    total_timesteps          | 2854912       |
| train/                      |               |
|    approx_kl                | 0.007309832   |
|    approx_ln(kl)            | -4.9185348    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.01         |
|    ln(policy_gradient_loss) | -9.3          |
|    loss                     | 0.134         |
|    n_updates                | 1446          |
|    policy_gradient_loss     | 9.16e-05      |
|    std                      | 0.969         |
|    value_loss               | 0.366         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.450522] |
| time/                       |             |
|    fps                      | 151         |
|    iterations               | 23          |
|    time_elapsed             | 311         |
|    total_timesteps          | 2856960     |
| train/                      |             |
|    approx_kl                | 0.008497002 |
|    approx_ln(kl)            | -4.768042   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.77       |
|    explained_variance       | 0.988       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -0.837      |
|    ln(policy_gradient_loss) | -6.81       |
|    loss                     | 0.433       |
|    n_updates                | 1447        |
|    policy_gradient_loss     | 0.00111     |
|    std                      | 0.969       |
|    value_loss               | 0.767       |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34651524] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 24            |
|    time_elapsed             | 324           |
|    total_timesteps          | 2859008       |
| train/                      |               |
|    approx_kl                | 0.007975013   |
|    approx_ln(kl)            | -4.831442     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.808        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.446         |
|    n_updates                | 1448          |
|    policy_gradient_loss     | -0.00107      |
|    std                      | 0.97          |
|    value_loss               | 1.41          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.30233175] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 25            |
|    time_elapsed             | 338           |
|    total_timesteps          | 2861056       |
| train/                      |               |
|    approx_kl                | 0.008106902   |
|    approx_ln(kl)            | -4.8150396    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.973         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.403         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.5           |
|    n_updates                | 1449          |
|    policy_gradient_loss     | -0.00282      |
|    std                      | 0.972         |
|    value_loss               | 1.93          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2609978] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 26           |
|    time_elapsed             | 351          |
|    total_timesteps          | 2863104      |
| train/                      |              |
|    approx_kl                | 0.005759827  |
|    approx_ln(kl)            | -5.156848    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.949        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.07        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.126        |
|    n_updates                | 1450         |
|    policy_gradient_loss     | -0.0094      |
|    std                      | 0.974        |
|    value_loss               | 0.383        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.2831195] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 27           |
|    time_elapsed             | 364          |
|    total_timesteps          | 2865152      |
| train/                      |              |
|    approx_kl                | 0.008889834  |
|    approx_ln(kl)            | -4.722847    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.78        |
|    explained_variance       | 0.965        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.423        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.53         |
|    n_updates                | 1451         |
|    policy_gradient_loss     | -0.0131      |
|    std                      | 0.974        |
|    value_loss               | 2.25         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.46334514] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 28            |
|    time_elapsed             | 377           |
|    total_timesteps          | 2867200       |
| train/                      |               |
|    approx_kl                | 0.006901674   |
|    approx_ln(kl)            | -4.9759912    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.679        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.507         |
|    n_updates                | 1452          |
|    policy_gradient_loss     | -0.0022       |
|    std                      | 0.974         |
|    value_loss               | 1.38          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23451559] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 29            |
|    time_elapsed             | 391           |
|    total_timesteps          | 2869248       |
| train/                      |               |
|    approx_kl                | 0.0051652393  |
|    approx_ln(kl)            | -5.265804     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.683        |
|    ln(policy_gradient_loss) | -4.9          |
|    loss                     | 0.505         |
|    n_updates                | 1453          |
|    policy_gradient_loss     | 0.00748       |
|    std                      | 0.974         |
|    value_loss               | 1.22          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.28937724] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 30            |
|    time_elapsed             | 404           |
|    total_timesteps          | 2871296       |
| train/                      |               |
|    approx_kl                | 0.011418351   |
|    approx_ln(kl)            | -4.4725337    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.971         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.03         |
|    ln(policy_gradient_loss) | -4.43         |
|    loss                     | 0.358         |
|    n_updates                | 1454          |
|    policy_gradient_loss     | 0.0119        |
|    std                      | 0.973         |
|    value_loss               | 0.907         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40926522] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 31            |
|    time_elapsed             | 418           |
|    total_timesteps          | 2873344       |
| train/                      |               |
|    approx_kl                | 0.0077871443  |
|    approx_ln(kl)            | -4.855281     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.323        |
|    ln(policy_gradient_loss) | -6.4          |
|    loss                     | 0.724         |
|    n_updates                | 1455          |
|    policy_gradient_loss     | 0.00166       |
|    std                      | 0.972         |
|    value_loss               | 1.47          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.53078]   |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 32           |
|    time_elapsed             | 431          |
|    total_timesteps          | 2875392      |
| train/                      |              |
|    approx_kl                | 0.0088163335 |
|    approx_ln(kl)            | -4.731149    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.963        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.11        |
|    ln(policy_gradient_loss) | -5.5         |
|    loss                     | 0.331        |
|    n_updates                | 1456         |
|    policy_gradient_loss     | 0.00408      |
|    std                      | 0.971        |
|    value_loss               | 0.671        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36153665] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 33            |
|    time_elapsed             | 445           |
|    total_timesteps          | 2877440       |
| train/                      |               |
|    approx_kl                | 0.006669119   |
|    approx_ln(kl)            | -5.0102677    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.31         |
|    ln(policy_gradient_loss) | -6.21         |
|    loss                     | 0.269         |
|    n_updates                | 1457          |
|    policy_gradient_loss     | 0.002         |
|    std                      | 0.97          |
|    value_loss               | 0.976         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29444826] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 34            |
|    time_elapsed             | 458           |
|    total_timesteps          | 2879488       |
| train/                      |               |
|    approx_kl                | 0.005568196   |
|    approx_ln(kl)            | -5.1906843    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.281        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.755         |
|    n_updates                | 1458          |
|    policy_gradient_loss     | -0.00571      |
|    std                      | 0.969         |
|    value_loss               | 1.32          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22782066] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 35            |
|    time_elapsed             | 472           |
|    total_timesteps          | 2881536       |
| train/                      |               |
|    approx_kl                | 0.007154555   |
|    approx_ln(kl)            | -4.9400063    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.977         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.746        |
|    ln(policy_gradient_loss) | -4.26         |
|    loss                     | 0.474         |
|    n_updates                | 1459          |
|    policy_gradient_loss     | 0.0141        |
|    std                      | 0.969         |
|    value_loss               | 2.04          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27109888] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 36            |
|    time_elapsed             | 485           |
|    total_timesteps          | 2883584       |
| train/                      |               |
|    approx_kl                | 0.006218365   |
|    approx_ln(kl)            | -5.0802484    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.62         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.198         |
|    n_updates                | 1460          |
|    policy_gradient_loss     | -0.00235      |
|    std                      | 0.968         |
|    value_loss               | 0.43          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31073046] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 37            |
|    time_elapsed             | 498           |
|    total_timesteps          | 2885632       |
| train/                      |               |
|    approx_kl                | 0.008410564   |
|    approx_ln(kl)            | -4.778267     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.03         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.131         |
|    n_updates                | 1461          |
|    policy_gradient_loss     | -0.00728      |
|    std                      | 0.968         |
|    value_loss               | 0.438         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.46140605] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 38            |
|    time_elapsed             | 512           |
|    total_timesteps          | 2887680       |
| train/                      |               |
|    approx_kl                | 0.009348796   |
|    approx_ln(kl)            | -4.672508     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.88         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.153         |
|    n_updates                | 1462          |
|    policy_gradient_loss     | -0.00868      |
|    std                      | 0.967         |
|    value_loss               | 0.38          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.32292834] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 39            |
|    time_elapsed             | 526           |
|    total_timesteps          | 2889728       |
| train/                      |               |
|    approx_kl                | 0.011248159   |
|    approx_ln(kl)            | -4.4875507    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.62         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.198         |
|    n_updates                | 1463          |
|    policy_gradient_loss     | -0.00431      |
|    std                      | 0.966         |
|    value_loss               | 1.03          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4033105] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 40           |
|    time_elapsed             | 540          |
|    total_timesteps          | 2891776      |
| train/                      |              |
|    approx_kl                | 0.006112594  |
|    approx_ln(kl)            | -5.097404    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.76        |
|    explained_variance       | 0.987        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.44        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0872       |
|    n_updates                | 1464         |
|    policy_gradient_loss     | -0.00527     |
|    std                      | 0.966        |
|    value_loss               | 0.29         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.23630644] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 41            |
|    time_elapsed             | 553           |
|    total_timesteps          | 2893824       |
| train/                      |               |
|    approx_kl                | 0.009254369   |
|    approx_ln(kl)            | -4.6826596    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.25         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0389        |
|    n_updates                | 1465          |
|    policy_gradient_loss     | -0.000766     |
|    std                      | 0.965         |
|    value_loss               | 0.101         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22823687] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 42            |
|    time_elapsed             | 566           |
|    total_timesteps          | 2895872       |
| train/                      |               |
|    approx_kl                | 0.0101303505  |
|    approx_ln(kl)            | -4.5922194    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.1          |
|    ln(policy_gradient_loss) | -5.43         |
|    loss                     | 0.122         |
|    n_updates                | 1466          |
|    policy_gradient_loss     | 0.00438       |
|    std                      | 0.965         |
|    value_loss               | 0.268         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.16815236] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 43            |
|    time_elapsed             | 580           |
|    total_timesteps          | 2897920       |
| train/                      |               |
|    approx_kl                | 0.011139266   |
|    approx_ln(kl)            | -4.4972787    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.97         |
|    ln(policy_gradient_loss) | -5.2          |
|    loss                     | 0.14          |
|    n_updates                | 1467          |
|    policy_gradient_loss     | 0.00551       |
|    std                      | 0.964         |
|    value_loss               | 0.319         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29071122] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 44            |
|    time_elapsed             | 593           |
|    total_timesteps          | 2899968       |
| train/                      |               |
|    approx_kl                | 0.0050419644  |
|    approx_ln(kl)            | -5.2899594    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.629        |
|    ln(policy_gradient_loss) | -6.33         |
|    loss                     | 0.533         |
|    n_updates                | 1468          |
|    policy_gradient_loss     | 0.00178       |
|    std                      | 0.965         |
|    value_loss               | 0.736         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29573748] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 45            |
|    time_elapsed             | 607           |
|    total_timesteps          | 2902016       |
| train/                      |               |
|    approx_kl                | 0.008823229   |
|    approx_ln(kl)            | -4.730367     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.964         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.309        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.734         |
|    n_updates                | 1469          |
|    policy_gradient_loss     | -0.00226      |
|    std                      | 0.965         |
|    value_loss               | 1.95          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23975387] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 46            |
|    time_elapsed             | 620           |
|    total_timesteps          | 2904064       |
| train/                      |               |
|    approx_kl                | 0.0030900012  |
|    approx_ln(kl)            | -5.779584     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.97          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.74         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.175         |
|    n_updates                | 1470          |
|    policy_gradient_loss     | -0.00378      |
|    std                      | 0.965         |
|    value_loss               | 1.13          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.24585965] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 47            |
|    time_elapsed             | 633           |
|    total_timesteps          | 2906112       |
| train/                      |               |
|    approx_kl                | 0.006292809   |
|    approx_ln(kl)            | -5.068348     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.231        |
|    ln(policy_gradient_loss) | -5.39         |
|    loss                     | 0.793         |
|    n_updates                | 1471          |
|    policy_gradient_loss     | 0.00457       |
|    std                      | 0.966         |
|    value_loss               | 0.925         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49736407] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 48            |
|    time_elapsed             | 646           |
|    total_timesteps          | 2908160       |
| train/                      |               |
|    approx_kl                | 0.004786997   |
|    approx_ln(kl)            | -5.341852     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.974         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.01         |
|    ln(policy_gradient_loss) | -6.38         |
|    loss                     | 0.365         |
|    n_updates                | 1472          |
|    policy_gradient_loss     | 0.0017        |
|    std                      | 0.965         |
|    value_loss               | 0.897         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34109733] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 49            |
|    time_elapsed             | 660           |
|    total_timesteps          | 2910208       |
| train/                      |               |
|    approx_kl                | 0.005410948   |
|    approx_ln(kl)            | -5.219331     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.971         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.762        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.467         |
|    n_updates                | 1473          |
|    policy_gradient_loss     | -0.00306      |
|    std                      | 0.966         |
|    value_loss               | 0.825         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
--------------------------------------
| reward             | [-0.29999366] |
| time/              |               |
|    fps             | 157           |
|    iterations      | 1             |
|    time_elapsed    | 12            |
|    total_timesteps | 2912256       |
--------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27611148] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 2             |
|    time_elapsed             | 26            |
|    total_timesteps          | 2914304       |
| train/                      |               |
|    approx_kl                | 0.005731466   |
|    approx_ln(kl)            | -5.1617837    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.46         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.232         |
|    n_updates                | 1475          |
|    policy_gradient_loss     | -0.00714      |
|    std                      | 0.966         |
|    value_loss               | 0.497         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27380654] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 3             |
|    time_elapsed             | 39            |
|    total_timesteps          | 2916352       |
| train/                      |               |
|    approx_kl                | 0.008360073   |
|    approx_ln(kl)            | -4.784288     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.938         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.921        |
|    ln(policy_gradient_loss) | -5.51         |
|    loss                     | 0.398         |
|    n_updates                | 1476          |
|    policy_gradient_loss     | 0.00404       |
|    std                      | 0.967         |
|    value_loss               | 1.07          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32416362] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 4             |
|    time_elapsed             | 52            |
|    total_timesteps          | 2918400       |
| train/                      |               |
|    approx_kl                | 0.00679829    |
|    approx_ln(kl)            | -4.991084     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.971         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.48         |
|    ln(policy_gradient_loss) | -3.57         |
|    loss                     | 0.619         |
|    n_updates                | 1477          |
|    policy_gradient_loss     | 0.0283        |
|    std                      | 0.967         |
|    value_loss               | 1.13          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34992135] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 5             |
|    time_elapsed             | 65            |
|    total_timesteps          | 2920448       |
| train/                      |               |
|    approx_kl                | 0.0074749975  |
|    approx_ln(kl)            | -4.8961916    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.902         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.23         |
|    ln(policy_gradient_loss) | -6.86         |
|    loss                     | 0.293         |
|    n_updates                | 1478          |
|    policy_gradient_loss     | 0.00105       |
|    std                      | 0.967         |
|    value_loss               | 0.874         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.20692456] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 6             |
|    time_elapsed             | 78            |
|    total_timesteps          | 2922496       |
| train/                      |               |
|    approx_kl                | 0.008743052   |
|    approx_ln(kl)            | -4.7394958    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.953         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.995         |
|    ln(policy_gradient_loss) | -5.27         |
|    loss                     | 2.71          |
|    n_updates                | 1479          |
|    policy_gradient_loss     | 0.00514       |
|    std                      | 0.968         |
|    value_loss               | 3.2           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31547856] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 7             |
|    time_elapsed             | 92            |
|    total_timesteps          | 2924544       |
| train/                      |               |
|    approx_kl                | 0.0055343993  |
|    approx_ln(kl)            | -5.196772     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.968         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.882        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.414         |
|    n_updates                | 1480          |
|    policy_gradient_loss     | -0.00305      |
|    std                      | 0.969         |
|    value_loss               | 1.05          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.26442772] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 8             |
|    time_elapsed             | 106           |
|    total_timesteps          | 2926592       |
| train/                      |               |
|    approx_kl                | 0.00991452    |
|    approx_ln(kl)            | -4.6137547    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.895         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0302       |
|    ln(policy_gradient_loss) | -5.16         |
|    loss                     | 0.97          |
|    n_updates                | 1481          |
|    policy_gradient_loss     | 0.00573       |
|    std                      | 0.969         |
|    value_loss               | 3.63          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.21515305] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 9             |
|    time_elapsed             | 120           |
|    total_timesteps          | 2928640       |
| train/                      |               |
|    approx_kl                | 0.008588603   |
|    approx_ln(kl)            | -4.7573195    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.972         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.43         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.24          |
|    n_updates                | 1482          |
|    policy_gradient_loss     | -0.00442      |
|    std                      | 0.97          |
|    value_loss               | 0.83          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.20018625] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 10            |
|    time_elapsed             | 133           |
|    total_timesteps          | 2930688       |
| train/                      |               |
|    approx_kl                | 0.0046980698  |
|    approx_ln(kl)            | -5.3606033    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.932         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.2          |
|    ln(policy_gradient_loss) | -5.15         |
|    loss                     | 0.302         |
|    n_updates                | 1483          |
|    policy_gradient_loss     | 0.00582       |
|    std                      | 0.971         |
|    value_loss               | 0.618         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2825384] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 11           |
|    time_elapsed             | 146          |
|    total_timesteps          | 2932736      |
| train/                      |              |
|    approx_kl                | 0.007241437  |
|    approx_ln(kl)            | -4.9279356   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.966        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.17        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.311        |
|    n_updates                | 1484         |
|    policy_gradient_loss     | -0.0044      |
|    std                      | 0.971        |
|    value_loss               | 2.9          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.34892258] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 12            |
|    time_elapsed             | 160           |
|    total_timesteps          | 2934784       |
| train/                      |               |
|    approx_kl                | 0.009627486   |
|    approx_ln(kl)            | -4.643133     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.936         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.706         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 2.03          |
|    n_updates                | 1485          |
|    policy_gradient_loss     | -0.0136       |
|    std                      | 0.971         |
|    value_loss               | 2.41          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2604692] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 13           |
|    time_elapsed             | 174          |
|    total_timesteps          | 2936832      |
| train/                      |              |
|    approx_kl                | 0.0080307145 |
|    approx_ln(kl)            | -4.824482    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.981        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.1         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.334        |
|    n_updates                | 1486         |
|    policy_gradient_loss     | -0.00496     |
|    std                      | 0.971        |
|    value_loss               | 0.779        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34447828] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 14            |
|    time_elapsed             | 187           |
|    total_timesteps          | 2938880       |
| train/                      |               |
|    approx_kl                | 0.008908773   |
|    approx_ln(kl)            | -4.720719     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.963         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.671        |
|    ln(policy_gradient_loss) | -5.21         |
|    loss                     | 0.511         |
|    n_updates                | 1487          |
|    policy_gradient_loss     | 0.00544       |
|    std                      | 0.971         |
|    value_loss               | 1.08          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3602145] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 15           |
|    time_elapsed             | 203          |
|    total_timesteps          | 2940928      |
| train/                      |              |
|    approx_kl                | 0.008030713  |
|    approx_ln(kl)            | -4.824482    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.962        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.32        |
|    ln(policy_gradient_loss) | -5.76        |
|    loss                     | 0.268        |
|    n_updates                | 1488         |
|    policy_gradient_loss     | 0.00317      |
|    std                      | 0.97         |
|    value_loss               | 0.697        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.52302665] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 16            |
|    time_elapsed             | 217           |
|    total_timesteps          | 2942976       |
| train/                      |               |
|    approx_kl                | 0.010946374   |
|    approx_ln(kl)            | -4.514747     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.967         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.994        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.37          |
|    n_updates                | 1489          |
|    policy_gradient_loss     | -0.0173       |
|    std                      | 0.97          |
|    value_loss               | 1.16          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31652692] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 17            |
|    time_elapsed             | 231           |
|    total_timesteps          | 2945024       |
| train/                      |               |
|    approx_kl                | 0.009783185   |
|    approx_ln(kl)            | -4.62709      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.969         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.08         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.338         |
|    n_updates                | 1490          |
|    policy_gradient_loss     | -0.000843     |
|    std                      | 0.97          |
|    value_loss               | 0.721         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.20398036] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 18            |
|    time_elapsed             | 246           |
|    total_timesteps          | 2947072       |
| train/                      |               |
|    approx_kl                | 0.012623424   |
|    approx_ln(kl)            | -4.372201     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.936         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.184         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.2           |
|    n_updates                | 1491          |
|    policy_gradient_loss     | -0.00454      |
|    std                      | 0.97          |
|    value_loss               | 1.73          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2905123] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 19           |
|    time_elapsed             | 260          |
|    total_timesteps          | 2949120      |
| train/                      |              |
|    approx_kl                | 0.008841482  |
|    approx_ln(kl)            | -4.7283006   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.76        |
|    explained_variance       | 0.957        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.08        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.339        |
|    n_updates                | 1492         |
|    policy_gradient_loss     | -0.0115      |
|    std                      | 0.97         |
|    value_loss               | 0.864        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.32130745] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 20            |
|    time_elapsed             | 274           |
|    total_timesteps          | 2951168       |
| train/                      |               |
|    approx_kl                | 0.011307648   |
|    approx_ln(kl)            | -4.482276     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2            |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.135         |
|    n_updates                | 1493          |
|    policy_gradient_loss     | -0.0294       |
|    std                      | 0.969         |
|    value_loss               | 0.404         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.29865864] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 21            |
|    time_elapsed             | 288           |
|    total_timesteps          | 2953216       |
| train/                      |               |
|    approx_kl                | 0.011467707   |
|    approx_ln(kl)            | -4.46822      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.965         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.97         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.139         |
|    n_updates                | 1494          |
|    policy_gradient_loss     | -0.0038       |
|    std                      | 0.969         |
|    value_loss               | 0.58          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.21013571] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 22            |
|    time_elapsed             | 301           |
|    total_timesteps          | 2955264       |
| train/                      |               |
|    approx_kl                | 0.009434362   |
|    approx_ln(kl)            | -4.663397     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.927         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.235        |
|    ln(policy_gradient_loss) | -5            |
|    loss                     | 0.791         |
|    n_updates                | 1495          |
|    policy_gradient_loss     | 0.00673       |
|    std                      | 0.969         |
|    value_loss               | 4.32          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24313237] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 23            |
|    time_elapsed             | 314           |
|    total_timesteps          | 2957312       |
| train/                      |               |
|    approx_kl                | 0.008524616   |
|    approx_ln(kl)            | -4.764797     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.954         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.684         |
|    ln(policy_gradient_loss) | -5.73         |
|    loss                     | 1.98          |
|    n_updates                | 1496          |
|    policy_gradient_loss     | 0.00326       |
|    std                      | 0.969         |
|    value_loss               | 2.97          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.47964796] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 24            |
|    time_elapsed             | 328           |
|    total_timesteps          | 2959360       |
| train/                      |               |
|    approx_kl                | 0.008494895   |
|    approx_ln(kl)            | -4.76829      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.964         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.14         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.118         |
|    n_updates                | 1497          |
|    policy_gradient_loss     | -0.00442      |
|    std                      | 0.969         |
|    value_loss               | 0.455         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.52910423] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 25            |
|    time_elapsed             | 342           |
|    total_timesteps          | 2961408       |
| train/                      |               |
|    approx_kl                | 0.006443597   |
|    approx_ln(kl)            | -5.044668     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.923         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.592        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.553         |
|    n_updates                | 1498          |
|    policy_gradient_loss     | -0.00144      |
|    std                      | 0.969         |
|    value_loss               | 1.76          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30044648] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 26            |
|    time_elapsed             | 355           |
|    total_timesteps          | 2963456       |
| train/                      |               |
|    approx_kl                | 0.0076603834  |
|    approx_ln(kl)            | -4.871693     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.97          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.02         |
|    ln(policy_gradient_loss) | -4.93         |
|    loss                     | 0.361         |
|    n_updates                | 1499          |
|    policy_gradient_loss     | 0.00725       |
|    std                      | 0.97          |
|    value_loss               | 0.558         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5268562] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 27           |
|    time_elapsed             | 369          |
|    total_timesteps          | 2965504      |
| train/                      |              |
|    approx_kl                | 0.010172957  |
|    approx_ln(kl)            | -4.588022    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.977        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.853        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 2.35         |
|    n_updates                | 1500         |
|    policy_gradient_loss     | -0.00814     |
|    std                      | 0.971        |
|    value_loss               | 3.91         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27621266] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 28            |
|    time_elapsed             | 383           |
|    total_timesteps          | 2967552       |
| train/                      |               |
|    approx_kl                | 0.006922182   |
|    approx_ln(kl)            | -4.9730244    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.756        |
|    ln(policy_gradient_loss) | -5.02         |
|    loss                     | 0.47          |
|    n_updates                | 1501          |
|    policy_gradient_loss     | 0.00663       |
|    std                      | 0.972         |
|    value_loss               | 0.639         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.18685272] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 29            |
|    time_elapsed             | 397           |
|    total_timesteps          | 2969600       |
| train/                      |               |
|    approx_kl                | 0.009120174   |
|    approx_ln(kl)            | -4.6972666    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.95          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.62         |
|    ln(policy_gradient_loss) | -5.32         |
|    loss                     | 0.198         |
|    n_updates                | 1502          |
|    policy_gradient_loss     | 0.0049        |
|    std                      | 0.974         |
|    value_loss               | 2.24          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24183999] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 30            |
|    time_elapsed             | 410           |
|    total_timesteps          | 2971648       |
| train/                      |               |
|    approx_kl                | 0.006628399   |
|    approx_ln(kl)            | -5.016392     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.85         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.157         |
|    n_updates                | 1503          |
|    policy_gradient_loss     | -0.00317      |
|    std                      | 0.973         |
|    value_loss               | 0.296         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26352268] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 31            |
|    time_elapsed             | 423           |
|    total_timesteps          | 2973696       |
| train/                      |               |
|    approx_kl                | 0.008956612   |
|    approx_ln(kl)            | -4.715363     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.76         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.172         |
|    n_updates                | 1504          |
|    policy_gradient_loss     | -0.00421      |
|    std                      | 0.973         |
|    value_loss               | 0.48          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.26651898] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 32            |
|    time_elapsed             | 437           |
|    total_timesteps          | 2975744       |
| train/                      |               |
|    approx_kl                | 0.0062863887  |
|    approx_ln(kl)            | -5.0693684    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.57         |
|    ln(policy_gradient_loss) | -5.19         |
|    loss                     | 0.209         |
|    n_updates                | 1505          |
|    policy_gradient_loss     | 0.0056        |
|    std                      | 0.972         |
|    value_loss               | 1             |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.05
-----------------------------------------------
| reward                      | [-0.19623907] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 33            |
|    time_elapsed             | 451           |
|    total_timesteps          | 2977792       |
| train/                      |               |
|    approx_kl                | 0.009277027   |
|    approx_ln(kl)            | -4.680214     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.17         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.309         |
|    n_updates                | 1506          |
|    policy_gradient_loss     | -0.00967      |
|    std                      | 0.971         |
|    value_loss               | 1.19          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.15171602] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 34            |
|    time_elapsed             | 464           |
|    total_timesteps          | 2979840       |
| train/                      |               |
|    approx_kl                | 0.0069738054  |
|    approx_ln(kl)            | -4.9655943    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.143        |
|    ln(policy_gradient_loss) | -5.11         |
|    loss                     | 0.866         |
|    n_updates                | 1507          |
|    policy_gradient_loss     | 0.00604       |
|    std                      | 0.971         |
|    value_loss               | 1             |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.20188834] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 35            |
|    time_elapsed             | 477           |
|    total_timesteps          | 2981888       |
| train/                      |               |
|    approx_kl                | 0.0050149085  |
|    approx_ln(kl)            | -5.29534      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.608        |
|    ln(policy_gradient_loss) | -5.6          |
|    loss                     | 0.544         |
|    n_updates                | 1508          |
|    policy_gradient_loss     | 0.00371       |
|    std                      | 0.971         |
|    value_loss               | 1.11          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.34825718] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 36            |
|    time_elapsed             | 491           |
|    total_timesteps          | 2983936       |
| train/                      |               |
|    approx_kl                | 0.008448501   |
|    approx_ln(kl)            | -4.773766     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.94         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.144         |
|    n_updates                | 1509          |
|    policy_gradient_loss     | -0.00381      |
|    std                      | 0.969         |
|    value_loss               | 0.376         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2850395] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 37           |
|    time_elapsed             | 504          |
|    total_timesteps          | 2985984      |
| train/                      |              |
|    approx_kl                | 0.0073665716 |
|    approx_ln(kl)            | -4.910803    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.76        |
|    explained_variance       | 0.963        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.0626      |
|    ln(policy_gradient_loss) | -4.83        |
|    loss                     | 0.939        |
|    n_updates                | 1510         |
|    policy_gradient_loss     | 0.00802      |
|    std                      | 0.969        |
|    value_loss               | 1.31         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.5297506] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 38           |
|    time_elapsed             | 517          |
|    total_timesteps          | 2988032      |
| train/                      |              |
|    approx_kl                | 0.008845665  |
|    approx_ln(kl)            | -4.727828    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.76        |
|    explained_variance       | 0.97         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.03        |
|    ln(policy_gradient_loss) | -5.6         |
|    loss                     | 0.356        |
|    n_updates                | 1511         |
|    policy_gradient_loss     | 0.00369      |
|    std                      | 0.969        |
|    value_loss               | 1.16         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.43623832] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 39            |
|    time_elapsed             | 530           |
|    total_timesteps          | 2990080       |
| train/                      |               |
|    approx_kl                | 0.012329916   |
|    approx_ln(kl)            | -4.3957267    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.639        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.528         |
|    n_updates                | 1512          |
|    policy_gradient_loss     | -0.0228       |
|    std                      | 0.969         |
|    value_loss               | 0.915         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2842706] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 40           |
|    time_elapsed             | 544          |
|    total_timesteps          | 2992128      |
| train/                      |              |
|    approx_kl                | 0.008824064  |
|    approx_ln(kl)            | -4.730273    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.76        |
|    explained_variance       | 0.98         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.0121      |
|    ln(policy_gradient_loss) | -4.36        |
|    loss                     | 0.988        |
|    n_updates                | 1513         |
|    policy_gradient_loss     | 0.0128       |
|    std                      | 0.97         |
|    value_loss               | 1.47         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3098075] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 41           |
|    time_elapsed             | 557          |
|    total_timesteps          | 2994176      |
| train/                      |              |
|    approx_kl                | 0.006228241  |
|    approx_ln(kl)            | -5.0786614   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.76        |
|    explained_variance       | 0.975        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.47        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.625        |
|    n_updates                | 1514         |
|    policy_gradient_loss     | -0.00334     |
|    std                      | 0.97         |
|    value_loss               | 1.14         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32775337] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 42            |
|    time_elapsed             | 571           |
|    total_timesteps          | 2996224       |
| train/                      |               |
|    approx_kl                | 0.005064803   |
|    approx_ln(kl)            | -5.28544      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.173         |
|    ln(policy_gradient_loss) | -4.81         |
|    loss                     | 1.19          |
|    n_updates                | 1515          |
|    policy_gradient_loss     | 0.00815       |
|    std                      | 0.969         |
|    value_loss               | 1.53          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.52850306] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 43            |
|    time_elapsed             | 584           |
|    total_timesteps          | 2998272       |
| train/                      |               |
|    approx_kl                | 0.009502132   |
|    approx_ln(kl)            | -4.656239     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.81         |
|    ln(policy_gradient_loss) | -6.24         |
|    loss                     | 0.164         |
|    n_updates                | 1516          |
|    policy_gradient_loss     | 0.00194       |
|    std                      | 0.968         |
|    value_loss               | 0.92          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28387317] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 44            |
|    time_elapsed             | 597           |
|    total_timesteps          | 3000320       |
| train/                      |               |
|    approx_kl                | 0.0072630914  |
|    approx_ln(kl)            | -4.9249496    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.975         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.000845      |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1             |
|    n_updates                | 1517          |
|    policy_gradient_loss     | -0.00281      |
|    std                      | 0.968         |
|    value_loss               | 3.23          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3052066] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 45           |
|    time_elapsed             | 610          |
|    total_timesteps          | 3002368      |
| train/                      |              |
|    approx_kl                | 0.007585055  |
|    approx_ln(kl)            | -4.8815756   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.76        |
|    explained_variance       | 0.986        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.223       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.8          |
|    n_updates                | 1518         |
|    policy_gradient_loss     | -0.0014      |
|    std                      | 0.967        |
|    value_loss               | 0.777        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.20921662] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 46            |
|    time_elapsed             | 624           |
|    total_timesteps          | 3004416       |
| train/                      |               |
|    approx_kl                | 0.009892033   |
|    approx_ln(kl)            | -4.6160254    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.753        |
|    ln(policy_gradient_loss) | -5.26         |
|    loss                     | 0.471         |
|    n_updates                | 1519          |
|    policy_gradient_loss     | 0.00519       |
|    std                      | 0.967         |
|    value_loss               | 0.739         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33272713] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 47            |
|    time_elapsed             | 637           |
|    total_timesteps          | 3006464       |
| train/                      |               |
|    approx_kl                | 0.006515371   |
|    approx_ln(kl)            | -5.0335913    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.298        |
|    ln(policy_gradient_loss) | -9.92         |
|    loss                     | 0.743         |
|    n_updates                | 1520          |
|    policy_gradient_loss     | 4.91e-05      |
|    std                      | 0.967         |
|    value_loss               | 2.47          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35118002] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 48            |
|    time_elapsed             | 650           |
|    total_timesteps          | 3008512       |
| train/                      |               |
|    approx_kl                | 0.009286717   |
|    approx_ln(kl)            | -4.67917      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.82         |
|    ln(policy_gradient_loss) | -7.61         |
|    loss                     | 0.44          |
|    n_updates                | 1521          |
|    policy_gradient_loss     | 0.000497      |
|    std                      | 0.968         |
|    value_loss               | 1.26          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22269674] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 49            |
|    time_elapsed             | 664           |
|    total_timesteps          | 3010560       |
| train/                      |               |
|    approx_kl                | 0.008648075   |
|    approx_ln(kl)            | -4.7504187    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.95          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.15         |
|    ln(policy_gradient_loss) | -6.06         |
|    loss                     | 0.317         |
|    n_updates                | 1522          |
|    policy_gradient_loss     | 0.00234       |
|    std                      | 0.968         |
|    value_loss               | 0.908         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
--------------------------------------
| reward             | [-0.29380128] |
| time/              |               |
|    fps             | 153           |
|    iterations      | 1             |
|    time_elapsed    | 13            |
|    total_timesteps | 3012608       |
--------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24933878] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 2             |
|    time_elapsed             | 26            |
|    total_timesteps          | 3014656       |
| train/                      |               |
|    approx_kl                | 0.0075148335  |
|    approx_ln(kl)            | -4.8908763    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.963         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.122         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.13          |
|    n_updates                | 1524          |
|    policy_gradient_loss     | -0.00146      |
|    std                      | 0.972         |
|    value_loss               | 2.51          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3548356] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 3            |
|    time_elapsed             | 39           |
|    total_timesteps          | 3016704      |
| train/                      |              |
|    approx_kl                | 0.0052978136 |
|    approx_ln(kl)            | -5.240461    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.974        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.531       |
|    ln(policy_gradient_loss) | -5.89        |
|    loss                     | 0.588        |
|    n_updates                | 1525         |
|    policy_gradient_loss     | 0.00277      |
|    std                      | 0.974        |
|    value_loss               | 1.16         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
-----------------------------------------------
| reward                      | [-0.34379143] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 4             |
|    time_elapsed             | 52            |
|    total_timesteps          | 3018752       |
| train/                      |               |
|    approx_kl                | 0.01215008    |
|    approx_ln(kl)            | -4.4104195    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.949         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.785         |
|    ln(policy_gradient_loss) | -5.1          |
|    loss                     | 2.19          |
|    n_updates                | 1526          |
|    policy_gradient_loss     | 0.00608       |
|    std                      | 0.974         |
|    value_loss               | 1.85          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.29796016] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 5             |
|    time_elapsed             | 65            |
|    total_timesteps          | 3020800       |
| train/                      |               |
|    approx_kl                | 0.013454309   |
|    approx_ln(kl)            | -4.308456     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.14         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.32          |
|    n_updates                | 1527          |
|    policy_gradient_loss     | -0.0087       |
|    std                      | 0.974         |
|    value_loss               | 0.671         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42239425] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 6             |
|    time_elapsed             | 78            |
|    total_timesteps          | 3022848       |
| train/                      |               |
|    approx_kl                | 0.009048116   |
|    approx_ln(kl)            | -4.705199     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.961         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.00471      |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.995         |
|    n_updates                | 1528          |
|    policy_gradient_loss     | -0.0243       |
|    std                      | 0.975         |
|    value_loss               | 1.25          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.21542709] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 7             |
|    time_elapsed             | 91            |
|    total_timesteps          | 3024896       |
| train/                      |               |
|    approx_kl                | 0.0100365905  |
|    approx_ln(kl)            | -4.6015177    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.63         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.196         |
|    n_updates                | 1529          |
|    policy_gradient_loss     | -0.0029       |
|    std                      | 0.975         |
|    value_loss               | 1.28          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.28214332] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 8             |
|    time_elapsed             | 105           |
|    total_timesteps          | 3026944       |
| train/                      |               |
|    approx_kl                | 0.011367213   |
|    approx_ln(kl)            | -4.477022     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.378        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.685         |
|    n_updates                | 1530          |
|    policy_gradient_loss     | -0.00862      |
|    std                      | 0.975         |
|    value_loss               | 0.974         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
----------------------------------------------
| reward                      | [-0.2910098] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 9            |
|    time_elapsed             | 119          |
|    total_timesteps          | 3028992      |
| train/                      |              |
|    approx_kl                | 0.011076422  |
|    approx_ln(kl)            | -4.5029364   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.986        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.47        |
|    ln(policy_gradient_loss) | -5.93        |
|    loss                     | 0.229        |
|    n_updates                | 1531         |
|    policy_gradient_loss     | 0.00267      |
|    std                      | 0.976        |
|    value_loss               | 0.96         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.466298] |
| time/                       |             |
|    fps                      | 154         |
|    iterations               | 10          |
|    time_elapsed             | 132         |
|    total_timesteps          | 3031040     |
| train/                      |             |
|    approx_kl                | 0.010827498 |
|    approx_ln(kl)            | -4.525666   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.78       |
|    explained_variance       | 0.973       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 0.103       |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 1.11        |
|    n_updates                | 1532        |
|    policy_gradient_loss     | -0.00141    |
|    std                      | 0.977       |
|    value_loss               | 1.84        |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.34336245] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 11            |
|    time_elapsed             | 145           |
|    total_timesteps          | 3033088       |
| train/                      |               |
|    approx_kl                | 0.015563905   |
|    approx_ln(kl)            | -4.162801     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.971         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 1.14          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 3.13          |
|    n_updates                | 1533          |
|    policy_gradient_loss     | -0.00636      |
|    std                      | 0.978         |
|    value_loss               | 4.08          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.33327222] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 12            |
|    time_elapsed             | 158           |
|    total_timesteps          | 3035136       |
| train/                      |               |
|    approx_kl                | 0.018083626   |
|    approx_ln(kl)            | -4.0127482    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.573         |
|    ln(policy_gradient_loss) | -4.85         |
|    loss                     | 1.77          |
|    n_updates                | 1534          |
|    policy_gradient_loss     | 0.00784       |
|    std                      | 0.978         |
|    value_loss               | 2.63          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33380762] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 13            |
|    time_elapsed             | 172           |
|    total_timesteps          | 3037184       |
| train/                      |               |
|    approx_kl                | 0.007336192   |
|    approx_ln(kl)            | -4.9149356    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.976         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.32         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.267         |
|    n_updates                | 1535          |
|    policy_gradient_loss     | -0.0136       |
|    std                      | 0.979         |
|    value_loss               | 0.897         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24455415] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 14            |
|    time_elapsed             | 185           |
|    total_timesteps          | 3039232       |
| train/                      |               |
|    approx_kl                | 0.009425775   |
|    approx_ln(kl)            | -4.664307     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.486        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.615         |
|    n_updates                | 1536          |
|    policy_gradient_loss     | -0.0195       |
|    std                      | 0.979         |
|    value_loss               | 0.953         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30508375] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 15            |
|    time_elapsed             | 198           |
|    total_timesteps          | 3041280       |
| train/                      |               |
|    approx_kl                | 0.006946024   |
|    approx_ln(kl)            | -4.969586     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.881        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.414         |
|    n_updates                | 1537          |
|    policy_gradient_loss     | -0.00235      |
|    std                      | 0.979         |
|    value_loss               | 1.45          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2948383] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 16           |
|    time_elapsed             | 211          |
|    total_timesteps          | 3043328      |
| train/                      |              |
|    approx_kl                | 0.008807287  |
|    approx_ln(kl)            | -4.732176    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.78        |
|    explained_variance       | 0.982        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.41        |
|    ln(policy_gradient_loss) | -5.25        |
|    loss                     | 0.244        |
|    n_updates                | 1538         |
|    policy_gradient_loss     | 0.00523      |
|    std                      | 0.98         |
|    value_loss               | 0.994        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29413494] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 17            |
|    time_elapsed             | 224           |
|    total_timesteps          | 3045376       |
| train/                      |               |
|    approx_kl                | 0.004861769   |
|    approx_ln(kl)            | -5.326353     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.93         |
|    ln(policy_gradient_loss) | -5.26         |
|    loss                     | 0.145         |
|    n_updates                | 1539          |
|    policy_gradient_loss     | 0.00521       |
|    std                      | 0.981         |
|    value_loss               | 0.323         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.07
-----------------------------------------------
| reward                      | [-0.23228063] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 18            |
|    time_elapsed             | 237           |
|    total_timesteps          | 3047424       |
| train/                      |               |
|    approx_kl                | 0.018067598   |
|    approx_ln(kl)            | -4.013635     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.798        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.45          |
|    n_updates                | 1540          |
|    policy_gradient_loss     | -0.00139      |
|    std                      | 0.982         |
|    value_loss               | 0.789         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.34817287] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 19            |
|    time_elapsed             | 251           |
|    total_timesteps          | 3049472       |
| train/                      |               |
|    approx_kl                | 0.013409474   |
|    approx_ln(kl)            | -4.311794     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.74         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0648        |
|    n_updates                | 1541          |
|    policy_gradient_loss     | -0.00543      |
|    std                      | 0.983         |
|    value_loss               | 0.157         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31071246] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 20            |
|    time_elapsed             | 264           |
|    total_timesteps          | 3051520       |
| train/                      |               |
|    approx_kl                | 0.0058049485  |
|    approx_ln(kl)            | -5.1490445    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.759        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.468         |
|    n_updates                | 1542          |
|    policy_gradient_loss     | -0.0385       |
|    std                      | 0.984         |
|    value_loss               | 0.508         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.28390807] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 21            |
|    time_elapsed             | 277           |
|    total_timesteps          | 3053568       |
| train/                      |               |
|    approx_kl                | 0.008385571   |
|    approx_ln(kl)            | -4.781243     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.6          |
|    ln(policy_gradient_loss) | -4.43         |
|    loss                     | 0.201         |
|    n_updates                | 1543          |
|    policy_gradient_loss     | 0.0119        |
|    std                      | 0.985         |
|    value_loss               | 0.415         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39844027] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 22            |
|    time_elapsed             | 290           |
|    total_timesteps          | 3055616       |
| train/                      |               |
|    approx_kl                | 0.008619851   |
|    approx_ln(kl)            | -4.7536874    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.377        |
|    ln(policy_gradient_loss) | -5.2          |
|    loss                     | 0.686         |
|    n_updates                | 1544          |
|    policy_gradient_loss     | 0.00552       |
|    std                      | 0.987         |
|    value_loss               | 1.28          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.3580015] |
| time/                       |              |
|    fps                      | 155          |
|    iterations               | 23           |
|    time_elapsed             | 303          |
|    total_timesteps          | 3057664      |
| train/                      |              |
|    approx_kl                | 0.014388327  |
|    approx_ln(kl)            | -4.2413383   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.968        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.43        |
|    ln(policy_gradient_loss) | -8.12        |
|    loss                     | 0.0877       |
|    n_updates                | 1545         |
|    policy_gradient_loss     | 0.000297     |
|    std                      | 0.988        |
|    value_loss               | 0.318        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.2664384] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 24           |
|    time_elapsed             | 317          |
|    total_timesteps          | 3059712      |
| train/                      |              |
|    approx_kl                | 0.010043513  |
|    approx_ln(kl)            | -4.600828    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.986        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.26        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.104        |
|    n_updates                | 1546         |
|    policy_gradient_loss     | -0.0027      |
|    std                      | 0.988        |
|    value_loss               | 0.298        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3315839] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 25           |
|    time_elapsed             | 330          |
|    total_timesteps          | 3061760      |
| train/                      |              |
|    approx_kl                | 0.009273663  |
|    approx_ln(kl)            | -4.680577    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.962        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.33        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.263        |
|    n_updates                | 1547         |
|    policy_gradient_loss     | -0.00565     |
|    std                      | 0.989        |
|    value_loss               | 0.409        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24789476] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 26            |
|    time_elapsed             | 344           |
|    total_timesteps          | 3063808       |
| train/                      |               |
|    approx_kl                | 0.010521166   |
|    approx_ln(kl)            | -4.554366     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.593        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.553         |
|    n_updates                | 1548          |
|    policy_gradient_loss     | -0.00351      |
|    std                      | 0.989         |
|    value_loss               | 1.48          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36921152] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 27            |
|    time_elapsed             | 358           |
|    total_timesteps          | 3065856       |
| train/                      |               |
|    approx_kl                | 0.0068760216  |
|    approx_ln(kl)            | -4.979715     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.714        |
|    ln(policy_gradient_loss) | -6.4          |
|    loss                     | 0.49          |
|    n_updates                | 1549          |
|    policy_gradient_loss     | 0.00167       |
|    std                      | 0.989         |
|    value_loss               | 1.4           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3293452] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 28           |
|    time_elapsed             | 371          |
|    total_timesteps          | 3067904      |
| train/                      |              |
|    approx_kl                | 0.009079903  |
|    approx_ln(kl)            | -4.7016916   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.289        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.34         |
|    n_updates                | 1550         |
|    policy_gradient_loss     | -0.000367    |
|    std                      | 0.989        |
|    value_loss               | 1.45         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24242578] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 29            |
|    time_elapsed             | 384           |
|    total_timesteps          | 3069952       |
| train/                      |               |
|    approx_kl                | 0.007883078   |
|    approx_ln(kl)            | -4.8430367    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.296        |
|    ln(policy_gradient_loss) | -5.38         |
|    loss                     | 0.744         |
|    n_updates                | 1551          |
|    policy_gradient_loss     | 0.00463       |
|    std                      | 0.989         |
|    value_loss               | 1.69          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45192838] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 30            |
|    time_elapsed             | 397           |
|    total_timesteps          | 3072000       |
| train/                      |               |
|    approx_kl                | 0.0033918254  |
|    approx_ln(kl)            | -5.686387     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.973         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.505        |
|    ln(policy_gradient_loss) | -5.99         |
|    loss                     | 0.603         |
|    n_updates                | 1552          |
|    policy_gradient_loss     | 0.00251       |
|    std                      | 0.989         |
|    value_loss               | 1.23          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.53723603] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 31            |
|    time_elapsed             | 411           |
|    total_timesteps          | 3074048       |
| train/                      |               |
|    approx_kl                | 0.011388946   |
|    approx_ln(kl)            | -4.475112     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.836        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.433         |
|    n_updates                | 1553          |
|    policy_gradient_loss     | -0.0133       |
|    std                      | 0.99          |
|    value_loss               | 0.938         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.49228528] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 32            |
|    time_elapsed             | 424           |
|    total_timesteps          | 3076096       |
| train/                      |               |
|    approx_kl                | 0.011284277   |
|    approx_ln(kl)            | -4.484345     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.27         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.282         |
|    n_updates                | 1554          |
|    policy_gradient_loss     | -0.00263      |
|    std                      | 0.991         |
|    value_loss               | 0.982         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3527193] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 33           |
|    time_elapsed             | 437          |
|    total_timesteps          | 3078144      |
| train/                      |              |
|    approx_kl                | 0.0072971    |
|    approx_ln(kl)            | -4.920278    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.81        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.88        |
|    ln(policy_gradient_loss) | -5.16        |
|    loss                     | 0.152        |
|    n_updates                | 1555         |
|    policy_gradient_loss     | 0.00577      |
|    std                      | 0.991        |
|    value_loss               | 0.326        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.5455758] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 34           |
|    time_elapsed             | 450          |
|    total_timesteps          | 3080192      |
| train/                      |              |
|    approx_kl                | 0.010394667  |
|    approx_ln(kl)            | -4.5664625   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.81        |
|    explained_variance       | 0.985        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.533       |
|    ln(policy_gradient_loss) | -6.34        |
|    loss                     | 0.587        |
|    n_updates                | 1556         |
|    policy_gradient_loss     | 0.00176      |
|    std                      | 0.991        |
|    value_loss               | 1.85         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.20233305] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 35            |
|    time_elapsed             | 463           |
|    total_timesteps          | 3082240       |
| train/                      |               |
|    approx_kl                | 0.005904086   |
|    approx_ln(kl)            | -5.1321106    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.975         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.168         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.18          |
|    n_updates                | 1557          |
|    policy_gradient_loss     | -0.00471      |
|    std                      | 0.991         |
|    value_loss               | 2.87          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30097637] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 36            |
|    time_elapsed             | 476           |
|    total_timesteps          | 3084288       |
| train/                      |               |
|    approx_kl                | 0.006269171   |
|    approx_ln(kl)            | -5.072111     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.81         |
|    explained_variance       | 0.963         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.381        |
|    ln(policy_gradient_loss) | -8.18         |
|    loss                     | 0.683         |
|    n_updates                | 1558          |
|    policy_gradient_loss     | 0.00028       |
|    std                      | 0.99          |
|    value_loss               | 1.49          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34515923] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 37            |
|    time_elapsed             | 490           |
|    total_timesteps          | 3086336       |
| train/                      |               |
|    approx_kl                | 0.0064895283  |
|    approx_ln(kl)            | -5.037565     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.821        |
|    ln(policy_gradient_loss) | -6.4          |
|    loss                     | 0.44          |
|    n_updates                | 1559          |
|    policy_gradient_loss     | 0.00166       |
|    std                      | 0.988         |
|    value_loss               | 0.965         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48552203] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 38            |
|    time_elapsed             | 503           |
|    total_timesteps          | 3088384       |
| train/                      |               |
|    approx_kl                | 0.007102841   |
|    approx_ln(kl)            | -4.9472604    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.975         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.199         |
|    ln(policy_gradient_loss) | -4.84         |
|    loss                     | 1.22          |
|    n_updates                | 1560          |
|    policy_gradient_loss     | 0.00787       |
|    std                      | 0.987         |
|    value_loss               | 2.4           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.263046]  |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 39           |
|    time_elapsed             | 516          |
|    total_timesteps          | 3090432      |
| train/                      |              |
|    approx_kl                | 0.0058664577 |
|    approx_ln(kl)            | -5.1385045   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.993        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.561       |
|    ln(policy_gradient_loss) | -5.64        |
|    loss                     | 0.571        |
|    n_updates                | 1561         |
|    policy_gradient_loss     | 0.00354      |
|    std                      | 0.988        |
|    value_loss               | 0.565        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23276122] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 40            |
|    time_elapsed             | 530           |
|    total_timesteps          | 3092480       |
| train/                      |               |
|    approx_kl                | 0.008867425   |
|    approx_ln(kl)            | -4.725371     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.316         |
|    ln(policy_gradient_loss) | -5.03         |
|    loss                     | 1.37          |
|    n_updates                | 1562          |
|    policy_gradient_loss     | 0.00651       |
|    std                      | 0.988         |
|    value_loss               | 1.23          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.23622377] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 41            |
|    time_elapsed             | 543           |
|    total_timesteps          | 3094528       |
| train/                      |               |
|    approx_kl                | 0.009845742   |
|    approx_ln(kl)            | -4.620716     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.07         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.126         |
|    n_updates                | 1563          |
|    policy_gradient_loss     | -0.0071       |
|    std                      | 0.988         |
|    value_loss               | 0.257         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.14240266] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 42            |
|    time_elapsed             | 556           |
|    total_timesteps          | 3096576       |
| train/                      |               |
|    approx_kl                | 0.0066736178  |
|    approx_ln(kl)            | -5.009593     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.0847        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.09          |
|    n_updates                | 1564          |
|    policy_gradient_loss     | -0.000705     |
|    std                      | 0.988         |
|    value_loss               | 2.22          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4017915] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 43           |
|    time_elapsed             | 569          |
|    total_timesteps          | 3098624      |
| train/                      |              |
|    approx_kl                | 0.0063142166 |
|    approx_ln(kl)            | -5.0649514   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.983        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.29        |
|    ln(policy_gradient_loss) | -7.83        |
|    loss                     | 0.275        |
|    n_updates                | 1565         |
|    policy_gradient_loss     | 0.0004       |
|    std                      | 0.987        |
|    value_loss               | 1.09         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42066842] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 44            |
|    time_elapsed             | 583           |
|    total_timesteps          | 3100672       |
| train/                      |               |
|    approx_kl                | 0.009106208   |
|    approx_ln(kl)            | -4.698799     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.58         |
|    ln(policy_gradient_loss) | -5.67         |
|    loss                     | 0.0756        |
|    n_updates                | 1566          |
|    policy_gradient_loss     | 0.00343       |
|    std                      | 0.987         |
|    value_loss               | 0.741         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.16931912] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 45            |
|    time_elapsed             | 596           |
|    total_timesteps          | 3102720       |
| train/                      |               |
|    approx_kl                | 0.0076370216  |
|    approx_ln(kl)            | -4.8747478    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.22         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.297         |
|    n_updates                | 1567          |
|    policy_gradient_loss     | -0.0213       |
|    std                      | 0.988         |
|    value_loss               | 0.623         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4001285] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 46           |
|    time_elapsed             | 609          |
|    total_timesteps          | 3104768      |
| train/                      |              |
|    approx_kl                | 0.007307066  |
|    approx_ln(kl)            | -4.9189134   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.966        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.186        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.2          |
|    n_updates                | 1568         |
|    policy_gradient_loss     | -0.000721    |
|    std                      | 0.988        |
|    value_loss               | 1.76         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.2546074] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 47           |
|    time_elapsed             | 623          |
|    total_timesteps          | 3106816      |
| train/                      |              |
|    approx_kl                | 0.009150487  |
|    approx_ln(kl)            | -4.6939483   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.8         |
|    explained_variance       | 0.989        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.73        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.177        |
|    n_updates                | 1569         |
|    policy_gradient_loss     | -0.018       |
|    std                      | 0.988        |
|    value_loss               | 0.357        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.28460938] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 48            |
|    time_elapsed             | 636           |
|    total_timesteps          | 3108864       |
| train/                      |               |
|    approx_kl                | 0.016655797   |
|    approx_ln(kl)            | -4.094997     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.346        |
|    ln(policy_gradient_loss) | -3.77         |
|    loss                     | 0.707         |
|    n_updates                | 1570          |
|    policy_gradient_loss     | 0.023         |
|    std                      | 0.988         |
|    value_loss               | 0.831         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28012162] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 49            |
|    time_elapsed             | 649           |
|    total_timesteps          | 3110912       |
| train/                      |               |
|    approx_kl                | 0.008986731   |
|    approx_ln(kl)            | -4.712006     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.88         |
|    ln(policy_gradient_loss) | -4.9          |
|    loss                     | 0.415         |
|    n_updates                | 1571          |
|    policy_gradient_loss     | 0.00743       |
|    std                      | 0.987         |
|    value_loss               | 1.34          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------
| reward             | [-0.5364363] |
| time/              |              |
|    fps             | 158          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 3112960      |
-------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26078942] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 2             |
|    time_elapsed             | 26            |
|    total_timesteps          | 3115008       |
| train/                      |               |
|    approx_kl                | 0.0071997023  |
|    approx_ln(kl)            | -4.933716     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.853        |
|    ln(policy_gradient_loss) | -5.18         |
|    loss                     | 0.426         |
|    n_updates                | 1573          |
|    policy_gradient_loss     | 0.00561       |
|    std                      | 0.987         |
|    value_loss               | 1.57          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41550642] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 3             |
|    time_elapsed             | 40            |
|    total_timesteps          | 3117056       |
| train/                      |               |
|    approx_kl                | 0.007828803   |
|    approx_ln(kl)            | -4.8499455    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.47         |
|    ln(policy_gradient_loss) | -5.37         |
|    loss                     | 0.23          |
|    n_updates                | 1574          |
|    policy_gradient_loss     | 0.00467       |
|    std                      | 0.987         |
|    value_loss               | 0.561         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.48385873] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 4             |
|    time_elapsed             | 53            |
|    total_timesteps          | 3119104       |
| train/                      |               |
|    approx_kl                | 0.0060736435  |
|    approx_ln(kl)            | -5.1037965    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.8          |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.41         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.244         |
|    n_updates                | 1575          |
|    policy_gradient_loss     | -0.00583      |
|    std                      | 0.986         |
|    value_loss               | 0.59          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.26611558] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 5             |
|    time_elapsed             | 66            |
|    total_timesteps          | 3121152       |
| train/                      |               |
|    approx_kl                | 0.00922826    |
|    approx_ln(kl)            | -4.685485     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.79         |
|    ln(policy_gradient_loss) | -4.44         |
|    loss                     | 0.166         |
|    n_updates                | 1576          |
|    policy_gradient_loss     | 0.0118        |
|    std                      | 0.984         |
|    value_loss               | 0.238         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.21771665] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 6             |
|    time_elapsed             | 80            |
|    total_timesteps          | 3123200       |
| train/                      |               |
|    approx_kl                | 0.0064841188  |
|    approx_ln(kl)            | -5.038399     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.14         |
|    ln(policy_gradient_loss) | -5.38         |
|    loss                     | 0.319         |
|    n_updates                | 1577          |
|    policy_gradient_loss     | 0.0046        |
|    std                      | 0.984         |
|    value_loss               | 0.686         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41242513] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 7             |
|    time_elapsed             | 94            |
|    total_timesteps          | 3125248       |
| train/                      |               |
|    approx_kl                | 0.0071505764  |
|    approx_ln(kl)            | -4.9405622    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.3          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.274         |
|    n_updates                | 1578          |
|    policy_gradient_loss     | -0.000194     |
|    std                      | 0.983         |
|    value_loss               | 1.25          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23871833] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 8             |
|    time_elapsed             | 107           |
|    total_timesteps          | 3127296       |
| train/                      |               |
|    approx_kl                | 0.0077502327  |
|    approx_ln(kl)            | -4.8600326    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.61         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.199         |
|    n_updates                | 1579          |
|    policy_gradient_loss     | -0.000224     |
|    std                      | 0.983         |
|    value_loss               | 0.565         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.12844399] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 9             |
|    time_elapsed             | 120           |
|    total_timesteps          | 3129344       |
| train/                      |               |
|    approx_kl                | 0.009446923   |
|    approx_ln(kl)            | -4.6620665    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.08         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.125         |
|    n_updates                | 1580          |
|    policy_gradient_loss     | -0.00196      |
|    std                      | 0.982         |
|    value_loss               | 0.643         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3197557] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 10           |
|    time_elapsed             | 133          |
|    total_timesteps          | 3131392      |
| train/                      |              |
|    approx_kl                | 0.005154045  |
|    approx_ln(kl)            | -5.2679734   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.991        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.92        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.147        |
|    n_updates                | 1581         |
|    policy_gradient_loss     | -0.00378     |
|    std                      | 0.981        |
|    value_loss               | 0.39         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.25452948] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 11            |
|    time_elapsed             | 147           |
|    total_timesteps          | 3133440       |
| train/                      |               |
|    approx_kl                | 0.008969147   |
|    approx_ln(kl)            | -4.713965     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.0244        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.02          |
|    n_updates                | 1582          |
|    policy_gradient_loss     | -0.0168       |
|    std                      | 0.981         |
|    value_loss               | 1.76          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24599798] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 12            |
|    time_elapsed             | 160           |
|    total_timesteps          | 3135488       |
| train/                      |               |
|    approx_kl                | 0.008283192   |
|    approx_ln(kl)            | -4.7935266    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.46         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.631         |
|    n_updates                | 1583          |
|    policy_gradient_loss     | -0.00893      |
|    std                      | 0.981         |
|    value_loss               | 1.28          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28164145] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 13            |
|    time_elapsed             | 174           |
|    total_timesteps          | 3137536       |
| train/                      |               |
|    approx_kl                | 0.007832213   |
|    approx_ln(kl)            | -4.84951      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.5          |
|    ln(policy_gradient_loss) | -6.03         |
|    loss                     | 0.0302        |
|    n_updates                | 1584          |
|    policy_gradient_loss     | 0.00241       |
|    std                      | 0.982         |
|    value_loss               | 0.534         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.33121789] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 14            |
|    time_elapsed             | 187           |
|    total_timesteps          | 3139584       |
| train/                      |               |
|    approx_kl                | 0.010312713   |
|    approx_ln(kl)            | -4.574378     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.983        |
|    ln(policy_gradient_loss) | -4.28         |
|    loss                     | 0.374         |
|    n_updates                | 1585          |
|    policy_gradient_loss     | 0.0139        |
|    std                      | 0.982         |
|    value_loss               | 1.12          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
-----------------------------------------------
| reward                      | [-0.26493734] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 15            |
|    time_elapsed             | 201           |
|    total_timesteps          | 3141632       |
| train/                      |               |
|    approx_kl                | 0.023288485   |
|    approx_ln(kl)            | -3.7597961    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.19         |
|    ln(policy_gradient_loss) | -5.05         |
|    loss                     | 0.306         |
|    n_updates                | 1586          |
|    policy_gradient_loss     | 0.00639       |
|    std                      | 0.982         |
|    value_loss               | 1.25          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.3066685] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 16           |
|    time_elapsed             | 215          |
|    total_timesteps          | 3143680      |
| train/                      |              |
|    approx_kl                | 0.011537794  |
|    approx_ln(kl)            | -4.462127    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.977        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.05        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.349        |
|    n_updates                | 1587         |
|    policy_gradient_loss     | -0.0119      |
|    std                      | 0.981        |
|    value_loss               | 0.663        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.2987062] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 17           |
|    time_elapsed             | 229          |
|    total_timesteps          | 3145728      |
| train/                      |              |
|    approx_kl                | 0.010190495  |
|    approx_ln(kl)            | -4.5863      |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.964        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.338        |
|    ln(policy_gradient_loss) | -4.23        |
|    loss                     | 1.4          |
|    n_updates                | 1588         |
|    policy_gradient_loss     | 0.0145       |
|    std                      | 0.981        |
|    value_loss               | 2.23         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33542246] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 18            |
|    time_elapsed             | 244           |
|    total_timesteps          | 3147776       |
| train/                      |               |
|    approx_kl                | 0.0056483364  |
|    approx_ln(kl)            | -5.1763945    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.952         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0986       |
|    ln(policy_gradient_loss) | -6.6          |
|    loss                     | 0.906         |
|    n_updates                | 1589          |
|    policy_gradient_loss     | 0.00135       |
|    std                      | 0.981         |
|    value_loss               | 2.37          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.35399732] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 19            |
|    time_elapsed             | 258           |
|    total_timesteps          | 3149824       |
| train/                      |               |
|    approx_kl                | 0.009889515   |
|    approx_ln(kl)            | -4.61628      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.58         |
|    ln(policy_gradient_loss) | -5.08         |
|    loss                     | 0.56          |
|    n_updates                | 1590          |
|    policy_gradient_loss     | 0.00622       |
|    std                      | 0.981         |
|    value_loss               | 0.947         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32832772] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 20            |
|    time_elapsed             | 274           |
|    total_timesteps          | 3151872       |
| train/                      |               |
|    approx_kl                | 0.0074364245  |
|    approx_ln(kl)            | -4.9013653    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.16         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.314         |
|    n_updates                | 1591          |
|    policy_gradient_loss     | -0.000695     |
|    std                      | 0.98          |
|    value_loss               | 0.6           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24635935] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 21            |
|    time_elapsed             | 288           |
|    total_timesteps          | 3153920       |
| train/                      |               |
|    approx_kl                | 0.0076708524  |
|    approx_ln(kl)            | -4.8703275    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.977         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.78         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.168         |
|    n_updates                | 1592          |
|    policy_gradient_loss     | -0.00279      |
|    std                      | 0.98          |
|    value_loss               | 0.269         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.502988] |
| time/                       |             |
|    fps                      | 148         |
|    iterations               | 22          |
|    time_elapsed             | 302         |
|    total_timesteps          | 3155968     |
| train/                      |             |
|    approx_kl                | 0.008160717 |
|    approx_ln(kl)            | -4.808423   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.78       |
|    explained_variance       | 0.97        |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -1.6        |
|    ln(policy_gradient_loss) | -5.63       |
|    loss                     | 0.201       |
|    n_updates                | 1593        |
|    policy_gradient_loss     | 0.00358     |
|    std                      | 0.979       |
|    value_loss               | 0.685       |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30495912] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 23            |
|    time_elapsed             | 316           |
|    total_timesteps          | 3158016       |
| train/                      |               |
|    approx_kl                | 0.006781901   |
|    approx_ln(kl)            | -4.993498     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.3          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.272         |
|    n_updates                | 1594          |
|    policy_gradient_loss     | -0.0083       |
|    std                      | 0.979         |
|    value_loss               | 0.591         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22642957] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 24            |
|    time_elapsed             | 330           |
|    total_timesteps          | 3160064       |
| train/                      |               |
|    approx_kl                | 0.006145295   |
|    approx_ln(kl)            | -5.0920687    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.912         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.09         |
|    ln(policy_gradient_loss) | -6.18         |
|    loss                     | 0.335         |
|    n_updates                | 1595          |
|    policy_gradient_loss     | 0.00208       |
|    std                      | 0.978         |
|    value_loss               | 1.01          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34044668] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 25            |
|    time_elapsed             | 346           |
|    total_timesteps          | 3162112       |
| train/                      |               |
|    approx_kl                | 0.004841805   |
|    approx_ln(kl)            | -5.3304677    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.971         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.1          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.332         |
|    n_updates                | 1596          |
|    policy_gradient_loss     | -0.00502      |
|    std                      | 0.978         |
|    value_loss               | 1.03          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.05
-----------------------------------------------
| reward                      | [-0.31894293] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 26            |
|    time_elapsed             | 362           |
|    total_timesteps          | 3164160       |
| train/                      |               |
|    approx_kl                | 0.013452694   |
|    approx_ln(kl)            | -4.308576     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.921         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.934         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 2.54          |
|    n_updates                | 1597          |
|    policy_gradient_loss     | -0.0116       |
|    std                      | 0.977         |
|    value_loss               | 2.75          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2726222] |
| time/                       |              |
|    fps                      | 146          |
|    iterations               | 27           |
|    time_elapsed             | 376          |
|    total_timesteps          | 3166208      |
| train/                      |              |
|    approx_kl                | 0.007813025  |
|    approx_ln(kl)            | -4.851963    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.78        |
|    explained_variance       | 0.967        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.7         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0673       |
|    n_updates                | 1598         |
|    policy_gradient_loss     | -0.0135      |
|    std                      | 0.977        |
|    value_loss               | 0.606        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3393058] |
| time/                       |              |
|    fps                      | 147          |
|    iterations               | 28           |
|    time_elapsed             | 390          |
|    total_timesteps          | 3168256      |
| train/                      |              |
|    approx_kl                | 0.00877904   |
|    approx_ln(kl)            | -4.7353883   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.897        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 1.23         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 3.42         |
|    n_updates                | 1599         |
|    policy_gradient_loss     | -0.00196     |
|    std                      | 0.976        |
|    value_loss               | 6.2          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.07
-----------------------------------------------
| reward                      | [-0.41555187] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 29            |
|    time_elapsed             | 403           |
|    total_timesteps          | 3170304       |
| train/                      |               |
|    approx_kl                | 0.017269367   |
|    approx_ln(kl)            | -4.058821     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.72         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.179         |
|    n_updates                | 1600          |
|    policy_gradient_loss     | -0.00836      |
|    std                      | 0.975         |
|    value_loss               | 0.427         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32202503] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 30            |
|    time_elapsed             | 418           |
|    total_timesteps          | 3172352       |
| train/                      |               |
|    approx_kl                | 0.008829968   |
|    approx_ln(kl)            | -4.729604     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.904         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.167        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.846         |
|    n_updates                | 1601          |
|    policy_gradient_loss     | -0.00474      |
|    std                      | 0.975         |
|    value_loss               | 4.89          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24014911] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 31            |
|    time_elapsed             | 432           |
|    total_timesteps          | 3174400       |
| train/                      |               |
|    approx_kl                | 0.0064105657  |
|    approx_ln(kl)            | -5.0498075    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.37         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0935        |
|    n_updates                | 1602          |
|    policy_gradient_loss     | -0.00306      |
|    std                      | 0.975         |
|    value_loss               | 0.325         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.43933395] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 32            |
|    time_elapsed             | 446           |
|    total_timesteps          | 3176448       |
| train/                      |               |
|    approx_kl                | 0.012771348   |
|    approx_ln(kl)            | -4.360551     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.953         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.1          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.122         |
|    n_updates                | 1603          |
|    policy_gradient_loss     | -0.0149       |
|    std                      | 0.975         |
|    value_loss               | 1.73          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39044484] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 33            |
|    time_elapsed             | 460           |
|    total_timesteps          | 3178496       |
| train/                      |               |
|    approx_kl                | 0.00573374    |
|    approx_ln(kl)            | -5.1613874    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.929         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.397         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.49          |
|    n_updates                | 1604          |
|    policy_gradient_loss     | -0.00375      |
|    std                      | 0.975         |
|    value_loss               | 3.14          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29817578] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 34            |
|    time_elapsed             | 474           |
|    total_timesteps          | 3180544       |
| train/                      |               |
|    approx_kl                | 0.005588653   |
|    approx_ln(kl)            | -5.187017     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.13          |
|    ln(policy_gradient_loss) | -5.31         |
|    loss                     | 1.14          |
|    n_updates                | 1605          |
|    policy_gradient_loss     | 0.00494       |
|    std                      | 0.974         |
|    value_loss               | 1.79          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.30995995] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 35            |
|    time_elapsed             | 489           |
|    total_timesteps          | 3182592       |
| train/                      |               |
|    approx_kl                | 0.007625149   |
|    approx_ln(kl)            | -4.876303     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.01         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.365         |
|    n_updates                | 1606          |
|    policy_gradient_loss     | -0.00118      |
|    std                      | 0.975         |
|    value_loss               | 0.714         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34426722] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 36            |
|    time_elapsed             | 504           |
|    total_timesteps          | 3184640       |
| train/                      |               |
|    approx_kl                | 0.006043591   |
|    approx_ln(kl)            | -5.108757     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.961         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.29         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.275         |
|    n_updates                | 1607          |
|    policy_gradient_loss     | -0.00268      |
|    std                      | 0.975         |
|    value_loss               | 0.917         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25575384] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 37            |
|    time_elapsed             | 518           |
|    total_timesteps          | 3186688       |
| train/                      |               |
|    approx_kl                | 0.006438437   |
|    approx_ln(kl)            | -5.0454693    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.352         |
|    ln(policy_gradient_loss) | -4.52         |
|    loss                     | 1.42          |
|    n_updates                | 1608          |
|    policy_gradient_loss     | 0.0109        |
|    std                      | 0.975         |
|    value_loss               | 1.01          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.21258567] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 38            |
|    time_elapsed             | 531           |
|    total_timesteps          | 3188736       |
| train/                      |               |
|    approx_kl                | 0.012017937   |
|    approx_ln(kl)            | -4.421355     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.962         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.2          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.302         |
|    n_updates                | 1609          |
|    policy_gradient_loss     | -0.000281     |
|    std                      | 0.975         |
|    value_loss               | 2.18          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28382027] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 39            |
|    time_elapsed             | 545           |
|    total_timesteps          | 3190784       |
| train/                      |               |
|    approx_kl                | 0.0075195893  |
|    approx_ln(kl)            | -4.890244     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.961         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.825         |
|    ln(policy_gradient_loss) | -5.42         |
|    loss                     | 2.28          |
|    n_updates                | 1610          |
|    policy_gradient_loss     | 0.00441       |
|    std                      | 0.975         |
|    value_loss               | 2.06          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35776374] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 40            |
|    time_elapsed             | 558           |
|    total_timesteps          | 3192832       |
| train/                      |               |
|    approx_kl                | 0.0070544514  |
|    approx_ln(kl)            | -4.9540963    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.93          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.274        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.76          |
|    n_updates                | 1611          |
|    policy_gradient_loss     | -0.00666      |
|    std                      | 0.975         |
|    value_loss               | 0.921         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.50306433] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 41            |
|    time_elapsed             | 571           |
|    total_timesteps          | 3194880       |
| train/                      |               |
|    approx_kl                | 0.013562572   |
|    approx_ln(kl)            | -4.3004413    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.959         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.972         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 2.64          |
|    n_updates                | 1612          |
|    policy_gradient_loss     | -0.0114       |
|    std                      | 0.975         |
|    value_loss               | 3.9           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41065836] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 42            |
|    time_elapsed             | 584           |
|    total_timesteps          | 3196928       |
| train/                      |               |
|    approx_kl                | 0.00930117    |
|    approx_ln(kl)            | -4.677615     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.962         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.204        |
|    ln(policy_gradient_loss) | -4.42         |
|    loss                     | 0.816         |
|    n_updates                | 1613          |
|    policy_gradient_loss     | 0.0121        |
|    std                      | 0.976         |
|    value_loss               | 2.64          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.32004273] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 43            |
|    time_elapsed             | 597           |
|    total_timesteps          | 3198976       |
| train/                      |               |
|    approx_kl                | 0.009504453   |
|    approx_ln(kl)            | -4.655995     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.6          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.201         |
|    n_updates                | 1614          |
|    policy_gradient_loss     | -0.00534      |
|    std                      | 0.976         |
|    value_loss               | 0.398         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28515223] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 44            |
|    time_elapsed             | 611           |
|    total_timesteps          | 3201024       |
| train/                      |               |
|    approx_kl                | 0.006794196   |
|    approx_ln(kl)            | -4.9916863    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.965         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.42         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.242         |
|    n_updates                | 1615          |
|    policy_gradient_loss     | -0.0192       |
|    std                      | 0.976         |
|    value_loss               | 1.17          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.36284423] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 45            |
|    time_elapsed             | 624           |
|    total_timesteps          | 3203072       |
| train/                      |               |
|    approx_kl                | 0.0099709425  |
|    approx_ln(kl)            | -4.6080804    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.89         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.15          |
|    n_updates                | 1616          |
|    policy_gradient_loss     | -0.00943      |
|    std                      | 0.976         |
|    value_loss               | 0.506         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2945138] |
| time/                       |              |
|    fps                      | 147          |
|    iterations               | 46           |
|    time_elapsed             | 637          |
|    total_timesteps          | 3205120      |
| train/                      |              |
|    approx_kl                | 0.008688226  |
|    approx_ln(kl)            | -4.7457867   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.78        |
|    explained_variance       | 0.935        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.0357      |
|    ln(policy_gradient_loss) | -5.2         |
|    loss                     | 0.965        |
|    n_updates                | 1617         |
|    policy_gradient_loss     | 0.0055       |
|    std                      | 0.977        |
|    value_loss               | 1.53         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.41248664] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 47            |
|    time_elapsed             | 650           |
|    total_timesteps          | 3207168       |
| train/                      |               |
|    approx_kl                | 0.009543235   |
|    approx_ln(kl)            | -4.6519227    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.976         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.328         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.39          |
|    n_updates                | 1618          |
|    policy_gradient_loss     | -0.00328      |
|    std                      | 0.977         |
|    value_loss               | 2.08          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35919127] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 48            |
|    time_elapsed             | 663           |
|    total_timesteps          | 3209216       |
| train/                      |               |
|    approx_kl                | 0.0080709895  |
|    approx_ln(kl)            | -4.819479     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.964         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.807        |
|    ln(policy_gradient_loss) | -5.37         |
|    loss                     | 0.446         |
|    n_updates                | 1619          |
|    policy_gradient_loss     | 0.00467       |
|    std                      | 0.977         |
|    value_loss               | 1.32          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.33808726] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 49            |
|    time_elapsed             | 677           |
|    total_timesteps          | 3211264       |
| train/                      |               |
|    approx_kl                | 0.008163705   |
|    approx_ln(kl)            | -4.8080573    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.949         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.178        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.837         |
|    n_updates                | 1620          |
|    policy_gradient_loss     | -0.00755      |
|    std                      | 0.977         |
|    value_loss               | 1.33          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-------------------------------------
| reward             | [-0.3083087] |
| time/              |              |
|    fps             | 156          |
|    iterations      | 1            |
|    time_elapsed    | 13           |
|    total_timesteps | 3213312      |
-------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.23142461] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 2             |
|    time_elapsed             | 26            |
|    total_timesteps          | 3215360       |
| train/                      |               |
|    approx_kl                | 0.010900709   |
|    approx_ln(kl)            | -4.5189276    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.964         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.3          |
|    ln(policy_gradient_loss) | -5.32         |
|    loss                     | 0.273         |
|    n_updates                | 1622          |
|    policy_gradient_loss     | 0.0049        |
|    std                      | 0.977         |
|    value_loss               | 1.48          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.50614846] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 3             |
|    time_elapsed             | 39            |
|    total_timesteps          | 3217408       |
| train/                      |               |
|    approx_kl                | 0.006247186   |
|    approx_ln(kl)            | -5.075624     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.48         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.227         |
|    n_updates                | 1623          |
|    policy_gradient_loss     | -0.00461      |
|    std                      | 0.978         |
|    value_loss               | 0.431         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45300403] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 4             |
|    time_elapsed             | 52            |
|    total_timesteps          | 3219456       |
| train/                      |               |
|    approx_kl                | 0.009410661   |
|    approx_ln(kl)            | -4.665912     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.45         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.235         |
|    n_updates                | 1624          |
|    policy_gradient_loss     | -0.000114     |
|    std                      | 0.979         |
|    value_loss               | 0.337         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22762278] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 5             |
|    time_elapsed             | 66            |
|    total_timesteps          | 3221504       |
| train/                      |               |
|    approx_kl                | 0.006824094   |
|    approx_ln(kl)            | -4.9872956    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.934         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.65         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.522         |
|    n_updates                | 1625          |
|    policy_gradient_loss     | -0.00187      |
|    std                      | 0.979         |
|    value_loss               | 2.49          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2577569] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 6            |
|    time_elapsed             | 79           |
|    total_timesteps          | 3223552      |
| train/                      |              |
|    approx_kl                | 0.005722297  |
|    approx_ln(kl)            | -5.163385    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.78        |
|    explained_variance       | 0.954        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.897        |
|    ln(policy_gradient_loss) | -5.54        |
|    loss                     | 2.45         |
|    n_updates                | 1626         |
|    policy_gradient_loss     | 0.00391      |
|    std                      | 0.979        |
|    value_loss               | 2.38         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39465743] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 7             |
|    time_elapsed             | 93            |
|    total_timesteps          | 3225600       |
| train/                      |               |
|    approx_kl                | 0.0061989794  |
|    approx_ln(kl)            | -5.0833707    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.891         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.84         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.158         |
|    n_updates                | 1627          |
|    policy_gradient_loss     | -0.00321      |
|    std                      | 0.979         |
|    value_loss               | 3.15          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22043589] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 8             |
|    time_elapsed             | 106           |
|    total_timesteps          | 3227648       |
| train/                      |               |
|    approx_kl                | 0.007752368   |
|    approx_ln(kl)            | -4.859757     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.939         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.753         |
|    ln(policy_gradient_loss) | -4.06         |
|    loss                     | 2.12          |
|    n_updates                | 1628          |
|    policy_gradient_loss     | 0.0173        |
|    std                      | 0.98          |
|    value_loss               | 2.67          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
-----------------------------------------------
| reward                      | [-0.32537973] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 9             |
|    time_elapsed             | 119           |
|    total_timesteps          | 3229696       |
| train/                      |               |
|    approx_kl                | 0.01400972    |
|    approx_ln(kl)            | -4.268004     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.966         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.35         |
|    ln(policy_gradient_loss) | -5.41         |
|    loss                     | 0.26          |
|    n_updates                | 1629          |
|    policy_gradient_loss     | 0.00447       |
|    std                      | 0.98          |
|    value_loss               | 0.683         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24566895] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 10            |
|    time_elapsed             | 132           |
|    total_timesteps          | 3231744       |
| train/                      |               |
|    approx_kl                | 0.0070438883  |
|    approx_ln(kl)            | -4.955595     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.14         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.118         |
|    n_updates                | 1630          |
|    policy_gradient_loss     | -0.002        |
|    std                      | 0.98          |
|    value_loss               | 0.241         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.2799523] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 11           |
|    time_elapsed             | 145          |
|    total_timesteps          | 3233792      |
| train/                      |              |
|    approx_kl                | 0.012335517  |
|    approx_ln(kl)            | -4.3952727   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.78        |
|    explained_variance       | 0.974        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.93        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0533       |
|    n_updates                | 1631         |
|    policy_gradient_loss     | -0.011       |
|    std                      | 0.981        |
|    value_loss               | 0.18         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.19533451] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 12            |
|    time_elapsed             | 158           |
|    total_timesteps          | 3235840       |
| train/                      |               |
|    approx_kl                | 0.00964216    |
|    approx_ln(kl)            | -4.64161      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.08         |
|    ln(policy_gradient_loss) | -4.13         |
|    loss                     | 0.125         |
|    n_updates                | 1632          |
|    policy_gradient_loss     | 0.016         |
|    std                      | 0.983         |
|    value_loss               | 0.223         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.4513371] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 13           |
|    time_elapsed             | 171          |
|    total_timesteps          | 3237888      |
| train/                      |              |
|    approx_kl                | 0.011450538  |
|    approx_ln(kl)            | -4.4697185   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.12        |
|    ln(policy_gradient_loss) | -5.42        |
|    loss                     | 0.328        |
|    n_updates                | 1633         |
|    policy_gradient_loss     | 0.00443      |
|    std                      | 0.984        |
|    value_loss               | 0.526        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.21603633] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 14            |
|    time_elapsed             | 184           |
|    total_timesteps          | 3239936       |
| train/                      |               |
|    approx_kl                | 0.007816087   |
|    approx_ln(kl)            | -4.851571     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.947         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.954        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.385         |
|    n_updates                | 1634          |
|    policy_gradient_loss     | -0.00738      |
|    std                      | 0.984         |
|    value_loss               | 4.12          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.30270284] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 15            |
|    time_elapsed             | 198           |
|    total_timesteps          | 3241984       |
| train/                      |               |
|    approx_kl                | 0.011634159   |
|    approx_ln(kl)            | -4.4538097    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.948         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.32         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.268         |
|    n_updates                | 1635          |
|    policy_gradient_loss     | -0.00393      |
|    std                      | 0.984         |
|    value_loss               | 1.15          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30507028] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 16            |
|    time_elapsed             | 211           |
|    total_timesteps          | 3244032       |
| train/                      |               |
|    approx_kl                | 0.0074307146  |
|    approx_ln(kl)            | -4.9021335    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.969         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.19         |
|    ln(policy_gradient_loss) | -5.34         |
|    loss                     | 0.111         |
|    n_updates                | 1636          |
|    policy_gradient_loss     | 0.00478       |
|    std                      | 0.984         |
|    value_loss               | 0.683         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29568538] |
| time/                       |               |
|    fps                      | 155           |
|    iterations               | 17            |
|    time_elapsed             | 224           |
|    total_timesteps          | 3246080       |
| train/                      |               |
|    approx_kl                | 0.006807424   |
|    approx_ln(kl)            | -4.9897413    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.971         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.64         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.193         |
|    n_updates                | 1637          |
|    policy_gradient_loss     | -0.00232      |
|    std                      | 0.983         |
|    value_loss               | 0.814         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.18054673] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 18            |
|    time_elapsed             | 238           |
|    total_timesteps          | 3248128       |
| train/                      |               |
|    approx_kl                | 0.008295334   |
|    approx_ln(kl)            | -4.7920623    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.965         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.0742        |
|    ln(policy_gradient_loss) | -5.12         |
|    loss                     | 1.08          |
|    n_updates                | 1638          |
|    policy_gradient_loss     | 0.00598       |
|    std                      | 0.983         |
|    value_loss               | 2.35          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.26278147] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 19            |
|    time_elapsed             | 251           |
|    total_timesteps          | 3250176       |
| train/                      |               |
|    approx_kl                | 0.009716914   |
|    approx_ln(kl)            | -4.6338873    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.972         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.34         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.712         |
|    n_updates                | 1639          |
|    policy_gradient_loss     | -0.0217       |
|    std                      | 0.982         |
|    value_loss               | 1.2           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3536094] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 20           |
|    time_elapsed             | 264          |
|    total_timesteps          | 3252224      |
| train/                      |              |
|    approx_kl                | 0.008013736  |
|    approx_ln(kl)            | -4.826598    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.981        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.301        |
|    ln(policy_gradient_loss) | -4.15        |
|    loss                     | 1.35         |
|    n_updates                | 1640         |
|    policy_gradient_loss     | 0.0158       |
|    std                      | 0.982        |
|    value_loss               | 2.9          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.3829448] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 21           |
|    time_elapsed             | 277          |
|    total_timesteps          | 3254272      |
| train/                      |              |
|    approx_kl                | 0.010094205  |
|    approx_ln(kl)            | -4.5957937   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.987        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.918       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.399        |
|    n_updates                | 1641         |
|    policy_gradient_loss     | -0.0148      |
|    std                      | 0.982        |
|    value_loss               | 0.875        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.33331755] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 22            |
|    time_elapsed             | 291           |
|    total_timesteps          | 3256320       |
| train/                      |               |
|    approx_kl                | 0.008220863   |
|    approx_ln(kl)            | -4.80108      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.33         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.265         |
|    n_updates                | 1642          |
|    policy_gradient_loss     | -0.0138       |
|    std                      | 0.982         |
|    value_loss               | 0.402         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.26598668] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 23            |
|    time_elapsed             | 305           |
|    total_timesteps          | 3258368       |
| train/                      |               |
|    approx_kl                | 0.010117313   |
|    approx_ln(kl)            | -4.5935073    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.05         |
|    ln(policy_gradient_loss) | -4.22         |
|    loss                     | 0.349         |
|    n_updates                | 1643          |
|    policy_gradient_loss     | 0.0147        |
|    std                      | 0.982         |
|    value_loss               | 0.728         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.29625654] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 24            |
|    time_elapsed             | 318           |
|    total_timesteps          | 3260416       |
| train/                      |               |
|    approx_kl                | 0.010487832   |
|    approx_ln(kl)            | -4.5575395    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.919        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.399         |
|    n_updates                | 1644          |
|    policy_gradient_loss     | -0.00812      |
|    std                      | 0.983         |
|    value_loss               | 0.913         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30786484] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 25            |
|    time_elapsed             | 331           |
|    total_timesteps          | 3262464       |
| train/                      |               |
|    approx_kl                | 0.007690425   |
|    approx_ln(kl)            | -4.8677793    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.14          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.15          |
|    n_updates                | 1645          |
|    policy_gradient_loss     | -0.00325      |
|    std                      | 0.983         |
|    value_loss               | 1.69          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41778177] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 26            |
|    time_elapsed             | 344           |
|    total_timesteps          | 3264512       |
| train/                      |               |
|    approx_kl                | 0.0075807013  |
|    approx_ln(kl)            | -4.8821497    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.166        |
|    ln(policy_gradient_loss) | -6.21         |
|    loss                     | 0.847         |
|    n_updates                | 1646          |
|    policy_gradient_loss     | 0.00201       |
|    std                      | 0.984         |
|    value_loss               | 1.71          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24843307] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 27            |
|    time_elapsed             | 358           |
|    total_timesteps          | 3266560       |
| train/                      |               |
|    approx_kl                | 0.00831008    |
|    approx_ln(kl)            | -4.790286     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.975        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.377         |
|    n_updates                | 1647          |
|    policy_gradient_loss     | -0.00419      |
|    std                      | 0.984         |
|    value_loss               | 1.67          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.39897242] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 28            |
|    time_elapsed             | 371           |
|    total_timesteps          | 3268608       |
| train/                      |               |
|    approx_kl                | 0.009601489   |
|    approx_ln(kl)            | -4.6458373    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.33         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.264         |
|    n_updates                | 1648          |
|    policy_gradient_loss     | -0.000628     |
|    std                      | 0.984         |
|    value_loss               | 0.964         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.33271852] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 29            |
|    time_elapsed             | 384           |
|    total_timesteps          | 3270656       |
| train/                      |               |
|    approx_kl                | 0.011064184   |
|    approx_ln(kl)            | -4.504042     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.14         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.118         |
|    n_updates                | 1649          |
|    policy_gradient_loss     | -0.00658      |
|    std                      | 0.984         |
|    value_loss               | 0.303         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.46278825] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 30            |
|    time_elapsed             | 398           |
|    total_timesteps          | 3272704       |
| train/                      |               |
|    approx_kl                | 0.0081305895  |
|    approx_ln(kl)            | -4.812122     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.732        |
|    ln(policy_gradient_loss) | -5.81         |
|    loss                     | 0.481         |
|    n_updates                | 1650          |
|    policy_gradient_loss     | 0.00299       |
|    std                      | 0.984         |
|    value_loss               | 1.59          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.2804177] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 31           |
|    time_elapsed             | 411          |
|    total_timesteps          | 3274752      |
| train/                      |              |
|    approx_kl                | 0.013432013  |
|    approx_ln(kl)            | -4.3101144   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.99         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.27        |
|    ln(policy_gradient_loss) | -5.91        |
|    loss                     | 0.104        |
|    n_updates                | 1651         |
|    policy_gradient_loss     | 0.0027       |
|    std                      | 0.984        |
|    value_loss               | 0.199        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32588843] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 32            |
|    time_elapsed             | 424           |
|    total_timesteps          | 3276800       |
| train/                      |               |
|    approx_kl                | 0.0061337464  |
|    approx_ln(kl)            | -5.09395      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.974         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.22         |
|    ln(policy_gradient_loss) | -7.04         |
|    loss                     | 0.109         |
|    n_updates                | 1652          |
|    policy_gradient_loss     | 0.000873      |
|    std                      | 0.984         |
|    value_loss               | 0.356         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32282323] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 33            |
|    time_elapsed             | 437           |
|    total_timesteps          | 3278848       |
| train/                      |               |
|    approx_kl                | 0.005251348   |
|    approx_ln(kl)            | -5.2492704    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.32         |
|    ln(policy_gradient_loss) | -5.34         |
|    loss                     | 0.266         |
|    n_updates                | 1653          |
|    policy_gradient_loss     | 0.00479       |
|    std                      | 0.984         |
|    value_loss               | 0.622         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.35830092] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 34            |
|    time_elapsed             | 450           |
|    total_timesteps          | 3280896       |
| train/                      |               |
|    approx_kl                | 0.012191473   |
|    approx_ln(kl)            | -4.4070187    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.64         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.193         |
|    n_updates                | 1654          |
|    policy_gradient_loss     | -0.00965      |
|    std                      | 0.984         |
|    value_loss               | 0.501         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.39236668] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 35            |
|    time_elapsed             | 463           |
|    total_timesteps          | 3282944       |
| train/                      |               |
|    approx_kl                | 0.009474674   |
|    approx_ln(kl)            | -4.659133     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.467        |
|    ln(policy_gradient_loss) | -4.44         |
|    loss                     | 0.627         |
|    n_updates                | 1655          |
|    policy_gradient_loss     | 0.0118        |
|    std                      | 0.984         |
|    value_loss               | 0.821         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44032624] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 36            |
|    time_elapsed             | 476           |
|    total_timesteps          | 3284992       |
| train/                      |               |
|    approx_kl                | 0.0060778433  |
|    approx_ln(kl)            | -5.1031055    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.43         |
|    ln(policy_gradient_loss) | -4.51         |
|    loss                     | 0.24          |
|    n_updates                | 1656          |
|    policy_gradient_loss     | 0.011         |
|    std                      | 0.984         |
|    value_loss               | 1.15          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36345538] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 37            |
|    time_elapsed             | 490           |
|    total_timesteps          | 3287040       |
| train/                      |               |
|    approx_kl                | 0.0052844277  |
|    approx_ln(kl)            | -5.242991     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.405        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.667         |
|    n_updates                | 1657          |
|    policy_gradient_loss     | -0.00385      |
|    std                      | 0.984         |
|    value_loss               | 2.09          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.1551581] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 38           |
|    time_elapsed             | 504          |
|    total_timesteps          | 3289088      |
| train/                      |              |
|    approx_kl                | 0.0057429783 |
|    approx_ln(kl)            | -5.159777    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.989        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.71        |
|    ln(policy_gradient_loss) | -5.79        |
|    loss                     | 0.18         |
|    n_updates                | 1658         |
|    policy_gradient_loss     | 0.00307      |
|    std                      | 0.983        |
|    value_loss               | 0.457        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3268897] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 39           |
|    time_elapsed             | 517          |
|    total_timesteps          | 3291136      |
| train/                      |              |
|    approx_kl                | 0.010130126  |
|    approx_ln(kl)            | -4.5922413   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.988        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.661       |
|    ln(policy_gradient_loss) | -6.75        |
|    loss                     | 0.516        |
|    n_updates                | 1659         |
|    policy_gradient_loss     | 0.00117      |
|    std                      | 0.982        |
|    value_loss               | 1.01         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.06
----------------------------------------------
| reward                      | [-0.3218662] |
| time/                       |              |
|    fps                      | 154          |
|    iterations               | 40           |
|    time_elapsed             | 530          |
|    total_timesteps          | 3293184      |
| train/                      |              |
|    approx_kl                | 0.019075144  |
|    approx_ln(kl)            | -3.9593692   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.79        |
|    explained_variance       | 0.993        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.29        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.274        |
|    n_updates                | 1660         |
|    policy_gradient_loss     | -0.00332     |
|    std                      | 0.981        |
|    value_loss               | 0.662        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.06
---------------------------------------------
| reward                      | [-0.324201] |
| time/                       |             |
|    fps                      | 154         |
|    iterations               | 41          |
|    time_elapsed             | 544         |
|    total_timesteps          | 3295232     |
| train/                      |             |
|    approx_kl                | 0.018424045 |
|    approx_ln(kl)            | -3.9940987  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.79       |
|    explained_variance       | 0.994       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -0.0945     |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.91        |
|    n_updates                | 1661        |
|    policy_gradient_loss     | -0.0057     |
|    std                      | 0.981       |
|    value_loss               | 1.16        |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.35799092] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 42            |
|    time_elapsed             | 558           |
|    total_timesteps          | 3297280       |
| train/                      |               |
|    approx_kl                | 0.009233708   |
|    approx_ln(kl)            | -4.6848946    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.61         |
|    ln(policy_gradient_loss) | -7.09         |
|    loss                     | 0.201         |
|    n_updates                | 1662          |
|    policy_gradient_loss     | 0.000833      |
|    std                      | 0.981         |
|    value_loss               | 0.459         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28899238] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 43            |
|    time_elapsed             | 571           |
|    total_timesteps          | 3299328       |
| train/                      |               |
|    approx_kl                | 0.005552669   |
|    approx_ln(kl)            | -5.1934767    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.905        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.405         |
|    n_updates                | 1663          |
|    policy_gradient_loss     | -0.00463      |
|    std                      | 0.981         |
|    value_loss               | 0.593         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31737813] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 44            |
|    time_elapsed             | 584           |
|    total_timesteps          | 3301376       |
| train/                      |               |
|    approx_kl                | 0.00799915    |
|    approx_ln(kl)            | -4.82842      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.67         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.189         |
|    n_updates                | 1664          |
|    policy_gradient_loss     | -0.000274     |
|    std                      | 0.98          |
|    value_loss               | 0.28          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42767328] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 45            |
|    time_elapsed             | 598           |
|    total_timesteps          | 3303424       |
| train/                      |               |
|    approx_kl                | 0.007976826   |
|    approx_ln(kl)            | -4.831215     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.973         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.05         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.129         |
|    n_updates                | 1665          |
|    policy_gradient_loss     | -9.5e-05      |
|    std                      | 0.979         |
|    value_loss               | 0.2           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.36582267] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 46            |
|    time_elapsed             | 611           |
|    total_timesteps          | 3305472       |
| train/                      |               |
|    approx_kl                | 0.014454022   |
|    approx_ln(kl)            | -4.2367826    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.79         |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.53         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.217         |
|    n_updates                | 1666          |
|    policy_gradient_loss     | -0.00644      |
|    std                      | 0.979         |
|    value_loss               | 0.407         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.41682175] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 47            |
|    time_elapsed             | 625           |
|    total_timesteps          | 3307520       |
| train/                      |               |
|    approx_kl                | 0.011983236   |
|    approx_ln(kl)            | -4.424247     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.976         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.875         |
|    ln(policy_gradient_loss) | -5.64         |
|    loss                     | 2.4           |
|    n_updates                | 1667          |
|    policy_gradient_loss     | 0.00354       |
|    std                      | 0.979         |
|    value_loss               | 2.78          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.36749506] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 48            |
|    time_elapsed             | 638           |
|    total_timesteps          | 3309568       |
| train/                      |               |
|    approx_kl                | 0.013680809   |
|    approx_ln(kl)            | -4.2917614    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.14         |
|    ln(policy_gradient_loss) | -3.69         |
|    loss                     | 0.319         |
|    n_updates                | 1668          |
|    policy_gradient_loss     | 0.0249        |
|    std                      | 0.978         |
|    value_loss               | 0.519         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.29141656] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 49            |
|    time_elapsed             | 651           |
|    total_timesteps          | 3311616       |
| train/                      |               |
|    approx_kl                | 0.009369607   |
|    approx_ln(kl)            | -4.6702843    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.972         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.088         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.09          |
|    n_updates                | 1669          |
|    policy_gradient_loss     | -0.000847     |
|    std                      | 0.977         |
|    value_loss               | 1.36          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
--------------------------------------
| reward             | [-0.45669794] |
| time/              |               |
|    fps             | 152           |
|    iterations      | 1             |
|    time_elapsed    | 13            |
|    total_timesteps | 3313664       |
--------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34977257] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 2             |
|    time_elapsed             | 26            |
|    total_timesteps          | 3315712       |
| train/                      |               |
|    approx_kl                | 0.0043974393  |
|    approx_ln(kl)            | -5.426733     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.975         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.3          |
|    ln(policy_gradient_loss) | -8.91         |
|    loss                     | 0.273         |
|    n_updates                | 1671          |
|    policy_gradient_loss     | 0.000135      |
|    std                      | 0.975         |
|    value_loss               | 0.444         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3000776] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 3            |
|    time_elapsed             | 39           |
|    total_timesteps          | 3317760      |
| train/                      |              |
|    approx_kl                | 0.008641472  |
|    approx_ln(kl)            | -4.7511826   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.78        |
|    explained_variance       | 0.983        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.0154      |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.985        |
|    n_updates                | 1672         |
|    policy_gradient_loss     | -0.00796     |
|    std                      | 0.975        |
|    value_loss               | 1.58         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2413256] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 4            |
|    time_elapsed             | 53           |
|    total_timesteps          | 3319808      |
| train/                      |              |
|    approx_kl                | 0.0064584636 |
|    approx_ln(kl)            | -5.0423636   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.78        |
|    explained_variance       | 0.981        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.0987       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.1          |
|    n_updates                | 1673         |
|    policy_gradient_loss     | -0.00411     |
|    std                      | 0.975        |
|    value_loss               | 1.32         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24356392] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 5             |
|    time_elapsed             | 67            |
|    total_timesteps          | 3321856       |
| train/                      |               |
|    approx_kl                | 0.0075621344  |
|    approx_ln(kl)            | -4.8846016    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.759        |
|    ln(policy_gradient_loss) | -6.64         |
|    loss                     | 0.468         |
|    n_updates                | 1674          |
|    policy_gradient_loss     | 0.00131       |
|    std                      | 0.975         |
|    value_loss               | 0.733         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.43396434] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 6             |
|    time_elapsed             | 80            |
|    total_timesteps          | 3323904       |
| train/                      |               |
|    approx_kl                | 0.010306969   |
|    approx_ln(kl)            | -4.574935     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.84         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.16          |
|    n_updates                | 1675          |
|    policy_gradient_loss     | -0.0215       |
|    std                      | 0.975         |
|    value_loss               | 0.243         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3669927] |
| time/                       |              |
|    fps                      | 152          |
|    iterations               | 7            |
|    time_elapsed             | 94           |
|    total_timesteps          | 3325952      |
| train/                      |              |
|    approx_kl                | 0.008321886  |
|    approx_ln(kl)            | -4.7888665   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.78        |
|    explained_variance       | 0.982        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.83        |
|    ln(policy_gradient_loss) | -4.58        |
|    loss                     | 0.436        |
|    n_updates                | 1676         |
|    policy_gradient_loss     | 0.0102       |
|    std                      | 0.975        |
|    value_loss               | 1.09         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33086008] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 8             |
|    time_elapsed             | 107           |
|    total_timesteps          | 3328000       |
| train/                      |               |
|    approx_kl                | 0.008130737   |
|    approx_ln(kl)            | -4.8121037    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.48         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0837        |
|    n_updates                | 1677          |
|    policy_gradient_loss     | -0.00139      |
|    std                      | 0.976         |
|    value_loss               | 0.443         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30847588] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 9             |
|    time_elapsed             | 121           |
|    total_timesteps          | 3330048       |
| train/                      |               |
|    approx_kl                | 0.008728088   |
|    approx_ln(kl)            | -4.741209     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.46         |
|    ln(policy_gradient_loss) | -4.23         |
|    loss                     | 0.231         |
|    n_updates                | 1678          |
|    policy_gradient_loss     | 0.0145        |
|    std                      | 0.976         |
|    value_loss               | 0.357         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26043737] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 10            |
|    time_elapsed             | 135           |
|    total_timesteps          | 3332096       |
| train/                      |               |
|    approx_kl                | 0.006496038   |
|    approx_ln(kl)            | -5.036563     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.58         |
|    ln(policy_gradient_loss) | -5.08         |
|    loss                     | 0.207         |
|    n_updates                | 1679          |
|    policy_gradient_loss     | 0.00624       |
|    std                      | 0.976         |
|    value_loss               | 0.796         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.23988292] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 11            |
|    time_elapsed             | 148           |
|    total_timesteps          | 3334144       |
| train/                      |               |
|    approx_kl                | 0.007479049   |
|    approx_ln(kl)            | -4.89565      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.777        |
|    ln(policy_gradient_loss) | -5.87         |
|    loss                     | 0.46          |
|    n_updates                | 1680          |
|    policy_gradient_loss     | 0.00283       |
|    std                      | 0.976         |
|    value_loss               | 1.28          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.27752194] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 12            |
|    time_elapsed             | 162           |
|    total_timesteps          | 3336192       |
| train/                      |               |
|    approx_kl                | 0.00980385    |
|    approx_ln(kl)            | -4.62498      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.971         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.305        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.737         |
|    n_updates                | 1681          |
|    policy_gradient_loss     | -0.00701      |
|    std                      | 0.975         |
|    value_loss               | 1.95          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27200127] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 13            |
|    time_elapsed             | 176           |
|    total_timesteps          | 3338240       |
| train/                      |               |
|    approx_kl                | 0.0053077796  |
|    approx_ln(kl)            | -5.2385817    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.0109        |
|    ln(policy_gradient_loss) | -4.93         |
|    loss                     | 1.01          |
|    n_updates                | 1682          |
|    policy_gradient_loss     | 0.00722       |
|    std                      | 0.975         |
|    value_loss               | 2.02          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29813087] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 14            |
|    time_elapsed             | 190           |
|    total_timesteps          | 3340288       |
| train/                      |               |
|    approx_kl                | 0.0061684498  |
|    approx_ln(kl)            | -5.088308     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.32         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.267         |
|    n_updates                | 1683          |
|    policy_gradient_loss     | -0.00679      |
|    std                      | 0.974         |
|    value_loss               | 0.637         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2902108] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 15           |
|    time_elapsed             | 203          |
|    total_timesteps          | 3342336      |
| train/                      |              |
|    approx_kl                | 0.007205408  |
|    approx_ln(kl)            | -4.9329233   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.973        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.19        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.112        |
|    n_updates                | 1684         |
|    policy_gradient_loss     | -0.0124      |
|    std                      | 0.973        |
|    value_loss               | 0.38         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.2690629] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 16           |
|    time_elapsed             | 217          |
|    total_timesteps          | 3344384      |
| train/                      |              |
|    approx_kl                | 0.009137599  |
|    approx_ln(kl)            | -4.695358    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.976        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.859       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.424        |
|    n_updates                | 1685         |
|    policy_gradient_loss     | -0.0117      |
|    std                      | 0.973        |
|    value_loss               | 0.73         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26272053] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 17            |
|    time_elapsed             | 230           |
|    total_timesteps          | 3346432       |
| train/                      |               |
|    approx_kl                | 0.008816526   |
|    approx_ln(kl)            | -4.7311273    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.943         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.11         |
|    ln(policy_gradient_loss) | -4.63         |
|    loss                     | 0.121         |
|    n_updates                | 1686          |
|    policy_gradient_loss     | 0.00975       |
|    std                      | 0.972         |
|    value_loss               | 0.21          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.2936926] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 18           |
|    time_elapsed             | 243          |
|    total_timesteps          | 3348480      |
| train/                      |              |
|    approx_kl                | 0.01134872   |
|    approx_ln(kl)            | -4.4786506   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.942        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.31        |
|    ln(policy_gradient_loss) | -5.05        |
|    loss                     | 0.0988       |
|    n_updates                | 1687         |
|    policy_gradient_loss     | 0.00642      |
|    std                      | 0.972        |
|    value_loss               | 0.17         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3401797] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 19           |
|    time_elapsed             | 257          |
|    total_timesteps          | 3350528      |
| train/                      |              |
|    approx_kl                | 0.008763953  |
|    approx_ln(kl)            | -4.737108    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.979        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.391       |
|    ln(policy_gradient_loss) | -4.56        |
|    loss                     | 0.677        |
|    n_updates                | 1688         |
|    policy_gradient_loss     | 0.0104       |
|    std                      | 0.972        |
|    value_loss               | 0.889        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3258617] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 20           |
|    time_elapsed             | 270          |
|    total_timesteps          | 3352576      |
| train/                      |              |
|    approx_kl                | 0.0085398    |
|    approx_ln(kl)            | -4.7630177   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.988        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.28        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.278        |
|    n_updates                | 1689         |
|    policy_gradient_loss     | -0.00953     |
|    std                      | 0.972        |
|    value_loss               | 0.921        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23869602] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 21            |
|    time_elapsed             | 284           |
|    total_timesteps          | 3354624       |
| train/                      |               |
|    approx_kl                | 0.0085157575  |
|    approx_ln(kl)            | -4.765837     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.0796        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.08          |
|    n_updates                | 1690          |
|    policy_gradient_loss     | -0.00905      |
|    std                      | 0.972         |
|    value_loss               | 1.46          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3250791] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 22           |
|    time_elapsed             | 297          |
|    total_timesteps          | 3356672      |
| train/                      |              |
|    approx_kl                | 0.0087380875 |
|    approx_ln(kl)            | -4.740064    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.27        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.281        |
|    n_updates                | 1691         |
|    policy_gradient_loss     | -0.00523     |
|    std                      | 0.971        |
|    value_loss               | 0.599        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44767097] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 23            |
|    time_elapsed             | 310           |
|    total_timesteps          | 3358720       |
| train/                      |               |
|    approx_kl                | 0.0060925335  |
|    approx_ln(kl)            | -5.1006913    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.293         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.34          |
|    n_updates                | 1692          |
|    policy_gradient_loss     | -0.00624      |
|    std                      | 0.97          |
|    value_loss               | 2.01          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22790799] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 24            |
|    time_elapsed             | 324           |
|    total_timesteps          | 3360768       |
| train/                      |               |
|    approx_kl                | 0.006570224   |
|    approx_ln(kl)            | -5.0252075    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.971         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.157        |
|    ln(policy_gradient_loss) | -7.27         |
|    loss                     | 0.855         |
|    n_updates                | 1693          |
|    policy_gradient_loss     | 0.000697      |
|    std                      | 0.97          |
|    value_loss               | 1.99          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.1863019] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 25           |
|    time_elapsed             | 338          |
|    total_timesteps          | 3362816      |
| train/                      |              |
|    approx_kl                | 0.0059626284 |
|    approx_ln(kl)            | -5.122244    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.76        |
|    explained_variance       | 0.993        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.36        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0941       |
|    n_updates                | 1694         |
|    policy_gradient_loss     | -0.00451     |
|    std                      | 0.97         |
|    value_loss               | 0.384        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.21871983] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 26            |
|    time_elapsed             | 351           |
|    total_timesteps          | 3364864       |
| train/                      |               |
|    approx_kl                | 0.006664276   |
|    approx_ln(kl)            | -5.010994     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.39         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.092         |
|    n_updates                | 1695          |
|    policy_gradient_loss     | -0.0021       |
|    std                      | 0.971         |
|    value_loss               | 0.187         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2391988] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 27           |
|    time_elapsed             | 365          |
|    total_timesteps          | 3366912      |
| train/                      |              |
|    approx_kl                | 0.007037493  |
|    approx_ln(kl)            | -4.9565034   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.99         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.18        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.307        |
|    n_updates                | 1696         |
|    policy_gradient_loss     | -0.0123      |
|    std                      | 0.972        |
|    value_loss               | 1.07         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.20137122] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 28            |
|    time_elapsed             | 381           |
|    total_timesteps          | 3368960       |
| train/                      |               |
|    approx_kl                | 0.007967592   |
|    approx_ln(kl)            | -4.832373     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.91         |
|    ln(policy_gradient_loss) | -5.09         |
|    loss                     | 0.0544        |
|    n_updates                | 1697          |
|    policy_gradient_loss     | 0.00616       |
|    std                      | 0.972         |
|    value_loss               | 0.173         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28778198] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 29            |
|    time_elapsed             | 395           |
|    total_timesteps          | 3371008       |
| train/                      |               |
|    approx_kl                | 0.0070299357  |
|    approx_ln(kl)            | -4.9575777    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.611        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.543         |
|    n_updates                | 1698          |
|    policy_gradient_loss     | -0.00416      |
|    std                      | 0.972         |
|    value_loss               | 1.2           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44700158] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 30            |
|    time_elapsed             | 408           |
|    total_timesteps          | 3373056       |
| train/                      |               |
|    approx_kl                | 0.008398546   |
|    approx_ln(kl)            | -4.779697     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.758        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.468         |
|    n_updates                | 1699          |
|    policy_gradient_loss     | -0.000321     |
|    std                      | 0.972         |
|    value_loss               | 1.17          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2661541] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 31           |
|    time_elapsed             | 422          |
|    total_timesteps          | 3375104      |
| train/                      |              |
|    approx_kl                | 0.0038535148 |
|    approx_ln(kl)            | -5.5587697   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.991        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.14        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.321        |
|    n_updates                | 1700         |
|    policy_gradient_loss     | -0.000368    |
|    std                      | 0.972        |
|    value_loss               | 0.756        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2364037] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 32           |
|    time_elapsed             | 435          |
|    total_timesteps          | 3377152      |
| train/                      |              |
|    approx_kl                | 0.005423738  |
|    approx_ln(kl)            | -5.21697     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.983        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.4         |
|    ln(policy_gradient_loss) | -5.24        |
|    loss                     | 0.67         |
|    n_updates                | 1701         |
|    policy_gradient_loss     | 0.00531      |
|    std                      | 0.973        |
|    value_loss               | 1.17         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3835421] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 33           |
|    time_elapsed             | 449          |
|    total_timesteps          | 3379200      |
| train/                      |              |
|    approx_kl                | 0.008474406  |
|    approx_ln(kl)            | -4.7707047   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.998        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.62        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0725       |
|    n_updates                | 1702         |
|    policy_gradient_loss     | -0.000733    |
|    std                      | 0.974        |
|    value_loss               | 0.18         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3344321] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 34           |
|    time_elapsed             | 462          |
|    total_timesteps          | 3381248      |
| train/                      |              |
|    approx_kl                | 0.0077363397 |
|    approx_ln(kl)            | -4.8618264   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.981        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.202        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.22         |
|    n_updates                | 1703         |
|    policy_gradient_loss     | -0.00622     |
|    std                      | 0.974        |
|    value_loss               | 1.31         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.06
-----------------------------------------------
| reward                      | [-0.34134865] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 35            |
|    time_elapsed             | 475           |
|    total_timesteps          | 3383296       |
| train/                      |               |
|    approx_kl                | 0.014891381   |
|    approx_ln(kl)            | -4.2069726    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.42         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0328        |
|    n_updates                | 1704          |
|    policy_gradient_loss     | -0.024        |
|    std                      | 0.975         |
|    value_loss               | 0.276         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44559452] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 36            |
|    time_elapsed             | 489           |
|    total_timesteps          | 3385344       |
| train/                      |               |
|    approx_kl                | 0.007530801   |
|    approx_ln(kl)            | -4.888754     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.97          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.8          |
|    ln(policy_gradient_loss) | -6.37         |
|    loss                     | 0.165         |
|    n_updates                | 1705          |
|    policy_gradient_loss     | 0.00171       |
|    std                      | 0.975         |
|    value_loss               | 0.755         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3585261] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 37           |
|    time_elapsed             | 502          |
|    total_timesteps          | 3387392      |
| train/                      |              |
|    approx_kl                | 0.009773402  |
|    approx_ln(kl)            | -4.628091    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.982        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.03        |
|    ln(policy_gradient_loss) | -4.14        |
|    loss                     | 0.356        |
|    n_updates                | 1706         |
|    policy_gradient_loss     | 0.0159       |
|    std                      | 0.975        |
|    value_loss               | 1.63         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4082294] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 38           |
|    time_elapsed             | 516          |
|    total_timesteps          | 3389440      |
| train/                      |              |
|    approx_kl                | 0.008630698  |
|    approx_ln(kl)            | -4.75243     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.984        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.3         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.273        |
|    n_updates                | 1707         |
|    policy_gradient_loss     | -0.00351     |
|    std                      | 0.975        |
|    value_loss               | 0.501        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27262405] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 39            |
|    time_elapsed             | 529           |
|    total_timesteps          | 3391488       |
| train/                      |               |
|    approx_kl                | 0.008873882   |
|    approx_ln(kl)            | -4.7246428    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.505        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.603         |
|    n_updates                | 1708          |
|    policy_gradient_loss     | -0.00302      |
|    std                      | 0.975         |
|    value_loss               | 0.661         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.22896843] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 40            |
|    time_elapsed             | 543           |
|    total_timesteps          | 3393536       |
| train/                      |               |
|    approx_kl                | 0.009936129   |
|    approx_ln(kl)            | -4.611578     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.976         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.715        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.489         |
|    n_updates                | 1709          |
|    policy_gradient_loss     | -0.00835      |
|    std                      | 0.975         |
|    value_loss               | 1.34          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2771137] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 41           |
|    time_elapsed             | 557          |
|    total_timesteps          | 3395584      |
| train/                      |              |
|    approx_kl                | 0.00714951   |
|    approx_ln(kl)            | -4.9407115   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.63        |
|    ln(policy_gradient_loss) | -6.19        |
|    loss                     | 0.0722       |
|    n_updates                | 1710         |
|    policy_gradient_loss     | 0.00205      |
|    std                      | 0.975        |
|    value_loss               | 0.123        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2430094] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 42           |
|    time_elapsed             | 571          |
|    total_timesteps          | 3397632      |
| train/                      |              |
|    approx_kl                | 0.0059074312 |
|    approx_ln(kl)            | -5.131544    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.995        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.43        |
|    ln(policy_gradient_loss) | -4.19        |
|    loss                     | 0.24         |
|    n_updates                | 1711         |
|    policy_gradient_loss     | 0.0151       |
|    std                      | 0.976        |
|    value_loss               | 0.351        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25773144] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 43            |
|    time_elapsed             | 584           |
|    total_timesteps          | 3399680       |
| train/                      |               |
|    approx_kl                | 0.010071181   |
|    approx_ln(kl)            | -4.5980773    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.47         |
|    ln(policy_gradient_loss) | -4.76         |
|    loss                     | 0.0848        |
|    n_updates                | 1712          |
|    policy_gradient_loss     | 0.00859       |
|    std                      | 0.976         |
|    value_loss               | 0.118         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35369626] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 44            |
|    time_elapsed             | 598           |
|    total_timesteps          | 3401728       |
| train/                      |               |
|    approx_kl                | 0.008550503   |
|    approx_ln(kl)            | -4.761765     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.607        |
|    ln(policy_gradient_loss) | -4.85         |
|    loss                     | 0.545         |
|    n_updates                | 1713          |
|    policy_gradient_loss     | 0.00787       |
|    std                      | 0.976         |
|    value_loss               | 1.08          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5406085] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 45           |
|    time_elapsed             | 612          |
|    total_timesteps          | 3403776      |
| train/                      |              |
|    approx_kl                | 0.007345965  |
|    approx_ln(kl)            | -4.9136043   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.976        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.96        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.14         |
|    n_updates                | 1714         |
|    policy_gradient_loss     | -0.00705     |
|    std                      | 0.975        |
|    value_loss               | 0.585        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
-----------------------------------------------
| reward                      | [-0.47761714] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 46            |
|    time_elapsed             | 626           |
|    total_timesteps          | 3405824       |
| train/                      |               |
|    approx_kl                | 0.013927418   |
|    approx_ln(kl)            | -4.2738957    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.321        |
|    ln(policy_gradient_loss) | -4.92         |
|    loss                     | 0.725         |
|    n_updates                | 1715          |
|    policy_gradient_loss     | 0.00732       |
|    std                      | 0.975         |
|    value_loss               | 1.16          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24479479] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 47            |
|    time_elapsed             | 640           |
|    total_timesteps          | 3407872       |
| train/                      |               |
|    approx_kl                | 0.0056534163  |
|    approx_ln(kl)            | -5.175495     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.06         |
|    ln(policy_gradient_loss) | -5.88         |
|    loss                     | 0.127         |
|    n_updates                | 1716          |
|    policy_gradient_loss     | 0.0028        |
|    std                      | 0.975         |
|    value_loss               | 0.294         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40521637] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 48            |
|    time_elapsed             | 654           |
|    total_timesteps          | 3409920       |
| train/                      |               |
|    approx_kl                | 0.008127752   |
|    approx_ln(kl)            | -4.812471     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.966         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.833        |
|    ln(policy_gradient_loss) | -5.35         |
|    loss                     | 0.435         |
|    n_updates                | 1717          |
|    policy_gradient_loss     | 0.00474       |
|    std                      | 0.975         |
|    value_loss               | 1.2           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.2282851] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 49           |
|    time_elapsed             | 667          |
|    total_timesteps          | 3411968      |
| train/                      |              |
|    approx_kl                | 0.011945206  |
|    approx_ln(kl)            | -4.4274254   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.989        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.51        |
|    ln(policy_gradient_loss) | -6.5         |
|    loss                     | 0.0816       |
|    n_updates                | 1718         |
|    policy_gradient_loss     | 0.00151      |
|    std                      | 0.976        |
|    value_loss               | 0.176        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
------------------------------------
| reward             | [-0.538649] |
| time/              |             |
|    fps             | 147         |
|    iterations      | 1           |
|    time_elapsed    | 13          |
|    total_timesteps | 3414016     |
------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.275727] |
| time/                       |             |
|    fps                      | 148         |
|    iterations               | 2           |
|    time_elapsed             | 27          |
|    total_timesteps          | 3416064     |
| train/                      |             |
|    approx_kl                | 0.006588443 |
|    approx_ln(kl)            | -5.022438   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.78       |
|    explained_variance       | 0.931       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -0.885      |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.413       |
|    n_updates                | 1720        |
|    policy_gradient_loss     | -0.00473    |
|    std                      | 0.978       |
|    value_loss               | 1.62        |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.356119] |
| time/                       |             |
|    fps                      | 148         |
|    iterations               | 3           |
|    time_elapsed             | 41          |
|    total_timesteps          | 3418112     |
| train/                      |             |
|    approx_kl                | 0.005030458 |
|    approx_ln(kl)            | -5.2922444  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.78       |
|    explained_variance       | 0.982       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -1.54       |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.215       |
|    n_updates                | 1721        |
|    policy_gradient_loss     | -0.000467   |
|    std                      | 0.979       |
|    value_loss               | 0.554       |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.3423678] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 4            |
|    time_elapsed             | 54           |
|    total_timesteps          | 3420160      |
| train/                      |              |
|    approx_kl                | 0.0117434375 |
|    approx_ln(kl)            | -4.444461    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.78        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.54        |
|    ln(policy_gradient_loss) | -4.76        |
|    loss                     | 0.0788       |
|    n_updates                | 1722         |
|    policy_gradient_loss     | 0.00855      |
|    std                      | 0.979        |
|    value_loss               | 0.0637       |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.2513883] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 5            |
|    time_elapsed             | 68           |
|    total_timesteps          | 3422208      |
| train/                      |              |
|    approx_kl                | 0.01463179   |
|    approx_ln(kl)            | -4.224559    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.78        |
|    explained_variance       | 0.986        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.0632      |
|    ln(policy_gradient_loss) | -5.01        |
|    loss                     | 0.939        |
|    n_updates                | 1723         |
|    policy_gradient_loss     | 0.00666      |
|    std                      | 0.978        |
|    value_loss               | 0.792        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.40947446] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 6             |
|    time_elapsed             | 82            |
|    total_timesteps          | 3424256       |
| train/                      |               |
|    approx_kl                | 0.010608122   |
|    approx_ln(kl)            | -4.5461354    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.22         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.109         |
|    n_updates                | 1724          |
|    policy_gradient_loss     | -0.00043      |
|    std                      | 0.978         |
|    value_loss               | 0.186         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.23309447] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 7             |
|    time_elapsed             | 96            |
|    total_timesteps          | 3426304       |
| train/                      |               |
|    approx_kl                | 0.017088745   |
|    approx_ln(kl)            | -4.0693355    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.1          |
|    ln(policy_gradient_loss) | -6.24         |
|    loss                     | 0.122         |
|    n_updates                | 1725          |
|    policy_gradient_loss     | 0.00195       |
|    std                      | 0.978         |
|    value_loss               | 0.244         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.24896115] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 8             |
|    time_elapsed             | 109           |
|    total_timesteps          | 3428352       |
| train/                      |               |
|    approx_kl                | 0.010294003   |
|    approx_ln(kl)            | -4.576194     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.969         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.28         |
|    ln(policy_gradient_loss) | -5.23         |
|    loss                     | 0.277         |
|    n_updates                | 1726          |
|    policy_gradient_loss     | 0.00534       |
|    std                      | 0.977         |
|    value_loss               | 0.892         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30924585] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 9             |
|    time_elapsed             | 123           |
|    total_timesteps          | 3430400       |
| train/                      |               |
|    approx_kl                | 0.008928537   |
|    approx_ln(kl)            | -4.7185025    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.266         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.3           |
|    n_updates                | 1727          |
|    policy_gradient_loss     | -0.00343      |
|    std                      | 0.977         |
|    value_loss               | 1.07          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.25801373] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 10            |
|    time_elapsed             | 137           |
|    total_timesteps          | 3432448       |
| train/                      |               |
|    approx_kl                | 0.01489993    |
|    approx_ln(kl)            | -4.206399     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.968         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.793        |
|    ln(policy_gradient_loss) | -4.75         |
|    loss                     | 0.453         |
|    n_updates                | 1728          |
|    policy_gradient_loss     | 0.00868       |
|    std                      | 0.976         |
|    value_loss               | 1.08          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.22623955] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 11            |
|    time_elapsed             | 152           |
|    total_timesteps          | 3434496       |
| train/                      |               |
|    approx_kl                | 0.009998771   |
|    approx_ln(kl)            | -4.6052933    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.78         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.38         |
|    ln(policy_gradient_loss) | -5.12         |
|    loss                     | 0.0925        |
|    n_updates                | 1729          |
|    policy_gradient_loss     | 0.00597       |
|    std                      | 0.975         |
|    value_loss               | 0.237         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.46020028] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 12            |
|    time_elapsed             | 166           |
|    total_timesteps          | 3436544       |
| train/                      |               |
|    approx_kl                | 0.009708977   |
|    approx_ln(kl)            | -4.634704     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.52          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.68          |
|    n_updates                | 1730          |
|    policy_gradient_loss     | -0.00188      |
|    std                      | 0.974         |
|    value_loss               | 1.17          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.37000856] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 13            |
|    time_elapsed             | 180           |
|    total_timesteps          | 3438592       |
| train/                      |               |
|    approx_kl                | 0.010187249   |
|    approx_ln(kl)            | -4.5866184    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.42         |
|    ln(policy_gradient_loss) | -4.85         |
|    loss                     | 0.242         |
|    n_updates                | 1731          |
|    policy_gradient_loss     | 0.00779       |
|    std                      | 0.973         |
|    value_loss               | 1.68          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3821344] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 14           |
|    time_elapsed             | 193          |
|    total_timesteps          | 3440640      |
| train/                      |              |
|    approx_kl                | 0.0059735896 |
|    approx_ln(kl)            | -5.120407    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.991        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.361       |
|    ln(policy_gradient_loss) | -6.43        |
|    loss                     | 0.697        |
|    n_updates                | 1732         |
|    policy_gradient_loss     | 0.00162      |
|    std                      | 0.973        |
|    value_loss               | 1.14         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.33661288] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 15            |
|    time_elapsed             | 207           |
|    total_timesteps          | 3442688       |
| train/                      |               |
|    approx_kl                | 0.014039013   |
|    approx_ln(kl)            | -4.2659154    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.26         |
|    ln(policy_gradient_loss) | -8.08         |
|    loss                     | 0.105         |
|    n_updates                | 1733          |
|    policy_gradient_loss     | 0.000309      |
|    std                      | 0.973         |
|    value_loss               | 0.258         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34637487] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 16            |
|    time_elapsed             | 220           |
|    total_timesteps          | 3444736       |
| train/                      |               |
|    approx_kl                | 0.00865348    |
|    approx_ln(kl)            | -4.749794     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.368         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.44          |
|    n_updates                | 1734          |
|    policy_gradient_loss     | -0.0278       |
|    std                      | 0.973         |
|    value_loss               | 2.86          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28507757] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 17            |
|    time_elapsed             | 233           |
|    total_timesteps          | 3446784       |
| train/                      |               |
|    approx_kl                | 0.007056322   |
|    approx_ln(kl)            | -4.953831     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.195        |
|    ln(policy_gradient_loss) | -5.48         |
|    loss                     | 0.823         |
|    n_updates                | 1735          |
|    policy_gradient_loss     | 0.00419       |
|    std                      | 0.972         |
|    value_loss               | 0.704         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25273323] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 18            |
|    time_elapsed             | 247           |
|    total_timesteps          | 3448832       |
| train/                      |               |
|    approx_kl                | 0.009761124   |
|    approx_ln(kl)            | -4.629348     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.76         |
|    ln(policy_gradient_loss) | -4.42         |
|    loss                     | 0.0636        |
|    n_updates                | 1736          |
|    policy_gradient_loss     | 0.0121        |
|    std                      | 0.972         |
|    value_loss               | 0.0852        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35072947] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 19            |
|    time_elapsed             | 260           |
|    total_timesteps          | 3450880       |
| train/                      |               |
|    approx_kl                | 0.0053803404  |
|    approx_ln(kl)            | -5.2250037    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.77         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.64         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.194         |
|    n_updates                | 1737          |
|    policy_gradient_loss     | -0.0047       |
|    std                      | 0.971         |
|    value_loss               | 0.798         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3027061] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 20           |
|    time_elapsed             | 273          |
|    total_timesteps          | 3452928      |
| train/                      |              |
|    approx_kl                | 0.008411758  |
|    approx_ln(kl)            | -4.778125    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.77        |
|    explained_variance       | 0.991        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.116        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 1.12         |
|    n_updates                | 1738         |
|    policy_gradient_loss     | -0.00175     |
|    std                      | 0.97         |
|    value_loss               | 1.14         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34021837] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 21            |
|    time_elapsed             | 286           |
|    total_timesteps          | 3454976       |
| train/                      |               |
|    approx_kl                | 0.008556304   |
|    approx_ln(kl)            | -4.761087     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.35         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0351        |
|    n_updates                | 1739          |
|    policy_gradient_loss     | -0.0148       |
|    std                      | 0.97          |
|    value_loss               | 0.151         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3495414] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 22           |
|    time_elapsed             | 300          |
|    total_timesteps          | 3457024      |
| train/                      |              |
|    approx_kl                | 0.0102647515 |
|    approx_ln(kl)            | -4.5790396   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.76        |
|    explained_variance       | 0.986        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.601       |
|    ln(policy_gradient_loss) | -5.68        |
|    loss                     | 0.548        |
|    n_updates                | 1740         |
|    policy_gradient_loss     | 0.0034       |
|    std                      | 0.969        |
|    value_loss               | 0.899        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36773884] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 23            |
|    time_elapsed             | 313           |
|    total_timesteps          | 3459072       |
| train/                      |               |
|    approx_kl                | 0.00948307    |
|    approx_ln(kl)            | -4.658247     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.8          |
|    ln(policy_gradient_loss) | -4.84         |
|    loss                     | 0.165         |
|    n_updates                | 1741          |
|    policy_gradient_loss     | 0.00792       |
|    std                      | 0.969         |
|    value_loss               | 0.486         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.36248207] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 24            |
|    time_elapsed             | 327           |
|    total_timesteps          | 3461120       |
| train/                      |               |
|    approx_kl                | 0.00785173    |
|    approx_ln(kl)            | -4.8470216    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.67         |
|    ln(policy_gradient_loss) | -8.88         |
|    loss                     | 0.0696        |
|    n_updates                | 1742          |
|    policy_gradient_loss     | 0.000139      |
|    std                      | 0.967         |
|    value_loss               | 0.124         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5074579] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 25           |
|    time_elapsed             | 340          |
|    total_timesteps          | 3463168      |
| train/                      |              |
|    approx_kl                | 0.008378901  |
|    approx_ln(kl)            | -4.7820387   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.76        |
|    explained_variance       | 0.993        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.836       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.434        |
|    n_updates                | 1743         |
|    policy_gradient_loss     | -0.0043      |
|    std                      | 0.967        |
|    value_loss               | 0.381        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.21424235] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 26            |
|    time_elapsed             | 353           |
|    total_timesteps          | 3465216       |
| train/                      |               |
|    approx_kl                | 0.010570544   |
|    approx_ln(kl)            | -4.549684     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.3          |
|    ln(policy_gradient_loss) | -4.58         |
|    loss                     | 0.741         |
|    n_updates                | 1744          |
|    policy_gradient_loss     | 0.0103        |
|    std                      | 0.966         |
|    value_loss               | 1.08          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
----------------------------------------------
| reward                      | [-0.3364647] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 27           |
|    time_elapsed             | 367          |
|    total_timesteps          | 3467264      |
| train/                      |              |
|    approx_kl                | 0.015610166  |
|    approx_ln(kl)            | -4.159833    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.76        |
|    explained_variance       | 0.988        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.06        |
|    ln(policy_gradient_loss) | -6.68        |
|    loss                     | 0.128        |
|    n_updates                | 1745         |
|    policy_gradient_loss     | 0.00125      |
|    std                      | 0.966        |
|    value_loss               | 0.514        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37639084] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 28            |
|    time_elapsed             | 381           |
|    total_timesteps          | 3469312       |
| train/                      |               |
|    approx_kl                | 0.0094414335  |
|    approx_ln(kl)            | -4.6626472    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.973         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.101         |
|    ln(policy_gradient_loss) | -4.87         |
|    loss                     | 1.11          |
|    n_updates                | 1746          |
|    policy_gradient_loss     | 0.00768       |
|    std                      | 0.966         |
|    value_loss               | 1.91          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4299559] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 29           |
|    time_elapsed             | 395          |
|    total_timesteps          | 3471360      |
| train/                      |              |
|    approx_kl                | 0.005735458  |
|    approx_ln(kl)            | -5.1610875   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.76        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.62        |
|    ln(policy_gradient_loss) | -9.03        |
|    loss                     | 0.073        |
|    n_updates                | 1747         |
|    policy_gradient_loss     | 0.00012      |
|    std                      | 0.966        |
|    value_loss               | 0.125        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34090883] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 30            |
|    time_elapsed             | 410           |
|    total_timesteps          | 3473408       |
| train/                      |               |
|    approx_kl                | 0.0077433717  |
|    approx_ln(kl)            | -4.860918     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.13         |
|    ln(policy_gradient_loss) | -4.64         |
|    loss                     | 0.119         |
|    n_updates                | 1748          |
|    policy_gradient_loss     | 0.0097        |
|    std                      | 0.965         |
|    value_loss               | 0.305         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.30839297] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 31            |
|    time_elapsed             | 423           |
|    total_timesteps          | 3475456       |
| train/                      |               |
|    approx_kl                | 0.012358894   |
|    approx_ln(kl)            | -4.393379     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.78         |
|    ln(policy_gradient_loss) | -6.5          |
|    loss                     | 0.0228        |
|    n_updates                | 1749          |
|    policy_gradient_loss     | 0.0015        |
|    std                      | 0.964         |
|    value_loss               | 0.046         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.251919] |
| time/                       |             |
|    fps                      | 150         |
|    iterations               | 32          |
|    time_elapsed             | 436         |
|    total_timesteps          | 3477504     |
| train/                      |             |
|    approx_kl                | 0.010436371 |
|    approx_ln(kl)            | -4.5624585  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.75       |
|    explained_variance       | 0.992       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -0.806      |
|    ln(policy_gradient_loss) | -9.2        |
|    loss                     | 0.447       |
|    n_updates                | 1750        |
|    policy_gradient_loss     | 0.000101    |
|    std                      | 0.964       |
|    value_loss               | 0.486       |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32924736] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 33            |
|    time_elapsed             | 449           |
|    total_timesteps          | 3479552       |
| train/                      |               |
|    approx_kl                | 0.010989924   |
|    approx_ln(kl)            | -4.5107765    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.88         |
|    ln(policy_gradient_loss) | -5.5          |
|    loss                     | 0.153         |
|    n_updates                | 1751          |
|    policy_gradient_loss     | 0.00409       |
|    std                      | 0.964         |
|    value_loss               | 0.45          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.37029028] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 34            |
|    time_elapsed             | 463           |
|    total_timesteps          | 3481600       |
| train/                      |               |
|    approx_kl                | 0.010847805   |
|    approx_ln(kl)            | -4.5237927    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.997         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.89         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0204        |
|    n_updates                | 1752          |
|    policy_gradient_loss     | -0.004        |
|    std                      | 0.964         |
|    value_loss               | 0.0976        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.33529398] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 35            |
|    time_elapsed             | 476           |
|    total_timesteps          | 3483648       |
| train/                      |               |
|    approx_kl                | 0.015252488   |
|    approx_ln(kl)            | -4.1830125    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.91         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.148         |
|    n_updates                | 1753          |
|    policy_gradient_loss     | -0.000309     |
|    std                      | 0.964         |
|    value_loss               | 0.298         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.28741312] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 36            |
|    time_elapsed             | 490           |
|    total_timesteps          | 3485696       |
| train/                      |               |
|    approx_kl                | 0.016654031   |
|    approx_ln(kl)            | -4.095103     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.44         |
|    ln(policy_gradient_loss) | -6.49         |
|    loss                     | 0.0876        |
|    n_updates                | 1754          |
|    policy_gradient_loss     | 0.00153       |
|    std                      | 0.963         |
|    value_loss               | 0.0964        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.44592732] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 37            |
|    time_elapsed             | 503           |
|    total_timesteps          | 3487744       |
| train/                      |               |
|    approx_kl                | 0.011151991   |
|    approx_ln(kl)            | -4.496137     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.31         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0997        |
|    n_updates                | 1755          |
|    policy_gradient_loss     | -0.0104       |
|    std                      | 0.963         |
|    value_loss               | 0.515         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38900205] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 38            |
|    time_elapsed             | 517           |
|    total_timesteps          | 3489792       |
| train/                      |               |
|    approx_kl                | 0.0065663555  |
|    approx_ln(kl)            | -5.0257964    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.3          |
|    ln(policy_gradient_loss) | -6.33         |
|    loss                     | 0.0999        |
|    n_updates                | 1756          |
|    policy_gradient_loss     | 0.00179       |
|    std                      | 0.963         |
|    value_loss               | 0.14          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.16470787] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 39            |
|    time_elapsed             | 530           |
|    total_timesteps          | 3491840       |
| train/                      |               |
|    approx_kl                | 0.007460955   |
|    approx_ln(kl)            | -4.898072     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.48         |
|    ln(policy_gradient_loss) | -5            |
|    loss                     | 0.228         |
|    n_updates                | 1757          |
|    policy_gradient_loss     | 0.00671       |
|    std                      | 0.963         |
|    value_loss               | 0.362         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.36510465] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 40            |
|    time_elapsed             | 544           |
|    total_timesteps          | 3493888       |
| train/                      |               |
|    approx_kl                | 0.0112590585  |
|    approx_ln(kl)            | -4.4865823    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.26         |
|    ln(policy_gradient_loss) | -7.42         |
|    loss                     | 0.285         |
|    n_updates                | 1758          |
|    policy_gradient_loss     | 0.000601      |
|    std                      | 0.964         |
|    value_loss               | 0.498         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.43889385] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 41            |
|    time_elapsed             | 558           |
|    total_timesteps          | 3495936       |
| train/                      |               |
|    approx_kl                | 0.008964219   |
|    approx_ln(kl)            | -4.7145143    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.31         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0997        |
|    n_updates                | 1759          |
|    policy_gradient_loss     | -0.0166       |
|    std                      | 0.964         |
|    value_loss               | 0.442         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36469835] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 42            |
|    time_elapsed             | 572           |
|    total_timesteps          | 3497984       |
| train/                      |               |
|    approx_kl                | 0.00729132    |
|    approx_ln(kl)            | -4.9210706    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.6          |
|    ln(policy_gradient_loss) | -5.58         |
|    loss                     | 0.202         |
|    n_updates                | 1760          |
|    policy_gradient_loss     | 0.00376       |
|    std                      | 0.964         |
|    value_loss               | 0.408         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26119062] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 43            |
|    time_elapsed             | 586           |
|    total_timesteps          | 3500032       |
| train/                      |               |
|    approx_kl                | 0.005766359   |
|    approx_ln(kl)            | -5.1557145    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.49         |
|    ln(policy_gradient_loss) | -7.52         |
|    loss                     | 0.225         |
|    n_updates                | 1761          |
|    policy_gradient_loss     | 0.00054       |
|    std                      | 0.964         |
|    value_loss               | 0.613         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30261445] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 44            |
|    time_elapsed             | 599           |
|    total_timesteps          | 3502080       |
| train/                      |               |
|    approx_kl                | 0.0064139166  |
|    approx_ln(kl)            | -5.0492854    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.18         |
|    ln(policy_gradient_loss) | -7.32         |
|    loss                     | 0.0415        |
|    n_updates                | 1762          |
|    policy_gradient_loss     | 0.000664      |
|    std                      | 0.964         |
|    value_loss               | 0.153         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24994743] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 45            |
|    time_elapsed             | 613           |
|    total_timesteps          | 3504128       |
| train/                      |               |
|    approx_kl                | 0.0049430686  |
|    approx_ln(kl)            | -5.309769     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.44         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0871        |
|    n_updates                | 1763          |
|    policy_gradient_loss     | -0.0033       |
|    std                      | 0.965         |
|    value_loss               | 0.259         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28946215] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 46            |
|    time_elapsed             | 628           |
|    total_timesteps          | 3506176       |
| train/                      |               |
|    approx_kl                | 0.007105495   |
|    approx_ln(kl)            | -4.946887     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.51         |
|    ln(policy_gradient_loss) | -6.09         |
|    loss                     | 0.22          |
|    n_updates                | 1764          |
|    policy_gradient_loss     | 0.00227       |
|    std                      | 0.964         |
|    value_loss               | 0.538         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33259904] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 47            |
|    time_elapsed             | 642           |
|    total_timesteps          | 3508224       |
| train/                      |               |
|    approx_kl                | 0.0072667305  |
|    approx_ln(kl)            | -4.924449     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.35         |
|    ln(policy_gradient_loss) | -10.6         |
|    loss                     | 0.0954        |
|    n_updates                | 1765          |
|    policy_gradient_loss     | 2.4e-05       |
|    std                      | 0.965         |
|    value_loss               | 0.306         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39544666] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 48            |
|    time_elapsed             | 656           |
|    total_timesteps          | 3510272       |
| train/                      |               |
|    approx_kl                | 0.0072887726  |
|    approx_ln(kl)            | -4.92142      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.92         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.147         |
|    n_updates                | 1766          |
|    policy_gradient_loss     | -0.00207      |
|    std                      | 0.966         |
|    value_loss               | 0.505         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.36427915] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 49            |
|    time_elapsed             | 669           |
|    total_timesteps          | 3512320       |
| train/                      |               |
|    approx_kl                | 0.0109442305  |
|    approx_ln(kl)            | -4.514943     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.8          |
|    ln(policy_gradient_loss) | -4.49         |
|    loss                     | 0.0606        |
|    n_updates                | 1767          |
|    policy_gradient_loss     | 0.0112        |
|    std                      | 0.966         |
|    value_loss               | 0.0868        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------
| reward             | [-0.3257342] |
| time/              |              |
|    fps             | 151          |
|    iterations      | 1            |
|    time_elapsed    | 13           |
|    total_timesteps | 3514368      |
-------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31336066] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 2             |
|    time_elapsed             | 26            |
|    total_timesteps          | 3516416       |
| train/                      |               |
|    approx_kl                | 0.008106393   |
|    approx_ln(kl)            | -4.815102     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.963         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.249        |
|    ln(policy_gradient_loss) | -4.89         |
|    loss                     | 0.78          |
|    n_updates                | 1769          |
|    policy_gradient_loss     | 0.00755       |
|    std                      | 0.968         |
|    value_loss               | 1.21          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2753491] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 3            |
|    time_elapsed             | 40           |
|    total_timesteps          | 3518464      |
| train/                      |              |
|    approx_kl                | 0.004565008  |
|    approx_ln(kl)            | -5.389335    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.76        |
|    explained_variance       | 0.989        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.44        |
|    ln(policy_gradient_loss) | -5.75        |
|    loss                     | 0.0875       |
|    n_updates                | 1770         |
|    policy_gradient_loss     | 0.00319      |
|    std                      | 0.967        |
|    value_loss               | 0.198        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43014482] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 4             |
|    time_elapsed             | 53            |
|    total_timesteps          | 3520512       |
| train/                      |               |
|    approx_kl                | 0.007960753   |
|    approx_ln(kl)            | -4.833232     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.32         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.267         |
|    n_updates                | 1771          |
|    policy_gradient_loss     | -0.00421      |
|    std                      | 0.967         |
|    value_loss               | 0.806         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.24632563] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 5             |
|    time_elapsed             | 67            |
|    total_timesteps          | 3522560       |
| train/                      |               |
|    approx_kl                | 0.007205864   |
|    approx_ln(kl)            | -4.9328604    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.03         |
|    ln(policy_gradient_loss) | -5.44         |
|    loss                     | 0.132         |
|    n_updates                | 1772          |
|    policy_gradient_loss     | 0.00435       |
|    std                      | 0.967         |
|    value_loss               | 0.39          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36447972] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 6             |
|    time_elapsed             | 80            |
|    total_timesteps          | 3524608       |
| train/                      |               |
|    approx_kl                | 0.006354282   |
|    approx_ln(kl)            | -5.058626     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | nan           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | -0.0177       |
|    n_updates                | 1773          |
|    policy_gradient_loss     | -0.0153       |
|    std                      | 0.967         |
|    value_loss               | 0.0531        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32124355] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 7             |
|    time_elapsed             | 93            |
|    total_timesteps          | 3526656       |
| train/                      |               |
|    approx_kl                | 0.008524008   |
|    approx_ln(kl)            | -4.7648687    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.37         |
|    ln(policy_gradient_loss) | -6.45         |
|    loss                     | 0.255         |
|    n_updates                | 1774          |
|    policy_gradient_loss     | 0.00159       |
|    std                      | 0.966         |
|    value_loss               | 0.912         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30328783] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 8             |
|    time_elapsed             | 107           |
|    total_timesteps          | 3528704       |
| train/                      |               |
|    approx_kl                | 0.0073531857  |
|    approx_ln(kl)            | -4.9126215    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.35         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.013         |
|    n_updates                | 1775          |
|    policy_gradient_loss     | -0.00631      |
|    std                      | 0.966         |
|    value_loss               | 0.0928        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.35298905] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 9             |
|    time_elapsed             | 121           |
|    total_timesteps          | 3530752       |
| train/                      |               |
|    approx_kl                | 0.008051344   |
|    approx_ln(kl)            | -4.821916     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.99         |
|    ln(policy_gradient_loss) | -5.56         |
|    loss                     | 0.137         |
|    n_updates                | 1776          |
|    policy_gradient_loss     | 0.00383       |
|    std                      | 0.966         |
|    value_loss               | 0.424         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28829667] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 10            |
|    time_elapsed             | 135           |
|    total_timesteps          | 3532800       |
| train/                      |               |
|    approx_kl                | 0.007694032   |
|    approx_ln(kl)            | -4.8673105    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.78         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0621        |
|    n_updates                | 1777          |
|    policy_gradient_loss     | -0.00624      |
|    std                      | 0.965         |
|    value_loss               | 0.597         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:291: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(loss)", np.log(loss.item()))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.31956708] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 11            |
|    time_elapsed             | 149           |
|    total_timesteps          | 3534848       |
| train/                      |               |
|    approx_kl                | 0.009446919   |
|    approx_ln(kl)            | -4.6620665    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | nan           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | -0.00151      |
|    n_updates                | 1778          |
|    policy_gradient_loss     | -0.0116       |
|    std                      | 0.965         |
|    value_loss               | 0.0945        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25373876] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 12            |
|    time_elapsed             | 163           |
|    total_timesteps          | 3536896       |
| train/                      |               |
|    approx_kl                | 0.0075370697  |
|    approx_ln(kl)            | -4.887922     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.38         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.034         |
|    n_updates                | 1779          |
|    policy_gradient_loss     | -0.00607      |
|    std                      | 0.965         |
|    value_loss               | 0.0745        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27669206] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 13            |
|    time_elapsed             | 176           |
|    total_timesteps          | 3538944       |
| train/                      |               |
|    approx_kl                | 0.0066358447  |
|    approx_ln(kl)            | -5.0152693    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.34         |
|    ln(policy_gradient_loss) | -6.21         |
|    loss                     | 0.261         |
|    n_updates                | 1780          |
|    policy_gradient_loss     | 0.002         |
|    std                      | 0.965         |
|    value_loss               | 0.276         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------------
| reward                      | [-0.372797] |
| time/                       |             |
|    fps                      | 150         |
|    iterations               | 14          |
|    time_elapsed             | 190         |
|    total_timesteps          | 3540992     |
| train/                      |             |
|    approx_kl                | 0.008094269 |
|    approx_ln(kl)            | -4.816599   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.76       |
|    explained_variance       | 0.994       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -1.16       |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | 0.315       |
|    n_updates                | 1781        |
|    policy_gradient_loss     | -0.00475    |
|    std                      | 0.965       |
|    value_loss               | 0.607       |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.27272817] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 15            |
|    time_elapsed             | 204           |
|    total_timesteps          | 3543040       |
| train/                      |               |
|    approx_kl                | 0.012116032   |
|    approx_ln(kl)            | -4.4132257    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.61         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.199         |
|    n_updates                | 1782          |
|    policy_gradient_loss     | -0.00128      |
|    std                      | 0.965         |
|    value_loss               | 0.498         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.47335956] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 16            |
|    time_elapsed             | 217           |
|    total_timesteps          | 3545088       |
| train/                      |               |
|    approx_kl                | 0.008901557   |
|    approx_ln(kl)            | -4.721529     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.76         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.67         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0255        |
|    n_updates                | 1783          |
|    policy_gradient_loss     | -0.0229       |
|    std                      | 0.964         |
|    value_loss               | 0.146         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2833216] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 17           |
|    time_elapsed             | 231          |
|    total_timesteps          | 3547136      |
| train/                      |              |
|    approx_kl                | 0.007005373  |
|    approx_ln(kl)            | -4.9610777   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.76        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.39        |
|    ln(policy_gradient_loss) | -5.2         |
|    loss                     | 0.0914       |
|    n_updates                | 1784         |
|    policy_gradient_loss     | 0.00552      |
|    std                      | 0.964        |
|    value_loss               | 0.197        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.28687492] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 18            |
|    time_elapsed             | 244           |
|    total_timesteps          | 3549184       |
| train/                      |               |
|    approx_kl                | 0.011708488   |
|    approx_ln(kl)            | -4.447441     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.89         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0558        |
|    n_updates                | 1785          |
|    policy_gradient_loss     | -0.00346      |
|    std                      | 0.963         |
|    value_loss               | 0.143         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3830251] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 19           |
|    time_elapsed             | 257          |
|    total_timesteps          | 3551232      |
| train/                      |              |
|    approx_kl                | 0.011315395  |
|    approx_ln(kl)            | -4.481591    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.75        |
|    explained_variance       | 0.993        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.25        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.106        |
|    n_updates                | 1786         |
|    policy_gradient_loss     | -0.00556     |
|    std                      | 0.962        |
|    value_loss               | 0.383        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36170277] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 20            |
|    time_elapsed             | 271           |
|    total_timesteps          | 3553280       |
| train/                      |               |
|    approx_kl                | 0.007751779   |
|    approx_ln(kl)            | -4.859833     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.541        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.582         |
|    n_updates                | 1787          |
|    policy_gradient_loss     | -0.000202     |
|    std                      | 0.962         |
|    value_loss               | 2.59          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3342504] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 21           |
|    time_elapsed             | 285          |
|    total_timesteps          | 3555328      |
| train/                      |              |
|    approx_kl                | 0.01229859   |
|    approx_ln(kl)            | -4.3982706   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.75        |
|    explained_variance       | 0.99         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.05        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.35         |
|    n_updates                | 1788         |
|    policy_gradient_loss     | -0.00625     |
|    std                      | 0.961        |
|    value_loss               | 1.36         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2764289] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 22           |
|    time_elapsed             | 298          |
|    total_timesteps          | 3557376      |
| train/                      |              |
|    approx_kl                | 0.008359186  |
|    approx_ln(kl)            | -4.7843943   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.75        |
|    explained_variance       | 0.993        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.661       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.516        |
|    n_updates                | 1789         |
|    policy_gradient_loss     | -0.00522     |
|    std                      | 0.96         |
|    value_loss               | 1.18         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.14931442] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 23            |
|    time_elapsed             | 312           |
|    total_timesteps          | 3559424       |
| train/                      |               |
|    approx_kl                | 0.008218456   |
|    approx_ln(kl)            | -4.801373     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.75         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.1          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.122         |
|    n_updates                | 1790          |
|    policy_gradient_loss     | -0.00554      |
|    std                      | 0.96          |
|    value_loss               | 1.01          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5568534] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 24           |
|    time_elapsed             | 326          |
|    total_timesteps          | 3561472      |
| train/                      |              |
|    approx_kl                | 0.0058999383 |
|    approx_ln(kl)            | -5.1328135   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.75        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.36        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.257        |
|    n_updates                | 1791         |
|    policy_gradient_loss     | -0.0139      |
|    std                      | 0.959        |
|    value_loss               | 0.423        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24768144] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 25            |
|    time_elapsed             | 341           |
|    total_timesteps          | 3563520       |
| train/                      |               |
|    approx_kl                | 0.0072878925  |
|    approx_ln(kl)            | -4.9215407    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.526        |
|    ln(policy_gradient_loss) | -4.95         |
|    loss                     | 0.591         |
|    n_updates                | 1792          |
|    policy_gradient_loss     | 0.00706       |
|    std                      | 0.959         |
|    value_loss               | 0.818         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.20134662] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 26            |
|    time_elapsed             | 361           |
|    total_timesteps          | 3565568       |
| train/                      |               |
|    approx_kl                | 0.0044685607  |
|    approx_ln(kl)            | -5.410689     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.86         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.155         |
|    n_updates                | 1793          |
|    policy_gradient_loss     | -0.0039       |
|    std                      | 0.957         |
|    value_loss               | 0.591         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.28167468] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 27            |
|    time_elapsed             | 375           |
|    total_timesteps          | 3567616       |
| train/                      |               |
|    approx_kl                | 0.0064856866  |
|    approx_ln(kl)            | -5.0381575    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.066         |
|    ln(policy_gradient_loss) | -5.24         |
|    loss                     | 1.07          |
|    n_updates                | 1794          |
|    policy_gradient_loss     | 0.00531       |
|    std                      | 0.956         |
|    value_loss               | 0.722         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22571409] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 28            |
|    time_elapsed             | 392           |
|    total_timesteps          | 3569664       |
| train/                      |               |
|    approx_kl                | 0.004068114   |
|    approx_ln(kl)            | -5.5045757    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.39         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0914        |
|    n_updates                | 1795          |
|    policy_gradient_loss     | -0.002        |
|    std                      | 0.956         |
|    value_loss               | 0.462         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.1739649] |
| time/                       |              |
|    fps                      | 145          |
|    iterations               | 29           |
|    time_elapsed             | 407          |
|    total_timesteps          | 3571712      |
| train/                      |              |
|    approx_kl                | 0.008506473  |
|    approx_ln(kl)            | -4.7669277   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.74        |
|    explained_variance       | 0.99         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -4.18        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0153       |
|    n_updates                | 1796         |
|    policy_gradient_loss     | -0.00511     |
|    std                      | 0.956        |
|    value_loss               | 0.0815       |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38767174] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 30            |
|    time_elapsed             | 421           |
|    total_timesteps          | 3573760       |
| train/                      |               |
|    approx_kl                | 0.0068094037  |
|    approx_ln(kl)            | -4.989451     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.04         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.354         |
|    n_updates                | 1797          |
|    policy_gradient_loss     | -0.00153      |
|    std                      | 0.956         |
|    value_loss               | 0.902         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24967052] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 31            |
|    time_elapsed             | 435           |
|    total_timesteps          | 3575808       |
| train/                      |               |
|    approx_kl                | 0.007618428   |
|    approx_ln(kl)            | -4.8771853    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.394        |
|    ln(policy_gradient_loss) | -5.53         |
|    loss                     | 0.674         |
|    n_updates                | 1798          |
|    policy_gradient_loss     | 0.00396       |
|    std                      | 0.956         |
|    value_loss               | 1.19          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.21472572] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 32            |
|    time_elapsed             | 450           |
|    total_timesteps          | 3577856       |
| train/                      |               |
|    approx_kl                | 0.004570462   |
|    approx_ln(kl)            | -5.388141     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.47         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.23          |
|    n_updates                | 1799          |
|    policy_gradient_loss     | -0.00861      |
|    std                      | 0.955         |
|    value_loss               | 0.59          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.22245556] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 33            |
|    time_elapsed             | 464           |
|    total_timesteps          | 3579904       |
| train/                      |               |
|    approx_kl                | 0.008963039   |
|    approx_ln(kl)            | -4.714646     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.36         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.256         |
|    n_updates                | 1800          |
|    policy_gradient_loss     | -0.00749      |
|    std                      | 0.955         |
|    value_loss               | 0.344         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3034399] |
| time/                       |              |
|    fps                      | 145          |
|    iterations               | 34           |
|    time_elapsed             | 477          |
|    total_timesteps          | 3581952      |
| train/                      |              |
|    approx_kl                | 0.006319618  |
|    approx_ln(kl)            | -5.0640965   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.73        |
|    explained_variance       | 0.978        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.344       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.709        |
|    n_updates                | 1801         |
|    policy_gradient_loss     | -0.00442     |
|    std                      | 0.954        |
|    value_loss               | 0.859        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28332925] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 35            |
|    time_elapsed             | 491           |
|    total_timesteps          | 3584000       |
| train/                      |               |
|    approx_kl                | 0.007900791   |
|    approx_ln(kl)            | -4.840792     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.0288        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.03          |
|    n_updates                | 1802          |
|    policy_gradient_loss     | -0.00177      |
|    std                      | 0.954         |
|    value_loss               | 0.822         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.20835246] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 36            |
|    time_elapsed             | 505           |
|    total_timesteps          | 3586048       |
| train/                      |               |
|    approx_kl                | 0.009591408   |
|    approx_ln(kl)            | -4.646888     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.966         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.697         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 2.01          |
|    n_updates                | 1803          |
|    policy_gradient_loss     | -0.00479      |
|    std                      | 0.953         |
|    value_loss               | 2.05          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.05
----------------------------------------------
| reward                      | [-0.3551012] |
| time/                       |              |
|    fps                      | 146          |
|    iterations               | 37           |
|    time_elapsed             | 518          |
|    total_timesteps          | 3588096      |
| train/                      |              |
|    approx_kl                | 0.014644673  |
|    approx_ln(kl)            | -4.2236786   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.73        |
|    explained_variance       | 0.979        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.12        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0441       |
|    n_updates                | 1804         |
|    policy_gradient_loss     | -0.0279      |
|    std                      | 0.953        |
|    value_loss               | 0.644        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25110954] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 38            |
|    time_elapsed             | 532           |
|    total_timesteps          | 3590144       |
| train/                      |               |
|    approx_kl                | 0.0057416754  |
|    approx_ln(kl)            | -5.160004     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.22         |
|    ln(policy_gradient_loss) | -7.09         |
|    loss                     | 0.108         |
|    n_updates                | 1805          |
|    policy_gradient_loss     | 0.000833      |
|    std                      | 0.952         |
|    value_loss               | 0.811         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.1950664] |
| time/                       |              |
|    fps                      | 146          |
|    iterations               | 39           |
|    time_elapsed             | 545          |
|    total_timesteps          | 3592192      |
| train/                      |              |
|    approx_kl                | 0.008322651  |
|    approx_ln(kl)            | -4.7887745   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.73        |
|    explained_variance       | 0.976        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.07        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.344        |
|    n_updates                | 1806         |
|    policy_gradient_loss     | -0.00488     |
|    std                      | 0.952        |
|    value_loss               | 1.34         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23243539] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 40            |
|    time_elapsed             | 559           |
|    total_timesteps          | 3594240       |
| train/                      |               |
|    approx_kl                | 0.0076790424  |
|    approx_ln(kl)            | -4.8692603    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.422        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.656         |
|    n_updates                | 1807          |
|    policy_gradient_loss     | -0.00303      |
|    std                      | 0.952         |
|    value_loss               | 1.4           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4762694] |
| time/                       |              |
|    fps                      | 146          |
|    iterations               | 41           |
|    time_elapsed             | 572          |
|    total_timesteps          | 3596288      |
| train/                      |              |
|    approx_kl                | 0.006667697  |
|    approx_ln(kl)            | -5.010481    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.73        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.61        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.2          |
|    n_updates                | 1808         |
|    policy_gradient_loss     | -0.00855     |
|    std                      | 0.952        |
|    value_loss               | 0.71         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25311628] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 42            |
|    time_elapsed             | 586           |
|    total_timesteps          | 3598336       |
| train/                      |               |
|    approx_kl                | 0.0059027798  |
|    approx_ln(kl)            | -5.132332     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.91         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0542        |
|    n_updates                | 1809          |
|    policy_gradient_loss     | -0.0069       |
|    std                      | 0.952         |
|    value_loss               | 0.547         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.25608045] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 43            |
|    time_elapsed             | 599           |
|    total_timesteps          | 3600384       |
| train/                      |               |
|    approx_kl                | 0.009449796   |
|    approx_ln(kl)            | -4.661762     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.93         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.145         |
|    n_updates                | 1810          |
|    policy_gradient_loss     | -0.00971      |
|    std                      | 0.952         |
|    value_loss               | 0.309         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.19440767] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 44            |
|    time_elapsed             | 612           |
|    total_timesteps          | 3602432       |
| train/                      |               |
|    approx_kl                | 0.008773964   |
|    approx_ln(kl)            | -4.7359667    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.971         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.826        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.438         |
|    n_updates                | 1811          |
|    policy_gradient_loss     | -0.00605      |
|    std                      | 0.952         |
|    value_loss               | 1.92          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23318602] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 45            |
|    time_elapsed             | 626           |
|    total_timesteps          | 3604480       |
| train/                      |               |
|    approx_kl                | 0.006225112   |
|    approx_ln(kl)            | -5.079164     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.974         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.11         |
|    ln(policy_gradient_loss) | -5.17         |
|    loss                     | 0.896         |
|    n_updates                | 1812          |
|    policy_gradient_loss     | 0.00571       |
|    std                      | 0.953         |
|    value_loss               | 1.31          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24682128] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 46            |
|    time_elapsed             | 639           |
|    total_timesteps          | 3606528       |
| train/                      |               |
|    approx_kl                | 0.0071513937  |
|    approx_ln(kl)            | -4.940448     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.968         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.14         |
|    ln(policy_gradient_loss) | -5.35         |
|    loss                     | 0.32          |
|    n_updates                | 1813          |
|    policy_gradient_loss     | 0.00475       |
|    std                      | 0.952         |
|    value_loss               | 0.894         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.52648497] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 47            |
|    time_elapsed             | 652           |
|    total_timesteps          | 3608576       |
| train/                      |               |
|    approx_kl                | 0.006742793   |
|    approx_ln(kl)            | -4.999281     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.972         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.322        |
|    ln(policy_gradient_loss) | -6.39         |
|    loss                     | 0.725         |
|    n_updates                | 1814          |
|    policy_gradient_loss     | 0.00168       |
|    std                      | 0.951         |
|    value_loss               | 1.3           |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41169956] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 48            |
|    time_elapsed             | 666           |
|    total_timesteps          | 3610624       |
| train/                      |               |
|    approx_kl                | 0.0061015156  |
|    approx_ln(kl)            | -5.099218     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.859        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.424         |
|    n_updates                | 1815          |
|    policy_gradient_loss     | -0.00113      |
|    std                      | 0.951         |
|    value_loss               | 1.05          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.34391424] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 49            |
|    time_elapsed             | 679           |
|    total_timesteps          | 3612672       |
| train/                      |               |
|    approx_kl                | 0.007318669   |
|    approx_ln(kl)            | -4.917327     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.96          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.77         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.463         |
|    n_updates                | 1816          |
|    policy_gradient_loss     | -0.00133      |
|    std                      | 0.951         |
|    value_loss               | 1.03          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
--------------------------------------
| reward             | [-0.36513057] |
| time/              |               |
|    fps             | 156           |
|    iterations      | 1             |
|    time_elapsed    | 13            |
|    total_timesteps | 3614720       |
--------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3217194] |
| time/                       |              |
|    fps                      | 153          |
|    iterations               | 2            |
|    time_elapsed             | 26           |
|    total_timesteps          | 3616768      |
| train/                      |              |
|    approx_kl                | 0.0065486985 |
|    approx_ln(kl)            | -5.028489    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.73        |
|    explained_variance       | 0.986        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.22        |
|    ln(policy_gradient_loss) | -6           |
|    loss                     | 0.108        |
|    n_updates                | 1818         |
|    policy_gradient_loss     | 0.00248      |
|    std                      | 0.951        |
|    value_loss               | 0.558        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26468047] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 3             |
|    time_elapsed             | 40            |
|    total_timesteps          | 3618816       |
| train/                      |               |
|    approx_kl                | 0.0066272453  |
|    approx_ln(kl)            | -5.0165663    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.96          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.471        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.625         |
|    n_updates                | 1819          |
|    policy_gradient_loss     | -0.00375      |
|    std                      | 0.951         |
|    value_loss               | 1.27          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35431683] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 4             |
|    time_elapsed             | 53            |
|    total_timesteps          | 3620864       |
| train/                      |               |
|    approx_kl                | 0.008197601   |
|    approx_ln(kl)            | -4.8039136    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.959         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.14         |
|    ln(policy_gradient_loss) | -6.24         |
|    loss                     | 0.319         |
|    n_updates                | 1820          |
|    policy_gradient_loss     | 0.00194       |
|    std                      | 0.951         |
|    value_loss               | 1.64          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.23637469] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 5             |
|    time_elapsed             | 66            |
|    total_timesteps          | 3622912       |
| train/                      |               |
|    approx_kl                | 0.007653773   |
|    approx_ln(kl)            | -4.8725567    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.976         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.33         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0975        |
|    n_updates                | 1821          |
|    policy_gradient_loss     | -0.00627      |
|    std                      | 0.951         |
|    value_loss               | 0.213         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.42820206] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 6             |
|    time_elapsed             | 80            |
|    total_timesteps          | 3624960       |
| train/                      |               |
|    approx_kl                | 0.011226114   |
|    approx_ln(kl)            | -4.4895124    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.973         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.0164        |
|    ln(policy_gradient_loss) | -5.07         |
|    loss                     | 1.02          |
|    n_updates                | 1822          |
|    policy_gradient_loss     | 0.00626       |
|    std                      | 0.951         |
|    value_loss               | 2.21          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36135498] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 7             |
|    time_elapsed             | 93            |
|    total_timesteps          | 3627008       |
| train/                      |               |
|    approx_kl                | 0.00648461    |
|    approx_ln(kl)            | -5.0383234    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.66         |
|    ln(policy_gradient_loss) | -6            |
|    loss                     | 0.517         |
|    n_updates                | 1823          |
|    policy_gradient_loss     | 0.00249       |
|    std                      | 0.951         |
|    value_loss               | 1.26          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24857262] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 8             |
|    time_elapsed             | 106           |
|    total_timesteps          | 3629056       |
| train/                      |               |
|    approx_kl                | 0.0066044903  |
|    approx_ln(kl)            | -5.0200057    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.68         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.187         |
|    n_updates                | 1824          |
|    policy_gradient_loss     | -0.0123       |
|    std                      | 0.952         |
|    value_loss               | 0.549         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27609167] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 9             |
|    time_elapsed             | 120           |
|    total_timesteps          | 3631104       |
| train/                      |               |
|    approx_kl                | 0.007394064   |
|    approx_ln(kl)            | -4.907078     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.977         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.112        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.894         |
|    n_updates                | 1825          |
|    policy_gradient_loss     | -0.00196      |
|    std                      | 0.952         |
|    value_loss               | 0.742         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.18603505] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 10            |
|    time_elapsed             | 135           |
|    total_timesteps          | 3633152       |
| train/                      |               |
|    approx_kl                | 0.007875208   |
|    approx_ln(kl)            | -4.8440356    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.32         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.267         |
|    n_updates                | 1826          |
|    policy_gradient_loss     | -0.0131       |
|    std                      | 0.951         |
|    value_loss               | 0.713         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2504983] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 11           |
|    time_elapsed             | 149          |
|    total_timesteps          | 3635200      |
| train/                      |              |
|    approx_kl                | 0.007050803  |
|    approx_ln(kl)            | -4.9546137   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.73        |
|    explained_variance       | 0.983        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.28        |
|    ln(policy_gradient_loss) | -4.07        |
|    loss                     | 0.102        |
|    n_updates                | 1827         |
|    policy_gradient_loss     | 0.0171       |
|    std                      | 0.95         |
|    value_loss               | 0.214        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.2389551] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 12           |
|    time_elapsed             | 165          |
|    total_timesteps          | 3637248      |
| train/                      |              |
|    approx_kl                | 0.008658526  |
|    approx_ln(kl)            | -4.749211    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.73        |
|    explained_variance       | 0.975        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.72        |
|    ln(policy_gradient_loss) | -6.6         |
|    loss                     | 0.0656       |
|    n_updates                | 1828         |
|    policy_gradient_loss     | 0.00137      |
|    std                      | 0.95         |
|    value_loss               | 0.225        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25926355] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 13            |
|    time_elapsed             | 185           |
|    total_timesteps          | 3639296       |
| train/                      |               |
|    approx_kl                | 0.006844651   |
|    approx_ln(kl)            | -4.9842877    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.84         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0582        |
|    n_updates                | 1829          |
|    policy_gradient_loss     | -0.00631      |
|    std                      | 0.949         |
|    value_loss               | 0.178         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49263445] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 14            |
|    time_elapsed             | 203           |
|    total_timesteps          | 3641344       |
| train/                      |               |
|    approx_kl                | 0.008261918   |
|    approx_ln(kl)            | -4.7960987    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.974         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.482        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.617         |
|    n_updates                | 1830          |
|    policy_gradient_loss     | -0.00141      |
|    std                      | 0.948         |
|    value_loss               | 0.902         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.27570722] |
| time/                       |               |
|    fps                      | 140           |
|    iterations               | 15            |
|    time_elapsed             | 217           |
|    total_timesteps          | 3643392       |
| train/                      |               |
|    approx_kl                | 0.0151428385  |
|    approx_ln(kl)            | -4.1902275    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.93         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0534        |
|    n_updates                | 1831          |
|    policy_gradient_loss     | -0.0108       |
|    std                      | 0.948         |
|    value_loss               | 0.149         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32175314] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 16            |
|    time_elapsed             | 231           |
|    total_timesteps          | 3645440       |
| train/                      |               |
|    approx_kl                | 0.009183862   |
|    approx_ln(kl)            | -4.6903076    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.61         |
|    ln(policy_gradient_loss) | -5.28         |
|    loss                     | 0.2           |
|    n_updates                | 1832          |
|    policy_gradient_loss     | 0.0051        |
|    std                      | 0.947         |
|    value_loss               | 1.07          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40075108] |
| time/                       |               |
|    fps                      | 142           |
|    iterations               | 17            |
|    time_elapsed             | 245           |
|    total_timesteps          | 3647488       |
| train/                      |               |
|    approx_kl                | 0.007183352   |
|    approx_ln(kl)            | -4.9359894    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.52         |
|    ln(policy_gradient_loss) | -5.27         |
|    loss                     | 0.218         |
|    n_updates                | 1833          |
|    policy_gradient_loss     | 0.00513       |
|    std                      | 0.946         |
|    value_loss               | 0.73          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2656552] |
| time/                       |              |
|    fps                      | 142          |
|    iterations               | 18           |
|    time_elapsed             | 259          |
|    total_timesteps          | 3649536      |
| train/                      |              |
|    approx_kl                | 0.0074788583 |
|    approx_ln(kl)            | -4.895675    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.72        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.56        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0774       |
|    n_updates                | 1834         |
|    policy_gradient_loss     | -0.00814     |
|    std                      | 0.946        |
|    value_loss               | 0.258        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
---------------------------------------------
| reward                      | [-0.263055] |
| time/                       |             |
|    fps                      | 142         |
|    iterations               | 19          |
|    time_elapsed             | 273         |
|    total_timesteps          | 3651584     |
| train/                      |             |
|    approx_kl                | 0.013825916 |
|    approx_ln(kl)            | -4.2812104  |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.72       |
|    explained_variance       | 0.986       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | 0.0688      |
|    ln(policy_gradient_loss) | -4.37       |
|    loss                     | 1.07        |
|    n_updates                | 1835        |
|    policy_gradient_loss     | 0.0127      |
|    std                      | 0.946       |
|    value_loss               | 0.98        |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
-----------------------------------------------
| reward                      | [-0.30769312] |
| time/                       |               |
|    fps                      | 142           |
|    iterations               | 20            |
|    time_elapsed             | 286           |
|    total_timesteps          | 3653632       |
| train/                      |               |
|    approx_kl                | 0.013757092   |
|    approx_ln(kl)            | -4.286201     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.7          |
|    ln(policy_gradient_loss) | -5.29         |
|    loss                     | 0.183         |
|    n_updates                | 1836          |
|    policy_gradient_loss     | 0.00506       |
|    std                      | 0.946         |
|    value_loss               | 0.253         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42963797] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 21            |
|    time_elapsed             | 300           |
|    total_timesteps          | 3655680       |
| train/                      |               |
|    approx_kl                | 0.0069177556  |
|    approx_ln(kl)            | -4.973664     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.886        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.412         |
|    n_updates                | 1837          |
|    policy_gradient_loss     | -0.0075       |
|    std                      | 0.945         |
|    value_loss               | 0.723         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3654486] |
| time/                       |              |
|    fps                      | 143          |
|    iterations               | 22           |
|    time_elapsed             | 314          |
|    total_timesteps          | 3657728      |
| train/                      |              |
|    approx_kl                | 0.0058651282 |
|    approx_ln(kl)            | -5.138731    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.72        |
|    explained_variance       | 0.99         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.87        |
|    ln(policy_gradient_loss) | -4.26        |
|    loss                     | 0.155        |
|    n_updates                | 1838         |
|    policy_gradient_loss     | 0.0141       |
|    std                      | 0.945        |
|    value_loss               | 0.324        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24604587] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 23            |
|    time_elapsed             | 327           |
|    total_timesteps          | 3659776       |
| train/                      |               |
|    approx_kl                | 0.0074252672  |
|    approx_ln(kl)            | -4.902867     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.18         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.113         |
|    n_updates                | 1839          |
|    policy_gradient_loss     | -0.00492      |
|    std                      | 0.946         |
|    value_loss               | 0.191         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.49976686] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 24            |
|    time_elapsed             | 341           |
|    total_timesteps          | 3661824       |
| train/                      |               |
|    approx_kl                | 0.010309435   |
|    approx_ln(kl)            | -4.5746956    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.863        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.422         |
|    n_updates                | 1840          |
|    policy_gradient_loss     | -0.0161       |
|    std                      | 0.946         |
|    value_loss               | 0.789         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.24290158] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 25            |
|    time_elapsed             | 354           |
|    total_timesteps          | 3663872       |
| train/                      |               |
|    approx_kl                | 0.0074923704  |
|    approx_ln(kl)            | -4.89387      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.58         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0279        |
|    n_updates                | 1841          |
|    policy_gradient_loss     | -0.00866      |
|    std                      | 0.947         |
|    value_loss               | 0.11          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.28647324] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 26            |
|    time_elapsed             | 368           |
|    total_timesteps          | 3665920       |
| train/                      |               |
|    approx_kl                | 0.011495506   |
|    approx_ln(kl)            | -4.465799     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.949        |
|    ln(policy_gradient_loss) | -4.97         |
|    loss                     | 0.387         |
|    n_updates                | 1842          |
|    policy_gradient_loss     | 0.00694       |
|    std                      | 0.948         |
|    value_loss               | 0.78          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.54379475] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 27            |
|    time_elapsed             | 381           |
|    total_timesteps          | 3667968       |
| train/                      |               |
|    approx_kl                | 0.01208461    |
|    approx_ln(kl)            | -4.4158225    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.77         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.463         |
|    n_updates                | 1843          |
|    policy_gradient_loss     | -0.00851      |
|    std                      | 0.948         |
|    value_loss               | 0.685         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5604469] |
| time/                       |              |
|    fps                      | 145          |
|    iterations               | 28           |
|    time_elapsed             | 395          |
|    total_timesteps          | 3670016      |
| train/                      |              |
|    approx_kl                | 0.007175873  |
|    approx_ln(kl)            | -4.937031    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.72        |
|    explained_variance       | 0.997        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.76        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.171        |
|    n_updates                | 1844         |
|    policy_gradient_loss     | -0.00194     |
|    std                      | 0.948        |
|    value_loss               | 0.595        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2920961] |
| time/                       |              |
|    fps                      | 145          |
|    iterations               | 29           |
|    time_elapsed             | 408          |
|    total_timesteps          | 3672064      |
| train/                      |              |
|    approx_kl                | 0.00636332   |
|    approx_ln(kl)            | -5.057205    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.72        |
|    explained_variance       | 0.995        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.04        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.13         |
|    n_updates                | 1845         |
|    policy_gradient_loss     | -0.00261     |
|    std                      | 0.949        |
|    value_loss               | 0.346        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.22451505] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 30            |
|    time_elapsed             | 421           |
|    total_timesteps          | 3674112       |
| train/                      |               |
|    approx_kl                | 0.010012488   |
|    approx_ln(kl)            | -4.6039224    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.87         |
|    ln(policy_gradient_loss) | -5.16         |
|    loss                     | 0.0567        |
|    n_updates                | 1846          |
|    policy_gradient_loss     | 0.00573       |
|    std                      | 0.951         |
|    value_loss               | 0.133         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.39865354] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 31            |
|    time_elapsed             | 435           |
|    total_timesteps          | 3676160       |
| train/                      |               |
|    approx_kl                | 0.0073102117  |
|    approx_ln(kl)            | -4.9184833    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.69         |
|    ln(policy_gradient_loss) | -4.49         |
|    loss                     | 0.185         |
|    n_updates                | 1847          |
|    policy_gradient_loss     | 0.0112        |
|    std                      | 0.954         |
|    value_loss               | 0.907         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35476214] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 32            |
|    time_elapsed             | 448           |
|    total_timesteps          | 3678208       |
| train/                      |               |
|    approx_kl                | 0.007960447   |
|    approx_ln(kl)            | -4.83327      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.38         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0927        |
|    n_updates                | 1848          |
|    policy_gradient_loss     | -0.00538      |
|    std                      | 0.954         |
|    value_loss               | 0.185         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.5161285] |
| time/                       |              |
|    fps                      | 146          |
|    iterations               | 33           |
|    time_elapsed             | 461          |
|    total_timesteps          | 3680256      |
| train/                      |              |
|    approx_kl                | 0.0069501563 |
|    approx_ln(kl)            | -4.9689913   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.74        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.924       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.397        |
|    n_updates                | 1849         |
|    policy_gradient_loss     | -0.00524     |
|    std                      | 0.955        |
|    value_loss               | 0.46         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3215056] |
| time/                       |              |
|    fps                      | 146          |
|    iterations               | 34           |
|    time_elapsed             | 475          |
|    total_timesteps          | 3682304      |
| train/                      |              |
|    approx_kl                | 0.012553893  |
|    approx_ln(kl)            | -4.3777246   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.74        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.86        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.155        |
|    n_updates                | 1850         |
|    policy_gradient_loss     | -0.00893     |
|    std                      | 0.956        |
|    value_loss               | 0.344        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.07
----------------------------------------------
| reward                      | [-0.2939918] |
| time/                       |              |
|    fps                      | 146          |
|    iterations               | 35           |
|    time_elapsed             | 488          |
|    total_timesteps          | 3684352      |
| train/                      |              |
|    approx_kl                | 0.026654286  |
|    approx_ln(kl)            | -3.6248052   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.74        |
|    explained_variance       | 0.996        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.3         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.037        |
|    n_updates                | 1851         |
|    policy_gradient_loss     | -0.0192      |
|    std                      | 0.956        |
|    value_loss               | 0.155        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.37788227] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 36            |
|    time_elapsed             | 501           |
|    total_timesteps          | 3686400       |
| train/                      |               |
|    approx_kl                | 0.013801488   |
|    approx_ln(kl)            | -4.282979     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.41         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.033         |
|    n_updates                | 1852          |
|    policy_gradient_loss     | -0.0124       |
|    std                      | 0.956         |
|    value_loss               | 0.32          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25744188] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 37            |
|    time_elapsed             | 514           |
|    total_timesteps          | 3688448       |
| train/                      |               |
|    approx_kl                | 0.008843724   |
|    approx_ln(kl)            | -4.7280474    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.74         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.24         |
|    ln(policy_gradient_loss) | -5.76         |
|    loss                     | 0.106         |
|    n_updates                | 1853          |
|    policy_gradient_loss     | 0.00317       |
|    std                      | 0.955         |
|    value_loss               | 0.177         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31322074] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 38            |
|    time_elapsed             | 527           |
|    total_timesteps          | 3690496       |
| train/                      |               |
|    approx_kl                | 0.0059143677  |
|    approx_ln(kl)            | -5.1303706    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.962         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.24         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.291         |
|    n_updates                | 1854          |
|    policy_gradient_loss     | -0.00586      |
|    std                      | 0.952         |
|    value_loss               | 0.87          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26996157] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 39            |
|    time_elapsed             | 542           |
|    total_timesteps          | 3692544       |
| train/                      |               |
|    approx_kl                | 0.0056989505  |
|    approx_ln(kl)            | -5.1674733    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.945         |
|    ln(policy_gradient_loss) | -6.01         |
|    loss                     | 2.57          |
|    n_updates                | 1855          |
|    policy_gradient_loss     | 0.00245       |
|    std                      | 0.95          |
|    value_loss               | 1.82          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3400346] |
| time/                       |              |
|    fps                      | 147          |
|    iterations               | 40           |
|    time_elapsed             | 555          |
|    total_timesteps          | 3694592      |
| train/                      |              |
|    approx_kl                | 0.0061176703 |
|    approx_ln(kl)            | -5.096574    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.73        |
|    explained_variance       | 0.947        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.83        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.16         |
|    n_updates                | 1856         |
|    policy_gradient_loss     | -0.000826    |
|    std                      | 0.95         |
|    value_loss               | 0.26         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34983578] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 41            |
|    time_elapsed             | 569           |
|    total_timesteps          | 3696640       |
| train/                      |               |
|    approx_kl                | 0.0058508073  |
|    approx_ln(kl)            | -5.1411757    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.97          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.357        |
|    ln(policy_gradient_loss) | -6.75         |
|    loss                     | 0.7           |
|    n_updates                | 1857          |
|    policy_gradient_loss     | 0.00117       |
|    std                      | 0.95          |
|    value_loss               | 1.42          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.49636027] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 42            |
|    time_elapsed             | 582           |
|    total_timesteps          | 3698688       |
| train/                      |               |
|    approx_kl                | 0.0072745793  |
|    approx_ln(kl)            | -4.9233694    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.951         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.875        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.417         |
|    n_updates                | 1858          |
|    policy_gradient_loss     | -0.00178      |
|    std                      | 0.95          |
|    value_loss               | 1.05          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30588773] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 43            |
|    time_elapsed             | 596           |
|    total_timesteps          | 3700736       |
| train/                      |               |
|    approx_kl                | 0.004578783   |
|    approx_ln(kl)            | -5.386322     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.39         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0918        |
|    n_updates                | 1859          |
|    policy_gradient_loss     | -0.00131      |
|    std                      | 0.95          |
|    value_loss               | 0.275         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30504063] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 44            |
|    time_elapsed             | 609           |
|    total_timesteps          | 3702784       |
| train/                      |               |
|    approx_kl                | 0.0072323107  |
|    approx_ln(kl)            | -4.929197     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.965         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.69         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.502         |
|    n_updates                | 1860          |
|    policy_gradient_loss     | -0.00486      |
|    std                      | 0.949         |
|    value_loss               | 3.33          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32901752] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 45            |
|    time_elapsed             | 623           |
|    total_timesteps          | 3704832       |
| train/                      |               |
|    approx_kl                | 0.00673136    |
|    approx_ln(kl)            | -5.000978     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.73         |
|    explained_variance       | 0.955         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.41         |
|    ln(policy_gradient_loss) | -4.68         |
|    loss                     | 0.245         |
|    n_updates                | 1861          |
|    policy_gradient_loss     | 0.00929       |
|    std                      | 0.949         |
|    value_loss               | 0.645         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2506708] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 46           |
|    time_elapsed             | 636          |
|    total_timesteps          | 3706880      |
| train/                      |              |
|    approx_kl                | 0.006821046  |
|    approx_ln(kl)            | -4.9877424   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.72        |
|    explained_variance       | 0.986        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.11        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.121        |
|    n_updates                | 1862         |
|    policy_gradient_loss     | -0.0158      |
|    std                      | 0.949        |
|    value_loss               | 0.209        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2826637] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 47           |
|    time_elapsed             | 649          |
|    total_timesteps          | 3708928      |
| train/                      |              |
|    approx_kl                | 0.009061881  |
|    approx_ln(kl)            | -4.7036786   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.72        |
|    explained_variance       | 0.958        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.74        |
|    ln(policy_gradient_loss) | -5.86        |
|    loss                     | 0.0647       |
|    n_updates                | 1863         |
|    policy_gradient_loss     | 0.00285      |
|    std                      | 0.948        |
|    value_loss               | 0.153        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31826368] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 48            |
|    time_elapsed             | 663           |
|    total_timesteps          | 3710976       |
| train/                      |               |
|    approx_kl                | 0.007032417   |
|    approx_ln(kl)            | -4.957225     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.952         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.655        |
|    ln(policy_gradient_loss) | -6.18         |
|    loss                     | 0.519         |
|    n_updates                | 1864          |
|    policy_gradient_loss     | 0.00207       |
|    std                      | 0.948         |
|    value_loss               | 1.78          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.2331275] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 49           |
|    time_elapsed             | 676          |
|    total_timesteps          | 3713024      |
| train/                      |              |
|    approx_kl                | 0.009252881  |
|    approx_ln(kl)            | -4.6828203   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.72        |
|    explained_variance       | 0.968        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.783       |
|    ln(policy_gradient_loss) | -5.22        |
|    loss                     | 0.457        |
|    n_updates                | 1865         |
|    policy_gradient_loss     | 0.00538      |
|    std                      | 0.948        |
|    value_loss               | 1.2          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
--------------------------------------
| reward             | [-0.24906954] |
| time/              |               |
|    fps             | 161           |
|    iterations      | 1             |
|    time_elapsed    | 12            |
|    total_timesteps | 3715072       |
--------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.51102096] |
| time/                       |               |
|    fps                      | 154           |
|    iterations               | 2             |
|    time_elapsed             | 26            |
|    total_timesteps          | 3717120       |
| train/                      |               |
|    approx_kl                | 0.011382227   |
|    approx_ln(kl)            | -4.4757023    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.977         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.356        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.701         |
|    n_updates                | 1867          |
|    policy_gradient_loss     | -0.00296      |
|    std                      | 0.947         |
|    value_loss               | 1.97          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.27570626] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 3             |
|    time_elapsed             | 40            |
|    total_timesteps          | 3719168       |
| train/                      |               |
|    approx_kl                | 0.0099062     |
|    approx_ln(kl)            | -4.6145945    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.967         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.23         |
|    ln(policy_gradient_loss) | -6.33         |
|    loss                     | 0.293         |
|    n_updates                | 1868          |
|    policy_gradient_loss     | 0.00177       |
|    std                      | 0.946         |
|    value_loss               | 0.939         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.26342663] |
| time/                       |               |
|    fps                      | 136           |
|    iterations               | 4             |
|    time_elapsed             | 59            |
|    total_timesteps          | 3721216       |
| train/                      |               |
|    approx_kl                | 0.008152711   |
|    approx_ln(kl)            | -4.809405     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.97          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.07         |
|    ln(policy_gradient_loss) | -7.86         |
|    loss                     | 0.126         |
|    n_updates                | 1869          |
|    policy_gradient_loss     | 0.000386      |
|    std                      | 0.946         |
|    value_loss               | 1.27          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3947165] |
| time/                       |              |
|    fps                      | 138          |
|    iterations               | 5            |
|    time_elapsed             | 73           |
|    total_timesteps          | 3723264      |
| train/                      |              |
|    approx_kl                | 0.0062772026 |
|    approx_ln(kl)            | -5.070831    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.72        |
|    explained_variance       | 0.969        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.76        |
|    ln(policy_gradient_loss) | -5.3         |
|    loss                     | 0.172        |
|    n_updates                | 1870         |
|    policy_gradient_loss     | 0.00501      |
|    std                      | 0.946        |
|    value_loss               | 0.713        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.45066395] |
| time/                       |               |
|    fps                      | 141           |
|    iterations               | 6             |
|    time_elapsed             | 87            |
|    total_timesteps          | 3725312       |
| train/                      |               |
|    approx_kl                | 0.005586798   |
|    approx_ln(kl)            | -5.187349     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.925         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.6          |
|    ln(policy_gradient_loss) | -12.6         |
|    loss                     | 0.0739        |
|    n_updates                | 1871          |
|    policy_gradient_loss     | 3.51e-06      |
|    std                      | 0.947         |
|    value_loss               | 0.137         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31618348] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 7             |
|    time_elapsed             | 100           |
|    total_timesteps          | 3727360       |
| train/                      |               |
|    approx_kl                | 0.00730144    |
|    approx_ln(kl)            | -4.919684     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.51         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0298        |
|    n_updates                | 1872          |
|    policy_gradient_loss     | -0.00374      |
|    std                      | 0.948         |
|    value_loss               | 0.117         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38534665] |
| time/                       |               |
|    fps                      | 143           |
|    iterations               | 8             |
|    time_elapsed             | 114           |
|    total_timesteps          | 3729408       |
| train/                      |               |
|    approx_kl                | 0.010246232   |
|    approx_ln(kl)            | -4.5808454    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.65         |
|    ln(policy_gradient_loss) | -4.49         |
|    loss                     | 0.193         |
|    n_updates                | 1873          |
|    policy_gradient_loss     | 0.0112        |
|    std                      | 0.947         |
|    value_loss               | 0.426         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.31569543] |
| time/                       |               |
|    fps                      | 144           |
|    iterations               | 9             |
|    time_elapsed             | 127           |
|    total_timesteps          | 3731456       |
| train/                      |               |
|    approx_kl                | 0.011221657   |
|    approx_ln(kl)            | -4.4899096    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.43         |
|    ln(policy_gradient_loss) | -4.49         |
|    loss                     | 0.24          |
|    n_updates                | 1874          |
|    policy_gradient_loss     | 0.0112        |
|    std                      | 0.947         |
|    value_loss               | 0.325         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31074154] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 10            |
|    time_elapsed             | 141           |
|    total_timesteps          | 3733504       |
| train/                      |               |
|    approx_kl                | 0.0060920827  |
|    approx_ln(kl)            | -5.100765     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.31         |
|    ln(policy_gradient_loss) | -4.37         |
|    loss                     | 0.269         |
|    n_updates                | 1875          |
|    policy_gradient_loss     | 0.0126        |
|    std                      | 0.948         |
|    value_loss               | 2.37          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41410974] |
| time/                       |               |
|    fps                      | 145           |
|    iterations               | 11            |
|    time_elapsed             | 154           |
|    total_timesteps          | 3735552       |
| train/                      |               |
|    approx_kl                | 0.0061169975  |
|    approx_ln(kl)            | -5.096684     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.977         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.249        |
|    ln(policy_gradient_loss) | -5.01         |
|    loss                     | 0.779         |
|    n_updates                | 1876          |
|    policy_gradient_loss     | 0.0067        |
|    std                      | 0.948         |
|    value_loss               | 0.764         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.28809023] |
| time/                       |               |
|    fps                      | 146           |
|    iterations               | 12            |
|    time_elapsed             | 167           |
|    total_timesteps          | 3737600       |
| train/                      |               |
|    approx_kl                | 0.007390878   |
|    approx_ln(kl)            | -4.907509     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.965         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.446        |
|    ln(policy_gradient_loss) | -4.69         |
|    loss                     | 0.64          |
|    n_updates                | 1877          |
|    policy_gradient_loss     | 0.00923       |
|    std                      | 0.948         |
|    value_loss               | 1.08          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.3538881] |
| time/                       |              |
|    fps                      | 147          |
|    iterations               | 13           |
|    time_elapsed             | 180          |
|    total_timesteps          | 3739648      |
| train/                      |              |
|    approx_kl                | 0.0113974605 |
|    approx_ln(kl)            | -4.4743648   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.72        |
|    explained_variance       | 0.984        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.08        |
|    ln(policy_gradient_loss) | -4.56        |
|    loss                     | 0.125        |
|    n_updates                | 1878         |
|    policy_gradient_loss     | 0.0105       |
|    std                      | 0.949        |
|    value_loss               | 0.417        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38366538] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 14            |
|    time_elapsed             | 193           |
|    total_timesteps          | 3741696       |
| train/                      |               |
|    approx_kl                | 0.009425816   |
|    approx_ln(kl)            | -4.664303     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.8          |
|    ln(policy_gradient_loss) | -5.23         |
|    loss                     | 0.165         |
|    n_updates                | 1879          |
|    policy_gradient_loss     | 0.00533       |
|    std                      | 0.949         |
|    value_loss               | 0.86          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3498812] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 15           |
|    time_elapsed             | 206          |
|    total_timesteps          | 3743744      |
| train/                      |              |
|    approx_kl                | 0.008428217  |
|    approx_ln(kl)            | -4.7761703   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.72        |
|    explained_variance       | 0.966        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.626        |
|    ln(policy_gradient_loss) | -6.46        |
|    loss                     | 1.87         |
|    n_updates                | 1880         |
|    policy_gradient_loss     | 0.00156      |
|    std                      | 0.949        |
|    value_loss               | 3.11         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30692765] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 16            |
|    time_elapsed             | 220           |
|    total_timesteps          | 3745792       |
| train/                      |               |
|    approx_kl                | 0.0063326433  |
|    approx_ln(kl)            | -5.0620375    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.93         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.145         |
|    n_updates                | 1881          |
|    policy_gradient_loss     | -0.00518      |
|    std                      | 0.948         |
|    value_loss               | 0.261         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3378669] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 17           |
|    time_elapsed             | 233          |
|    total_timesteps          | 3747840      |
| train/                      |              |
|    approx_kl                | 0.0067634857 |
|    approx_ln(kl)            | -4.996217    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.72        |
|    explained_variance       | 0.985        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.72        |
|    ln(policy_gradient_loss) | -4.83        |
|    loss                     | 0.179        |
|    n_updates                | 1882         |
|    policy_gradient_loss     | 0.00801      |
|    std                      | 0.948        |
|    value_loss               | 0.831        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.27037558] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 18            |
|    time_elapsed             | 246           |
|    total_timesteps          | 3749888       |
| train/                      |               |
|    approx_kl                | 0.011287722   |
|    approx_ln(kl)            | -4.48404      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.14         |
|    ln(policy_gradient_loss) | -5.18         |
|    loss                     | 0.118         |
|    n_updates                | 1883          |
|    policy_gradient_loss     | 0.00565       |
|    std                      | 0.949         |
|    value_loss               | 0.311         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.2226423] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 19           |
|    time_elapsed             | 260          |
|    total_timesteps          | 3751936      |
| train/                      |              |
|    approx_kl                | 0.013125453  |
|    approx_ln(kl)            | -4.333202    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.72        |
|    explained_variance       | 0.984        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.455       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.634        |
|    n_updates                | 1884         |
|    policy_gradient_loss     | -0.0117      |
|    std                      | 0.949        |
|    value_loss               | 0.728        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2880935] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 20           |
|    time_elapsed             | 275          |
|    total_timesteps          | 3753984      |
| train/                      |              |
|    approx_kl                | 0.0099351555 |
|    approx_ln(kl)            | -4.6116757   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.72        |
|    explained_variance       | 0.988        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.525       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.592        |
|    n_updates                | 1885         |
|    policy_gradient_loss     | -0.0146      |
|    std                      | 0.949        |
|    value_loss               | 0.89         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2800497] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 21           |
|    time_elapsed             | 289          |
|    total_timesteps          | 3756032      |
| train/                      |              |
|    approx_kl                | 0.007626368  |
|    approx_ln(kl)            | -4.8761435   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.72        |
|    explained_variance       | 0.991        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.729       |
|    ln(policy_gradient_loss) | -4.49        |
|    loss                     | 0.482        |
|    n_updates                | 1886         |
|    policy_gradient_loss     | 0.0112       |
|    std                      | 0.949        |
|    value_loss               | 0.384        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23950034] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 22            |
|    time_elapsed             | 302           |
|    total_timesteps          | 3758080       |
| train/                      |               |
|    approx_kl                | 0.009508038   |
|    approx_ln(kl)            | -4.6556177    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.76         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.171         |
|    n_updates                | 1887          |
|    policy_gradient_loss     | -0.0141       |
|    std                      | 0.949         |
|    value_loss               | 0.907         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33163083] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 23            |
|    time_elapsed             | 316           |
|    total_timesteps          | 3760128       |
| train/                      |               |
|    approx_kl                | 0.0075013274  |
|    approx_ln(kl)            | -4.8926754    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.56         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0773        |
|    n_updates                | 1888          |
|    policy_gradient_loss     | -0.00436      |
|    std                      | 0.948         |
|    value_loss               | 0.745         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2519088] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 24           |
|    time_elapsed             | 329          |
|    total_timesteps          | 3762176      |
| train/                      |              |
|    approx_kl                | 0.007655413  |
|    approx_ln(kl)            | -4.872342    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.72        |
|    explained_variance       | 0.995        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.36        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0347       |
|    n_updates                | 1889         |
|    policy_gradient_loss     | -0.0128      |
|    std                      | 0.948        |
|    value_loss               | 0.233        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.2831178] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 25           |
|    time_elapsed             | 343          |
|    total_timesteps          | 3764224      |
| train/                      |              |
|    approx_kl                | 0.008989787  |
|    approx_ln(kl)            | -4.711666    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.72        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.7         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0671       |
|    n_updates                | 1890         |
|    policy_gradient_loss     | -0.000776    |
|    std                      | 0.947        |
|    value_loss               | 0.152        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.30035886] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 26            |
|    time_elapsed             | 356           |
|    total_timesteps          | 3766272       |
| train/                      |               |
|    approx_kl                | 0.009133359   |
|    approx_ln(kl)            | -4.695822     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.977         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.05         |
|    ln(policy_gradient_loss) | -4.63         |
|    loss                     | 0.348         |
|    n_updates                | 1891          |
|    policy_gradient_loss     | 0.00973       |
|    std                      | 0.947         |
|    value_loss               | 1.05          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30911922] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 27            |
|    time_elapsed             | 370           |
|    total_timesteps          | 3768320       |
| train/                      |               |
|    approx_kl                | 0.007690077   |
|    approx_ln(kl)            | -4.8678246    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.949         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.0967        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.1           |
|    n_updates                | 1892          |
|    policy_gradient_loss     | -0.00282      |
|    std                      | 0.947         |
|    value_loss               | 2.49          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3773263] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 28           |
|    time_elapsed             | 383          |
|    total_timesteps          | 3770368      |
| train/                      |              |
|    approx_kl                | 0.005063247  |
|    approx_ln(kl)            | -5.2857475   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.72        |
|    explained_variance       | 0.94         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.828       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.437        |
|    n_updates                | 1893         |
|    policy_gradient_loss     | -0.000241    |
|    std                      | 0.948        |
|    value_loss               | 3.74         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23750141] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 29            |
|    time_elapsed             | 397           |
|    total_timesteps          | 3772416       |
| train/                      |               |
|    approx_kl                | 0.0036200834  |
|    approx_ln(kl)            | -5.6212583    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.93          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.333        |
|    ln(policy_gradient_loss) | -6.65         |
|    loss                     | 0.717         |
|    n_updates                | 1894          |
|    policy_gradient_loss     | 0.00129       |
|    std                      | 0.949         |
|    value_loss               | 2.91          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30328855] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 30            |
|    time_elapsed             | 411           |
|    total_timesteps          | 3774464       |
| train/                      |               |
|    approx_kl                | 0.00739525    |
|    approx_ln(kl)            | -4.9069176    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.977         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.21         |
|    ln(policy_gradient_loss) | -6.46         |
|    loss                     | 0.299         |
|    n_updates                | 1895          |
|    policy_gradient_loss     | 0.00156       |
|    std                      | 0.949         |
|    value_loss               | 0.808         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34971458] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 31            |
|    time_elapsed             | 425           |
|    total_timesteps          | 3776512       |
| train/                      |               |
|    approx_kl                | 0.0039774566  |
|    approx_ln(kl)            | -5.527113     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.929         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.718        |
|    ln(policy_gradient_loss) | -6.85         |
|    loss                     | 0.488         |
|    n_updates                | 1896          |
|    policy_gradient_loss     | 0.00106       |
|    std                      | 0.949         |
|    value_loss               | 0.815         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22244458] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 32            |
|    time_elapsed             | 438           |
|    total_timesteps          | 3778560       |
| train/                      |               |
|    approx_kl                | 0.007422401   |
|    approx_ln(kl)            | -4.9032526    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.931         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.398         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.49          |
|    n_updates                | 1897          |
|    policy_gradient_loss     | -0.0014       |
|    std                      | 0.948         |
|    value_loss               | 2.58          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31045634] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 33            |
|    time_elapsed             | 451           |
|    total_timesteps          | 3780608       |
| train/                      |               |
|    approx_kl                | 0.0051840907  |
|    approx_ln(kl)            | -5.262161     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.974         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.33         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.266         |
|    n_updates                | 1898          |
|    policy_gradient_loss     | -0.00809      |
|    std                      | 0.948         |
|    value_loss               | 0.49          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38232976] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 34            |
|    time_elapsed             | 464           |
|    total_timesteps          | 3782656       |
| train/                      |               |
|    approx_kl                | 0.007096653   |
|    approx_ln(kl)            | -4.948132     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.975         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.223        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.8           |
|    n_updates                | 1899          |
|    policy_gradient_loss     | -0.00844      |
|    std                      | 0.948         |
|    value_loss               | 1.33          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.21579345] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 35            |
|    time_elapsed             | 478           |
|    total_timesteps          | 3784704       |
| train/                      |               |
|    approx_kl                | 0.007286884   |
|    approx_ln(kl)            | -4.9216795    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.958         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.425        |
|    ln(policy_gradient_loss) | -5.51         |
|    loss                     | 0.654         |
|    n_updates                | 1900          |
|    policy_gradient_loss     | 0.00406       |
|    std                      | 0.948         |
|    value_loss               | 1.45          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29694068] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 36            |
|    time_elapsed             | 493           |
|    total_timesteps          | 3786752       |
| train/                      |               |
|    approx_kl                | 0.008314447   |
|    approx_ln(kl)            | -4.7897606    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.8          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.165         |
|    n_updates                | 1901          |
|    policy_gradient_loss     | -0.0069       |
|    std                      | 0.947         |
|    value_loss               | 0.291         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35802856] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 37            |
|    time_elapsed             | 512           |
|    total_timesteps          | 3788800       |
| train/                      |               |
|    approx_kl                | 0.0075879646  |
|    approx_ln(kl)            | -4.8811917    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.966         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.906        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.404         |
|    n_updates                | 1902          |
|    policy_gradient_loss     | -0.00159      |
|    std                      | 0.946         |
|    value_loss               | 0.518         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.05
-----------------------------------------------
| reward                      | [-0.48668012] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 38            |
|    time_elapsed             | 527           |
|    total_timesteps          | 3790848       |
| train/                      |               |
|    approx_kl                | 0.01373964    |
|    approx_ln(kl)            | -4.2874703    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.72         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -4.22         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0147        |
|    n_updates                | 1903          |
|    policy_gradient_loss     | -0.0125       |
|    std                      | 0.945         |
|    value_loss               | 0.0903        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24230257] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 39            |
|    time_elapsed             | 540           |
|    total_timesteps          | 3792896       |
| train/                      |               |
|    approx_kl                | 0.006846775   |
|    approx_ln(kl)            | -4.983978     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.62         |
|    ln(policy_gradient_loss) | -5.67         |
|    loss                     | 0.0731        |
|    n_updates                | 1904          |
|    policy_gradient_loss     | 0.00346       |
|    std                      | 0.944         |
|    value_loss               | 0.157         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.39166427] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 40            |
|    time_elapsed             | 554           |
|    total_timesteps          | 3794944       |
| train/                      |               |
|    approx_kl                | 0.011472454   |
|    approx_ln(kl)            | -4.4678063    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.973         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.65         |
|    ln(policy_gradient_loss) | -4.16         |
|    loss                     | 0.192         |
|    n_updates                | 1905          |
|    policy_gradient_loss     | 0.0156        |
|    std                      | 0.944         |
|    value_loss               | 2.19          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24914746] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 41            |
|    time_elapsed             | 567           |
|    total_timesteps          | 3796992       |
| train/                      |               |
|    approx_kl                | 0.009586042   |
|    approx_ln(kl)            | -4.647447     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.381        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.683         |
|    n_updates                | 1906          |
|    policy_gradient_loss     | -0.0115       |
|    std                      | 0.944         |
|    value_loss               | 0.473         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.27642557] |
| time/                       |               |
|    fps                      | 147           |
|    iterations               | 42            |
|    time_elapsed             | 581           |
|    total_timesteps          | 3799040       |
| train/                      |               |
|    approx_kl                | 0.009796171   |
|    approx_ln(kl)            | -4.625764     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -6.19         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.00206       |
|    n_updates                | 1907          |
|    policy_gradient_loss     | -0.0144       |
|    std                      | 0.943         |
|    value_loss               | 0.0621        |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.30821204] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 43            |
|    time_elapsed             | 594           |
|    total_timesteps          | 3801088       |
| train/                      |               |
|    approx_kl                | 0.01697872    |
|    approx_ln(kl)            | -4.0757947    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.985         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.6          |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0274        |
|    n_updates                | 1908          |
|    policy_gradient_loss     | -0.0066       |
|    std                      | 0.943         |
|    value_loss               | 0.0999        |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.42003506] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 44            |
|    time_elapsed             | 607           |
|    total_timesteps          | 3803136       |
| train/                      |               |
|    approx_kl                | 0.012787727   |
|    approx_ln(kl)            | -4.3592696    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.71         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0225       |
|    ln(policy_gradient_loss) | -5.21         |
|    loss                     | 0.978         |
|    n_updates                | 1909          |
|    policy_gradient_loss     | 0.00548       |
|    std                      | 0.941         |
|    value_loss               | 1.37          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4122554] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 45           |
|    time_elapsed             | 621          |
|    total_timesteps          | 3805184      |
| train/                      |              |
|    approx_kl                | 0.0058505223 |
|    approx_ln(kl)            | -5.1412244   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.71        |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.87        |
|    ln(policy_gradient_loss) | -5.59        |
|    loss                     | 0.0569       |
|    n_updates                | 1910         |
|    policy_gradient_loss     | 0.00373      |
|    std                      | 0.94         |
|    value_loss               | 0.077        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32298273] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 46            |
|    time_elapsed             | 635           |
|    total_timesteps          | 3807232       |
| train/                      |               |
|    approx_kl                | 0.008244159   |
|    approx_ln(kl)            | -4.79825      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.66         |
|    ln(policy_gradient_loss) | -7.03         |
|    loss                     | 0.0702        |
|    n_updates                | 1911          |
|    policy_gradient_loss     | 0.000883      |
|    std                      | 0.939         |
|    value_loss               | 0.383         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.50912374] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 47            |
|    time_elapsed             | 649           |
|    total_timesteps          | 3809280       |
| train/                      |               |
|    approx_kl                | 0.008241248   |
|    approx_ln(kl)            | -4.7986035    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.671        |
|    ln(policy_gradient_loss) | -12.4         |
|    loss                     | 0.511         |
|    n_updates                | 1912          |
|    policy_gradient_loss     | 4.16e-06      |
|    std                      | 0.938         |
|    value_loss               | 0.741         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2776116] |
| time/                       |              |
|    fps                      | 148          |
|    iterations               | 48           |
|    time_elapsed             | 662          |
|    total_timesteps          | 3811328      |
| train/                      |              |
|    approx_kl                | 0.006668102  |
|    approx_ln(kl)            | -5.01042     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.995        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.29        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.275        |
|    n_updates                | 1913         |
|    policy_gradient_loss     | -0.00407     |
|    std                      | 0.936        |
|    value_loss               | 0.537        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.25405142] |
| time/                       |               |
|    fps                      | 148           |
|    iterations               | 49            |
|    time_elapsed             | 675           |
|    total_timesteps          | 3813376       |
| train/                      |               |
|    approx_kl                | 0.007617861   |
|    approx_ln(kl)            | -4.8772597    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.27         |
|    ln(policy_gradient_loss) | -5.14         |
|    loss                     | 0.281         |
|    n_updates                | 1914          |
|    policy_gradient_loss     | 0.00585       |
|    std                      | 0.936         |
|    value_loss               | 0.548         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-------------------------------------
| reward             | [-0.2834806] |
| time/              |              |
|    fps             | 158          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 3815424      |
-------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24524985] |
| time/                       |               |
|    fps                      | 156           |
|    iterations               | 2             |
|    time_elapsed             | 26            |
|    total_timesteps          | 3817472       |
| train/                      |               |
|    approx_kl                | 0.0054250695  |
|    approx_ln(kl)            | -5.2167244    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.976         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.136        |
|    ln(policy_gradient_loss) | -5.13         |
|    loss                     | 0.873         |
|    n_updates                | 1916          |
|    policy_gradient_loss     | 0.00589       |
|    std                      | 0.936         |
|    value_loss               | 0.555         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27024564] |
| time/                       |               |
|    fps                      | 152           |
|    iterations               | 3             |
|    time_elapsed             | 40            |
|    total_timesteps          | 3819520       |
| train/                      |               |
|    approx_kl                | 0.0049766693  |
|    approx_ln(kl)            | -5.3029943    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.9          |
|    ln(policy_gradient_loss) | -6.37         |
|    loss                     | 0.15          |
|    n_updates                | 1917          |
|    policy_gradient_loss     | 0.00171       |
|    std                      | 0.936         |
|    value_loss               | 0.641         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2953457] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 4            |
|    time_elapsed             | 54           |
|    total_timesteps          | 3821568      |
| train/                      |              |
|    approx_kl                | 0.007272187  |
|    approx_ln(kl)            | -4.9236984   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.975        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.31        |
|    ln(policy_gradient_loss) | -4.24        |
|    loss                     | 0.271        |
|    n_updates                | 1918         |
|    policy_gradient_loss     | 0.0143       |
|    std                      | 0.936        |
|    value_loss               | 0.726        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22918844] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 5             |
|    time_elapsed             | 67            |
|    total_timesteps          | 3823616       |
| train/                      |               |
|    approx_kl                | 0.004783205   |
|    approx_ln(kl)            | -5.342644     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.39         |
|    ln(policy_gradient_loss) | -7.11         |
|    loss                     | 0.092         |
|    n_updates                | 1919          |
|    policy_gradient_loss     | 0.000818      |
|    std                      | 0.935         |
|    value_loss               | 0.153         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25246456] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 6             |
|    time_elapsed             | 81            |
|    total_timesteps          | 3825664       |
| train/                      |               |
|    approx_kl                | 0.0072854282  |
|    approx_ln(kl)            | -4.9218793    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.41         |
|    ln(policy_gradient_loss) | -5.37         |
|    loss                     | 0.0894        |
|    n_updates                | 1920          |
|    policy_gradient_loss     | 0.00466       |
|    std                      | 0.935         |
|    value_loss               | 0.167         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3260656] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 7            |
|    time_elapsed             | 95           |
|    total_timesteps          | 3827712      |
| train/                      |              |
|    approx_kl                | 0.008113159  |
|    approx_ln(kl)            | -4.814268    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.69        |
|    explained_variance       | 0.973        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.52        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.219        |
|    n_updates                | 1921         |
|    policy_gradient_loss     | -0.0042      |
|    std                      | 0.934        |
|    value_loss               | 1.84         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.05
----------------------------------------------
| reward                      | [-0.3267407] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 8            |
|    time_elapsed             | 108          |
|    total_timesteps          | 3829760      |
| train/                      |              |
|    approx_kl                | 0.016143132  |
|    approx_ln(kl)            | -4.1262608   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.69        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -3.61        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0269       |
|    n_updates                | 1922         |
|    policy_gradient_loss     | -0.0142      |
|    std                      | 0.935        |
|    value_loss               | 0.0772       |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.32708043] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 9             |
|    time_elapsed             | 122           |
|    total_timesteps          | 3831808       |
| train/                      |               |
|    approx_kl                | 0.010567859   |
|    approx_ln(kl)            | -4.549938     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.69         |
|    explained_variance       | 0.982         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0741       |
|    ln(policy_gradient_loss) | -6.26         |
|    loss                     | 0.929         |
|    n_updates                | 1923          |
|    policy_gradient_loss     | 0.00191       |
|    std                      | 0.935         |
|    value_loss               | 0.88          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.37695867] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 10            |
|    time_elapsed             | 135           |
|    total_timesteps          | 3833856       |
| train/                      |               |
|    approx_kl                | 0.01267507    |
|    approx_ln(kl)            | -4.3681183    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.99         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.137         |
|    n_updates                | 1924          |
|    policy_gradient_loss     | -0.0215       |
|    std                      | 0.935         |
|    value_loss               | 0.668         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.19845909] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 11            |
|    time_elapsed             | 148           |
|    total_timesteps          | 3835904       |
| train/                      |               |
|    approx_kl                | 0.007852602   |
|    approx_ln(kl)            | -4.8469105    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.87         |
|    ln(policy_gradient_loss) | -4.2          |
|    loss                     | 0.155         |
|    n_updates                | 1925          |
|    policy_gradient_loss     | 0.0149        |
|    std                      | 0.936         |
|    value_loss               | 0.278         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35470262] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 12            |
|    time_elapsed             | 162           |
|    total_timesteps          | 3837952       |
| train/                      |               |
|    approx_kl                | 0.0069917124  |
|    approx_ln(kl)            | -4.96303      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.184        |
|    ln(policy_gradient_loss) | -4.56         |
|    loss                     | 0.832         |
|    n_updates                | 1926          |
|    policy_gradient_loss     | 0.0105        |
|    std                      | 0.937         |
|    value_loss               | 0.905         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.36012006] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 13            |
|    time_elapsed             | 176           |
|    total_timesteps          | 3840000       |
| train/                      |               |
|    approx_kl                | 0.004605304   |
|    approx_ln(kl)            | -5.3805466    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.965         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.26          |
|    ln(policy_gradient_loss) | -5.17         |
|    loss                     | 1.3           |
|    n_updates                | 1927          |
|    policy_gradient_loss     | 0.00567       |
|    std                      | 0.938         |
|    value_loss               | 3.48          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2492988] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 14           |
|    time_elapsed             | 190          |
|    total_timesteps          | 3842048      |
| train/                      |              |
|    approx_kl                | 0.008099791  |
|    approx_ln(kl)            | -4.815917    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.986        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | 0.519        |
|    ln(policy_gradient_loss) | -6.4         |
|    loss                     | 1.68         |
|    n_updates                | 1928         |
|    policy_gradient_loss     | 0.00167      |
|    std                      | 0.938        |
|    value_loss               | 1.96         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.47394252] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 15            |
|    time_elapsed             | 203           |
|    total_timesteps          | 3844096       |
| train/                      |               |
|    approx_kl                | 0.007810352   |
|    approx_ln(kl)            | -4.8523054    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -5.21         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.00546       |
|    n_updates                | 1929          |
|    policy_gradient_loss     | -0.0142       |
|    std                      | 0.936         |
|    value_loss               | 0.199         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35309842] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 16            |
|    time_elapsed             | 216           |
|    total_timesteps          | 3846144       |
| train/                      |               |
|    approx_kl                | 0.008471801   |
|    approx_ln(kl)            | -4.7710123    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.75         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0642        |
|    n_updates                | 1930          |
|    policy_gradient_loss     | -0.000896     |
|    std                      | 0.935         |
|    value_loss               | 0.327         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.20946397] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 17            |
|    time_elapsed             | 231           |
|    total_timesteps          | 3848192       |
| train/                      |               |
|    approx_kl                | 0.0075348825  |
|    approx_ln(kl)            | -4.888212     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.69         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.95         |
|    ln(policy_gradient_loss) | -4.03         |
|    loss                     | 0.142         |
|    n_updates                | 1931          |
|    policy_gradient_loss     | 0.0177        |
|    std                      | 0.934         |
|    value_loss               | 0.559         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26713923] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 18            |
|    time_elapsed             | 244           |
|    total_timesteps          | 3850240       |
| train/                      |               |
|    approx_kl                | 0.008713765   |
|    approx_ln(kl)            | -4.7428513    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.69         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.08         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.341         |
|    n_updates                | 1932          |
|    policy_gradient_loss     | -0.00777      |
|    std                      | 0.934         |
|    value_loss               | 0.992         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.5237926] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 19           |
|    time_elapsed             | 258          |
|    total_timesteps          | 3852288      |
| train/                      |              |
|    approx_kl                | 0.009994994  |
|    approx_ln(kl)            | -4.605671    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.69        |
|    explained_variance       | 0.99         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.621       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.538        |
|    n_updates                | 1933         |
|    policy_gradient_loss     | -0.00385     |
|    std                      | 0.933        |
|    value_loss               | 0.886        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.20890245] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 20            |
|    time_elapsed             | 272           |
|    total_timesteps          | 3854336       |
| train/                      |               |
|    approx_kl                | 0.011516577   |
|    approx_ln(kl)            | -4.463968     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.69         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.417        |
|    ln(policy_gradient_loss) | -9.57         |
|    loss                     | 0.659         |
|    n_updates                | 1934          |
|    policy_gradient_loss     | 6.99e-05      |
|    std                      | 0.933         |
|    value_loss               | 0.87          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2440065] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 21           |
|    time_elapsed             | 285          |
|    total_timesteps          | 3856384      |
| train/                      |              |
|    approx_kl                | 0.0053303475 |
|    approx_ln(kl)            | -5.2343388   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.69        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.09        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.123        |
|    n_updates                | 1935         |
|    policy_gradient_loss     | -0.0073      |
|    std                      | 0.934        |
|    value_loss               | 0.602        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3795001] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 22           |
|    time_elapsed             | 299          |
|    total_timesteps          | 3858432      |
| train/                      |              |
|    approx_kl                | 0.008321549  |
|    approx_ln(kl)            | -4.788907    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.69        |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.309       |
|    ln(policy_gradient_loss) | -4.28        |
|    loss                     | 0.734        |
|    n_updates                | 1936         |
|    policy_gradient_loss     | 0.0139       |
|    std                      | 0.934        |
|    value_loss               | 0.942        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.12729949] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 23            |
|    time_elapsed             | 313           |
|    total_timesteps          | 3860480       |
| train/                      |               |
|    approx_kl                | 0.0051994673  |
|    approx_ln(kl)            | -5.259199     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.69         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.569        |
|    ln(policy_gradient_loss) | -8.53         |
|    loss                     | 0.566         |
|    n_updates                | 1937          |
|    policy_gradient_loss     | 0.000198      |
|    std                      | 0.935         |
|    value_loss               | 1.12          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32992834] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 24            |
|    time_elapsed             | 326           |
|    total_timesteps          | 3862528       |
| train/                      |               |
|    approx_kl                | 0.007997943   |
|    approx_ln(kl)            | -4.828571     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.425        |
|    ln(policy_gradient_loss) | -5.52         |
|    loss                     | 0.654         |
|    n_updates                | 1938          |
|    policy_gradient_loss     | 0.00403       |
|    std                      | 0.935         |
|    value_loss               | 1.05          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30646223] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 25            |
|    time_elapsed             | 340           |
|    total_timesteps          | 3864576       |
| train/                      |               |
|    approx_kl                | 0.007441795   |
|    approx_ln(kl)            | -4.9006433    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.79         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.168         |
|    n_updates                | 1939          |
|    policy_gradient_loss     | -0.00401      |
|    std                      | 0.935         |
|    value_loss               | 0.742         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.34609103] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 26            |
|    time_elapsed             | 354           |
|    total_timesteps          | 3866624       |
| train/                      |               |
|    approx_kl                | 0.0115148     |
|    approx_ln(kl)            | -4.4641223    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.27         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.104         |
|    n_updates                | 1940          |
|    policy_gradient_loss     | -0.01         |
|    std                      | 0.936         |
|    value_loss               | 0.247         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3835089] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 27           |
|    time_elapsed             | 368          |
|    total_timesteps          | 3868672      |
| train/                      |              |
|    approx_kl                | 0.0067026205 |
|    approx_ln(kl)            | -5.0052567   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.991        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.39        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.0913       |
|    n_updates                | 1941         |
|    policy_gradient_loss     | -0.0033      |
|    std                      | 0.936        |
|    value_loss               | 0.335        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.1848908] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 28           |
|    time_elapsed             | 382          |
|    total_timesteps          | 3870720      |
| train/                      |              |
|    approx_kl                | 0.010146339  |
|    approx_ln(kl)            | -4.5906425   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.991        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.346       |
|    ln(policy_gradient_loss) | -5.26        |
|    loss                     | 0.707        |
|    n_updates                | 1942         |
|    policy_gradient_loss     | 0.00518      |
|    std                      | 0.936        |
|    value_loss               | 1.11         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.16360962] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 29            |
|    time_elapsed             | 395           |
|    total_timesteps          | 3872768       |
| train/                      |               |
|    approx_kl                | 0.008862918   |
|    approx_ln(kl)            | -4.725879     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.28         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.277         |
|    n_updates                | 1943          |
|    policy_gradient_loss     | -0.00283      |
|    std                      | 0.936         |
|    value_loss               | 1.37          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.30772346] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 30            |
|    time_elapsed             | 409           |
|    total_timesteps          | 3874816       |
| train/                      |               |
|    approx_kl                | 0.009957523   |
|    approx_ln(kl)            | -4.609427     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.368        |
|    ln(policy_gradient_loss) | -7.28         |
|    loss                     | 0.692         |
|    n_updates                | 1944          |
|    policy_gradient_loss     | 0.000692      |
|    std                      | 0.936         |
|    value_loss               | 1.46          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------------
| reward                      | [-0.22543705] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 31            |
|    time_elapsed             | 424           |
|    total_timesteps          | 3876864       |
| train/                      |               |
|    approx_kl                | 0.0125942305  |
|    approx_ln(kl)            | -4.3745165    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.54         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0791        |
|    n_updates                | 1945          |
|    policy_gradient_loss     | -0.0106       |
|    std                      | 0.936         |
|    value_loss               | 0.469         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.39472455] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 32            |
|    time_elapsed             | 439           |
|    total_timesteps          | 3878912       |
| train/                      |               |
|    approx_kl                | 0.00820649    |
|    approx_ln(kl)            | -4.8028297    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.32         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.267         |
|    n_updates                | 1946          |
|    policy_gradient_loss     | -0.0059       |
|    std                      | 0.936         |
|    value_loss               | 0.47          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30610022] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 33            |
|    time_elapsed             | 453           |
|    total_timesteps          | 3880960       |
| train/                      |               |
|    approx_kl                | 0.008633403   |
|    approx_ln(kl)            | -4.7521167    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.14         |
|    ln(policy_gradient_loss) | -5.29         |
|    loss                     | 0.118         |
|    n_updates                | 1947          |
|    policy_gradient_loss     | 0.00506       |
|    std                      | 0.936         |
|    value_loss               | 0.289         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3863788] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 34           |
|    time_elapsed             | 466          |
|    total_timesteps          | 3883008      |
| train/                      |              |
|    approx_kl                | 0.0066819014 |
|    approx_ln(kl)            | -5.0083528   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.7         |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.05        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.129        |
|    n_updates                | 1948         |
|    policy_gradient_loss     | -0.00338     |
|    std                      | 0.936        |
|    value_loss               | 0.183        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32036814] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 35            |
|    time_elapsed             | 479           |
|    total_timesteps          | 3885056       |
| train/                      |               |
|    approx_kl                | 0.006525921   |
|    approx_ln(kl)            | -5.0319734    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | 0.977         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.52         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.218         |
|    n_updates                | 1949          |
|    policy_gradient_loss     | -0.00722      |
|    std                      | 0.936         |
|    value_loss               | 0.574         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2215691] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 36           |
|    time_elapsed             | 492          |
|    total_timesteps          | 3887104      |
| train/                      |              |
|    approx_kl                | 0.007281631  |
|    approx_ln(kl)            | -4.9224005   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.69        |
|    explained_variance       | 0.972        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.641       |
|    ln(policy_gradient_loss) | -5.69        |
|    loss                     | 0.527        |
|    n_updates                | 1950         |
|    policy_gradient_loss     | 0.00336      |
|    std                      | 0.935        |
|    value_loss               | 1.78         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3492343] |
| time/                       |              |
|    fps                      | 149          |
|    iterations               | 37           |
|    time_elapsed             | 505          |
|    total_timesteps          | 3889152      |
| train/                      |              |
|    approx_kl                | 0.009043319  |
|    approx_ln(kl)            | -4.705729    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.69        |
|    explained_variance       | 0.978        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.713       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.49         |
|    n_updates                | 1951         |
|    policy_gradient_loss     | -0.00384     |
|    std                      | 0.934        |
|    value_loss               | 0.964        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40191424] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 38            |
|    time_elapsed             | 518           |
|    total_timesteps          | 3891200       |
| train/                      |               |
|    approx_kl                | 0.007902753   |
|    approx_ln(kl)            | -4.840544     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.69         |
|    explained_variance       | 0.964         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.18         |
|    ln(policy_gradient_loss) | -6.53         |
|    loss                     | 0.308         |
|    n_updates                | 1952          |
|    policy_gradient_loss     | 0.00146       |
|    std                      | 0.932         |
|    value_loss               | 1.18          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.42032552] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 39            |
|    time_elapsed             | 531           |
|    total_timesteps          | 3893248       |
| train/                      |               |
|    approx_kl                | 0.006740817   |
|    approx_ln(kl)            | -4.999574     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.69         |
|    explained_variance       | 0.956         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.763        |
|    ln(policy_gradient_loss) | -6.23         |
|    loss                     | 0.466         |
|    n_updates                | 1953          |
|    policy_gradient_loss     | 0.00198       |
|    std                      | 0.932         |
|    value_loss               | 0.797         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.38201582] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 40            |
|    time_elapsed             | 545           |
|    total_timesteps          | 3895296       |
| train/                      |               |
|    approx_kl                | 0.0075579644  |
|    approx_ln(kl)            | -4.8851533    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.68         |
|    explained_variance       | 0.953         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0171       |
|    ln(policy_gradient_loss) | -5.68         |
|    loss                     | 0.983         |
|    n_updates                | 1954          |
|    policy_gradient_loss     | 0.00341       |
|    std                      | 0.931         |
|    value_loss               | 1.51          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43269402] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 41            |
|    time_elapsed             | 558           |
|    total_timesteps          | 3897344       |
| train/                      |               |
|    approx_kl                | 0.0060469806  |
|    approx_ln(kl)            | -5.1081963    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.68         |
|    explained_variance       | 0.961         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.468        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.626         |
|    n_updates                | 1955          |
|    policy_gradient_loss     | -0.00518      |
|    std                      | 0.931         |
|    value_loss               | 2.68          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.31331202] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 42            |
|    time_elapsed             | 571           |
|    total_timesteps          | 3899392       |
| train/                      |               |
|    approx_kl                | 0.004950864   |
|    approx_ln(kl)            | -5.308193     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.68         |
|    explained_variance       | 0.972         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0766       |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.926         |
|    n_updates                | 1956          |
|    policy_gradient_loss     | -0.00199      |
|    std                      | 0.931         |
|    value_loss               | 1.8           |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.42872596] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 43            |
|    time_elapsed             | 585           |
|    total_timesteps          | 3901440       |
| train/                      |               |
|    approx_kl                | 0.00748775    |
|    approx_ln(kl)            | -4.894487     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.68         |
|    explained_variance       | 0.964         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.59         |
|    ln(policy_gradient_loss) | -6.39         |
|    loss                     | 0.205         |
|    n_updates                | 1957          |
|    policy_gradient_loss     | 0.00168       |
|    std                      | 0.931         |
|    value_loss               | 0.519         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2714704] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 44           |
|    time_elapsed             | 598          |
|    total_timesteps          | 3903488      |
| train/                      |              |
|    approx_kl                | 0.009419132  |
|    approx_ln(kl)            | -4.6650124   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.68        |
|    explained_variance       | 0.978        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.02        |
|    ln(policy_gradient_loss) | -6.19        |
|    loss                     | 0.36         |
|    n_updates                | 1958         |
|    policy_gradient_loss     | 0.00205      |
|    std                      | 0.93         |
|    value_loss               | 2.26         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.3487382] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 45           |
|    time_elapsed             | 612          |
|    total_timesteps          | 3905536      |
| train/                      |              |
|    approx_kl                | 0.008094907  |
|    approx_ln(kl)            | -4.81652     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.68        |
|    explained_variance       | 0.986        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.239       |
|    ln(policy_gradient_loss) | -4.88        |
|    loss                     | 0.787        |
|    n_updates                | 1959         |
|    policy_gradient_loss     | 0.00762      |
|    std                      | 0.93         |
|    value_loss               | 2.38         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-32.885193] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 46           |
|    time_elapsed             | 625          |
|    total_timesteps          | 3907584      |
| train/                      |              |
|    approx_kl                | 0.007011305  |
|    approx_ln(kl)            | -4.9602313   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.68        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.12        |
|    ln(policy_gradient_loss) | -5.29        |
|    loss                     | 0.327        |
|    n_updates                | 1960         |
|    policy_gradient_loss     | 0.00502      |
|    std                      | 0.929        |
|    value_loss               | 0.625        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
----------------------------------------------
| reward                      | [-0.2633992] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 47           |
|    time_elapsed             | 638          |
|    total_timesteps          | 3909632      |
| train/                      |              |
|    approx_kl                | 0.007652799  |
|    approx_ln(kl)            | -4.872684    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.68        |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -2.22        |
|    ln(policy_gradient_loss) | -6.11        |
|    loss                     | 0.108        |
|    n_updates                | 1961         |
|    policy_gradient_loss     | 0.00222      |
|    std                      | 0.927        |
|    value_loss               | 0.285        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2912045] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 48           |
|    time_elapsed             | 652          |
|    total_timesteps          | 3911680      |
| train/                      |              |
|    approx_kl                | 0.007507326  |
|    approx_ln(kl)            | -4.8918757   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.67        |
|    explained_variance       | 0.987        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.31        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.27         |
|    n_updates                | 1962         |
|    policy_gradient_loss     | -0.00214     |
|    std                      | 0.927        |
|    value_loss               | 1.01         |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33970276] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 49            |
|    time_elapsed             | 666           |
|    total_timesteps          | 3913728       |
| train/                      |               |
|    approx_kl                | 0.008071006   |
|    approx_ln(kl)            | -4.819477     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.67         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.98         |
|    ln(policy_gradient_loss) | -5.95         |
|    loss                     | 0.0507        |
|    n_updates                | 1963          |
|    policy_gradient_loss     | 0.00261       |
|    std                      | 0.926         |
|    value_loss               | 0.174         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-------------------------------------
| reward             | [-0.2757685] |
| time/              |              |
|    fps             | 148          |
|    iterations      | 1            |
|    time_elapsed    | 13           |
|    total_timesteps | 3915776      |
-------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.40538844] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 2             |
|    time_elapsed             | 27            |
|    total_timesteps          | 3917824       |
| train/                      |               |
|    approx_kl                | 0.0052672513  |
|    approx_ln(kl)            | -5.246247     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.67         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.19         |
|    ln(policy_gradient_loss) | -8.17         |
|    loss                     | 0.303         |
|    n_updates                | 1965          |
|    policy_gradient_loss     | 0.000282      |
|    std                      | 0.925         |
|    value_loss               | 0.603         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37104347] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 3             |
|    time_elapsed             | 40            |
|    total_timesteps          | 3919872       |
| train/                      |               |
|    approx_kl                | 0.0061150035  |
|    approx_ln(kl)            | -5.09701      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.67         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.16         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.312         |
|    n_updates                | 1966          |
|    policy_gradient_loss     | -0.00885      |
|    std                      | 0.925         |
|    value_loss               | 1.01          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4176543] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 4            |
|    time_elapsed             | 54           |
|    total_timesteps          | 3921920      |
| train/                      |              |
|    approx_kl                | 0.009670921  |
|    approx_ln(kl)            | -4.638632    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.67        |
|    explained_variance       | 0.994        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.726       |
|    ln(policy_gradient_loss) | -5.2         |
|    loss                     | 0.484        |
|    n_updates                | 1967         |
|    policy_gradient_loss     | 0.00551      |
|    std                      | 0.924        |
|    value_loss               | 1.15         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27868664] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 5             |
|    time_elapsed             | 67            |
|    total_timesteps          | 3923968       |
| train/                      |               |
|    approx_kl                | 0.008074222   |
|    approx_ln(kl)            | -4.819079     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.67         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.33         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0969        |
|    n_updates                | 1968          |
|    policy_gradient_loss     | -0.0157       |
|    std                      | 0.924         |
|    value_loss               | 0.787         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33275717] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 6             |
|    time_elapsed             | 81            |
|    total_timesteps          | 3926016       |
| train/                      |               |
|    approx_kl                | 0.006179456   |
|    approx_ln(kl)            | -5.086525     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.67         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.25         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.105         |
|    n_updates                | 1969          |
|    policy_gradient_loss     | -0.000534     |
|    std                      | 0.924         |
|    value_loss               | 0.47          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33110008] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 7             |
|    time_elapsed             | 95            |
|    total_timesteps          | 3928064       |
| train/                      |               |
|    approx_kl                | 0.0078026303  |
|    approx_ln(kl)            | -4.8532944    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.67         |
|    explained_variance       | 0.98          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.255         |
|    ln(policy_gradient_loss) | -4.46         |
|    loss                     | 1.29          |
|    n_updates                | 1970          |
|    policy_gradient_loss     | 0.0116        |
|    std                      | 0.923         |
|    value_loss               | 1.57          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3707674] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 8            |
|    time_elapsed             | 109          |
|    total_timesteps          | 3930112      |
| train/                      |              |
|    approx_kl                | 0.006467024  |
|    approx_ln(kl)            | -5.0410395   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.67        |
|    explained_variance       | 0.992        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.627       |
|    ln(policy_gradient_loss) | -6.56        |
|    loss                     | 0.534        |
|    n_updates                | 1971         |
|    policy_gradient_loss     | 0.00141      |
|    std                      | 0.923        |
|    value_loss               | 0.681        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3701275] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 9            |
|    time_elapsed             | 122          |
|    total_timesteps          | 3932160      |
| train/                      |              |
|    approx_kl                | 0.008489413  |
|    approx_ln(kl)            | -4.768935    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.66        |
|    explained_variance       | 0.987        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.33        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.719        |
|    n_updates                | 1972         |
|    policy_gradient_loss     | -0.00694     |
|    std                      | 0.922        |
|    value_loss               | 2.14         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.22135153] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 10            |
|    time_elapsed             | 136           |
|    total_timesteps          | 3934208       |
| train/                      |               |
|    approx_kl                | 0.0062089725  |
|    approx_ln(kl)            | -5.08176      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.66         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.39         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0913        |
|    n_updates                | 1973          |
|    policy_gradient_loss     | -0.00183      |
|    std                      | 0.921         |
|    value_loss               | 0.48          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.24727464] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 11            |
|    time_elapsed             | 149           |
|    total_timesteps          | 3936256       |
| train/                      |               |
|    approx_kl                | 0.010145139   |
|    approx_ln(kl)            | -4.5907607    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.66         |
|    explained_variance       | 0.998         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.33         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0974        |
|    n_updates                | 1974          |
|    policy_gradient_loss     | -0.00606      |
|    std                      | 0.921         |
|    value_loss               | 0.391         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.18663464] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 12            |
|    time_elapsed             | 163           |
|    total_timesteps          | 3938304       |
| train/                      |               |
|    approx_kl                | 0.006852693   |
|    approx_ln(kl)            | -4.983114     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.66         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.51         |
|    ln(policy_gradient_loss) | -5.02         |
|    loss                     | 0.222         |
|    n_updates                | 1975          |
|    policy_gradient_loss     | 0.00663       |
|    std                      | 0.922         |
|    value_loss               | 0.402         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.393948]  |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 13           |
|    time_elapsed             | 176          |
|    total_timesteps          | 3940352      |
| train/                      |              |
|    approx_kl                | 0.0065338654 |
|    approx_ln(kl)            | -5.0307565   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.66        |
|    explained_variance       | 0.989        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.634       |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.531        |
|    n_updates                | 1976         |
|    policy_gradient_loss     | -0.011       |
|    std                      | 0.921        |
|    value_loss               | 0.557        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.19362876] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 14            |
|    time_elapsed             | 190           |
|    total_timesteps          | 3942400       |
| train/                      |               |
|    approx_kl                | 0.0072852043  |
|    approx_ln(kl)            | -4.92191      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.66         |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.256        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.774         |
|    n_updates                | 1977          |
|    policy_gradient_loss     | -0.00166      |
|    std                      | 0.919         |
|    value_loss               | 0.733         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26951113] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 15            |
|    time_elapsed             | 204           |
|    total_timesteps          | 3944448       |
| train/                      |               |
|    approx_kl                | 0.005354523   |
|    approx_ln(kl)            | -5.2298136    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.66         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.24         |
|    ln(policy_gradient_loss) | -5.22         |
|    loss                     | 0.107         |
|    n_updates                | 1978          |
|    policy_gradient_loss     | 0.00542       |
|    std                      | 0.919         |
|    value_loss               | 0.307         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.38652182] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 16            |
|    time_elapsed             | 217           |
|    total_timesteps          | 3946496       |
| train/                      |               |
|    approx_kl                | 0.00601222    |
|    approx_ln(kl)            | -5.113961     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.66         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.71         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.492         |
|    n_updates                | 1979          |
|    policy_gradient_loss     | -0.00192      |
|    std                      | 0.918         |
|    value_loss               | 0.777         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.33611125] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 17            |
|    time_elapsed             | 230           |
|    total_timesteps          | 3948544       |
| train/                      |               |
|    approx_kl                | 0.00916225    |
|    approx_ln(kl)            | -4.6926637    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.31         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.0996        |
|    n_updates                | 1980          |
|    policy_gradient_loss     | -0.00408      |
|    std                      | 0.918         |
|    value_loss               | 0.936         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24362198] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 18            |
|    time_elapsed             | 244           |
|    total_timesteps          | 3950592       |
| train/                      |               |
|    approx_kl                | 0.0056862845  |
|    approx_ln(kl)            | -5.169698     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.994         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.54         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.215         |
|    n_updates                | 1981          |
|    policy_gradient_loss     | -0.0027       |
|    std                      | 0.918         |
|    value_loss               | 0.65          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.19099517] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 19            |
|    time_elapsed             | 257           |
|    total_timesteps          | 3952640       |
| train/                      |               |
|    approx_kl                | 0.0069787214  |
|    approx_ln(kl)            | -4.9648895    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.24         |
|    ln(policy_gradient_loss) | -5.41         |
|    loss                     | 0.107         |
|    n_updates                | 1982          |
|    policy_gradient_loss     | 0.00447       |
|    std                      | 0.917         |
|    value_loss               | 0.169         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25483522] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 20            |
|    time_elapsed             | 271           |
|    total_timesteps          | 3954688       |
| train/                      |               |
|    approx_kl                | 0.006944635   |
|    approx_ln(kl)            | -4.9697857    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.996         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.97         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.14          |
|    n_updates                | 1983          |
|    policy_gradient_loss     | -0.0053       |
|    std                      | 0.918         |
|    value_loss               | 0.237         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.43967423] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 21            |
|    time_elapsed             | 286           |
|    total_timesteps          | 3956736       |
| train/                      |               |
|    approx_kl                | 0.0071036243  |
|    approx_ln(kl)            | -4.94715      |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -3.04         |
|    ln(policy_gradient_loss) | -6.55         |
|    loss                     | 0.0481        |
|    n_updates                | 1984          |
|    policy_gradient_loss     | 0.00144       |
|    std                      | 0.918         |
|    value_loss               | 0.471         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.4850156] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 22           |
|    time_elapsed             | 300          |
|    total_timesteps          | 3958784      |
| train/                      |              |
|    approx_kl                | 0.0060575935 |
|    approx_ln(kl)            | -5.1064425   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.65        |
|    explained_variance       | 0.993        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.74        |
|    ln(policy_gradient_loss) | -6.23        |
|    loss                     | 0.477        |
|    n_updates                | 1985         |
|    policy_gradient_loss     | 0.00198      |
|    std                      | 0.918        |
|    value_loss               | 0.569        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3275526] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 23           |
|    time_elapsed             | 313          |
|    total_timesteps          | 3960832      |
| train/                      |              |
|    approx_kl                | 0.005779082  |
|    approx_ln(kl)            | -5.1535106   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.65        |
|    explained_variance       | 0.989        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.06        |
|    ln(policy_gradient_loss) | -4.72        |
|    loss                     | 0.348        |
|    n_updates                | 1986         |
|    policy_gradient_loss     | 0.00888      |
|    std                      | 0.918        |
|    value_loss               | 0.757        |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.19241504] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 24            |
|    time_elapsed             | 327           |
|    total_timesteps          | 3962880       |
| train/                      |               |
|    approx_kl                | 0.004336169   |
|    approx_ln(kl)            | -5.440764     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.995         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.09         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.124         |
|    n_updates                | 1987          |
|    policy_gradient_loss     | -0.00471      |
|    std                      | 0.918         |
|    value_loss               | 0.244         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32160586] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 25            |
|    time_elapsed             | 340           |
|    total_timesteps          | 3964928       |
| train/                      |               |
|    approx_kl                | 0.00831922    |
|    approx_ln(kl)            | -4.789187     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.44         |
|    ln(policy_gradient_loss) | -4.57         |
|    loss                     | 0.0872        |
|    n_updates                | 1988          |
|    policy_gradient_loss     | 0.0103        |
|    std                      | 0.917         |
|    value_loss               | 0.351         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.26894552] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 26            |
|    time_elapsed             | 353           |
|    total_timesteps          | 3966976       |
| train/                      |               |
|    approx_kl                | 0.006564715   |
|    approx_ln(kl)            | -5.0260463    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.2          |
|    ln(policy_gradient_loss) | -5.12         |
|    loss                     | 0.3           |
|    n_updates                | 1989          |
|    policy_gradient_loss     | 0.00598       |
|    std                      | 0.916         |
|    value_loss               | 3.16          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.32972777] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 27            |
|    time_elapsed             | 367           |
|    total_timesteps          | 3969024       |
| train/                      |               |
|    approx_kl                | 0.008110607   |
|    approx_ln(kl)            | -4.8145823    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.986         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.12         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.327         |
|    n_updates                | 1990          |
|    policy_gradient_loss     | -0.00547      |
|    std                      | 0.915         |
|    value_loss               | 0.728         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------------
| reward                      | [-0.2515024] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 28           |
|    time_elapsed             | 380          |
|    total_timesteps          | 3971072      |
| train/                      |              |
|    approx_kl                | 0.009155032  |
|    approx_ln(kl)            | -4.6934514   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.65        |
|    explained_variance       | 0.989        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.52        |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.219        |
|    n_updates                | 1991         |
|    policy_gradient_loss     | -0.00373     |
|    std                      | 0.915        |
|    value_loss               | 0.607        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.29924473] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 29            |
|    time_elapsed             | 393           |
|    total_timesteps          | 3973120       |
| train/                      |               |
|    approx_kl                | 0.008756513   |
|    approx_ln(kl)            | -4.7379575    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.984         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.105        |
|    ln(policy_gradient_loss) | -6.02         |
|    loss                     | 0.901         |
|    n_updates                | 1992          |
|    policy_gradient_loss     | 0.00244       |
|    std                      | 0.914         |
|    value_loss               | 1.37          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32315624] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 30            |
|    time_elapsed             | 407           |
|    total_timesteps          | 3975168       |
| train/                      |               |
|    approx_kl                | 0.0061886185  |
|    approx_ln(kl)            | -5.0850434    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.0825        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.09          |
|    n_updates                | 1993          |
|    policy_gradient_loss     | -0.000868     |
|    std                      | 0.915         |
|    value_loss               | 1.56          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.38301665] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 31            |
|    time_elapsed             | 420           |
|    total_timesteps          | 3977216       |
| train/                      |               |
|    approx_kl                | 0.007264664   |
|    approx_ln(kl)            | -4.924733     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.996        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.369         |
|    n_updates                | 1994          |
|    policy_gradient_loss     | -0.00227      |
|    std                      | 0.915         |
|    value_loss               | 0.818         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.35315725] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 32            |
|    time_elapsed             | 433           |
|    total_timesteps          | 3979264       |
| train/                      |               |
|    approx_kl                | 0.0073979124  |
|    approx_ln(kl)            | -4.9065576    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.978         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.348         |
|    ln(policy_gradient_loss) | -5.7          |
|    loss                     | 1.42          |
|    n_updates                | 1995          |
|    policy_gradient_loss     | 0.00335       |
|    std                      | 0.915         |
|    value_loss               | 1.47          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.3288864] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 33           |
|    time_elapsed             | 447          |
|    total_timesteps          | 3981312      |
| train/                      |              |
|    approx_kl                | 0.006231167  |
|    approx_ln(kl)            | -5.0781918   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.65        |
|    explained_variance       | 0.987        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -1.7         |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | 0.182        |
|    n_updates                | 1996         |
|    policy_gradient_loss     | -0.00556     |
|    std                      | 0.915        |
|    value_loss               | 0.476        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.19525385] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 34            |
|    time_elapsed             | 460           |
|    total_timesteps          | 3983360       |
| train/                      |               |
|    approx_kl                | 0.0076701283  |
|    approx_ln(kl)            | -4.870422     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.979         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.0627       |
|    ln(policy_gradient_loss) | -4.34         |
|    loss                     | 0.939         |
|    n_updates                | 1997          |
|    policy_gradient_loss     | 0.013         |
|    std                      | 0.915         |
|    value_loss               | 1.65          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.34241438] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 35            |
|    time_elapsed             | 473           |
|    total_timesteps          | 3985408       |
| train/                      |               |
|    approx_kl                | 0.007277131   |
|    approx_ln(kl)            | -4.9230185    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.442         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.56          |
|    n_updates                | 1998          |
|    policy_gradient_loss     | -0.00171      |
|    std                      | 0.916         |
|    value_loss               | 1.99          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.23736767] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 36            |
|    time_elapsed             | 487           |
|    total_timesteps          | 3987456       |
| train/                      |               |
|    approx_kl                | 0.0059790798  |
|    approx_ln(kl)            | -5.1194887    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.981         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.587        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.556         |
|    n_updates                | 1999          |
|    policy_gradient_loss     | -0.00235      |
|    std                      | 0.916         |
|    value_loss               | 1.34          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.25744542] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 37            |
|    time_elapsed             | 500           |
|    total_timesteps          | 3989504       |
| train/                      |               |
|    approx_kl                | 0.006515009   |
|    approx_ln(kl)            | -5.0336466    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.975         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.353         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 1.42          |
|    n_updates                | 2000          |
|    policy_gradient_loss     | -0.00618      |
|    std                      | 0.915         |
|    value_loss               | 2.36          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.27596083] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 38            |
|    time_elapsed             | 513           |
|    total_timesteps          | 3991552       |
| train/                      |               |
|    approx_kl                | 0.007319277   |
|    approx_ln(kl)            | -4.917244     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.14         |
|    ln(policy_gradient_loss) | -5.26         |
|    loss                     | 0.321         |
|    n_updates                | 2001          |
|    policy_gradient_loss     | 0.0052        |
|    std                      | 0.915         |
|    value_loss               | 1.62          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32631198] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 39            |
|    time_elapsed             | 527           |
|    total_timesteps          | 3993600       |
| train/                      |               |
|    approx_kl                | 0.008034401   |
|    approx_ln(kl)            | -4.824023     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.964         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | 0.574         |
|    ln(policy_gradient_loss) | -4.64         |
|    loss                     | 1.78          |
|    n_updates                | 2002          |
|    policy_gradient_loss     | 0.00964       |
|    std                      | 0.914         |
|    value_loss               | 2.37          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.30679446] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 40            |
|    time_elapsed             | 540           |
|    total_timesteps          | 3995648       |
| train/                      |               |
|    approx_kl                | 0.006142373   |
|    approx_ln(kl)            | -5.092544     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.966         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.76         |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.173         |
|    n_updates                | 2003          |
|    policy_gradient_loss     | -0.00348      |
|    std                      | 0.914         |
|    value_loss               | 1.03          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.23826516] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 41            |
|    time_elapsed             | 554           |
|    total_timesteps          | 3997696       |
| train/                      |               |
|    approx_kl                | 0.00911666    |
|    approx_ln(kl)            | -4.697652     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.988         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.5          |
|    ln(policy_gradient_loss) | -5.18         |
|    loss                     | 0.224         |
|    n_updates                | 2004          |
|    policy_gradient_loss     | 0.00563       |
|    std                      | 0.914         |
|    value_loss               | 0.768         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------------
| reward                      | [-0.2680917] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 42           |
|    time_elapsed             | 567          |
|    total_timesteps          | 3999744      |
| train/                      |              |
|    approx_kl                | 0.0082326    |
|    approx_ln(kl)            | -4.7996535   |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.64        |
|    explained_variance       | 0.982        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | -0.85        |
|    ln(policy_gradient_loss) | -5.9         |
|    loss                     | 0.428        |
|    n_updates                | 2005         |
|    policy_gradient_loss     | 0.00273      |
|    std                      | 0.913        |
|    value_loss               | 0.521        |
----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
---------------------------------------------
| reward                      | [-0.275849] |
| time/                       |             |
|    fps                      | 151         |
|    iterations               | 43          |
|    time_elapsed             | 582         |
|    total_timesteps          | 4001792     |
| train/                      |             |
|    approx_kl                | 0.008391229 |
|    approx_ln(kl)            | -4.780568   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.64       |
|    explained_variance       | 0.984       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | -1.08       |
|    ln(policy_gradient_loss) | -5.2        |
|    loss                     | 0.341       |
|    n_updates                | 2006        |
|    policy_gradient_loss     | 0.00551     |
|    std                      | 0.914       |
|    value_loss               | 0.971       |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.24780226] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 44            |
|    time_elapsed             | 599           |
|    total_timesteps          | 4003840       |
| train/                      |               |
|    approx_kl                | 0.0064561744  |
|    approx_ln(kl)            | -5.0427184    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.318        |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | 0.728         |
|    n_updates                | 2007          |
|    policy_gradient_loss     | -0.00169      |
|    std                      | 0.915         |
|    value_loss               | 1.24          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------------
| reward                      | [-0.43725443] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 45            |
|    time_elapsed             | 613           |
|    total_timesteps          | 4005888       |
| train/                      |               |
|    approx_kl                | 0.009151344   |
|    approx_ln(kl)            | -4.6938543    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.993         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -0.847        |
|    ln(policy_gradient_loss) | -5.97         |
|    loss                     | 0.429         |
|    n_updates                | 2008          |
|    policy_gradient_loss     | 0.00254       |
|    std                      | 0.915         |
|    value_loss               | 1.02          |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.20840068] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 46            |
|    time_elapsed             | 626           |
|    total_timesteps          | 4007936       |
| train/                      |               |
|    approx_kl                | 0.0055092843  |
|    approx_ln(kl)            | -5.2013206    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.987         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.09         |
|    ln(policy_gradient_loss) | -5.35         |
|    loss                     | 0.338         |
|    n_updates                | 2009          |
|    policy_gradient_loss     | 0.00475       |
|    std                      | 0.915         |
|    value_loss               | 0.421         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.41771358] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 47            |
|    time_elapsed             | 640           |
|    total_timesteps          | 4009984       |
| train/                      |               |
|    approx_kl                | 0.0065536276  |
|    approx_ln(kl)            | -5.0277367    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.65         |
|    explained_variance       | 0.989         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.35         |
|    ln(policy_gradient_loss) | -7.54         |
|    loss                     | 0.261         |
|    n_updates                | 2010          |
|    policy_gradient_loss     | 0.000533      |
|    std                      | 0.915         |
|    value_loss               | 0.885         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.37838823] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 48            |
|    time_elapsed             | 656           |
|    total_timesteps          | 4012032       |
| train/                      |               |
|    approx_kl                | 0.008600806   |
|    approx_ln(kl)            | -4.7558994    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.64         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -1.18         |
|    ln(policy_gradient_loss) | -6.36         |
|    loss                     | 0.306         |
|    n_updates                | 2011          |
|    policy_gradient_loss     | 0.00173       |
|    std                      | 0.91          |
|    value_loss               | 0.707         |
-----------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------------
| reward                      | [-0.32806027] |
| time/                       |               |
|    fps                      | 149           |
|    iterations               | 49            |
|    time_elapsed             | 669           |
|    total_timesteps          | 4014080       |
| train/                      |               |
|    approx_kl                | 0.0047467523  |
|    approx_ln(kl)            | -5.3502946    |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.63         |
|    explained_variance       | 0.991         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | -2.23         |
|    ln(policy_gradient_loss) | -7.39         |
|    loss                     | 0.107         |
|    n_updates                | 2012          |
|    policy_gradient_loss     | 0.000614      |
|    std                      | 0.908         |
|    value_loss               | 0.404         |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:286: RuntimeWarning: invalid value encountered in log
  self.logger.record("train/ln(policy_gradient_loss)", np.log(np.mean(pg_losses)))
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
Early stopping at step 0 due to reaching max kl: 0.04
Final reward: -0.5499500632286072
Final reward: -0.5507607460021973
Final reward: -0.5501314997673035
Final reward: -0.5488815903663635
Final reward: -0.5475829243659973
Final reward: -0.5462930798530579
Final reward: -0.5445482730865479
Final reward: -0.5419527888298035
Final reward: -0.5384818315505981
Final reward: -0.5317128896713257
Final reward: -0.5262051224708557
Final reward: -0.5211533308029175
Final reward: -0.5139166116714478
Final reward: -0.5078539252281189
Final reward: -0.5003527402877808
Final reward: -0.4941984713077545
Final reward: -0.48625171184539795
Final reward: -0.4806714355945587
Final reward: -0.47361767292022705
Final reward: -0.47406241297721863
Final reward: -0.47473859786987305
Final reward: -0.47523248195648193
Final reward: -0.47576794028282166
Final reward: -0.4760129451751709
Final reward: -0.4759916663169861
Final reward: -0.47595757246017456
Final reward: -0.4760053753852844
Final reward: -0.476065456867218
Final reward: -0.4760240316390991
Final reward: -0.4756920635700226
Final reward: -0.47560933232307434
Final reward: -0.48025158047676086
Final reward: -0.4695854187011719
Final reward: -0.4710712134838104
Final reward: -0.48136577010154724
Final reward: -0.4706921875476837
Final reward: -0.4862108528614044
Final reward: -0.47355711460113525
Final reward: -0.48964938521385193
Final reward: -0.47865498065948486
Final reward: -0.4883936941623688
Final reward: -0.4794626832008362
Final reward: -0.49043524265289307
Final reward: -0.48446404933929443
Final reward: -0.4911445081233978
Final reward: -0.49539366364479065
Final reward: -0.497526079416275
Final reward: -0.49773678183555603
Final reward: -0.4969663619995117
Final reward: -0.49609965085983276
Final reward: -0.49333545565605164
Final reward: -0.4893314242362976
Final reward: -0.4846402704715729
Final reward: -0.4810539484024048
Final reward: -0.4780615270137787
Final reward: -0.47376593947410583
Final reward: -0.4668843150138855
Final reward: -0.46089082956314087
Final reward: -0.45193761587142944
Final reward: -0.44545501470565796
Final reward: -0.4385508596897125
Final reward: -0.4312138557434082
Final reward: -0.4256829023361206
Final reward: -0.4214399755001068
Final reward: -0.4224180579185486
Final reward: -0.4225277900695801
Final reward: -0.4222830832004547
Final reward: -0.4219915270805359
Final reward: -0.4217386543750763
Final reward: -0.42176052927970886
Final reward: -0.42200326919555664
Final reward: -0.42228636145591736
Final reward: -0.42255160212516785
Final reward: -0.4226701557636261
Final reward: -0.42256683111190796
Final reward: -0.4220734238624573
Final reward: -0.4239627420902252
Final reward: -0.4351082742214203
Final reward: -0.4257478415966034
Final reward: -0.4412250220775604
Final reward: -0.42913222312927246
Final reward: -0.44417300820350647
Final reward: -0.43508222699165344
Final reward: -0.4463665187358856
Final reward: -0.43851011991500854
Final reward: -0.44719693064689636
Final reward: -0.45307061076164246
Final reward: -0.4565329849720001
Final reward: -0.45792731642723083
Final reward: -0.4584241807460785
Final reward: -0.45904722809791565
Final reward: -0.4578219950199127
Final reward: -0.4548501670360565
Final reward: -0.4502226710319519
Final reward: -0.44478359818458557
Final reward: -0.4397725760936737
Final reward: -0.4346979260444641
Final reward: -0.4314418137073517
Final reward: -0.4272307753562927
Final reward: -0.42083820700645447
Final reward: -0.4146234691143036
Final reward: -0.4058414697647095
Final reward: -0.39984050393104553
Final reward: -0.39298078417778015
Final reward: -0.3881337344646454
Final reward: -0.38638919591903687
Final reward: -0.38697949051856995
Final reward: -0.3869524300098419
Final reward: -0.3867636024951935
Final reward: -0.38665032386779785
Final reward: -0.3866422474384308
Final reward: -0.386806458234787
Final reward: -0.38699132204055786
Final reward: -0.3870489001274109
Final reward: -0.38712450861930847
Final reward: -0.3871302306652069
Final reward: -0.38688015937805176
Final reward: -0.38626009225845337
Final reward: -0.3869718015193939
Final reward: -0.39453360438346863
Final reward: -0.40349578857421875
Final reward: -0.39449575543403625
Final reward: -0.4082114100456238
Final reward: -0.3964144289493561
Final reward: -0.4027566611766815
Final reward: -0.39809149503707886
Final reward: -0.40624427795410156
Final reward: -0.41481128334999084
Final reward: -0.41472217440605164
Final reward: -0.4170558452606201
Final reward: -0.4198454022407532
Final reward: -0.4217626750469208
Final reward: -0.4219728410243988
Final reward: -0.42101776599884033
Final reward: -0.42054829001426697
Final reward: -0.4192388653755188
Final reward: -0.41646087169647217
Final reward: -0.41295936703681946
Final reward: -0.40728995203971863
Final reward: -0.4013530910015106
Final reward: -0.399020254611969
Final reward: -0.3960317075252533
Final reward: -0.39297792315483093
Final reward: -0.39074403047561646
Final reward: -0.3878834843635559
Final reward: -0.3829334080219269
Final reward: -0.3769955337047577
Final reward: -0.37069132924079895
Final reward: -0.36469361186027527
Final reward: -0.3566714823246002
Final reward: -0.35151949524879456
Final reward: -0.3475728929042816
Final reward: -0.3461463451385498
Final reward: -0.3460789620876312
Final reward: -0.34626543521881104
Final reward: -0.3462865352630615
Final reward: -0.34621912240982056
Final reward: -0.3461669683456421
Final reward: -0.3462180495262146
Final reward: -0.3462936580181122
Final reward: -0.3462935984134674
Final reward: -0.3463071882724762
Final reward: -0.34636038541793823
Final reward: -0.3462609350681305
Final reward: -0.3456737995147705
Final reward: -0.3510664701461792
Final reward: -0.36403462290763855
Final reward: -0.3543505072593689
Final reward: -0.36809247732162476
Final reward: -0.35988649725914
Final reward: -0.368120402097702
Final reward: -0.37209025025367737
Final reward: -0.3756633996963501
Final reward: -0.3791360557079315
Final reward: -0.3758469521999359
Final reward: -0.3698921501636505
Final reward: -0.3717558979988098
Final reward: -0.37392523884773254
Final reward: -0.3752134442329407
Final reward: -0.3772895336151123
Final reward: -0.3775137662887573
Final reward: -0.377678245306015
Final reward: -0.3793569505214691
Final reward: -0.37999874353408813
Final reward: -0.38112691044807434
Final reward: -0.3817601501941681
Final reward: -0.3801444172859192
Final reward: -0.37616363167762756
Final reward: -0.369989812374115
Final reward: -0.3619953989982605
Final reward: -0.3533296585083008
Final reward: -0.34821975231170654
Final reward: -0.3429329991340637
Final reward: -0.33798477053642273
Final reward: -0.33184659481048584
Final reward: -0.32530030608177185
Final reward: -0.31847888231277466
Final reward: -0.31992092728614807
Final reward: -0.32099494338035583
Final reward: -0.32222384214401245
Final reward: -0.32292503118515015
Final reward: -0.3231504261493683
Final reward: -0.3230660557746887
Final reward: -0.3228556513786316
Final reward: -0.322801798582077
Final reward: -0.32291075587272644
Final reward: -0.3230558931827545
Final reward: -0.32323578000068665
Final reward: -0.323300302028656
Final reward: -0.323112815618515
Final reward: -0.3222504258155823
Final reward: -0.32586169242858887
Final reward: -0.3221077620983124
Final reward: -0.3076939582824707
Final reward: -0.3222021162509918
Final reward: -0.3156469762325287
Final reward: -0.3324452042579651
Final reward: -0.324900358915329
Final reward: -0.3399350345134735
Final reward: -0.332536518573761
Final reward: -0.3442988991737366
Final reward: -0.34779778122901917
Final reward: -0.34210747480392456
Final reward: -0.34881576895713806
Final reward: -0.3543608486652374
Final reward: -0.35955148935317993
Final reward: -0.3576448857784271
Final reward: -0.3614434003829956
Final reward: -0.36133748292922974
Final reward: -0.3617558181285858
Final reward: -0.3597925305366516
Final reward: -0.3560521602630615
Final reward: -0.3513098657131195
Final reward: -0.3433672785758972
Final reward: -0.3366963863372803
Final reward: -0.3311620056629181
Final reward: -0.3248318135738373
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
Final reward: -0.3193845748901367
Final reward: -0.3133603036403656
Final reward: -0.3066006898880005
Final reward: -0.29851487278938293
Final reward: -0.2909300625324249
Final reward: -0.2815316319465637
Final reward: -0.2770521342754364
Final reward: -0.280219167470932
Final reward: -0.28230947256088257
Final reward: -0.2849949896335602
Final reward: -0.2867404520511627
Final reward: -0.2889074981212616
Final reward: -0.29018476605415344
Final reward: -0.291631817817688
Final reward: -0.29227155447006226
Final reward: -0.2925119400024414
Final reward: -0.29251256585121155
Final reward: -0.2925121486186981
Final reward: -0.29252976179122925
Final reward: -0.29252535104751587
Final reward: -0.2924983501434326
Final reward: -0.29233863949775696
Final reward: -0.29185664653778076
Final reward: -0.29069599509239197
Final reward: -0.29675978422164917
Final reward: -0.28116899728775024
Final reward: -0.288786381483078
Final reward: -0.2670380473136902
Final reward: -0.27591457962989807
Final reward: -0.2548580467700958
Final reward: -0.28143084049224854
Final reward: -0.2689674198627472
Final reward: -0.2941208481788635
Final reward: -0.28251171112060547
Final reward: -0.3052416145801544
Final reward: -0.2949376702308655
Final reward: -0.3137072026729584
Final reward: -0.3050464987754822
Final reward: -0.3189966082572937
Final reward: -0.31225940585136414
Final reward: -0.3216857612133026
Final reward: -0.32900547981262207
Final reward: -0.33294206857681274
Final reward: -0.333439439535141
Final reward: -0.3315001130104065
Final reward: -0.32652172446250916
Final reward: -0.31809020042419434
Final reward: -0.31106579303741455
Final reward: -0.3049684166908264
Final reward: -0.29706329107284546
Final reward: -0.28842562437057495
Final reward: -0.27780282497406006
Final reward: -0.26674187183380127
Final reward: -0.2525060176849365
Final reward: -0.23793846368789673
Final reward: -0.21963846683502197
Final reward: -0.19822773337364197
Final reward: -0.19850924611091614
Final reward: -0.20350965857505798
Final reward: -0.2075166255235672
Final reward: -0.2109268456697464
Final reward: -0.21528124809265137
Final reward: -0.2188384234905243
Final reward: -0.22199372947216034
Final reward: -0.2242247611284256
Final reward: -0.22661764919757843
Final reward: -0.23019225895404816
Final reward: -0.23153331875801086
Final reward: -0.23200282454490662
Final reward: -0.23223260045051575
Final reward: -0.23215824365615845
Final reward: -0.2318003922700882
Final reward: -0.23075580596923828
Final reward: -0.2344255894422531
Final reward: -0.22105146944522858
Final reward: -0.22687938809394836
Final reward: -0.2058519423007965
Final reward: -0.21443629264831543
Final reward: -0.18462474644184113
Final reward: -0.1922779530286789
Final reward: -0.16475753486156464
Final reward: -0.17429932951927185
Final reward: -0.13994762301445007
Final reward: -0.15249322354793549
Final reward: -0.1441759616136551
Final reward: -0.1642685830593109
Final reward: -0.16074897348880768
Final reward: -0.1825989931821823
Final reward: -0.1719752699136734
Final reward: -0.18917852640151978
Final reward: -0.18015480041503906
Final reward: -0.18981178104877472
Final reward: -0.19462111592292786
Final reward: -0.1971215009689331
Final reward: -0.1961032599210739
Final reward: -0.18971848487854004
Final reward: -0.17915748059749603
Final reward: -0.1738167703151703
Final reward: -0.1667863130569458
Final reward: -0.157377228140831
Final reward: -0.15044499933719635
Final reward: -0.14429669082164764
Final reward: -0.13785791397094727
Final reward: -0.12796488404273987
Final reward: -0.11749202758073807
Final reward: -0.5116263628005981
Final reward: -0.5106721520423889
Final reward: -0.5089013576507568
Final reward: -0.5061484575271606
Final reward: -0.5022183060646057
Final reward: -0.4976125955581665
Final reward: -0.49465411901474
Final reward: -0.48827657103538513
Final reward: -0.4854448437690735
Final reward: -0.47609153389930725
Final reward: -0.4730307161808014
Final reward: -0.46111199259757996
Final reward: -0.4563562273979187
Final reward: -0.4566340148448944
Final reward: -0.4620186984539032
Final reward: -0.46289125084877014
Final reward: -0.46793150901794434
Final reward: -0.4686608910560608
Final reward: -0.4713883399963379
Final reward: -0.47297924757003784
Final reward: -0.476351261138916
Final reward: -0.4773457646369934
Final reward: -0.47867268323898315
Final reward: -0.47963133454322815
Final reward: -0.48017480969429016
Final reward: -0.4802756607532501
Final reward: -0.48026221990585327
Final reward: -0.4802742004394531
Final reward: -0.4802955090999603
Final reward: -0.4801567494869232
Final reward: -0.47953861951828003
Final reward: -0.4825597405433655
Final reward: -0.48713019490242004
Final reward: -0.4703838527202606
Final reward: -0.4770234525203705
Final reward: -0.4548589587211609
Final reward: -0.46309441328048706
Final reward: -0.43768513202667236
Final reward: -0.44610244035720825
Final reward: -0.42247238755226135
Final reward: -0.43231576681137085
Final reward: -0.41871267557144165
Final reward: -0.43366968631744385
Final reward: -0.42362380027770996
Final reward: -0.43380042910575867
Final reward: -0.43384724855422974
Final reward: -0.4397425353527069
Final reward: -0.44340723752975464
Final reward: -0.4465787410736084
Final reward: -0.4409703314304352
Final reward: -0.4411882758140564
Final reward: -0.4349183440208435
Final reward: -0.43865907192230225
Final reward: -0.4406699538230896
Final reward: -0.4407608211040497
Final reward: -0.43896859884262085
Final reward: -0.4351169764995575
Final reward: -0.4288381040096283
Final reward: -0.4196247458457947
Final reward: -0.4127166271209717
Final reward: -0.4077424108982086
Final reward: -0.4081941545009613
Final reward: -0.3988840579986572
Final reward: -0.39939427375793457
Final reward: -0.3935821056365967
Final reward: -0.4028574824333191
Final reward: -0.4015815854072571
Final reward: -0.4098360240459442
Final reward: -0.4087725281715393
Final reward: -0.41467827558517456
Final reward: -0.414162278175354
Final reward: -0.4154336452484131
Final reward: -0.41736942529678345
Final reward: -0.41821616888046265
Final reward: -0.4185827374458313
Final reward: -0.41872113943099976
Final reward: -0.41872483491897583
Final reward: -0.41872575879096985
Final reward: -0.41873061656951904
Final reward: -0.4187158942222595
Final reward: -0.4185178279876709
Final reward: -0.41779714822769165
Final reward: -0.4212694466114044
Final reward: -0.4259110391139984
Final reward: -0.41180703043937683
Final reward: -0.41748344898223877
Final reward: -0.3978176414966583
Final reward: -0.40434589982032776
Final reward: -0.3856223225593567
Final reward: -0.3915552496910095
Final reward: -0.373768150806427
Final reward: -0.38739708065986633
Final reward: -0.3759109377861023
Final reward: -0.3904009461402893
Final reward: -0.38093358278274536
Final reward: -0.3911338150501251
Final reward: -0.38451480865478516
Final reward: -0.39588093757629395
Final reward: -0.3880845308303833
Final reward: -0.3944518268108368
Final reward: -0.3980676531791687
Final reward: -0.3995077610015869
Final reward: -0.3991371691226959
Final reward: -0.3974214494228363
Final reward: -0.395222932100296
Final reward: -0.3913666605949402
Final reward: -0.38645049929618835
Final reward: -0.37946513295173645
Final reward: -0.3741046190261841
Final reward: -0.3703843653202057
Final reward: -0.36623820662498474
Final reward: -0.362394779920578
Final reward: -0.3579462170600891
Final reward: -0.3530511260032654
Final reward: -0.34738901257514954
Final reward: -0.35362276434898376
Final reward: -0.3503989279270172
Final reward: -0.3535987138748169
Final reward: -0.36748480796813965
Final reward: -0.3637833893299103
Final reward: -0.3745819330215454
Final reward: -0.37141990661621094
Final reward: -0.37273043394088745
Final reward: -0.375128835439682
Final reward: -0.3763114809989929
Final reward: -0.3771617114543915
Final reward: -0.3773791491985321
Final reward: -0.3774092495441437
Final reward: -0.3773599863052368
Final reward: -0.37722286581993103
Final reward: -0.37684690952301025
Final reward: -0.37598031759262085
Final reward: -0.38080495595932007
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
Final reward: -0.3850550353527069
Final reward: -0.3675440847873688
Final reward: -0.3744790852069855
Final reward: -0.35381436347961426
Final reward: -0.36048373579978943
Final reward: -0.33679601550102234
Final reward: -0.3465459644794464
Final reward: -0.32597148418426514
Final reward: -0.3488330841064453
Final reward: -0.3304533064365387
Final reward: -0.35108229517936707
Final reward: -0.33682891726493835
Final reward: -0.35383132100105286
Final reward: -0.34215953946113586
Final reward: -0.3543356657028198
Final reward: -0.34685230255126953
Final reward: -0.35447949171066284
Final reward: -0.3572789132595062
Final reward: -0.35974758863449097
Final reward: -0.359616219997406
Final reward: -0.35690227150917053
Final reward: -0.3513694405555725
Final reward: -0.3429969549179077
Final reward: -0.33533817529678345
Final reward: -0.33046361804008484
Final reward: -0.32958418130874634
Final reward: -0.3196849524974823
Final reward: -0.31738510727882385
Final reward: -0.30899834632873535
Final reward: -0.3068430721759796
Final reward: -0.2998569905757904
Final reward: -0.2976755201816559
Final reward: -0.2900269627571106
Final reward: -0.2934049963951111
Final reward: -0.2942192256450653
Final reward: -0.2964347302913666
Final reward: -0.3018183708190918
Final reward: -0.30182361602783203
Final reward: -0.30485209822654724
Final reward: -0.30656737089157104
Final reward: -0.30840879678726196
Final reward: -0.3091227412223816
Final reward: -0.30946236848831177
Final reward: -0.30947762727737427
Final reward: -0.3093600869178772
Final reward: -0.308915913105011
Final reward: -0.3080366551876068
Final reward: -0.3129667639732361
Final reward: -0.31716132164001465
Final reward: -0.30394449830055237
Final reward: -0.30815500020980835
Final reward: -0.3115212917327881
Final reward: -0.2954329252243042
Final reward: -0.2937606871128082
Final reward: -0.27771228551864624
Final reward: -0.29464977979660034
Final reward: -0.28274545073509216
Final reward: -0.2976008653640747
Final reward: -0.28654244542121887
Final reward: -0.2977198362350464
Final reward: -0.3028107285499573
Final reward: -0.2981727123260498
Final reward: -0.3042811453342438
Final reward: -0.30701979994773865
Final reward: -0.3071441352367401
Final reward: -0.30382010340690613
Final reward: -0.2988707721233368
Final reward: -0.2950998842716217
Final reward: -0.29055097699165344
Final reward: -0.28443413972854614
Final reward: -0.27912014722824097
Final reward: -0.27624082565307617
Final reward: -0.27457740902900696
Final reward: -0.27414244413375854
Final reward: -0.2745423913002014
Final reward: -0.2752949595451355
Final reward: -0.2759735584259033
Final reward: -0.2771100401878357
Final reward: -0.2785380184650421
Final reward: -0.27998748421669006
Final reward: -0.28232455253601074
Final reward: -0.28744837641716003
Final reward: -0.293373703956604
Final reward: -0.2958217263221741
Final reward: -0.295635849237442
Final reward: -0.2958807647228241
Final reward: -0.29858994483947754
Final reward: -0.3006158769130707
Final reward: -0.30117475986480713
Final reward: -0.30206984281539917
Final reward: -0.2997715473175049
Final reward: -0.29457128047943115
Final reward: -0.28658977150917053
Final reward: -0.2783753573894501
Final reward: -0.281991183757782
Final reward: -0.2700348496437073
Final reward: -0.2753432095050812
Final reward: -0.258211612701416
Final reward: -0.27371057868003845
Final reward: -0.26917243003845215
Final reward: -0.27999570965766907
Final reward: -0.27770718932151794
Final reward: -0.2802101969718933
Final reward: -0.2880686819553375
Final reward: -0.2872963845729828
Final reward: -0.2884983420372009
Final reward: -0.2903079390525818
Final reward: -0.29199132323265076
Final reward: -0.29339680075645447
Final reward: -0.2939075827598572
Final reward: -0.2940085232257843
Final reward: -0.2938281297683716
Final reward: -0.29324302077293396
Final reward: -0.293863445520401
Final reward: -0.2999345660209656
Final reward: -0.2881161570549011
Final reward: -0.29642996191978455
Final reward: -0.26686620712280273
Final reward: -0.27858132123947144
Final reward: -0.23953256011009216
Final reward: -0.26085084676742554
Final reward: -0.2370297610759735
Final reward: -0.26276808977127075
Final reward: -0.24411627650260925
Final reward: -0.2649728059768677
Final reward: -0.25107070803642273
Final reward: -0.26480206847190857
Final reward: -0.25649312138557434
Final reward: -0.26386383175849915
Final reward: -0.26720380783081055
Final reward: -0.2669256925582886
Final reward: -0.26303842663764954
Final reward: -0.25514665246009827
Final reward: -0.24243135750293732
Final reward: -0.23506121337413788
Final reward: -0.23232688009738922
Final reward: -0.22359797358512878
Final reward: -0.22319495677947998
Final reward: -0.21047762036323547
Final reward: -0.22024141252040863
Final reward: -0.19981735944747925
Final reward: -0.2069990634918213
Final reward: -0.209660604596138
Final reward: -0.21463647484779358
Final reward: -0.215800479054451
Final reward: -0.21719221770763397
Final reward: -0.21784713864326477
Final reward: -0.21789269149303436
Final reward: -0.2178507298231125
Final reward: -0.2179185301065445
Final reward: -0.2182975858449936
Final reward: -0.2187628149986267
Final reward: -0.22002935409545898
Final reward: -0.22042974829673767
Final reward: -0.22054573893547058
Final reward: -0.2204403579235077
Final reward: -0.22006355226039886
Final reward: -0.21900852024555206
Final reward: -0.22182215750217438
Final reward: -0.22875666618347168
Final reward: -0.21692027151584625
Final reward: -0.22493571043014526
Final reward: -0.19582028687000275
Final reward: -0.2045094072818756
Final reward: -0.19247975945472717
Final reward: -0.22036440670490265
Final reward: -0.20657497644424438
Final reward: -0.23093579709529877
Final reward: -0.2191043496131897
Final reward: -0.24164913594722748
Final reward: -0.23068299889564514
Final reward: -0.24838635325431824
Final reward: -0.2658895254135132
Final reward: -0.25419750809669495
Final reward: -0.24694247543811798
Final reward: -0.24725224077701569
Final reward: -0.249697744846344
Final reward: -0.24845801293849945
Final reward: -0.24266798794269562
Final reward: -0.2372257262468338
Final reward: -0.23936744034290314
Final reward: -0.227898508310318
Final reward: -0.23028497397899628
Final reward: -0.2150658816099167
Final reward: -0.2175643891096115
Final reward: -0.20103523135185242
Final reward: -0.2069181501865387
Final reward: -0.1833241581916809
Final reward: -0.18792423605918884
Final reward: -0.1629646271467209
Final reward: -0.17219074070453644
Final reward: -0.16527655720710754
Final reward: -0.18682625889778137
Final reward: -0.18145880103111267
Final reward: -0.19866065680980682
Final reward: -0.19351713359355927
Final reward: -0.19682300090789795
Final reward: -0.19805338978767395
Final reward: -0.19841955602169037
Final reward: -0.19817695021629333
Final reward: -0.19727225601673126
Final reward: -0.20009779930114746
Final reward: -0.20776842534542084
Final reward: -0.18263600766658783
Final reward: -0.19560720026493073
Final reward: -0.1580069214105606
Final reward: -0.17654426395893097
Final reward: -0.148394376039505
Final reward: -0.19226756691932678
Final reward: -0.16623623669147491
Final reward: -0.20260928571224213
Final reward: -0.18182474374771118
Final reward: -0.20943783223628998
Final reward: -0.19382445514202118
Final reward: -0.2125272899866104
Final reward: -0.21501275897026062
Final reward: -0.21394892036914825
Final reward: -0.21634387969970703
Final reward: -0.2144291251897812
Final reward: -0.21033717691898346
Final reward: -0.1990552693605423
Final reward: -0.19672149419784546
Final reward: -0.1870947629213333
Final reward: -0.1906108260154724
Final reward: -0.17575323581695557
Final reward: -0.18084122240543365
Final reward: -0.16017068922519684
Final reward: -0.16626085340976715
Final reward: -0.14258505403995514
Final reward: -0.14807860553264618
Final reward: -0.12577570974826813
Final reward: -0.1309361457824707
Final reward: -0.11630933731794357
Final reward: -0.47143107652664185
Final reward: -0.4714595079421997
Final reward: -0.4705146551132202
Final reward: -0.46899470686912537
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
Final reward: -0.4666391909122467
Final reward: -0.4631192088127136
Final reward: -0.45756152272224426
Final reward: -0.4522058367729187
Final reward: -0.44419538974761963
Final reward: -0.43704503774642944
Final reward: -0.4263583719730377
Final reward: -0.41785091161727905
Final reward: -0.4061541259288788
Final reward: -0.3985559940338135
Final reward: -0.3892786502838135
Final reward: -0.3829449415206909
Final reward: -0.3803453743457794
Final reward: -0.3808363080024719
Final reward: -0.38139432668685913
Final reward: -0.3818332850933075
Final reward: -0.38202235102653503
Final reward: -0.3820068836212158
Final reward: -0.38198673725128174
Final reward: -0.3820394277572632
Final reward: -0.3820699155330658
Final reward: -0.38195091485977173
Final reward: -0.3816240727901459
Final reward: -0.3808029592037201
Final reward: -0.3848649859428406
Final reward: -0.37466534972190857
Final reward: -0.3849714696407318
Final reward: -0.37624093890190125
Final reward: -0.39152103662490845
Final reward: -0.37936243414878845
Final reward: -0.3987026810646057
Final reward: -0.3853660821914673
Final reward: -0.40235358476638794
Final reward: -0.3906996250152588
Final reward: -0.40474432706832886
Final reward: -0.39690473675727844
Final reward: -0.40552130341529846
Final reward: -0.41109421849250793
Final reward: -0.4113478660583496
Final reward: -0.41207969188690186
Final reward: -0.4120514690876007
Final reward: -0.412776380777359
Final reward: -0.41124260425567627
Final reward: -0.4082448184490204
Final reward: -0.4045830965042114
Final reward: -0.39821773767471313
Final reward: -0.39233294129371643
Final reward: -0.3882848918437958
Final reward: -0.38353297114372253
Final reward: -0.3782264292240143
Final reward: -0.3715240955352783
Final reward: -0.36439305543899536
Final reward: -0.35525986552238464
Final reward: -0.3486729860305786
Final reward: -0.3416387736797333
Final reward: -0.3353404998779297
Final reward: -0.32893386483192444
Final reward: -0.3301273584365845
Final reward: -0.3302830457687378
Final reward: -0.33052965998649597
Final reward: -0.33190155029296875
Final reward: -0.3326062858104706
Final reward: -0.33295851945877075
Final reward: -0.3330039083957672
Final reward: -0.33297649025917053
Final reward: -0.3329943120479584
Final reward: -0.3330412209033966
Final reward: -0.3328797221183777
Final reward: -0.33205512166023254
Final reward: -0.33585768938064575
Final reward: -0.32371845841407776
Final reward: -0.33245933055877686
Final reward: -0.32693156599998474
Final reward: -0.34185728430747986
Final reward: -0.3366633653640747
Final reward: -0.35152387619018555
Final reward: -0.3464469611644745
Final reward: -0.3572219908237457
Final reward: -0.35745003819465637
Final reward: -0.3651842772960663
Final reward: -0.3657393753528595
Final reward: -0.3692088723182678
Final reward: -0.3702923357486725
Final reward: -0.36873316764831543
Final reward: -0.36624521017074585
Final reward: -0.36504438519477844
Final reward: -0.3618890047073364
Final reward: -0.3565254509449005
Final reward: -0.3535929322242737
Final reward: -0.3522094488143921
Final reward: -0.3498564660549164
Final reward: -0.34661397337913513
Final reward: -0.3417482376098633
Final reward: -0.3364013433456421
Final reward: -0.3296857476234436
Final reward: -0.3225545883178711
Final reward: -0.31330880522727966
Final reward: -0.3063250780105591
Final reward: -0.2990749180316925
Final reward: -0.29365649819374084
Final reward: -0.2931472063064575
Final reward: -0.29441192746162415
Final reward: -0.2947671413421631
Final reward: -0.29485997557640076
Final reward: -0.2949260473251343
Final reward: -0.2949252724647522
Final reward: -0.29493048787117004
Final reward: -0.294785737991333
Final reward: -0.29476699233055115
Final reward: -0.29514241218566895
Final reward: -0.29525694251060486
Final reward: -0.29555466771125793
Final reward: -0.2955996096134186
Final reward: -0.2955890893936157
Final reward: -0.29547473788261414
Final reward: -0.2949593663215637
Final reward: -0.294523686170578
Final reward: -0.3008101284503937
Final reward: -0.28689783811569214
Final reward: -0.28076037764549255
Final reward: -0.29115694761276245
Final reward: -0.2838449478149414
Final reward: -0.2957959473133087
Final reward: -0.2930736541748047
Final reward: -0.29442986845970154
Final reward: -0.31072500348091125
Final reward: -0.2992815375328064
Final reward: -0.3146704137325287
Final reward: -0.3065827190876007
Final reward: -0.3155980110168457
Final reward: -0.3181329071521759
Final reward: -0.32357215881347656
Final reward: -0.32580459117889404
Final reward: -0.3268868029117584
Final reward: -0.32851138710975647
Final reward: -0.32902956008911133
Final reward: -0.326789915561676
Final reward: -0.32284775376319885
Final reward: -0.3191049098968506
Final reward: -0.31419873237609863
Final reward: -0.3069707155227661
Final reward: -0.3029041588306427
Final reward: -0.2984611988067627
Final reward: -0.2931440472602844
Final reward: -0.2865884006023407
Final reward: -0.279917448759079
Final reward: -0.2724224030971527
Final reward: -0.26508641242980957
Final reward: -0.25665009021759033
Final reward: -0.25753054022789
Final reward: -0.2594663202762604
Final reward: -0.2621402442455292
Final reward: -0.2635774612426758
Final reward: -0.2649574279785156
Final reward: -0.26536935567855835
Final reward: -0.26565396785736084
Final reward: -0.26564109325408936
Final reward: -0.2654627859592438
Final reward: -0.2648527920246124
Final reward: -0.26359274983406067
Final reward: -0.2687442898750305
Final reward: -0.2560088038444519
Final reward: -0.2632749676704407
Final reward: -0.2442345917224884
Final reward: -0.2574464678764343
Final reward: -0.25006890296936035
Final reward: -0.26581332087516785
Final reward: -0.2573893964290619
Final reward: -0.2670729160308838
Final reward: -0.26489022374153137
Final reward: -0.2768712043762207
Final reward: -0.2848295569419861
Final reward: -0.2889183759689331
Final reward: -0.29065024852752686
Final reward: -0.2912179231643677
Final reward: -0.29071319103240967
Final reward: -0.2916046679019928
Final reward: -0.2912052869796753
Final reward: -0.29029738903045654
Final reward: -0.28639090061187744
Final reward: -0.27966275811195374
Final reward: -0.2704296410083771
Final reward: -0.26255497336387634
Final reward: -0.25609302520751953
Final reward: -0.2485940307378769
Final reward: -0.23971502482891083
Final reward: -0.22910591959953308
Final reward: -0.2292938381433487
Final reward: -0.232461079955101
Final reward: -0.23451466858386993
Final reward: -0.236716628074646
Final reward: -0.2377634197473526
Final reward: -0.23886504769325256
Final reward: -0.2396053671836853
Final reward: -0.23976914584636688
Final reward: -0.23984089493751526
Final reward: -0.23975233733654022
Final reward: -0.2394111603498459
Final reward: -0.23889696598052979
Final reward: -0.23841215670108795
Final reward: -0.23766015470027924
Final reward: -0.23574206233024597
Final reward: -0.24051021039485931
Final reward: -0.23733873665332794
Final reward: -0.22151148319244385
Final reward: -0.21637974679470062
Final reward: -0.22347494959831238
Final reward: -0.21495908498764038
Final reward: -0.23228929936885834
Final reward: -0.2224903404712677
Final reward: -0.23565004765987396
Final reward: -0.23122139275074005
Final reward: -0.2478165626525879
Final reward: -0.24117273092269897
Final reward: -0.25569024682044983
Final reward: -0.25049978494644165
Final reward: -0.2609410285949707
Final reward: -0.2634418308734894
Final reward: -0.2646138370037079
Final reward: -0.26990315318107605
Final reward: -0.2758193016052246
Final reward: -0.2787609398365021
Final reward: -0.28009656071662903
Final reward: -0.2787306606769562
Final reward: -0.2744498550891876
Final reward: -0.2661496102809906
Final reward: -0.2592773735523224
Final reward: -0.25423315167427063
Final reward: -0.24903811514377594
Final reward: -0.2418118268251419
Final reward: -0.23318827152252197
Final reward: -0.22460094094276428
Final reward: -0.21663092076778412
Final reward: -0.20913273096084595
Final reward: -0.20309290289878845
Final reward: -0.19764091074466705
Final reward: -0.19050975143909454
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.
  warnings.warn("You tried to call render() but no `render_mode` was passed to the env constructor.")
Final reward: -0.18068885803222656
Final reward: -0.1763315051794052
Final reward: -0.18075823783874512
Final reward: -0.1831945776939392
Final reward: -0.18426696956157684
Final reward: -0.18584483861923218
Final reward: -0.18664130568504333
Final reward: -0.18779590725898743
Final reward: -0.18856610357761383
Final reward: -0.19025516510009766
Final reward: -0.19101594388484955
Final reward: -0.19112618267536163
Final reward: -0.19086095690727234
Final reward: -0.1901838630437851
Final reward: -0.1898619532585144
Final reward: -0.18920017778873444
Final reward: -0.1865340769290924
Final reward: -0.18665549159049988
Final reward: -0.17709150910377502
Final reward: -0.1760941445827484
Final reward: -0.16935555636882782
Final reward: -0.16592136025428772
Final reward: -0.1736181527376175
Final reward: -0.1571917086839676
Final reward: -0.1682654619216919
Final reward: -0.171347513794899
Final reward: -0.17667677998542786
Final reward: -0.1679837554693222
Final reward: -0.178917795419693
Final reward: -0.18742984533309937
Final reward: -0.18177010118961334
Final reward: -0.18025535345077515
Final reward: -0.1919044852256775
Final reward: -0.1941976249217987
Final reward: -0.19728907942771912
Final reward: -0.1972329318523407
Final reward: -0.1922987699508667
Final reward: -0.19494251906871796
Final reward: -0.19228486716747284
Final reward: -0.1854688972234726
Final reward: -0.1814827173948288
Final reward: -0.17798186838626862
Final reward: -0.1741253137588501
Final reward: -0.16935375332832336
Final reward: -0.16417765617370605
Final reward: -0.158230721950531
Final reward: -0.15069763362407684
Final reward: -0.14158259332180023
Final reward: -0.13074427843093872
Final reward: -0.12573759257793427
Final reward: -0.13277611136436462
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.027 MB of 0.027 MB uploaded (0.000 MB deduped)wandb: \ 0.027 MB of 0.027 MB uploaded (0.000 MB deduped)wandb: | 0.027 MB of 0.027 MB uploaded (0.000 MB deduped)wandb: / 0.027 MB of 0.027 MB uploaded (0.000 MB deduped)wandb: - 0.027 MB of 0.027 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                         reward ‚ñá‚ñÖ‚ñÜ‚ñÅ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÖ‚ñá‚ñá‚ñá
wandb:                train/approx_kl ‚ñà‚ñÇ‚ñÉ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÇ
wandb:            train/approx_ln(kl) ‚ñà‚ñÉ‚ñÑ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñá‚ñÇ‚ñÖ‚ñÑ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÇ
wandb:               train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:             train/entropy_loss ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà
wandb:       train/explained_variance ‚ñÇ‚ñà‚ñÅ‚ñá‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:            train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                 train/ln(loss) ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÉ ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÑ ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ
wandb: train/ln(policy_gradient_loss)   ‚ñá  ‚ñá ‚ñÖ          ‚ñà    ‚ñÑ‚ñÉ  ‚ñÑ‚ñÑ ‚ñá ‚ñá ‚ñÜ   ‚ñá‚ñÅ
wandb:                     train/loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:     train/policy_gradient_loss ‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñÇ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñà‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÜ‚ñÜ‚ñÅ‚ñá‚ñÉ‚ñá‚ñÖ‚ñá‚ñÇ‚ñÉ‚ñÇ‚ñá‚ñÖ
wandb:                      train/std ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:               train/value_loss ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                         reward -0.32806
wandb:                train/approx_kl 0.00475
wandb:            train/approx_ln(kl) -5.35029
wandb:               train/clip_range 0.2
wandb:             train/entropy_loss -2.63439
wandb:       train/explained_variance 0.99101
wandb:            train/learning_rate 0.001
wandb:                 train/ln(loss) -2.23396
wandb: train/ln(policy_gradient_loss) -7.39478
wandb:                     train/loss 0.1071
wandb:                train/n_updates 2012
wandb:     train/policy_gradient_loss 0.00061
wandb:                      train/std 0.90825
wandb:               train/value_loss 0.40393
wandb: 
wandb: üöÄ View run ppo-KLpenalty-beta(dynamic)-parking at: https://wandb.ai/ecrl/PPO-Penalty-Report5/runs/baompkgl
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231120_143521-baompkgl/logs
