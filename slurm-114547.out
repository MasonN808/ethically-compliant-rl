Running script with speed limit: 2
Running script with speed limit: 4
Running script with speed limit: 8
Running script with speed limit: 16
Running script with speed limit: 2
Running script with speed limit: 4
Running script with speed limit: 8
Running script with speed limit: 16
Running script with speed limit: 2
Running script with speed limit: 4
Running script with speed limit: 8
Running script with speed limit: 16
Running script with speed limit: 2
Running script with speed limit: 4
Running script with speed limit: 8
Running script with speed limit: 16
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_185750-fkhimvab
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/fkhimvab
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_185750-cwr6z585
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/cwr6z585
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231227_025751-h94pxu2w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/h94pxu2w
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_185750-wpgdmzjr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/wpgdmzjr
Using cpu device
------------------------------------
| reward             | [-0.545187] |
| time/              |             |
|    fps             | 134         |
|    iterations      | 1           |
|    time_elapsed    | 15          |
|    total_timesteps | 2048        |
------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.3494458] |
| time/              |              |
|    fps             | 131          |
|    iterations      | 1            |
|    time_elapsed    | 15           |
|    total_timesteps | 2048         |
-------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.26754978] |
| time/              |               |
|    fps             | 135           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.4521336] |
| time/              |              |
|    fps             | 119          |
|    iterations      | 1            |
|    time_elapsed    | 17           |
|    total_timesteps | 2048         |
-------------------------------------
-------------------------------------------
| reward                   | [-0.8718231] |
| time/                    |              |
|    fps                   | 130          |
|    iterations            | 2            |
|    time_elapsed          | 31           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0059735323 |
|    clip_fraction         | 0.0651       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 75.7         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0343      |
|    lagrangian_multiplier | 0.083        |
|    learning_rate         | 0.0003       |
|    loss                  | 35.6         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00632     |
|    std                   | 1            |
|    value_loss            | 358          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7350165] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 2            |
|    time_elapsed          | 32           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0064984583 |
|    clip_fraction         | 0.0477       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0254       |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0721      |
|    lagrangian_multiplier | 0.0772       |
|    learning_rate         | 0.0003       |
|    loss                  | 24.3         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00539     |
|    std                   | 0.995        |
|    value_loss            | 265          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.43292928] |
| time/                    |               |
|    fps                   | 122           |
|    iterations            | 2             |
|    time_elapsed          | 33            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.0056133135  |
|    clip_fraction         | 0.043         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0133        |
|    entropy_loss          | -2.86         |
|    explained_variance    | -0.0332       |
|    lagrangian_multiplier | 0.0716        |
|    learning_rate         | 0.0003        |
|    loss                  | 29.8          |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00552      |
|    std                   | 1.02          |
|    value_loss            | 326           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.6373291] |
| time/                    |              |
|    fps                   | 115          |
|    iterations            | 2            |
|    time_elapsed          | 35           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0057728435 |
|    clip_fraction         | 0.041        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.149        |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0212      |
|    lagrangian_multiplier | 0.0567       |
|    learning_rate         | 0.0003       |
|    loss                  | 40.4         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00492     |
|    std                   | 0.991        |
|    value_loss            | 350          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8387829] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 3            |
|    time_elapsed          | 47           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0058271773 |
|    clip_fraction         | 0.0492       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 71.1         |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0493      |
|    lagrangian_multiplier | 0.0431       |
|    learning_rate         | 0.0003       |
|    loss                  | 81.5         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00538     |
|    std                   | 1.01         |
|    value_loss            | 539          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0233114] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 3            |
|    time_elapsed          | 48           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.00433072   |
|    clip_fraction         | 0.0279       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0452       |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0441      |
|    lagrangian_multiplier | 0.0649       |
|    learning_rate         | 0.0003       |
|    loss                  | 23.8         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00389     |
|    std                   | 0.99         |
|    value_loss            | 277          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1686537] |
| time/                    |              |
|    fps                   | 123          |
|    iterations            | 3            |
|    time_elapsed          | 49           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0071705524 |
|    clip_fraction         | 0.0614       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 95.6         |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.0267      |
|    lagrangian_multiplier | 0.0791       |
|    learning_rate         | 0.0003       |
|    loss                  | 50.4         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00547     |
|    std                   | 1.02         |
|    value_loss            | 626          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6250393] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 3            |
|    time_elapsed          | 53           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0073456625 |
|    clip_fraction         | 0.0721       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.155        |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0184      |
|    lagrangian_multiplier | 0.0477       |
|    learning_rate         | 0.0003       |
|    loss                  | 70.1         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.0109      |
|    std                   | 1.02         |
|    value_loss            | 513          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.69196224] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 4             |
|    time_elapsed          | 63            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.0054773698  |
|    clip_fraction         | 0.0461        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 124           |
|    entropy_loss          | -2.86         |
|    explained_variance    | 0.0128        |
|    lagrangian_multiplier | 0.0763        |
|    learning_rate         | 0.0003        |
|    loss                  | 76            |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00673      |
|    std                   | 1.02          |
|    value_loss            | 654           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.81063753] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 4             |
|    time_elapsed          | 65            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.0062328884  |
|    clip_fraction         | 0.0598        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0462        |
|    entropy_loss          | -2.81         |
|    explained_variance    | -0.0905       |
|    lagrangian_multiplier | 0.0713        |
|    learning_rate         | 0.0003        |
|    loss                  | 75.1          |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00661      |
|    std                   | 0.982         |
|    value_loss            | 729           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.5089591] |
| time/                    |              |
|    fps                   | 123          |
|    iterations            | 4            |
|    time_elapsed          | 66           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.0055903327 |
|    clip_fraction         | 0.0419       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 107          |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.0293      |
|    lagrangian_multiplier | 0.0722       |
|    learning_rate         | 0.0003       |
|    loss                  | 67.4         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.0087      |
|    std                   | 1.02         |
|    value_loss            | 612          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.36992696] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 4             |
|    time_elapsed          | 71            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.007132026   |
|    clip_fraction         | 0.0625        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.324         |
|    entropy_loss          | -2.86         |
|    explained_variance    | -0.0232       |
|    lagrangian_multiplier | 0.057         |
|    learning_rate         | 0.0003        |
|    loss                  | 45.5          |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00774      |
|    std                   | 1.01          |
|    value_loss            | 396           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.8982667] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 5            |
|    time_elapsed          | 79           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0046311165 |
|    clip_fraction         | 0.0419       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 83.4         |
|    entropy_loss          | -2.9         |
|    explained_variance    | -0.0527      |
|    lagrangian_multiplier | 0.0461       |
|    learning_rate         | 0.0003       |
|    loss                  | 51.8         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.0066      |
|    std                   | 1.04         |
|    value_loss            | 401          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.665645]  |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 5            |
|    time_elapsed          | 81           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0073621934 |
|    clip_fraction         | 0.0687       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.13         |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0452       |
|    lagrangian_multiplier | 0.0539       |
|    learning_rate         | 0.0003       |
|    loss                  | 49.3         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00925     |
|    std                   | 0.987        |
|    value_loss            | 438          |
-------------------------------------------
------------------------------------------
| reward                   | [-0.982742] |
| time/                    |             |
|    fps                   | 124         |
|    iterations            | 5           |
|    time_elapsed          | 82          |
|    total_timesteps       | 10240       |
| train/                   |             |
|    approx_kl             | 0.007253404 |
|    clip_fraction         | 0.0627      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 46.5        |
|    entropy_loss          | -2.92       |
|    explained_variance    | 0.0157      |
|    lagrangian_multiplier | 0.0511      |
|    learning_rate         | 0.0003      |
|    loss                  | 117         |
|    n_updates             | 40          |
|    policy_gradient_loss  | -0.00805    |
|    std                   | 1.05        |
|    value_loss            | 818         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.73704803] |
| time/                    |               |
|    fps                   | 114           |
|    iterations            | 5             |
|    time_elapsed          | 89            |
|    total_timesteps       | 10240         |
| train/                   |               |
|    approx_kl             | 0.0063214973  |
|    clip_fraction         | 0.0609        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 3.56          |
|    entropy_loss          | -2.86         |
|    explained_variance    | 0.00703       |
|    lagrangian_multiplier | 0.064         |
|    learning_rate         | 0.0003        |
|    loss                  | 70.1          |
|    n_updates             | 40            |
|    policy_gradient_loss  | -0.00836      |
|    std                   | 1.01          |
|    value_loss            | 534           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.49741408] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 6             |
|    time_elapsed          | 96            |
|    total_timesteps       | 12288         |
| train/                   |               |
|    approx_kl             | 0.0051407143  |
|    clip_fraction         | 0.0426        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 39.6          |
|    entropy_loss          | -2.93         |
|    explained_variance    | -0.0745       |
|    lagrangian_multiplier | 0.0432        |
|    learning_rate         | 0.0003        |
|    loss                  | 58            |
|    n_updates             | 50            |
|    policy_gradient_loss  | -0.00446      |
|    std                   | 1.05          |
|    value_loss            | 398           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.4043996] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 6            |
|    time_elapsed          | 97           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0050836285 |
|    clip_fraction         | 0.047        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.34         |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0439      |
|    lagrangian_multiplier | 0.0573       |
|    learning_rate         | 0.0003       |
|    loss                  | 45.9         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00835     |
|    std                   | 0.987        |
|    value_loss            | 405          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.108077]  |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 6            |
|    time_elapsed          | 98           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0065446803 |
|    clip_fraction         | 0.0575       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 133          |
|    entropy_loss          | -2.94        |
|    explained_variance    | -0.0247      |
|    lagrangian_multiplier | 0.0693       |
|    learning_rate         | 0.0003       |
|    loss                  | 56.3         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00821     |
|    std                   | 1.05         |
|    value_loss            | 488          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4567741] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 6            |
|    time_elapsed          | 107          |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0049004694 |
|    clip_fraction         | 0.0401       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 8.55         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0184       |
|    lagrangian_multiplier | 0.0631       |
|    learning_rate         | 0.0003       |
|    loss                  | 66.5         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00637     |
|    std                   | 1            |
|    value_loss            | 633          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.463667] |
| time/                    |             |
|    fps                   | 127         |
|    iterations            | 7           |
|    time_elapsed          | 112         |
|    total_timesteps       | 14336       |
| train/                   |             |
|    approx_kl             | 0.004107261 |
|    clip_fraction         | 0.0182      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 125         |
|    entropy_loss          | -2.94       |
|    explained_variance    | -0.033      |
|    lagrangian_multiplier | 0.0754      |
|    learning_rate         | 0.0003      |
|    loss                  | 44.4        |
|    n_updates             | 60          |
|    policy_gradient_loss  | -0.00562    |
|    std                   | 1.05        |
|    value_loss            | 339         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.91549414] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 7             |
|    time_elapsed          | 114           |
|    total_timesteps       | 14336         |
| train/                   |               |
|    approx_kl             | 0.00693246    |
|    clip_fraction         | 0.0777        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0388        |
|    entropy_loss          | -2.8          |
|    explained_variance    | 0.0647        |
|    lagrangian_multiplier | 0.059         |
|    learning_rate         | 0.0003        |
|    loss                  | 47.9          |
|    n_updates             | 60            |
|    policy_gradient_loss  | -0.00914      |
|    std                   | 0.974         |
|    value_loss            | 455           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.69072735] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 7             |
|    time_elapsed          | 114           |
|    total_timesteps       | 14336         |
| train/                   |               |
|    approx_kl             | 0.0050184047  |
|    clip_fraction         | 0.0408        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 88.7          |
|    entropy_loss          | -2.92         |
|    explained_variance    | 0.019         |
|    lagrangian_multiplier | 0.0496        |
|    learning_rate         | 0.0003        |
|    loss                  | 89.1          |
|    n_updates             | 60            |
|    policy_gradient_loss  | -0.00722      |
|    std                   | 1.03          |
|    value_loss            | 571           |
--------------------------------------------
srun: Job 114547 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation temporarily disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.61622524] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 7             |
|    time_elapsed          | 125           |
|    total_timesteps       | 14336         |
| train/                   |               |
|    approx_kl             | 0.005448526   |
|    clip_fraction         | 0.0573        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 4.5           |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.0482       |
|    lagrangian_multiplier | 0.0619        |
|    learning_rate         | 0.0003        |
|    loss                  | 50.3          |
|    n_updates             | 60            |
|    policy_gradient_loss  | -0.00766      |
|    std                   | 0.994         |
|    value_loss            | 417           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.8626663] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 8            |
|    time_elapsed          | 128          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0069957324 |
|    clip_fraction         | 0.0584       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 39.8         |
|    entropy_loss          | -2.93        |
|    explained_variance    | 0.00696      |
|    lagrangian_multiplier | 0.0564       |
|    learning_rate         | 0.0003       |
|    loss                  | 31.2         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00862     |
|    std                   | 1.05         |
|    value_loss            | 243          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.30272]   |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 8            |
|    time_elapsed          | 130          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0047495468 |
|    clip_fraction         | 0.0353       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0297       |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0383      |
|    lagrangian_multiplier | 0.0616       |
|    learning_rate         | 0.0003       |
|    loss                  | 50.5         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00747     |
|    std                   | 0.959        |
|    value_loss            | 501          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5348923] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 8            |
|    time_elapsed          | 131          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0062941816 |
|    clip_fraction         | 0.0499       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 21.7         |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.00397      |
|    lagrangian_multiplier | 0.0594       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.8         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00668     |
|    std                   | 1.04         |
|    value_loss            | 539          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.323219] |
| time/                    |             |
|    fps                   | 127         |
|    iterations            | 9           |
|    time_elapsed          | 144         |
|    total_timesteps       | 18432       |
| train/                   |             |
|    approx_kl             | 0.00611872  |
|    clip_fraction         | 0.058       |
|    clip_range            | 0.2         |
|    cost_value_loss       | 136         |
|    entropy_loss          | -2.94       |
|    explained_variance    | 0.0207      |
|    lagrangian_multiplier | 0.0623      |
|    learning_rate         | 0.0003      |
|    loss                  | 70          |
|    n_updates             | 80          |
|    policy_gradient_loss  | -0.00712    |
|    std                   | 1.06        |
|    value_loss            | 562         |
------------------------------------------
-------------------------------------------
| reward                   | [-0.8347713] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 8            |
|    time_elapsed          | 144          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0071274485 |
|    clip_fraction         | 0.0542       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.171        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.00219      |
|    lagrangian_multiplier | 0.0593       |
|    learning_rate         | 0.0003       |
|    loss                  | 76.6         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00609     |
|    std                   | 0.98         |
|    value_loss            | 772          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6092933] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 9            |
|    time_elapsed          | 147          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.007078791  |
|    clip_fraction         | 0.0593       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.412        |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.00372     |
|    lagrangian_multiplier | 0.0513       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.9         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00869     |
|    std                   | 0.954        |
|    value_loss            | 374          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6339977] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 9            |
|    time_elapsed          | 147          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.004023133  |
|    clip_fraction         | 0.0365       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 48.5         |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.013        |
|    lagrangian_multiplier | 0.0567       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.4         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00572     |
|    std                   | 1.03         |
|    value_loss            | 518          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6428393] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 10           |
|    time_elapsed          | 160          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0061484585 |
|    clip_fraction         | 0.051        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 115          |
|    entropy_loss          | -2.97        |
|    explained_variance    | -0.00416     |
|    lagrangian_multiplier | 0.0626       |
|    learning_rate         | 0.0003       |
|    loss                  | 38.7         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00741     |
|    std                   | 1.07         |
|    value_loss            | 441          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.73276854] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 9             |
|    time_elapsed          | 162           |
|    total_timesteps       | 18432         |
| train/                   |               |
|    approx_kl             | 0.006698884   |
|    clip_fraction         | 0.0717        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.278         |
|    entropy_loss          | -2.79         |
|    explained_variance    | -0.016        |
|    lagrangian_multiplier | 0.0633        |
|    learning_rate         | 0.0003        |
|    loss                  | 56.7          |
|    n_updates             | 80            |
|    policy_gradient_loss  | -0.00728      |
|    std                   | 0.973         |
|    value_loss            | 540           |
--------------------------------------------
------------------------------------------
| reward                   | [-1.352463] |
| time/                    |             |
|    fps                   | 124         |
|    iterations            | 10          |
|    time_elapsed          | 163         |
|    total_timesteps       | 20480       |
| train/                   |             |
|    approx_kl             | 0.004233207 |
|    clip_fraction         | 0.0514      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0673      |
|    entropy_loss          | -2.75       |
|    explained_variance    | 0.000454    |
|    lagrangian_multiplier | 0.0571      |
|    learning_rate         | 0.0003      |
|    loss                  | 61.7        |
|    n_updates             | 90          |
|    policy_gradient_loss  | -0.00565    |
|    std                   | 0.958       |
|    value_loss            | 577         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.73928833] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 10            |
|    time_elapsed          | 163           |
|    total_timesteps       | 20480         |
| train/                   |               |
|    approx_kl             | 0.005688424   |
|    clip_fraction         | 0.0472        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 64.8          |
|    entropy_loss          | -2.9          |
|    explained_variance    | 0.0749        |
|    lagrangian_multiplier | 0.0631        |
|    learning_rate         | 0.0003        |
|    loss                  | 51            |
|    n_updates             | 90            |
|    policy_gradient_loss  | -0.00689      |
|    std                   | 1.03          |
|    value_loss            | 426           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2249688] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 11           |
|    time_elapsed          | 177          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.008346139  |
|    clip_fraction         | 0.0806       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 171          |
|    entropy_loss          | -2.98        |
|    explained_variance    | -0.0427      |
|    lagrangian_multiplier | 0.0486       |
|    learning_rate         | 0.0003       |
|    loss                  | 98.7         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.0107      |
|    std                   | 1.07         |
|    value_loss            | 667          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9253005] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 11           |
|    time_elapsed          | 180          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.007092514  |
|    clip_fraction         | 0.0601       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 5.28         |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.0146       |
|    lagrangian_multiplier | 0.0512       |
|    learning_rate         | 0.0003       |
|    loss                  | 130          |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00568     |
|    std                   | 0.948        |
|    value_loss            | 1.1e+03      |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3345532] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 11           |
|    time_elapsed          | 180          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0057352865 |
|    clip_fraction         | 0.0531       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 51.8         |
|    entropy_loss          | -2.9         |
|    explained_variance    | -0.00168     |
|    lagrangian_multiplier | 0.0749       |
|    learning_rate         | 0.0003       |
|    loss                  | 31.8         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00882     |
|    std                   | 1.04         |
|    value_loss            | 446          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.017339]  |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 10           |
|    time_elapsed          | 180          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0042004213 |
|    clip_fraction         | 0.0362       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.233        |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0204      |
|    lagrangian_multiplier | 0.0655       |
|    learning_rate         | 0.0003       |
|    loss                  | 40.6         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00611     |
|    std                   | 0.983        |
|    value_loss            | 512          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1475248] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 12           |
|    time_elapsed          | 193          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.006224468  |
|    clip_fraction         | 0.0552       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 176          |
|    entropy_loss          | -2.97        |
|    explained_variance    | -0.0262      |
|    lagrangian_multiplier | 0.0496       |
|    learning_rate         | 0.0003       |
|    loss                  | 82.6         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00822     |
|    std                   | 1.06         |
|    value_loss            | 513          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8262486] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 12           |
|    time_elapsed          | 196          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.005635701  |
|    clip_fraction         | 0.0505       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.4          |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.0211      |
|    lagrangian_multiplier | 0.0526       |
|    learning_rate         | 0.0003       |
|    loss                  | 77.3         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00532     |
|    std                   | 0.939        |
|    value_loss            | 668          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2814049] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 12           |
|    time_elapsed          | 196          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0067353225 |
|    clip_fraction         | 0.0665       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 64.5         |
|    entropy_loss          | -2.9         |
|    explained_variance    | -0.0098      |
|    lagrangian_multiplier | 0.065        |
|    learning_rate         | 0.0003       |
|    loss                  | 68.1         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00898     |
|    std                   | 1.03         |
|    value_loss            | 612          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.1537535] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 11           |
|    time_elapsed          | 198          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.004816805  |
|    clip_fraction         | 0.0304       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 10.9         |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0086      |
|    lagrangian_multiplier | 0.0663       |
|    learning_rate         | 0.0003       |
|    loss                  | 41.9         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00501     |
|    std                   | 0.986        |
|    value_loss            | 393          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6300924] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 13           |
|    time_elapsed          | 209          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0055457717 |
|    clip_fraction         | 0.0462       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 129          |
|    entropy_loss          | -2.96        |
|    explained_variance    | -0.0254      |
|    lagrangian_multiplier | 0.0628       |
|    learning_rate         | 0.0003       |
|    loss                  | 68.6         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.007       |
|    std                   | 1.07         |
|    value_loss            | 557          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.1466837] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 13           |
|    time_elapsed          | 213          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.007381505  |
|    clip_fraction         | 0.0622       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 2.73         |
|    entropy_loss          | -2.7         |
|    explained_variance    | 0.00449      |
|    lagrangian_multiplier | 0.057        |
|    learning_rate         | 0.0003       |
|    loss                  | 111          |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00791     |
|    std                   | 0.927        |
|    value_loss            | 979          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4657555] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 13           |
|    time_elapsed          | 212          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0054678163 |
|    clip_fraction         | 0.0523       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 64.4         |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.0232      |
|    lagrangian_multiplier | 0.0624       |
|    learning_rate         | 0.0003       |
|    loss                  | 43.7         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00847     |
|    std                   | 1.03         |
|    value_loss            | 396          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2541778] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 12           |
|    time_elapsed          | 216          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.005968958  |
|    clip_fraction         | 0.0526       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.96         |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0129      |
|    lagrangian_multiplier | 0.0696       |
|    learning_rate         | 0.0003       |
|    loss                  | 76.8         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00697     |
|    std                   | 0.984        |
|    value_loss            | 651          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.3251102] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 14           |
|    time_elapsed          | 225          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.003772643  |
|    clip_fraction         | 0.0213       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 117          |
|    entropy_loss          | -2.96        |
|    explained_variance    | -0.0562      |
|    lagrangian_multiplier | 0.0552       |
|    learning_rate         | 0.0003       |
|    loss                  | 54.3         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00455     |
|    std                   | 1.06         |
|    value_loss            | 373          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6942431] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 14           |
|    time_elapsed          | 229          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.005057202  |
|    clip_fraction         | 0.0523       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0535       |
|    entropy_loss          | -2.69        |
|    explained_variance    | 0.00933      |
|    lagrangian_multiplier | 0.0583       |
|    learning_rate         | 0.0003       |
|    loss                  | 100          |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.0065      |
|    std                   | 0.933        |
|    value_loss            | 1.08e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8311553] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 14           |
|    time_elapsed          | 229          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0064321165 |
|    clip_fraction         | 0.0775       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 74.7         |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0193       |
|    lagrangian_multiplier | 0.0551       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.4         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 1.02         |
|    value_loss            | 486          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0358793] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 13           |
|    time_elapsed          | 234          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0052200016 |
|    clip_fraction         | 0.0453       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.829        |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0037       |
|    lagrangian_multiplier | 0.0603       |
|    learning_rate         | 0.0003       |
|    loss                  | 102          |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00692     |
|    std                   | 0.968        |
|    value_loss            | 978          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.6512072] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 15           |
|    time_elapsed          | 242          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0074241315 |
|    clip_fraction         | 0.0773       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 200          |
|    entropy_loss          | -2.94        |
|    explained_variance    | -0.0147      |
|    lagrangian_multiplier | 0.0664       |
|    learning_rate         | 0.0003       |
|    loss                  | 81.8         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.0105      |
|    std                   | 1.05         |
|    value_loss            | 628          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.7251394] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 15           |
|    time_elapsed          | 246          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.007092585  |
|    clip_fraction         | 0.0694       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.5          |
|    entropy_loss          | -2.7         |
|    explained_variance    | -0.00368     |
|    lagrangian_multiplier | 0.0605       |
|    learning_rate         | 0.0003       |
|    loss                  | 94.7         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00922     |
|    std                   | 0.937        |
|    value_loss            | 881          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.997791]  |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 15           |
|    time_elapsed          | 245          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0060789324 |
|    clip_fraction         | 0.0546       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 99.4         |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.0225      |
|    lagrangian_multiplier | 0.0518       |
|    learning_rate         | 0.0003       |
|    loss                  | 83.7         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00747     |
|    std                   | 1.01         |
|    value_loss            | 546          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.7392908] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 14           |
|    time_elapsed          | 252          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0047969287 |
|    clip_fraction         | 0.0333       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.343        |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0313      |
|    lagrangian_multiplier | 0.0602       |
|    learning_rate         | 0.0003       |
|    loss                  | 59.5         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00401     |
|    std                   | 0.976        |
|    value_loss            | 599          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.3701261] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 16           |
|    time_elapsed          | 258          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0063317427 |
|    clip_fraction         | 0.0594       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 139          |
|    entropy_loss          | -2.93        |
|    explained_variance    | -0.023       |
|    lagrangian_multiplier | 0.0613       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.5         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.0106      |
|    std                   | 1.05         |
|    value_loss            | 533          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.767806]  |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 16           |
|    time_elapsed          | 262          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0066309306 |
|    clip_fraction         | 0.0729       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.55         |
|    entropy_loss          | -2.7         |
|    explained_variance    | -0.0518      |
|    lagrangian_multiplier | 0.063        |
|    learning_rate         | 0.0003       |
|    loss                  | 67.4         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00917     |
|    std                   | 0.933        |
|    value_loss            | 709          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.02894]   |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 16           |
|    time_elapsed          | 262          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0045281034 |
|    clip_fraction         | 0.0416       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 35.7         |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0501       |
|    lagrangian_multiplier | 0.0554       |
|    learning_rate         | 0.0003       |
|    loss                  | 50.7         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00596     |
|    std                   | 1.02         |
|    value_loss            | 517          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.94595146] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 15            |
|    time_elapsed          | 270           |
|    total_timesteps       | 30720         |
| train/                   |               |
|    approx_kl             | 0.006067439   |
|    clip_fraction         | 0.0609        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.802         |
|    entropy_loss          | -2.77         |
|    explained_variance    | 0.0146        |
|    lagrangian_multiplier | 0.0511        |
|    learning_rate         | 0.0003        |
|    loss                  | 56.6          |
|    n_updates             | 140           |
|    policy_gradient_loss  | -0.00657      |
|    std                   | 0.966         |
|    value_loss            | 550           |
--------------------------------------------
-------------------------------------------
| reward                   | [-3.0963295] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 17           |
|    time_elapsed          | 274          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0052437526 |
|    clip_fraction         | 0.0412       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 166          |
|    entropy_loss          | -2.94        |
|    explained_variance    | -0.0699      |
|    lagrangian_multiplier | 0.0811       |
|    learning_rate         | 0.0003       |
|    loss                  | 44.2         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00692     |
|    std                   | 1.05         |
|    value_loss            | 352          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.607123]  |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 17           |
|    time_elapsed          | 279          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0038137254 |
|    clip_fraction         | 0.031        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.979        |
|    entropy_loss          | -2.7         |
|    explained_variance    | 0.00463      |
|    lagrangian_multiplier | 0.0607       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.3         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00458     |
|    std                   | 0.936        |
|    value_loss            | 663          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5381535] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 17           |
|    time_elapsed          | 278          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.006243289  |
|    clip_fraction         | 0.0671       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 67.7         |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0124       |
|    lagrangian_multiplier | 0.0518       |
|    learning_rate         | 0.0003       |
|    loss                  | 104          |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00723     |
|    std                   | 1.02         |
|    value_loss            | 876          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.1657908] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 16           |
|    time_elapsed          | 288          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.005050692  |
|    clip_fraction         | 0.0601       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.252        |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0347      |
|    lagrangian_multiplier | 0.0645       |
|    learning_rate         | 0.0003       |
|    loss                  | 63.9         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00849     |
|    std                   | 0.96         |
|    value_loss            | 584          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9899366] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 18           |
|    time_elapsed          | 290          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0076199877 |
|    clip_fraction         | 0.0745       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 102          |
|    entropy_loss          | -2.91        |
|    explained_variance    | -0.0185      |
|    lagrangian_multiplier | 0.0485       |
|    learning_rate         | 0.0003       |
|    loss                  | 95.7         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.0135      |
|    std                   | 1.03         |
|    value_loss            | 674          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.6801283] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 18           |
|    time_elapsed          | 295          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0059398375 |
|    clip_fraction         | 0.056        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0396       |
|    entropy_loss          | -2.7         |
|    explained_variance    | 0.0153       |
|    lagrangian_multiplier | 0.0616       |
|    learning_rate         | 0.0003       |
|    loss                  | 79.1         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00798     |
|    std                   | 0.93         |
|    value_loss            | 716          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7267438] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 18           |
|    time_elapsed          | 295          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0050664423 |
|    clip_fraction         | 0.0307       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 33.3         |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.0182      |
|    lagrangian_multiplier | 0.0639       |
|    learning_rate         | 0.0003       |
|    loss                  | 32.9         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00418     |
|    std                   | 1.01         |
|    value_loss            | 291          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.063855]  |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 19           |
|    time_elapsed          | 306          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0049677733 |
|    clip_fraction         | 0.0381       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 26.6         |
|    entropy_loss          | -2.9         |
|    explained_variance    | -0.0401      |
|    lagrangian_multiplier | 0.0736       |
|    learning_rate         | 0.0003       |
|    loss                  | 43.7         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00975     |
|    std                   | 1.04         |
|    value_loss            | 393          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6681283] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 17           |
|    time_elapsed          | 306          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.005155341  |
|    clip_fraction         | 0.0495       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.368        |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0276      |
|    lagrangian_multiplier | 0.0709       |
|    learning_rate         | 0.0003       |
|    loss                  | 28.7         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00925     |
|    std                   | 0.971        |
|    value_loss            | 444          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7492067] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 19           |
|    time_elapsed          | 312          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0065979175 |
|    clip_fraction         | 0.0587       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.52         |
|    entropy_loss          | -2.7         |
|    explained_variance    | -0.000551    |
|    lagrangian_multiplier | 0.0557       |
|    learning_rate         | 0.0003       |
|    loss                  | 88.8         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00578     |
|    std                   | 0.935        |
|    value_loss            | 826          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6095622] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 19           |
|    time_elapsed          | 311          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.005791488  |
|    clip_fraction         | 0.059        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.2          |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.088       |
|    lagrangian_multiplier | 0.081        |
|    learning_rate         | 0.0003       |
|    loss                  | 29           |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00936     |
|    std                   | 1.01         |
|    value_loss            | 379          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3895245] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 20           |
|    time_elapsed          | 322          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0063490355 |
|    clip_fraction         | 0.0678       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 93.8         |
|    entropy_loss          | -2.91        |
|    explained_variance    | -0.0368      |
|    lagrangian_multiplier | 0.0487       |
|    learning_rate         | 0.0003       |
|    loss                  | 145          |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00784     |
|    std                   | 1.04         |
|    value_loss            | 913          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.6396503] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 18           |
|    time_elapsed          | 325          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0043130517 |
|    clip_fraction         | 0.0255       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 2.09         |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.00589      |
|    lagrangian_multiplier | 0.0553       |
|    learning_rate         | 0.0003       |
|    loss                  | 83.8         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00455     |
|    std                   | 0.964        |
|    value_loss            | 785          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6624212] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 20           |
|    time_elapsed          | 328          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0067014284 |
|    clip_fraction         | 0.0661       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.31         |
|    entropy_loss          | -2.71        |
|    explained_variance    | 0.0017       |
|    lagrangian_multiplier | 0.059        |
|    learning_rate         | 0.0003       |
|    loss                  | 73.7         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00769     |
|    std                   | 0.938        |
|    value_loss            | 766          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.5803344] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 20           |
|    time_elapsed          | 327          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0062495307 |
|    clip_fraction         | 0.0671       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 32           |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0268       |
|    lagrangian_multiplier | 0.0503       |
|    learning_rate         | 0.0003       |
|    loss                  | 69.7         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00643     |
|    std                   | 1            |
|    value_loss            | 602          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9551433] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 21           |
|    time_elapsed          | 339          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.009146634  |
|    clip_fraction         | 0.0781       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 130          |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.00784     |
|    lagrangian_multiplier | 0.046        |
|    learning_rate         | 0.0003       |
|    loss                  | 92.4         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00984     |
|    std                   | 1.02         |
|    value_loss            | 636          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.5404212] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 19           |
|    time_elapsed          | 343          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0065321843 |
|    clip_fraction         | 0.0638       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.207        |
|    entropy_loss          | -2.74        |
|    explained_variance    | -0.0167      |
|    lagrangian_multiplier | 0.0649       |
|    learning_rate         | 0.0003       |
|    loss                  | 98.8         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00898     |
|    std                   | 0.949        |
|    value_loss            | 1e+03        |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7198861] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 21           |
|    time_elapsed          | 344          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.006390892  |
|    clip_fraction         | 0.0586       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.666        |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.0161      |
|    lagrangian_multiplier | 0.0571       |
|    learning_rate         | 0.0003       |
|    loss                  | 73           |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00874     |
|    std                   | 0.948        |
|    value_loss            | 655          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.961037]  |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 21           |
|    time_elapsed          | 344          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0058814376 |
|    clip_fraction         | 0.0531       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 11.4         |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0517       |
|    lagrangian_multiplier | 0.0549       |
|    learning_rate         | 0.0003       |
|    loss                  | 73.7         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00686     |
|    std                   | 0.983        |
|    value_loss            | 723          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2654402] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 22           |
|    time_elapsed          | 355          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.0059943977 |
|    clip_fraction         | 0.0503       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 170          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0564      |
|    lagrangian_multiplier | 0.0492       |
|    learning_rate         | 0.0003       |
|    loss                  | 74           |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.0074      |
|    std                   | 1.01         |
|    value_loss            | 379          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.36425674] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 22            |
|    time_elapsed          | 360           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.005181354   |
|    clip_fraction         | 0.0595        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 92.5          |
|    entropy_loss          | -2.81         |
|    explained_variance    | -0.00358      |
|    lagrangian_multiplier | 0.0639        |
|    learning_rate         | 0.0003        |
|    loss                  | 68.6          |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.00783      |
|    std                   | 0.992         |
|    value_loss            | 560           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.30231366] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 22            |
|    time_elapsed          | 361           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.006266061   |
|    clip_fraction         | 0.044         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 1.79          |
|    entropy_loss          | -2.7          |
|    explained_variance    | -0.00876      |
|    lagrangian_multiplier | 0.0547        |
|    learning_rate         | 0.0003        |
|    loss                  | 62.9          |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.00721      |
|    std                   | 0.918         |
|    value_loss            | 537           |
--------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.5443573] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 20           |
|    time_elapsed          | 361          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0061472924 |
|    clip_fraction         | 0.0644       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.593        |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.0103      |
|    lagrangian_multiplier | 0.0535       |
|    learning_rate         | 0.0003       |
|    loss                  | 85.9         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.0087      |
|    std                   | 0.951        |
|    value_loss            | 704          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.8590789] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 23           |
|    time_elapsed          | 372          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.006252519  |
|    clip_fraction         | 0.0585       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 54.1         |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0197      |
|    lagrangian_multiplier | 0.0695       |
|    learning_rate         | 0.0003       |
|    loss                  | 49.6         |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.00597     |
|    std                   | 1.01         |
|    value_loss            | 492          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.50093025] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 23            |
|    time_elapsed          | 377           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.006072084   |
|    clip_fraction         | 0.0604        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 36.7          |
|    entropy_loss          | -2.81         |
|    explained_variance    | 0.0148        |
|    lagrangian_multiplier | 0.0564        |
|    learning_rate         | 0.0003        |
|    loss                  | 44.5          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00897      |
|    std                   | 0.983         |
|    value_loss            | 380           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.73920643] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 23            |
|    time_elapsed          | 378           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.0052603777  |
|    clip_fraction         | 0.0463        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0233        |
|    entropy_loss          | -2.66         |
|    explained_variance    | 0.0197        |
|    lagrangian_multiplier | 0.0648        |
|    learning_rate         | 0.0003        |
|    loss                  | 56.5          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00736      |
|    std                   | 0.909         |
|    value_loss            | 712           |
--------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.467673]  |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 21           |
|    time_elapsed          | 379          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0046277554 |
|    clip_fraction         | 0.0539       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.2          |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.00989      |
|    lagrangian_multiplier | 0.048        |
|    learning_rate         | 0.0003       |
|    loss                  | 84.4         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00692     |
|    std                   | 0.953        |
|    value_loss            | 798          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.5550744] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 24           |
|    time_elapsed          | 388          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.007048602  |
|    clip_fraction         | 0.0811       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 75.1         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0157      |
|    lagrangian_multiplier | 0.0412       |
|    learning_rate         | 0.0003       |
|    loss                  | 120          |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.0105      |
|    std                   | 0.998        |
|    value_loss            | 772          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3441743] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 24           |
|    time_elapsed          | 393          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.0073623895 |
|    clip_fraction         | 0.063        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 48.6         |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0685      |
|    lagrangian_multiplier | 0.0843       |
|    learning_rate         | 0.0003       |
|    loss                  | 80.9         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00943     |
|    std                   | 0.977        |
|    value_loss            | 970          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1701528] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 24           |
|    time_elapsed          | 394          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.0055069383 |
|    clip_fraction         | 0.0401       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.172        |
|    entropy_loss          | -2.65        |
|    explained_variance    | -0.0457      |
|    lagrangian_multiplier | 0.058        |
|    learning_rate         | 0.0003       |
|    loss                  | 78.7         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.0072      |
|    std                   | 0.91         |
|    value_loss            | 732          |
-------------------------------------------
------------------------------------------
| reward                   | [-0.691239] |
| time/                    |             |
|    fps                   | 113         |
|    iterations            | 22          |
|    time_elapsed          | 397         |
|    total_timesteps       | 45056       |
| train/                   |             |
|    approx_kl             | 0.007124788 |
|    clip_fraction         | 0.0691      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.237       |
|    entropy_loss          | -2.73       |
|    explained_variance    | 0.000718    |
|    lagrangian_multiplier | 0.0574      |
|    learning_rate         | 0.0003      |
|    loss                  | 48.4        |
|    n_updates             | 210         |
|    policy_gradient_loss  | -0.00874    |
|    std                   | 0.943       |
|    value_loss            | 448         |
------------------------------------------
-------------------------------------------
| reward                   | [-2.5015447] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 25           |
|    time_elapsed          | 404          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.0045537157 |
|    clip_fraction         | 0.0373       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 43.4         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0266      |
|    lagrangian_multiplier | 0.0463       |
|    learning_rate         | 0.0003       |
|    loss                  | 41.8         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00644     |
|    std                   | 1.01         |
|    value_loss            | 282          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6824674] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 25           |
|    time_elapsed          | 409          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.007753401  |
|    clip_fraction         | 0.0759       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 53.8         |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0401       |
|    lagrangian_multiplier | 0.055        |
|    learning_rate         | 0.0003       |
|    loss                  | 106          |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00887     |
|    std                   | 0.973        |
|    value_loss            | 863          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2463372] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 25           |
|    time_elapsed          | 411          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.0056645535 |
|    clip_fraction         | 0.0711       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.93         |
|    entropy_loss          | -2.64        |
|    explained_variance    | -0.0291      |
|    lagrangian_multiplier | 0.0558       |
|    learning_rate         | 0.0003       |
|    loss                  | 68.6         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00836     |
|    std                   | 0.907        |
|    value_loss            | 677          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0466402] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 23           |
|    time_elapsed          | 415          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.0062903734 |
|    clip_fraction         | 0.0577       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.28         |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.0166      |
|    lagrangian_multiplier | 0.045        |
|    learning_rate         | 0.0003       |
|    loss                  | 39.6         |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.00727     |
|    std                   | 0.948        |
|    value_loss            | 351          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.0502777] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 26           |
|    time_elapsed          | 420          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0063905744 |
|    clip_fraction         | 0.0701       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 100          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.00133     |
|    lagrangian_multiplier | 0.0583       |
|    learning_rate         | 0.0003       |
|    loss                  | 67.4         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 1.01         |
|    value_loss            | 534          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6376466] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 26           |
|    time_elapsed          | 426          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.008597116  |
|    clip_fraction         | 0.0821       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 94           |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0391      |
|    lagrangian_multiplier | 0.0609       |
|    learning_rate         | 0.0003       |
|    loss                  | 121          |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.972        |
|    value_loss            | 1.09e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3175701] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 26           |
|    time_elapsed          | 427          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0090346895 |
|    clip_fraction         | 0.0766       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 5.12         |
|    entropy_loss          | -2.64        |
|    explained_variance    | -0.0295      |
|    lagrangian_multiplier | 0.0505       |
|    learning_rate         | 0.0003       |
|    loss                  | 82.8         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.0107      |
|    std                   | 0.905        |
|    value_loss            | 729          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6657012] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 24           |
|    time_elapsed          | 434          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.007735943  |
|    clip_fraction         | 0.0806       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 2.64         |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.00967      |
|    lagrangian_multiplier | 0.0538       |
|    learning_rate         | 0.0003       |
|    loss                  | 57           |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.0117      |
|    std                   | 0.957        |
|    value_loss            | 503          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7769303] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 27           |
|    time_elapsed          | 437          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.009541277  |
|    clip_fraction         | 0.1          |
|    clip_range            | 0.2          |
|    cost_value_loss       | 195          |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0108       |
|    lagrangian_multiplier | 0.0409       |
|    learning_rate         | 0.0003       |
|    loss                  | 182          |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.0135      |
|    std                   | 1.01         |
|    value_loss            | 1.28e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8179786] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 27           |
|    time_elapsed          | 442          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.0053909263 |
|    clip_fraction         | 0.0667       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 62.8         |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0403       |
|    lagrangian_multiplier | 0.0499       |
|    learning_rate         | 0.0003       |
|    loss                  | 168          |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00773     |
|    std                   | 0.964        |
|    value_loss            | 1.59e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5182579] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 27           |
|    time_elapsed          | 444          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.0060484894 |
|    clip_fraction         | 0.07         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 8.34         |
|    entropy_loss          | -2.64        |
|    explained_variance    | -0.0236      |
|    lagrangian_multiplier | 0.0583       |
|    learning_rate         | 0.0003       |
|    loss                  | 102          |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00822     |
|    std                   | 0.905        |
|    value_loss            | 942          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9185116] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 28           |
|    time_elapsed          | 453          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.0077910507 |
|    clip_fraction         | 0.0737       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 172          |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0163      |
|    lagrangian_multiplier | 0.0452       |
|    learning_rate         | 0.0003       |
|    loss                  | 181          |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.0109      |
|    std                   | 1            |
|    value_loss            | 1.19e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5745952] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 25           |
|    time_elapsed          | 452          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.00593644   |
|    clip_fraction         | 0.0384       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.86         |
|    entropy_loss          | -2.74        |
|    explained_variance    | -0.0207      |
|    lagrangian_multiplier | 0.058        |
|    learning_rate         | 0.0003       |
|    loss                  | 66.8         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00738     |
|    std                   | 0.948        |
|    value_loss            | 607          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0009148] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 28           |
|    time_elapsed          | 458          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.008963415  |
|    clip_fraction         | 0.0925       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 181          |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.0364       |
|    lagrangian_multiplier | 0.0495       |
|    learning_rate         | 0.0003       |
|    loss                  | 303          |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.0107      |
|    std                   | 0.961        |
|    value_loss            | 2.29e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6603009] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 28           |
|    time_elapsed          | 460          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.0064566922 |
|    clip_fraction         | 0.0558       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 10           |
|    entropy_loss          | -2.63        |
|    explained_variance    | -0.0251      |
|    lagrangian_multiplier | 0.059        |
|    learning_rate         | 0.0003       |
|    loss                  | 73.9         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00634     |
|    std                   | 0.901        |
|    value_loss            | 768          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1953952] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 29           |
|    time_elapsed          | 469          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.0064921696 |
|    clip_fraction         | 0.0602       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 167          |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0456      |
|    lagrangian_multiplier | 0.0557       |
|    learning_rate         | 0.0003       |
|    loss                  | 93.5         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00888     |
|    std                   | 0.994        |
|    value_loss            | 773          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7450341] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 26           |
|    time_elapsed          | 470          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.005129368  |
|    clip_fraction         | 0.0477       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.36         |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.0161      |
|    lagrangian_multiplier | 0.0575       |
|    learning_rate         | 0.0003       |
|    loss                  | 32.4         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.0076      |
|    std                   | 0.94         |
|    value_loss            | 317          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.4298881] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 29           |
|    time_elapsed          | 477          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.0062258476 |
|    clip_fraction         | 0.0698       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 2.75         |
|    entropy_loss          | -2.64        |
|    explained_variance    | -0.0679      |
|    lagrangian_multiplier | 0.0571       |
|    learning_rate         | 0.0003       |
|    loss                  | 94.1         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00777     |
|    std                   | 0.91         |
|    value_loss            | 854          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.36471856] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 30            |
|    time_elapsed          | 485           |
|    total_timesteps       | 61440         |
| train/                   |               |
|    approx_kl             | 0.007501402   |
|    clip_fraction         | 0.0744        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 97.1          |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.0706       |
|    lagrangian_multiplier | 0.0637        |
|    learning_rate         | 0.0003        |
|    loss                  | 43.6          |
|    n_updates             | 290           |
|    policy_gradient_loss  | -0.0124       |
|    std                   | 1             |
|    value_loss            | 483           |
--------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.6268519] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 27           |
|    time_elapsed          | 488          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.005511703  |
|    clip_fraction         | 0.0481       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.104        |
|    entropy_loss          | -2.7         |
|    explained_variance    | -0.0119      |
|    lagrangian_multiplier | 0.0478       |
|    learning_rate         | 0.0003       |
|    loss                  | 101          |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00586     |
|    std                   | 0.933        |
|    value_loss            | 771          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.559966]  |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 30           |
|    time_elapsed          | 493          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0066236136 |
|    clip_fraction         | 0.0572       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.312        |
|    entropy_loss          | -2.65        |
|    explained_variance    | -0.0182      |
|    lagrangian_multiplier | 0.0658       |
|    learning_rate         | 0.0003       |
|    loss                  | 81.3         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00644     |
|    std                   | 0.908        |
|    value_loss            | 798          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.3635176] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 31           |
|    time_elapsed          | 501          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0051427907 |
|    clip_fraction         | 0.0408       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 74.5         |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0691      |
|    lagrangian_multiplier | 0.0759       |
|    learning_rate         | 0.0003       |
|    loss                  | 17.7         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00536     |
|    std                   | 0.989        |
|    value_loss            | 171          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.32972047] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 28            |
|    time_elapsed          | 507           |
|    total_timesteps       | 57344         |
| train/                   |               |
|    approx_kl             | 0.0066931066  |
|    clip_fraction         | 0.0656        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 19.2          |
|    entropy_loss          | -2.69         |
|    explained_variance    | 0.0165        |
|    lagrangian_multiplier | 0.0592        |
|    learning_rate         | 0.0003        |
|    loss                  | 129           |
|    n_updates             | 270           |
|    policy_gradient_loss  | -0.0114       |
|    std                   | 0.93          |
|    value_loss            | 1.03e+03      |
--------------------------------------------
------------------------------------------
| reward                   | [-1.542764] |
| time/                    |             |
|    fps                   | 124         |
|    iterations            | 31          |
|    time_elapsed          | 510         |
|    total_timesteps       | 63488       |
| train/                   |             |
|    approx_kl             | 0.008705378 |
|    clip_fraction         | 0.087       |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.613       |
|    entropy_loss          | -2.64       |
|    explained_variance    | 0.00571     |
|    lagrangian_multiplier | 0.0603      |
|    learning_rate         | 0.0003      |
|    loss                  | 62.3        |
|    n_updates             | 300         |
|    policy_gradient_loss  | -0.00907    |
|    std                   | 0.902       |
|    value_loss            | 637         |
------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-2.119744] |
| time/                    |             |
|    fps                   | 115         |
|    iterations            | 29          |
|    time_elapsed          | 515         |
|    total_timesteps       | 59392       |
| train/                   |             |
|    approx_kl             | 0.005429346 |
|    clip_fraction         | 0.049       |
|    clip_range            | 0.2         |
|    cost_value_loss       | 180         |
|    entropy_loss          | -2.75       |
|    explained_variance    | 0.0146      |
|    lagrangian_multiplier | 0.0538      |
|    learning_rate         | 0.0003      |
|    loss                  | 261         |
|    n_updates             | 280         |
|    policy_gradient_loss  | -0.00727    |
|    std                   | 0.954       |
|    value_loss            | 1.99e+03    |
------------------------------------------
-------------------------------------------
| reward                   | [-2.6768734] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 32           |
|    time_elapsed          | 517          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.0096947085 |
|    clip_fraction         | 0.0882       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 184          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0301      |
|    lagrangian_multiplier | 0.0556       |
|    learning_rate         | 0.0003       |
|    loss                  | 56.3         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.0138      |
|    std                   | 1.01         |
|    value_loss            | 309          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6487228] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 29           |
|    time_elapsed          | 525          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.008527286  |
|    clip_fraction         | 0.0931       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.17         |
|    entropy_loss          | -2.68        |
|    explained_variance    | -0.00414     |
|    lagrangian_multiplier | 0.0488       |
|    learning_rate         | 0.0003       |
|    loss                  | 108          |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.012       |
|    std                   | 0.921        |
|    value_loss            | 899          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6276398] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 32           |
|    time_elapsed          | 526          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.0061684423 |
|    clip_fraction         | 0.0662       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.948        |
|    entropy_loss          | -2.63        |
|    explained_variance    | -0.021       |
|    lagrangian_multiplier | 0.0619       |
|    learning_rate         | 0.0003       |
|    loss                  | 57.3         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00763     |
|    std                   | 0.9          |
|    value_loss            | 618          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.163206]  |
| time/                    |              |
|    fps                   | 115          |
|    iterations            | 30           |
|    time_elapsed          | 531          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0097765615 |
|    clip_fraction         | 0.1          |
|    clip_range            | 0.2          |
|    cost_value_loss       | 217          |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.031        |
|    lagrangian_multiplier | 0.05         |
|    learning_rate         | 0.0003       |
|    loss                  | 323          |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.0107      |
|    std                   | 0.954        |
|    value_loss            | 2.16e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.4629989] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 33           |
|    time_elapsed          | 534          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0055575683 |
|    clip_fraction         | 0.0516       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 186          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.00362     |
|    lagrangian_multiplier | 0.0533       |
|    learning_rate         | 0.0003       |
|    loss                  | 143          |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00781     |
|    std                   | 1.01         |
|    value_loss            | 941          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7047845] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 33           |
|    time_elapsed          | 543          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.007138357  |
|    clip_fraction         | 0.073        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.828        |
|    entropy_loss          | -2.62        |
|    explained_variance    | -0.0436      |
|    lagrangian_multiplier | 0.0599       |
|    learning_rate         | 0.0003       |
|    loss                  | 102          |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00826     |
|    std                   | 0.897        |
|    value_loss            | 966          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6922114] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 30           |
|    time_elapsed          | 543          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0052452404 |
|    clip_fraction         | 0.0525       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.861        |
|    entropy_loss          | -2.67        |
|    explained_variance    | -0.000496    |
|    lagrangian_multiplier | 0.0599       |
|    learning_rate         | 0.0003       |
|    loss                  | 72           |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00705     |
|    std                   | 0.924        |
|    value_loss            | 694          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.71202]   |
| time/                    |              |
|    fps                   | 115          |
|    iterations            | 31           |
|    time_elapsed          | 548          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0035468522 |
|    clip_fraction         | 0.058        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 187          |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.0159       |
|    lagrangian_multiplier | 0.0465       |
|    learning_rate         | 0.0003       |
|    loss                  | 277          |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00519     |
|    std                   | 0.953        |
|    value_loss            | 1.86e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5328857] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 34           |
|    time_elapsed          | 550          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0074475436 |
|    clip_fraction         | 0.0687       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 192          |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0286      |
|    lagrangian_multiplier | 0.0586       |
|    learning_rate         | 0.0003       |
|    loss                  | 55.6         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.0109      |
|    std                   | 1.01         |
|    value_loss            | 370          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5862832] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 34           |
|    time_elapsed          | 559          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.006974224  |
|    clip_fraction         | 0.0722       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0411       |
|    entropy_loss          | -2.61        |
|    explained_variance    | -0.0412      |
|    lagrangian_multiplier | 0.0642       |
|    learning_rate         | 0.0003       |
|    loss                  | 84.8         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00891     |
|    std                   | 0.891        |
|    value_loss            | 759          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.87174076] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 31            |
|    time_elapsed          | 561           |
|    total_timesteps       | 63488         |
| train/                   |               |
|    approx_kl             | 0.007997518   |
|    clip_fraction         | 0.0819        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.392         |
|    entropy_loss          | -2.68         |
|    explained_variance    | 0.00154       |
|    lagrangian_multiplier | 0.0477        |
|    learning_rate         | 0.0003        |
|    loss                  | 79.7          |
|    n_updates             | 300           |
|    policy_gradient_loss  | -0.0111       |
|    std                   | 0.928         |
|    value_loss            | 601           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.9381515] |
| time/                    |              |
|    fps                   | 116          |
|    iterations            | 32           |
|    time_elapsed          | 564          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.005713555  |
|    clip_fraction         | 0.0773       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 233          |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.0341       |
|    lagrangian_multiplier | 0.0424       |
|    learning_rate         | 0.0003       |
|    loss                  | 324          |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00744     |
|    std                   | 0.954        |
|    value_loss            | 2.04e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.37878352] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 35            |
|    time_elapsed          | 566           |
|    total_timesteps       | 71680         |
| train/                   |               |
|    approx_kl             | 0.008143317   |
|    clip_fraction         | 0.0743        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 253           |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.0511       |
|    lagrangian_multiplier | 0.0451        |
|    learning_rate         | 0.0003        |
|    loss                  | 90.3          |
|    n_updates             | 340           |
|    policy_gradient_loss  | -0.00903      |
|    std                   | 1.01          |
|    value_loss            | 396           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.8268669] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 35           |
|    time_elapsed          | 576          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.006510453  |
|    clip_fraction         | 0.0742       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.948        |
|    entropy_loss          | -2.6         |
|    explained_variance    | -0.00608     |
|    lagrangian_multiplier | 0.0645       |
|    learning_rate         | 0.0003       |
|    loss                  | 67.3         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.0079      |
|    std                   | 0.889        |
|    value_loss            | 710          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2354047] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 32           |
|    time_elapsed          | 579          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.005369573  |
|    clip_fraction         | 0.0503       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.933        |
|    entropy_loss          | -2.68        |
|    explained_variance    | -0.0266      |
|    lagrangian_multiplier | 0.0533       |
|    learning_rate         | 0.0003       |
|    loss                  | 39           |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.0096      |
|    std                   | 0.925        |
|    value_loss            | 350          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.014182]  |
| time/                    |              |
|    fps                   | 116          |
|    iterations            | 33           |
|    time_elapsed          | 580          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0072462833 |
|    clip_fraction         | 0.0718       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 200          |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.0252       |
|    lagrangian_multiplier | 0.0469       |
|    learning_rate         | 0.0003       |
|    loss                  | 215          |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.0103      |
|    std                   | 0.955        |
|    value_loss            | 1.75e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.43080267] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 36            |
|    time_elapsed          | 583           |
|    total_timesteps       | 73728         |
| train/                   |               |
|    approx_kl             | 0.006113456   |
|    clip_fraction         | 0.0566        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 85            |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.0828       |
|    lagrangian_multiplier | 0.0671        |
|    learning_rate         | 0.0003        |
|    loss                  | 30.9          |
|    n_updates             | 350           |
|    policy_gradient_loss  | -0.00818      |
|    std                   | 1.01          |
|    value_loss            | 285           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.4788947] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 36           |
|    time_elapsed          | 592          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.004796391  |
|    clip_fraction         | 0.0433       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0266       |
|    entropy_loss          | -2.59        |
|    explained_variance    | -0.00714     |
|    lagrangian_multiplier | 0.0627       |
|    learning_rate         | 0.0003       |
|    loss                  | 60.9         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00422     |
|    std                   | 0.882        |
|    value_loss            | 652          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-2.325364] |
| time/                    |             |
|    fps                   | 116         |
|    iterations            | 34          |
|    time_elapsed          | 597         |
|    total_timesteps       | 69632       |
| train/                   |             |
|    approx_kl             | 0.008717349 |
|    clip_fraction         | 0.0941      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 184         |
|    entropy_loss          | -2.74       |
|    explained_variance    | 0.0164      |
|    lagrangian_multiplier | 0.0495      |
|    learning_rate         | 0.0003      |
|    loss                  | 217         |
|    n_updates             | 330         |
|    policy_gradient_loss  | -0.01       |
|    std                   | 0.95        |
|    value_loss            | 1.72e+03    |
------------------------------------------
-------------------------------------------
| reward                   | [-1.2869325] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 33           |
|    time_elapsed          | 597          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0056437748 |
|    clip_fraction         | 0.05         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.399        |
|    entropy_loss          | -2.68        |
|    explained_variance    | -0.0119      |
|    lagrangian_multiplier | 0.0572       |
|    learning_rate         | 0.0003       |
|    loss                  | 55.5         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00813     |
|    std                   | 0.925        |
|    value_loss            | 549          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.3623933] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 37           |
|    time_elapsed          | 599          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.0055950405 |
|    clip_fraction         | 0.0536       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 125          |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.0717      |
|    lagrangian_multiplier | 0.0509       |
|    learning_rate         | 0.0003       |
|    loss                  | 65.2         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00769     |
|    std                   | 1.03         |
|    value_loss            | 412          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.7034657] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 37           |
|    time_elapsed          | 608          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.0054201204 |
|    clip_fraction         | 0.0434       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.031        |
|    entropy_loss          | -2.59        |
|    explained_variance    | -0.0129      |
|    lagrangian_multiplier | 0.0629       |
|    learning_rate         | 0.0003       |
|    loss                  | 66.3         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00409     |
|    std                   | 0.884        |
|    value_loss            | 669          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.6536068] |
| time/                    |              |
|    fps                   | 116          |
|    iterations            | 35           |
|    time_elapsed          | 613          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.008836018  |
|    clip_fraction         | 0.101        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 146          |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0441       |
|    lagrangian_multiplier | 0.05         |
|    learning_rate         | 0.0003       |
|    loss                  | 170          |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.0125      |
|    std                   | 0.94         |
|    value_loss            | 1.38e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.3223085] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 38           |
|    time_elapsed          | 615          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0050941133 |
|    clip_fraction         | 0.0396       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 78.3         |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.0569      |
|    lagrangian_multiplier | 0.0696       |
|    learning_rate         | 0.0003       |
|    loss                  | 42.7         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00481     |
|    std                   | 1.02         |
|    value_loss            | 354          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.7879978] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 34           |
|    time_elapsed          | 615          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.006484671  |
|    clip_fraction         | 0.0619       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.34         |
|    entropy_loss          | -2.68        |
|    explained_variance    | 0.000111     |
|    lagrangian_multiplier | 0.0574       |
|    learning_rate         | 0.0003       |
|    loss                  | 70.1         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.0109      |
|    std                   | 0.929        |
|    value_loss            | 704          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.5801524] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 38           |
|    time_elapsed          | 625          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.005540101  |
|    clip_fraction         | 0.0464       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0111       |
|    entropy_loss          | -2.6         |
|    explained_variance    | 0.00463      |
|    lagrangian_multiplier | 0.0649       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.1         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00271     |
|    std                   | 0.889        |
|    value_loss            | 485          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.1182177] |
| time/                    |              |
|    fps                   | 117          |
|    iterations            | 36           |
|    time_elapsed          | 630          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0069830148 |
|    clip_fraction         | 0.0756       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 177          |
|    entropy_loss          | -2.7         |
|    explained_variance    | 0.0273       |
|    lagrangian_multiplier | 0.0543       |
|    learning_rate         | 0.0003       |
|    loss                  | 130          |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0111      |
|    std                   | 0.933        |
|    value_loss            | 1.08e+03     |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.74801624] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 39            |
|    time_elapsed          | 632           |
|    total_timesteps       | 79872         |
| train/                   |               |
|    approx_kl             | 0.007926565   |
|    clip_fraction         | 0.0826        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 225           |
|    entropy_loss          | -2.89         |
|    explained_variance    | -0.0259       |
|    lagrangian_multiplier | 0.0561        |
|    learning_rate         | 0.0003        |
|    loss                  | 194           |
|    n_updates             | 380           |
|    policy_gradient_loss  | -0.00894      |
|    std                   | 1.03          |
|    value_loss            | 1.6e+03       |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.5021405] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 35           |
|    time_elapsed          | 634          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0098284865 |
|    clip_fraction         | 0.0802       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 2.6          |
|    entropy_loss          | -2.69        |
|    explained_variance    | -0.000392    |
|    lagrangian_multiplier | 0.0644       |
|    learning_rate         | 0.0003       |
|    loss                  | 33.3         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00946     |
|    std                   | 0.931        |
|    value_loss            | 323          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.6765376] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 39           |
|    time_elapsed          | 641          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0062728585 |
|    clip_fraction         | 0.0541       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0135       |
|    entropy_loss          | -2.6         |
|    explained_variance    | -0.0309      |
|    lagrangian_multiplier | 0.0673       |
|    learning_rate         | 0.0003       |
|    loss                  | 67.5         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00669     |
|    std                   | 0.885        |
|    value_loss            | 663          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.1021323] |
| time/                    |              |
|    fps                   | 117          |
|    iterations            | 37           |
|    time_elapsed          | 646          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.005847023  |
|    clip_fraction         | 0.0471       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 82.2         |
|    entropy_loss          | -2.68        |
|    explained_variance    | 0.0319       |
|    lagrangian_multiplier | 0.0541       |
|    learning_rate         | 0.0003       |
|    loss                  | 98.6         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00784     |
|    std                   | 0.923        |
|    value_loss            | 817          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8511275] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 40           |
|    time_elapsed          | 648          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.004822542  |
|    clip_fraction         | 0.0474       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 128          |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.0416      |
|    lagrangian_multiplier | 0.0643       |
|    learning_rate         | 0.0003       |
|    loss                  | 42.7         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00695     |
|    std                   | 1.03         |
|    value_loss            | 349          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6798657] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 36           |
|    time_elapsed          | 652          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0078101633 |
|    clip_fraction         | 0.0769       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 8.91         |
|    entropy_loss          | -2.68        |
|    explained_variance    | 0.00615      |
|    lagrangian_multiplier | 0.0542       |
|    learning_rate         | 0.0003       |
|    loss                  | 36           |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00728     |
|    std                   | 0.923        |
|    value_loss            | 324          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8679341] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 40           |
|    time_elapsed          | 658          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.008155931  |
|    clip_fraction         | 0.0959       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0192       |
|    entropy_loss          | -2.61        |
|    explained_variance    | -0.00165     |
|    lagrangian_multiplier | 0.0636       |
|    learning_rate         | 0.0003       |
|    loss                  | 66.3         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.0078      |
|    std                   | 0.894        |
|    value_loss            | 629          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2299466] |
| time/                    |              |
|    fps                   | 117          |
|    iterations            | 38           |
|    time_elapsed          | 663          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.008686996  |
|    clip_fraction         | 0.1          |
|    clip_range            | 0.2          |
|    cost_value_loss       | 19.9         |
|    entropy_loss          | -2.68        |
|    explained_variance    | 0.0587       |
|    lagrangian_multiplier | 0.0487       |
|    learning_rate         | 0.0003       |
|    loss                  | 115          |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.0117      |
|    std                   | 0.936        |
|    value_loss            | 903          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6117826] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 41           |
|    time_elapsed          | 664          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0073755924 |
|    clip_fraction         | 0.071        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 115          |
|    entropy_loss          | -2.91        |
|    explained_variance    | -0.0296      |
|    lagrangian_multiplier | 0.06         |
|    learning_rate         | 0.0003       |
|    loss                  | 34.6         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 1.04         |
|    value_loss            | 285          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3883497] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 37           |
|    time_elapsed          | 670          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.006826536  |
|    clip_fraction         | 0.0654       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.66         |
|    entropy_loss          | -2.67        |
|    explained_variance    | -0.00687     |
|    lagrangian_multiplier | 0.0618       |
|    learning_rate         | 0.0003       |
|    loss                  | 21.3         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00811     |
|    std                   | 0.921        |
|    value_loss            | 188          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7812176] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 41           |
|    time_elapsed          | 674          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0063077523 |
|    clip_fraction         | 0.0674       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.016        |
|    entropy_loss          | -2.62        |
|    explained_variance    | -0.00191     |
|    lagrangian_multiplier | 0.0723       |
|    learning_rate         | 0.0003       |
|    loss                  | 51.5         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00642     |
|    std                   | 0.898        |
|    value_loss            | 585          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.4962149] |
| time/                    |              |
|    fps                   | 117          |
|    iterations            | 39           |
|    time_elapsed          | 679          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0070172804 |
|    clip_fraction         | 0.0721       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 104          |
|    entropy_loss          | -2.7         |
|    explained_variance    | 0.0198       |
|    lagrangian_multiplier | 0.0518       |
|    learning_rate         | 0.0003       |
|    loss                  | 163          |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00768     |
|    std                   | 0.938        |
|    value_loss            | 1.25e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.29364]   |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 42           |
|    time_elapsed          | 680          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0062135365 |
|    clip_fraction         | 0.0708       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 33           |
|    entropy_loss          | -2.93        |
|    explained_variance    | 0.0332       |
|    lagrangian_multiplier | 0.0717       |
|    learning_rate         | 0.0003       |
|    loss                  | 30.6         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00904     |
|    std                   | 1.05         |
|    value_loss            | 335          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.493716]  |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 38           |
|    time_elapsed          | 689          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0060115433 |
|    clip_fraction         | 0.0637       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.59         |
|    entropy_loss          | -2.66        |
|    explained_variance    | 0.00178      |
|    lagrangian_multiplier | 0.0512       |
|    learning_rate         | 0.0003       |
|    loss                  | 71.5         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00821     |
|    std                   | 0.917        |
|    value_loss            | 647          |
-------------------------------------------
------------------------------------------
| reward                   | [-2.01272]  |
| time/                    |             |
|    fps                   | 124         |
|    iterations            | 42          |
|    time_elapsed          | 691         |
|    total_timesteps       | 86016       |
| train/                   |             |
|    approx_kl             | 0.005004098 |
|    clip_fraction         | 0.06        |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0153      |
|    entropy_loss          | -2.62       |
|    explained_variance    | -0.00835    |
|    lagrangian_multiplier | 0.0654      |
|    learning_rate         | 0.0003      |
|    loss                  | 42.3        |
|    n_updates             | 410         |
|    policy_gradient_loss  | -0.00513    |
|    std                   | 0.898       |
|    value_loss            | 484         |
------------------------------------------
------------------------------------------
| reward                   | [-2.975634] |
| time/                    |             |
|    fps                   | 117         |
|    iterations            | 40          |
|    time_elapsed          | 696         |
|    total_timesteps       | 81920       |
| train/                   |             |
|    approx_kl             | 0.008910412 |
|    clip_fraction         | 0.1         |
|    clip_range            | 0.2         |
|    cost_value_loss       | 130         |
|    entropy_loss          | -2.71       |
|    explained_variance    | 0.0319      |
|    lagrangian_multiplier | 0.0579      |
|    learning_rate         | 0.0003      |
|    loss                  | 139         |
|    n_updates             | 390         |
|    policy_gradient_loss  | -0.0133     |
|    std                   | 0.946       |
|    value_loss            | 1.24e+03    |
------------------------------------------
--------------------------------------------
| reward                   | [-0.79743046] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 43            |
|    time_elapsed          | 696           |
|    total_timesteps       | 88064         |
| train/                   |               |
|    approx_kl             | 0.006275464   |
|    clip_fraction         | 0.0587        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 215           |
|    entropy_loss          | -2.93         |
|    explained_variance    | 0.00385       |
|    lagrangian_multiplier | 0.0495        |
|    learning_rate         | 0.0003        |
|    loss                  | 88.3          |
|    n_updates             | 420           |
|    policy_gradient_loss  | -0.00827      |
|    std                   | 1.05          |
|    value_loss            | 569           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.9762806] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 43           |
|    time_elapsed          | 708          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.007647117  |
|    clip_fraction         | 0.0803       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.4          |
|    entropy_loss          | -2.63        |
|    explained_variance    | -0.0193      |
|    lagrangian_multiplier | 0.0633       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.7         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00849     |
|    std                   | 0.903        |
|    value_loss            | 532          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7315481] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 39           |
|    time_elapsed          | 707          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0059428234 |
|    clip_fraction         | 0.0693       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 10.8         |
|    entropy_loss          | -2.66        |
|    explained_variance    | 0.0115       |
|    lagrangian_multiplier | 0.0598       |
|    learning_rate         | 0.0003       |
|    loss                  | 64.7         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.0084      |
|    std                   | 0.917        |
|    value_loss            | 490          |
-------------------------------------------
------------------------------------------
| reward                   | [-3.170161] |
| time/                    |             |
|    fps                   | 117         |
|    iterations            | 41          |
|    time_elapsed          | 712         |
|    total_timesteps       | 83968       |
| train/                   |             |
|    approx_kl             | 0.009370181 |
|    clip_fraction         | 0.0806      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 164         |
|    entropy_loss          | -2.7        |
|    explained_variance    | 0.0109      |
|    lagrangian_multiplier | 0.045       |
|    learning_rate         | 0.0003      |
|    loss                  | 199         |
|    n_updates             | 400         |
|    policy_gradient_loss  | -0.00994    |
|    std                   | 0.937       |
|    value_loss            | 1.21e+03    |
------------------------------------------
--------------------------------------------
| reward                   | [-0.66087294] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 44            |
|    time_elapsed          | 713           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.004283009   |
|    clip_fraction         | 0.0404        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.734         |
|    entropy_loss          | -2.93         |
|    explained_variance    | 0.0113        |
|    lagrangian_multiplier | 0.078         |
|    learning_rate         | 0.0003        |
|    loss                  | 10.7          |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.00583      |
|    std                   | 1.05          |
|    value_loss            | 119           |
--------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.48295137] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 44            |
|    time_elapsed          | 724           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.0068475273  |
|    clip_fraction         | 0.0833        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0325        |
|    entropy_loss          | -2.62         |
|    explained_variance    | -0.0172       |
|    lagrangian_multiplier | 0.0637        |
|    learning_rate         | 0.0003        |
|    loss                  | 74.1          |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.00804      |
|    std                   | 0.891         |
|    value_loss            | 803           |
--------------------------------------------
------------------------------------------
| reward                   | [-1.289644] |
| time/                    |             |
|    fps                   | 112         |
|    iterations            | 40          |
|    time_elapsed          | 725         |
|    total_timesteps       | 81920       |
| train/                   |             |
|    approx_kl             | 0.008059742 |
|    clip_fraction         | 0.0689      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 1.95        |
|    entropy_loss          | -2.66       |
|    explained_variance    | -0.000109   |
|    lagrangian_multiplier | 0.0589      |
|    learning_rate         | 0.0003      |
|    loss                  | 80.4        |
|    n_updates             | 390         |
|    policy_gradient_loss  | -0.00993    |
|    std                   | 0.917       |
|    value_loss            | 718         |
------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.6102642] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 45           |
|    time_elapsed          | 729          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.0057008658 |
|    clip_fraction         | 0.0505       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 17.4         |
|    entropy_loss          | -2.94        |
|    explained_variance    | -0.0457      |
|    lagrangian_multiplier | 0.0701       |
|    learning_rate         | 0.0003       |
|    loss                  | 11           |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00934     |
|    std                   | 1.06         |
|    value_loss            | 181          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9660372] |
| time/                    |              |
|    fps                   | 118          |
|    iterations            | 42           |
|    time_elapsed          | 728          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.00897523   |
|    clip_fraction         | 0.0964       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 152          |
|    entropy_loss          | -2.69        |
|    explained_variance    | 0.0347       |
|    lagrangian_multiplier | 0.0511       |
|    learning_rate         | 0.0003       |
|    loss                  | 153          |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.0118      |
|    std                   | 0.939        |
|    value_loss            | 1.12e+03     |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.7622491] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 45           |
|    time_elapsed          | 740          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.0076013147 |
|    clip_fraction         | 0.0615       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.64         |
|    entropy_loss          | -2.61        |
|    explained_variance    | -0.0218      |
|    lagrangian_multiplier | 0.0629       |
|    learning_rate         | 0.0003       |
|    loss                  | 100          |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00639     |
|    std                   | 0.89         |
|    value_loss            | 1.2e+03      |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.87448233] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 41            |
|    time_elapsed          | 743           |
|    total_timesteps       | 83968         |
| train/                   |               |
|    approx_kl             | 0.0044395383  |
|    clip_fraction         | 0.0392        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 13.5          |
|    entropy_loss          | -2.65         |
|    explained_variance    | -0.018        |
|    lagrangian_multiplier | 0.0716        |
|    learning_rate         | 0.0003        |
|    loss                  | 30            |
|    n_updates             | 400           |
|    policy_gradient_loss  | -0.00583      |
|    std                   | 0.909         |
|    value_loss            | 363           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.8232921] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 46           |
|    time_elapsed          | 745          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0060404204 |
|    clip_fraction         | 0.058        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 108          |
|    entropy_loss          | -2.95        |
|    explained_variance    | 0.00115      |
|    lagrangian_multiplier | 0.0603       |
|    learning_rate         | 0.0003       |
|    loss                  | 65           |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.00768     |
|    std                   | 1.06         |
|    value_loss            | 469          |
-------------------------------------------
-------------------------------------------
| reward                   | [-4.208246]  |
| time/                    |              |
|    fps                   | 118          |
|    iterations            | 43           |
|    time_elapsed          | 745          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0070148283 |
|    clip_fraction         | 0.0657       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 115          |
|    entropy_loss          | -2.71        |
|    explained_variance    | 0.0328       |
|    lagrangian_multiplier | 0.0453       |
|    learning_rate         | 0.0003       |
|    loss                  | 143          |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00861     |
|    std                   | 0.949        |
|    value_loss            | 925          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.9835744] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 46           |
|    time_elapsed          | 757          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.008081459  |
|    clip_fraction         | 0.0772       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.1          |
|    entropy_loss          | -2.6         |
|    explained_variance    | -0.0188      |
|    lagrangian_multiplier | 0.064        |
|    learning_rate         | 0.0003       |
|    loss                  | 85.5         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 0.89         |
|    value_loss            | 862          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.9293286] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 47           |
|    time_elapsed          | 761          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.008470989  |
|    clip_fraction         | 0.0722       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 258          |
|    entropy_loss          | -2.95        |
|    explained_variance    | -0.00195     |
|    lagrangian_multiplier | 0.0567       |
|    learning_rate         | 0.0003       |
|    loss                  | 74.9         |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 1.05         |
|    value_loss            | 382          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.47442287] |
| time/                    |               |
|    fps                   | 118           |
|    iterations            | 44            |
|    time_elapsed          | 761           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.0064918473  |
|    clip_fraction         | 0.0759        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 179           |
|    entropy_loss          | -2.71         |
|    explained_variance    | 0.0243        |
|    lagrangian_multiplier | 0.0497        |
|    learning_rate         | 0.0003        |
|    loss                  | 211           |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.0109       |
|    std                   | 0.939         |
|    value_loss            | 1.57e+03      |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.0242628] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 42           |
|    time_elapsed          | 762          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0069407457 |
|    clip_fraction         | 0.0703       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 8.68         |
|    entropy_loss          | -2.64        |
|    explained_variance    | -0.00519     |
|    lagrangian_multiplier | 0.067        |
|    learning_rate         | 0.0003       |
|    loss                  | 20.5         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.905        |
|    value_loss            | 203          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.0391005] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 47           |
|    time_elapsed          | 773          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.007530569  |
|    clip_fraction         | 0.072        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.1          |
|    entropy_loss          | -2.61        |
|    explained_variance    | -0.0239      |
|    lagrangian_multiplier | 0.0646       |
|    learning_rate         | 0.0003       |
|    loss                  | 96.7         |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.00592     |
|    std                   | 0.893        |
|    value_loss            | 885          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.44946188] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 48            |
|    time_elapsed          | 777           |
|    total_timesteps       | 98304         |
| train/                   |               |
|    approx_kl             | 0.008512276   |
|    clip_fraction         | 0.0688        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 132           |
|    entropy_loss          | -2.96         |
|    explained_variance    | -0.0665       |
|    lagrangian_multiplier | 0.0757        |
|    learning_rate         | 0.0003        |
|    loss                  | 30.9          |
|    n_updates             | 470           |
|    policy_gradient_loss  | -0.0108       |
|    std                   | 1.08          |
|    value_loss            | 215           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.46355012] |
| time/                    |               |
|    fps                   | 118           |
|    iterations            | 45            |
|    time_elapsed          | 778           |
|    total_timesteps       | 92160         |
| train/                   |               |
|    approx_kl             | 0.008589317   |
|    clip_fraction         | 0.0947        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 107           |
|    entropy_loss          | -2.69         |
|    explained_variance    | 0.02          |
|    lagrangian_multiplier | 0.0523        |
|    learning_rate         | 0.0003        |
|    loss                  | 121           |
|    n_updates             | 440           |
|    policy_gradient_loss  | -0.0116       |
|    std                   | 0.935         |
|    value_loss            | 799           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.4624648] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 43           |
|    time_elapsed          | 780          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0057635345 |
|    clip_fraction         | 0.0602       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 9.57         |
|    entropy_loss          | -2.65        |
|    explained_variance    | -0.00109     |
|    lagrangian_multiplier | 0.0584       |
|    learning_rate         | 0.0003       |
|    loss                  | 84.1         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00977     |
|    std                   | 0.922        |
|    value_loss            | 852          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9531815] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 48           |
|    time_elapsed          | 790          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.0068539    |
|    clip_fraction         | 0.0682       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0191       |
|    entropy_loss          | -2.6         |
|    explained_variance    | 0.0062       |
|    lagrangian_multiplier | 0.06         |
|    learning_rate         | 0.0003       |
|    loss                  | 60.7         |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.0064      |
|    std                   | 0.88         |
|    value_loss            | 603          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.80095613] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 49            |
|    time_elapsed          | 793           |
|    total_timesteps       | 100352        |
| train/                   |               |
|    approx_kl             | 0.003838488   |
|    clip_fraction         | 0.0383        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 49.9          |
|    entropy_loss          | -3            |
|    explained_variance    | -0.0295       |
|    lagrangian_multiplier | 0.0783        |
|    learning_rate         | 0.0003        |
|    loss                  | 26.6          |
|    n_updates             | 480           |
|    policy_gradient_loss  | -0.00781      |
|    std                   | 1.09          |
|    value_loss            | 165           |
--------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
-------------------------------------------
| reward                   | [-0.8999092] |
| time/                    |              |
|    fps                   | 118          |
|    iterations            | 46           |
|    time_elapsed          | 794          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0065943557 |
|    clip_fraction         | 0.0746       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 79.7         |
|    entropy_loss          | -2.7         |
|    explained_variance    | 0.0341       |
|    lagrangian_multiplier | 0.0493       |
|    learning_rate         | 0.0003       |
|    loss                  | 126          |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.00904     |
|    std                   | 0.941        |
|    value_loss            | 942          |
-------------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñá‚ñÅ‚ñÖ‚ñà‚ñÉ‚ñá‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñà‚ñÇ‚ñÜ‚ñà‚ñà‚ñÜ‚ñÖ‚ñá‚ñá‚ñÇ‚ñá‚ñá‚ñÖ‚ñà‚ñà‚ñá
wandb:             train/approx_kl ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñà‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñá‚ñá‚ñÅ
wandb:         train/clip_fraction ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÅ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÉ‚ñá‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÉ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÇ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñá‚ñÑ‚ñÇ‚ñá‚ñÅ‚ñÅ‚ñà‚ñÖ‚ñÇ
wandb:          train/entropy_loss ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ
wandb:    train/explained_variance ‚ñÑ‚ñÉ‚ñá‚ñÉ‚ñÇ‚ñÜ‚ñá‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñÑ‚ñÜ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñà‚ñÜ‚ñá‚ñÉ‚ñÜ‚ñÇ‚ñÑ
wandb: train/lagrangian_multiplier ‚ñà‚ñÅ‚ñá‚ñÇ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñà‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÇ‚ñá‚ñÜ‚ñÑ‚ñá‚ñá
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñà‚ñá‚ñÇ‚ñÅ‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñà‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñá‚ñÅ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÖ
wandb:                   train/std ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb:            train/value_loss ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñá‚ñÜ‚ñÉ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                      reward -0.80096
wandb:             train/approx_kl 0.00384
wandb:         train/clip_fraction 0.03833
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 49.94988
wandb:          train/entropy_loss -2.99547
wandb:    train/explained_variance -0.02952
wandb: train/lagrangian_multiplier 0.07829
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 26.57865
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00781
wandb:                   train/std 1.08966
wandb:            train/value_loss 164.72195
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/fkhimvab
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_185750-fkhimvab/logs
-------------------------------------------
| reward                   | [-1.9453671] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 44           |
|    time_elapsed          | 798          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.007484545  |
|    clip_fraction         | 0.0799       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 18.1         |
|    entropy_loss          | -2.67        |
|    explained_variance    | -0.0055      |
|    lagrangian_multiplier | 0.0746       |
|    learning_rate         | 0.0003       |
|    loss                  | 39.2         |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.0113      |
|    std                   | 0.922        |
|    value_loss            | 455          |
-------------------------------------------
srun: error: gail.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-1.1122332] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 49           |
|    time_elapsed          | 806          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.006059357  |
|    clip_fraction         | 0.0615       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 10           |
|    entropy_loss          | -2.59        |
|    explained_variance    | -0.0344      |
|    lagrangian_multiplier | 0.0632       |
|    learning_rate         | 0.0003       |
|    loss                  | 94.2         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.00827     |
|    std                   | 0.885        |
|    value_loss            | 1.19e+03     |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
-------------------------------------------
| reward                   | [-1.5577657] |
| time/                    |              |
|    fps                   | 118          |
|    iterations            | 47           |
|    time_elapsed          | 811          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.006933133  |
|    clip_fraction         | 0.068        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 184          |
|    entropy_loss          | -2.7         |
|    explained_variance    | 0.0277       |
|    lagrangian_multiplier | 0.0517       |
|    learning_rate         | 0.0003       |
|    loss                  | 181          |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.00807     |
|    std                   | 0.938        |
|    value_loss            | 1.31e+03     |
-------------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñá‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñà‚ñá‚ñÖ‚ñÜ‚ñÖ
wandb:             train/approx_kl ‚ñÖ‚ñÇ‚ñÑ‚ñÜ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ
wandb:         train/clip_fraction ‚ñÉ‚ñÅ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÑ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÜ‚ñÉ‚ñÅ‚ñà
wandb:          train/entropy_loss ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:    train/explained_variance ‚ñÇ‚ñÉ‚ñÅ‚ñà‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÑ
wandb: train/lagrangian_multiplier ‚ñà‚ñÖ‚ñÜ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÉ‚ñÜ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÜ‚ñá‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÇ‚ñÇ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñá‚ñà‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÉ
wandb:                   train/std ‚ñà‚ñà‚ñá‚ñà‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:            train/value_loss ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñá‚ñÑ‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÜ‚ñÑ‚ñà
wandb: 
wandb: Run summary:
wandb:                      reward -1.11223
wandb:             train/approx_kl 0.00606
wandb:         train/clip_fraction 0.06152
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 10.02029
wandb:          train/entropy_loss -2.58756
wandb:    train/explained_variance -0.03439
wandb: train/lagrangian_multiplier 0.06317
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 94.20225
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00827
wandb:                   train/std 0.88512
wandb:            train/value_loss 1192.18993
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/cwr6z585
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_185750-cwr6z585/logs
srun: error: dqn.ist.berkeley.edu: task 0: Exited with exit code 1
--------------------------------------------
| reward                   | [-0.87653536] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 45            |
|    time_elapsed          | 816           |
|    total_timesteps       | 92160         |
| train/                   |               |
|    approx_kl             | 0.007967891   |
|    clip_fraction         | 0.0753        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 6.98          |
|    entropy_loss          | -2.67         |
|    explained_variance    | 0.015         |
|    lagrangian_multiplier | 0.0653        |
|    learning_rate         | 0.0003        |
|    loss                  | 69.6          |
|    n_updates             | 440           |
|    policy_gradient_loss  | -0.0126       |
|    std                   | 0.923         |
|    value_loss            | 666           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3935989] |
| time/                    |              |
|    fps                   | 118          |
|    iterations            | 48           |
|    time_elapsed          | 827          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.006749282  |
|    clip_fraction         | 0.0624       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 106          |
|    entropy_loss          | -2.71        |
|    explained_variance    | 0.0296       |
|    lagrangian_multiplier | 0.0526       |
|    learning_rate         | 0.0003       |
|    loss                  | 104          |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00789     |
|    std                   | 0.947        |
|    value_loss            | 954          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9821692] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 46           |
|    time_elapsed          | 834          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0051093707 |
|    clip_fraction         | 0.056        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 2.96         |
|    entropy_loss          | -2.67        |
|    explained_variance    | -0.00319     |
|    lagrangian_multiplier | 0.0672       |
|    learning_rate         | 0.0003       |
|    loss                  | 28.2         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.00841     |
|    std                   | 0.919        |
|    value_loss            | 284          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114547.4
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_191151-6en5g80o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/6en5g80o
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114547.5
-------------------------------------------
| reward                   | [-1.4839727] |
| time/                    |              |
|    fps                   | 118          |
|    iterations            | 49           |
|    time_elapsed          | 843          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.0072510773 |
|    clip_fraction         | 0.0902       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 51           |
|    entropy_loss          | -2.71        |
|    explained_variance    | 0.0459       |
|    lagrangian_multiplier | 0.0502       |
|    learning_rate         | 0.0003       |
|    loss                  | 97.4         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.0109      |
|    std                   | 0.944        |
|    value_loss            | 698          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_191158-x6uwljc0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2/runs/x6uwljc0
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñá‚ñá‚ñà‚ñá‚ñÖ‚ñÖ‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñÇ‚ñÖ‚ñà‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñá‚ñà‚ñà‚ñÜ‚ñà‚ñÖ
wandb:             train/approx_kl ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñÉ‚ñá‚ñà‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñá‚ñá‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÖ
wandb:         train/clip_fraction ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñà‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÇ‚ñà‚ñÖ‚ñà‚ñà‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñá
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñá‚ñÑ‚ñÉ
wandb:          train/entropy_loss ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá
wandb:    train/explained_variance ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÅ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÉ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá
wandb: train/lagrangian_multiplier ‚ñÜ‚ñà‚ñÜ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñà‚ñá‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÉ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñà‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñà‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÇ‚ñÜ‚ñÅ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÜ‚ñÜ‚ñÉ
wandb:                   train/std ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:            train/value_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñà‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÇ
wandb: 
wandb: Run summary:
wandb:                      reward -1.48397
wandb:             train/approx_kl 0.00725
wandb:         train/clip_fraction 0.09023
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 51.03365
wandb:          train/entropy_loss -2.70809
wandb:    train/explained_variance 0.04592
wandb: train/lagrangian_multiplier 0.05017
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 97.38811
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.01089
wandb:                   train/std 0.94443
wandb:            train/value_loss 697.8761
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/h94pxu2w
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231227_025751-h94pxu2w/logs
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: error: airl.ist.berkeley.edu: task 0: Exited with exit code 1
--------------------------------------------
| reward                   | [-0.82042915] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 47            |
|    time_elapsed          | 852           |
|    total_timesteps       | 96256         |
| train/                   |               |
|    approx_kl             | 0.006820323   |
|    clip_fraction         | 0.0633        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 8             |
|    entropy_loss          | -2.67         |
|    explained_variance    | 0.00489       |
|    lagrangian_multiplier | 0.0619        |
|    learning_rate         | 0.0003        |
|    loss                  | 60.5          |
|    n_updates             | 460           |
|    policy_gradient_loss  | -0.0102       |
|    std                   | 0.921         |
|    value_loss            | 669           |
--------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.62891793] |
| time/              |               |
|    fps             | 134           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114547.6
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231227_031212-l79bu81z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2/runs/l79bu81z
Using cpu device
--------------------------------------
| reward             | [-0.43732417] |
| time/              |               |
|    fps             | 135           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.50593096] |
| time/                    |               |
|    fps                   | 130           |
|    iterations            | 2             |
|    time_elapsed          | 31            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.0055040037  |
|    clip_fraction         | 0.0471        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 17            |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.0356        |
|    lagrangian_multiplier | 0.047         |
|    learning_rate         | 0.0003        |
|    loss                  | 47            |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00634      |
|    std                   | 0.998         |
|    value_loss            | 322           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.98406816] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 48            |
|    time_elapsed          | 870           |
|    total_timesteps       | 98304         |
| train/                   |               |
|    approx_kl             | 0.00744537    |
|    clip_fraction         | 0.0534        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 2.06          |
|    entropy_loss          | -2.68         |
|    explained_variance    | -0.028        |
|    lagrangian_multiplier | 0.0687        |
|    learning_rate         | 0.0003        |
|    loss                  | 43.6          |
|    n_updates             | 470           |
|    policy_gradient_loss  | -0.0122       |
|    std                   | 0.933         |
|    value_loss            | 373           |
--------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.41502884] |
| time/              |               |
|    fps             | 136           |
|    iterations      | 1             |
|    time_elapsed    | 14            |
|    total_timesteps | 2048          |
--------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.595916]  |
| time/                    |              |
|    fps                   | 132          |
|    iterations            | 2            |
|    time_elapsed          | 31           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0074495077 |
|    clip_fraction         | 0.0536       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 194          |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0127      |
|    lagrangian_multiplier | 0.0471       |
|    learning_rate         | 0.0003       |
|    loss                  | 79.9         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00812     |
|    std                   | 0.995        |
|    value_loss            | 484          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.7954705] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 3            |
|    time_elapsed          | 47           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0052832356 |
|    clip_fraction         | 0.0399       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 86           |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0566       |
|    lagrangian_multiplier | 0.0559       |
|    learning_rate         | 0.0003       |
|    loss                  | 59           |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00411     |
|    std                   | 1.01         |
|    value_loss            | 449          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.8044748] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 49           |
|    time_elapsed          | 888          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.008780118  |
|    clip_fraction         | 0.103        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 12.5         |
|    entropy_loss          | -2.69        |
|    explained_variance    | -0.0356      |
|    lagrangian_multiplier | 0.0817       |
|    learning_rate         | 0.0003       |
|    loss                  | 29.1         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.0125      |
|    std                   | 0.928        |
|    value_loss            | 312          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.7409919] |
| time/                    |              |
|    fps                   | 130          |
|    iterations            | 2            |
|    time_elapsed          | 31           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.006440034  |
|    clip_fraction         | 0.042        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 32.7         |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0101      |
|    lagrangian_multiplier | 0.086        |
|    learning_rate         | 0.0003       |
|    loss                  | 53.3         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00727     |
|    std                   | 0.978        |
|    value_loss            | 665          |
-------------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÅ
wandb:             train/approx_kl ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñá‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñà
wandb:         train/clip_fraction ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñà
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÜ
wandb:          train/entropy_loss ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ
wandb:    train/explained_variance ‚ñÉ‚ñÉ‚ñÉ‚ñá‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñá‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÇ‚ñÅ
wandb: train/lagrangian_multiplier ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñà
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÜ‚ñà‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÇ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñá‚ñÇ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÇ‚ñÜ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ
wandb:                   train/std ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ
wandb:            train/value_loss ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñà‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÜ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÖ‚ñÅ‚ñá‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÇ
wandb: 
wandb: Run summary:
wandb:                      reward -2.80447
wandb:             train/approx_kl 0.00878
wandb:         train/clip_fraction 0.10327
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 12.53465
wandb:          train/entropy_loss -2.6891
wandb:    train/explained_variance -0.03556
wandb: train/lagrangian_multiplier 0.08167
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 29.10266
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.01254
wandb:                   train/std 0.92782
wandb:            train/value_loss 311.57341
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/wpgdmzjr
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_185750-wpgdmzjr/logs
--------------------------------------------
| reward                   | [-0.78119606] |
| time/                    |               |
|    fps                   | 130           |
|    iterations            | 3             |
|    time_elapsed          | 47            |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.0052231085  |
|    clip_fraction         | 0.0303        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 156           |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.0293        |
|    lagrangian_multiplier | 0.0579        |
|    learning_rate         | 0.0003        |
|    loss                  | 56.9          |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00633      |
|    std                   | 0.989         |
|    value_loss            | 493           |
--------------------------------------------
srun: error: ddpg.ist.berkeley.edu: task 0: Exited with exit code 1
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114547.7
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_191254-xzjrcf8f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2/runs/xzjrcf8f
-------------------------------------------
| reward                   | [-1.206927]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 4            |
|    time_elapsed          | 63           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.0053753834 |
|    clip_fraction         | 0.0429       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 167          |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.000317     |
|    lagrangian_multiplier | 0.0682       |
|    learning_rate         | 0.0003       |
|    loss                  | 106          |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00619     |
|    std                   | 1.01         |
|    value_loss            | 841          |
-------------------------------------------
------------------------------------------
| reward                   | [-0.809096] |
| time/                    |             |
|    fps                   | 129         |
|    iterations            | 3           |
|    time_elapsed          | 47          |
|    total_timesteps       | 6144        |
| train/                   |             |
|    approx_kl             | 0.005538356 |
|    clip_fraction         | 0.0421      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 128         |
|    entropy_loss          | -2.78       |
|    explained_variance    | -0.0304     |
|    lagrangian_multiplier | 0.0664      |
|    learning_rate         | 0.0003      |
|    loss                  | 68.2        |
|    n_updates             | 20          |
|    policy_gradient_loss  | -0.00578    |
|    std                   | 0.964       |
|    value_loss            | 548         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.40541643] |
| time/                    |               |
|    fps                   | 129           |
|    iterations            | 4             |
|    time_elapsed          | 63            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.005770586   |
|    clip_fraction         | 0.0402        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 241           |
|    entropy_loss          | -2.81         |
|    explained_variance    | -0.045        |
|    lagrangian_multiplier | 0.073         |
|    learning_rate         | 0.0003        |
|    loss                  | 85.7          |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00538      |
|    std                   | 0.986         |
|    value_loss            | 675           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.367084]  |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 5            |
|    time_elapsed          | 80           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0071896007 |
|    clip_fraction         | 0.0591       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 29.7         |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.00233      |
|    lagrangian_multiplier | 0.0411       |
|    learning_rate         | 0.0003       |
|    loss                  | 104          |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00476     |
|    std                   | 1.03         |
|    value_loss            | 689          |
-------------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.6486393] |
| time/              |              |
|    fps             | 117          |
|    iterations      | 1            |
|    time_elapsed    | 17           |
|    total_timesteps | 2048         |
-------------------------------------
--------------------------------------------
| reward                   | [-0.83331287] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 4             |
|    time_elapsed          | 63            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.00530147    |
|    clip_fraction         | 0.0744        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 78            |
|    entropy_loss          | -2.76         |
|    explained_variance    | 0.0157        |
|    lagrangian_multiplier | 0.0946        |
|    learning_rate         | 0.0003        |
|    loss                  | 33.8          |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00673      |
|    std                   | 0.964         |
|    value_loss            | 410           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.6579991] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 5            |
|    time_elapsed          | 79           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0067394655 |
|    clip_fraction         | 0.072        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 148          |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.000183     |
|    lagrangian_multiplier | 0.0548       |
|    learning_rate         | 0.0003       |
|    loss                  | 56.6         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00889     |
|    std                   | 0.984        |
|    value_loss            | 338          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8911868] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 6            |
|    time_elapsed          | 96           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0057788705 |
|    clip_fraction         | 0.0434       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 144          |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.0186      |
|    lagrangian_multiplier | 0.045        |
|    learning_rate         | 0.0003       |
|    loss                  | 61.7         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00782     |
|    std                   | 1.02         |
|    value_loss            | 342          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9948328] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 2            |
|    time_elapsed          | 35           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0063465685 |
|    clip_fraction         | 0.0562       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 70.4         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0137      |
|    lagrangian_multiplier | 0.0583       |
|    learning_rate         | 0.0003       |
|    loss                  | 75.8         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00503     |
|    std                   | 1            |
|    value_loss            | 611          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0825089] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 5            |
|    time_elapsed          | 80           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0052988846 |
|    clip_fraction         | 0.0451       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 186          |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.0139       |
|    lagrangian_multiplier | 0.0815       |
|    learning_rate         | 0.0003       |
|    loss                  | 68.6         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00511     |
|    std                   | 0.958        |
|    value_loss            | 688          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7270307] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 6            |
|    time_elapsed          | 95           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.006881878  |
|    clip_fraction         | 0.0559       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 198          |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.00373     |
|    lagrangian_multiplier | 0.0475       |
|    learning_rate         | 0.0003       |
|    loss                  | 140          |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00757     |
|    std                   | 0.988        |
|    value_loss            | 964          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1348164] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 7            |
|    time_elapsed          | 112          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.0048335027 |
|    clip_fraction         | 0.0376       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.78         |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0276       |
|    lagrangian_multiplier | 0.0643       |
|    learning_rate         | 0.0003       |
|    loss                  | 20.9         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00458     |
|    std                   | 1.01         |
|    value_loss            | 213          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5511621] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 3            |
|    time_elapsed          | 53           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.005137896  |
|    clip_fraction         | 0.0432       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 52.4         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.00994      |
|    lagrangian_multiplier | 0.0756       |
|    learning_rate         | 0.0003       |
|    loss                  | 33.4         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00685     |
|    std                   | 1            |
|    value_loss            | 340          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.139338]  |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 6            |
|    time_elapsed          | 96           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0053899307 |
|    clip_fraction         | 0.0551       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 192          |
|    entropy_loss          | -2.75        |
|    explained_variance    | 0.0382       |
|    lagrangian_multiplier | 0.0701       |
|    learning_rate         | 0.0003       |
|    loss                  | 53.4         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00631     |
|    std                   | 0.955        |
|    value_loss            | 361          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.682582] |
| time/                    |             |
|    fps                   | 129         |
|    iterations            | 7           |
|    time_elapsed          | 111         |
|    total_timesteps       | 14336       |
| train/                   |             |
|    approx_kl             | 0.006064713 |
|    clip_fraction         | 0.0477      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 224         |
|    entropy_loss          | -2.81       |
|    explained_variance    | -0.0161     |
|    lagrangian_multiplier | 0.0634      |
|    learning_rate         | 0.0003      |
|    loss                  | 112         |
|    n_updates             | 60          |
|    policy_gradient_loss  | -0.00571    |
|    std                   | 0.988       |
|    value_loss            | 1e+03       |
------------------------------------------
--------------------------------------------
| reward                   | [-0.48270297] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 8             |
|    time_elapsed          | 128           |
|    total_timesteps       | 16384         |
| train/                   |               |
|    approx_kl             | 0.005871588   |
|    clip_fraction         | 0.0413        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 123           |
|    entropy_loss          | -2.88         |
|    explained_variance    | 0.00465       |
|    lagrangian_multiplier | 0.0553        |
|    learning_rate         | 0.0003        |
|    loss                  | 43.5          |
|    n_updates             | 70            |
|    policy_gradient_loss  | -0.00473      |
|    std                   | 1.02          |
|    value_loss            | 335           |
--------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.663588]  |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 7            |
|    time_elapsed          | 112          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.0055093216 |
|    clip_fraction         | 0.0448       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 140          |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.0395      |
|    lagrangian_multiplier | 0.0626       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.9         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00679     |
|    std                   | 0.944        |
|    value_loss            | 481          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8257266] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 8            |
|    time_elapsed          | 127          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0063108136 |
|    clip_fraction         | 0.0647       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 229          |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0107       |
|    lagrangian_multiplier | 0.0605       |
|    learning_rate         | 0.0003       |
|    loss                  | 127          |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00991     |
|    std                   | 0.995        |
|    value_loss            | 911          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.70457745] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 4             |
|    time_elapsed          | 72            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.0077707134  |
|    clip_fraction         | 0.0563        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 214           |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.02         |
|    lagrangian_multiplier | 0.0759        |
|    learning_rate         | 0.0003        |
|    loss                  | 58.9          |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00848      |
|    std                   | 1             |
|    value_loss            | 482           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.71658945] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 9             |
|    time_elapsed          | 145           |
|    total_timesteps       | 18432         |
| train/                   |               |
|    approx_kl             | 0.008528448   |
|    clip_fraction         | 0.0693        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 131           |
|    entropy_loss          | -2.86         |
|    explained_variance    | 0.0274        |
|    lagrangian_multiplier | 0.0527        |
|    learning_rate         | 0.0003        |
|    loss                  | 59.7          |
|    n_updates             | 80            |
|    policy_gradient_loss  | -0.0116       |
|    std                   | 1             |
|    value_loss            | 351           |
--------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-2.53282]  |
| time/                    |             |
|    fps                   | 128         |
|    iterations            | 9           |
|    time_elapsed          | 143         |
|    total_timesteps       | 18432       |
| train/                   |             |
|    approx_kl             | 0.008731915 |
|    clip_fraction         | 0.0977      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 207         |
|    entropy_loss          | -2.83       |
|    explained_variance    | -0.0218     |
|    lagrangian_multiplier | 0.0622      |
|    learning_rate         | 0.0003      |
|    loss                  | 123         |
|    n_updates             | 80          |
|    policy_gradient_loss  | -0.0107     |
|    std                   | 0.995       |
|    value_loss            | 1.01e+03    |
------------------------------------------
-------------------------------------------
| reward                   | [-1.5372381] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 8            |
|    time_elapsed          | 128          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.004804776  |
|    clip_fraction         | 0.0327       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 149          |
|    entropy_loss          | -2.71        |
|    explained_variance    | -0.0196      |
|    lagrangian_multiplier | 0.077        |
|    learning_rate         | 0.0003       |
|    loss                  | 64           |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00478     |
|    std                   | 0.936        |
|    value_loss            | 585          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9358174] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 5            |
|    time_elapsed          | 90           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0049325367 |
|    clip_fraction         | 0.037        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 128          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0445      |
|    lagrangian_multiplier | 0.0429       |
|    learning_rate         | 0.0003       |
|    loss                  | 65.3         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00315     |
|    std                   | 1            |
|    value_loss            | 380          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.50413567] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 10            |
|    time_elapsed          | 161           |
|    total_timesteps       | 20480         |
| train/                   |               |
|    approx_kl             | 0.006850017   |
|    clip_fraction         | 0.048         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 71.1          |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.0447        |
|    lagrangian_multiplier | 0.0689        |
|    learning_rate         | 0.0003        |
|    loss                  | 31.4          |
|    n_updates             | 90            |
|    policy_gradient_loss  | -0.00861      |
|    std                   | 0.984         |
|    value_loss            | 260           |
--------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.6762654] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 10           |
|    time_elapsed          | 159          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.006492552  |
|    clip_fraction         | 0.0544       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 242          |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0064      |
|    lagrangian_multiplier | 0.0652       |
|    learning_rate         | 0.0003       |
|    loss                  | 146          |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00788     |
|    std                   | 0.98         |
|    value_loss            | 1.14e+03     |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8025921] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 6            |
|    time_elapsed          | 108          |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.007397238  |
|    clip_fraction         | 0.068        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 219          |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.027       |
|    lagrangian_multiplier | 0.0685       |
|    learning_rate         | 0.0003       |
|    loss                  | 51.8         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00801     |
|    std                   | 1.01         |
|    value_loss            | 305          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1289848] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 11           |
|    time_elapsed          | 177          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.005458137  |
|    clip_fraction         | 0.0537       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 83.6         |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.00906      |
|    lagrangian_multiplier | 0.0614       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.8         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00812     |
|    std                   | 0.998        |
|    value_loss            | 639          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.0031033] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 11           |
|    time_elapsed          | 175          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0070551466 |
|    clip_fraction         | 0.0683       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 222          |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.00306     |
|    lagrangian_multiplier | 0.0583       |
|    learning_rate         | 0.0003       |
|    loss                  | 127          |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.0086      |
|    std                   | 0.979        |
|    value_loss            | 1.08e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5715353] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 9            |
|    time_elapsed          | 165          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0051793926 |
|    clip_fraction         | 0.0613       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 194          |
|    entropy_loss          | -2.7         |
|    explained_variance    | -0.0332      |
|    lagrangian_multiplier | 0.0681       |
|    learning_rate         | 0.0003       |
|    loss                  | 80.7         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00705     |
|    std                   | 0.929        |
|    value_loss            | 674          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8504882] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 7            |
|    time_elapsed          | 127          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.0065246783 |
|    clip_fraction         | 0.0502       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 210          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0314      |
|    lagrangian_multiplier | 0.0678       |
|    learning_rate         | 0.0003       |
|    loss                  | 61.2         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00549     |
|    std                   | 1            |
|    value_loss            | 467          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.1309497] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 12           |
|    time_elapsed          | 194          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.006152686  |
|    clip_fraction         | 0.0488       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 82.5         |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0126       |
|    lagrangian_multiplier | 0.0791       |
|    learning_rate         | 0.0003       |
|    loss                  | 32.7         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00798     |
|    std                   | 0.988        |
|    value_loss            | 468          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0121589] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 12           |
|    time_elapsed          | 191          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.007773425  |
|    clip_fraction         | 0.0726       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 235          |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0145       |
|    lagrangian_multiplier | 0.0629       |
|    learning_rate         | 0.0003       |
|    loss                  | 119          |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00848     |
|    std                   | 0.987        |
|    value_loss            | 890          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1832683] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 8            |
|    time_elapsed          | 145          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0040979767 |
|    clip_fraction         | 0.0335       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 215          |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.00737     |
|    lagrangian_multiplier | 0.0555       |
|    learning_rate         | 0.0003       |
|    loss                  | 117          |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00589     |
|    std                   | 0.991        |
|    value_loss            | 785          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8528025] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 13           |
|    time_elapsed          | 210          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.008363482  |
|    clip_fraction         | 0.0893       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 204          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.00537      |
|    lagrangian_multiplier | 0.0629       |
|    learning_rate         | 0.0003       |
|    loss                  | 147          |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.0105      |
|    std                   | 0.999        |
|    value_loss            | 1.27e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.6245446] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 13           |
|    time_elapsed          | 207          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.006584     |
|    clip_fraction         | 0.0646       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 227          |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.00113     |
|    lagrangian_multiplier | 0.0719       |
|    learning_rate         | 0.0003       |
|    loss                  | 124          |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00766     |
|    std                   | 0.993        |
|    value_loss            | 964          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.9718137] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 14           |
|    time_elapsed          | 226          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.006658382  |
|    clip_fraction         | 0.0587       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 134          |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0178       |
|    lagrangian_multiplier | 0.0746       |
|    learning_rate         | 0.0003       |
|    loss                  | 91.1         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00815     |
|    std                   | 1.01         |
|    value_loss            | 896          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2311692] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 9            |
|    time_elapsed          | 163          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0065768007 |
|    clip_fraction         | 0.0478       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 208          |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0428      |
|    lagrangian_multiplier | 0.0521       |
|    learning_rate         | 0.0003       |
|    loss                  | 115          |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00872     |
|    std                   | 0.991        |
|    value_loss            | 847          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5023272] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 14           |
|    time_elapsed          | 223          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.008139003  |
|    clip_fraction         | 0.0621       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 247          |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.00601      |
|    lagrangian_multiplier | 0.0623       |
|    learning_rate         | 0.0003       |
|    loss                  | 110          |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00807     |
|    std                   | 0.987        |
|    value_loss            | 1e+03        |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.415625]  |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 15           |
|    time_elapsed          | 242          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0058662565 |
|    clip_fraction         | 0.0402       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 125          |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0238       |
|    lagrangian_multiplier | 0.0725       |
|    learning_rate         | 0.0003       |
|    loss                  | 72.1         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00812     |
|    std                   | 1.01         |
|    value_loss            | 720          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.96124226] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 10            |
|    time_elapsed          | 182           |
|    total_timesteps       | 20480         |
| train/                   |               |
|    approx_kl             | 0.0034507364  |
|    clip_fraction         | 0.0312        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 210           |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.00662      |
|    lagrangian_multiplier | 0.0595        |
|    learning_rate         | 0.0003        |
|    loss                  | 124           |
|    n_updates             | 90            |
|    policy_gradient_loss  | -0.00449      |
|    std                   | 1             |
|    value_loss            | 867           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.2088904] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 15           |
|    time_elapsed          | 239          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0056786407 |
|    clip_fraction         | 0.0444       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 191          |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.00296     |
|    lagrangian_multiplier | 0.0431       |
|    learning_rate         | 0.0003       |
|    loss                  | 176          |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00503     |
|    std                   | 0.983        |
|    value_loss            | 1e+03        |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.3144332] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 16           |
|    time_elapsed          | 259          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0060796104 |
|    clip_fraction         | 0.0712       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 120          |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0074      |
|    lagrangian_multiplier | 0.0805       |
|    learning_rate         | 0.0003       |
|    loss                  | 87.6         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.0114      |
|    std                   | 1            |
|    value_loss            | 959          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.3231645] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 16           |
|    time_elapsed          | 255          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.008500511  |
|    clip_fraction         | 0.0858       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 187          |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0109      |
|    lagrangian_multiplier | 0.0567       |
|    learning_rate         | 0.0003       |
|    loss                  | 116          |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.0112      |
|    std                   | 0.984        |
|    value_loss            | 769          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5904491] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 11           |
|    time_elapsed          | 200          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0077528646 |
|    clip_fraction         | 0.0939       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 184          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0103      |
|    lagrangian_multiplier | 0.0585       |
|    learning_rate         | 0.0003       |
|    loss                  | 121          |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.0105      |
|    std                   | 1.02         |
|    value_loss            | 1.03e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.640815]  |
| time/                    |              |
|    fps                   | 82           |
|    iterations            | 10           |
|    time_elapsed          | 247          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0066904174 |
|    clip_fraction         | 0.0538       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 250          |
|    entropy_loss          | -2.69        |
|    explained_variance    | -0.0159      |
|    lagrangian_multiplier | 0.0532       |
|    learning_rate         | 0.0003       |
|    loss                  | 130          |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00986     |
|    std                   | 0.93         |
|    value_loss            | 851          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-1.960161] |
| time/                    |             |
|    fps                   | 126         |
|    iterations            | 17          |
|    time_elapsed          | 275         |
|    total_timesteps       | 34816       |
| train/                   |             |
|    approx_kl             | 0.00799142  |
|    clip_fraction         | 0.0855      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 64.8        |
|    entropy_loss          | -2.83       |
|    explained_variance    | 0.0271      |
|    lagrangian_multiplier | 0.0885      |
|    learning_rate         | 0.0003      |
|    loss                  | 84.3        |
|    n_updates             | 160         |
|    policy_gradient_loss  | -0.0105     |
|    std                   | 0.988       |
|    value_loss            | 975         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.7638719] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 17           |
|    time_elapsed          | 271          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0073539317 |
|    clip_fraction         | 0.0762       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 259          |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.00474     |
|    lagrangian_multiplier | 0.0588       |
|    learning_rate         | 0.0003       |
|    loss                  | 141          |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.0108      |
|    std                   | 0.973        |
|    value_loss            | 1.09e+03     |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.9998827] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 12           |
|    time_elapsed          | 218          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.006500625  |
|    clip_fraction         | 0.0502       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 166          |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.0197      |
|    lagrangian_multiplier | 0.0563       |
|    learning_rate         | 0.0003       |
|    loss                  | 92.4         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00791     |
|    std                   | 1.01         |
|    value_loss            | 718          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.97908324] |
| time/                    |               |
|    fps                   | 85            |
|    iterations            | 11            |
|    time_elapsed          | 263           |
|    total_timesteps       | 22528         |
| train/                   |               |
|    approx_kl             | 0.006515405   |
|    clip_fraction         | 0.0561        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 190           |
|    entropy_loss          | -2.67         |
|    explained_variance    | 0.0246        |
|    lagrangian_multiplier | 0.0498        |
|    learning_rate         | 0.0003        |
|    loss                  | 117           |
|    n_updates             | 100           |
|    policy_gradient_loss  | -0.00787      |
|    std                   | 0.914         |
|    value_loss            | 878           |
--------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.6854198] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 18           |
|    time_elapsed          | 291          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.008745052  |
|    clip_fraction         | 0.0713       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 67.4         |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0223       |
|    lagrangian_multiplier | 0.061        |
|    learning_rate         | 0.0003       |
|    loss                  | 70.2         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.0089      |
|    std                   | 0.987        |
|    value_loss            | 783          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0067105] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 18           |
|    time_elapsed          | 287          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0075511774 |
|    clip_fraction         | 0.0722       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 227          |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.00121      |
|    lagrangian_multiplier | 0.0463       |
|    learning_rate         | 0.0003       |
|    loss                  | 145          |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00868     |
|    std                   | 0.97         |
|    value_loss            | 1.05e+03     |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.3192039] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 13           |
|    time_elapsed          | 237          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.006289113  |
|    clip_fraction         | 0.0605       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 199          |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.00869      |
|    lagrangian_multiplier | 0.0811       |
|    learning_rate         | 0.0003       |
|    loss                  | 65.9         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00724     |
|    std                   | 1            |
|    value_loss            | 532          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0959883] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 19           |
|    time_elapsed          | 307          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.006560901  |
|    clip_fraction         | 0.0483       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 105          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0186       |
|    lagrangian_multiplier | 0.0638       |
|    learning_rate         | 0.0003       |
|    loss                  | 105          |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00718     |
|    std                   | 1            |
|    value_loss            | 969          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-3.1268191] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 19           |
|    time_elapsed          | 303          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.007799051  |
|    clip_fraction         | 0.0755       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 247          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.00775      |
|    lagrangian_multiplier | 0.0749       |
|    learning_rate         | 0.0003       |
|    loss                  | 78.7         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00676     |
|    std                   | 0.974        |
|    value_loss            | 694          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9006367] |
| time/                    |              |
|    fps                   | 84           |
|    iterations            | 12           |
|    time_elapsed          | 289          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.004622101  |
|    clip_fraction         | 0.0331       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 242          |
|    entropy_loss          | -2.67        |
|    explained_variance    | -0.0129      |
|    lagrangian_multiplier | 0.0639       |
|    learning_rate         | 0.0003       |
|    loss                  | 65.1         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00803     |
|    std                   | 0.927        |
|    value_loss            | 413          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.6246994] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 14           |
|    time_elapsed          | 255          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0068860827 |
|    clip_fraction         | 0.0563       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 238          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0142      |
|    lagrangian_multiplier | 0.0545       |
|    learning_rate         | 0.0003       |
|    loss                  | 136          |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00709     |
|    std                   | 1            |
|    value_loss            | 1.05e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2594419] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 20           |
|    time_elapsed          | 324          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.005635751  |
|    clip_fraction         | 0.0556       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 108          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0126       |
|    lagrangian_multiplier | 0.0655       |
|    learning_rate         | 0.0003       |
|    loss                  | 135          |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00832     |
|    std                   | 0.998        |
|    value_loss            | 1.11e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.120058]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 20           |
|    time_elapsed          | 319          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0076993974 |
|    clip_fraction         | 0.0721       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 229          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.00962      |
|    lagrangian_multiplier | 0.0667       |
|    learning_rate         | 0.0003       |
|    loss                  | 104          |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.0103      |
|    std                   | 0.97         |
|    value_loss            | 870          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0355675] |
| time/                    |              |
|    fps                   | 86           |
|    iterations            | 13           |
|    time_elapsed          | 306          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.00609484   |
|    clip_fraction         | 0.0614       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 237          |
|    entropy_loss          | -2.69        |
|    explained_variance    | 0.0201       |
|    lagrangian_multiplier | 0.0645       |
|    learning_rate         | 0.0003       |
|    loss                  | 96.7         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00884     |
|    std                   | 0.927        |
|    value_loss            | 733          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9704889] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 15           |
|    time_elapsed          | 273          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.006240708  |
|    clip_fraction         | 0.0774       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 212          |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.000268     |
|    lagrangian_multiplier | 0.0563       |
|    learning_rate         | 0.0003       |
|    loss                  | 152          |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 1.01         |
|    value_loss            | 1.37e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.4667218] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 21           |
|    time_elapsed          | 340          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.006186605  |
|    clip_fraction         | 0.0609       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 84.4         |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0208       |
|    lagrangian_multiplier | 0.0624       |
|    learning_rate         | 0.0003       |
|    loss                  | 59           |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00804     |
|    std                   | 1.01         |
|    value_loss            | 507          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.6196]    |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 21           |
|    time_elapsed          | 335          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0060080066 |
|    clip_fraction         | 0.0556       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 227          |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.00589      |
|    lagrangian_multiplier | 0.0528       |
|    learning_rate         | 0.0003       |
|    loss                  | 119          |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00663     |
|    std                   | 0.966        |
|    value_loss            | 892          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1269778] |
| time/                    |              |
|    fps                   | 88           |
|    iterations            | 14           |
|    time_elapsed          | 322          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0060821082 |
|    clip_fraction         | 0.0731       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 181          |
|    entropy_loss          | -2.69        |
|    explained_variance    | -0.0189      |
|    lagrangian_multiplier | 0.0703       |
|    learning_rate         | 0.0003       |
|    loss                  | 59.8         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.0099      |
|    std                   | 0.932        |
|    value_loss            | 464          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.4045174] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 16           |
|    time_elapsed          | 292          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0054127434 |
|    clip_fraction         | 0.0359       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 213          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.00368     |
|    lagrangian_multiplier | 0.0567       |
|    learning_rate         | 0.0003       |
|    loss                  | 91.2         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00884     |
|    std                   | 1.02         |
|    value_loss            | 654          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.0945768] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 22           |
|    time_elapsed          | 356          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.0049025947 |
|    clip_fraction         | 0.0374       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 79.6         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0212       |
|    lagrangian_multiplier | 0.0744       |
|    learning_rate         | 0.0003       |
|    loss                  | 67.1         |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00605     |
|    std                   | 1.01         |
|    value_loss            | 729          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.9572017] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 22           |
|    time_elapsed          | 351          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.0066271983 |
|    clip_fraction         | 0.0578       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 263          |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.012        |
|    lagrangian_multiplier | 0.0747       |
|    learning_rate         | 0.0003       |
|    loss                  | 140          |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00691     |
|    std                   | 0.968        |
|    value_loss            | 1.25e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9185222] |
| time/                    |              |
|    fps                   | 90           |
|    iterations            | 15           |
|    time_elapsed          | 338          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.004951965  |
|    clip_fraction         | 0.0511       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 199          |
|    entropy_loss          | -2.7         |
|    explained_variance    | 0.00638      |
|    lagrangian_multiplier | 0.0566       |
|    learning_rate         | 0.0003       |
|    loss                  | 74.2         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.0083      |
|    std                   | 0.933        |
|    value_loss            | 551          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.47143114] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 23            |
|    time_elapsed          | 372           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.0052420394  |
|    clip_fraction         | 0.0432        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 159           |
|    entropy_loss          | -2.86         |
|    explained_variance    | 0.00997       |
|    lagrangian_multiplier | 0.0759        |
|    learning_rate         | 0.0003        |
|    loss                  | 217           |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00806      |
|    std                   | 1.02          |
|    value_loss            | 2.14e+03      |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.2141726] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 17           |
|    time_elapsed          | 310          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.005635936  |
|    clip_fraction         | 0.0443       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 239          |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.034       |
|    lagrangian_multiplier | 0.0459       |
|    learning_rate         | 0.0003       |
|    loss                  | 136          |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00912     |
|    std                   | 1            |
|    value_loss            | 832          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.4432887] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 23           |
|    time_elapsed          | 367          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.0070427656 |
|    clip_fraction         | 0.0646       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 263          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0104       |
|    lagrangian_multiplier | 0.0715       |
|    learning_rate         | 0.0003       |
|    loss                  | 171          |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.00711     |
|    std                   | 0.976        |
|    value_loss            | 1.45e+03     |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.6717982] |
| time/                    |              |
|    fps                   | 92           |
|    iterations            | 16           |
|    time_elapsed          | 354          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.005490616  |
|    clip_fraction         | 0.0419       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 231          |
|    entropy_loss          | -2.71        |
|    explained_variance    | -0.0121      |
|    lagrangian_multiplier | 0.0445       |
|    learning_rate         | 0.0003       |
|    loss                  | 105          |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00732     |
|    std                   | 0.94         |
|    value_loss            | 579          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.39088786] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 24            |
|    time_elapsed          | 389           |
|    total_timesteps       | 49152         |
| train/                   |               |
|    approx_kl             | 0.0065270476  |
|    clip_fraction         | 0.0614        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 142           |
|    entropy_loss          | -2.87         |
|    explained_variance    | 8.32e-05      |
|    lagrangian_multiplier | 0.0598        |
|    learning_rate         | 0.0003        |
|    loss                  | 158           |
|    n_updates             | 230           |
|    policy_gradient_loss  | -0.011        |
|    std                   | 1.02          |
|    value_loss            | 1.33e+03      |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.6126978] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 24           |
|    time_elapsed          | 383          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.006434921  |
|    clip_fraction         | 0.0621       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 242          |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0131       |
|    lagrangian_multiplier | 0.0638       |
|    learning_rate         | 0.0003       |
|    loss                  | 154          |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00657     |
|    std                   | 0.973        |
|    value_loss            | 1.23e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9318479] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 18           |
|    time_elapsed          | 328          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0056745633 |
|    clip_fraction         | 0.0373       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 231          |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0368      |
|    lagrangian_multiplier | 0.0645       |
|    learning_rate         | 0.0003       |
|    loss                  | 83.5         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00733     |
|    std                   | 1.01         |
|    value_loss            | 708          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5506053] |
| time/                    |              |
|    fps                   | 93           |
|    iterations            | 17           |
|    time_elapsed          | 371          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.005063035  |
|    clip_fraction         | 0.0425       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 236          |
|    entropy_loss          | -2.71        |
|    explained_variance    | -0.0071      |
|    lagrangian_multiplier | 0.0508       |
|    learning_rate         | 0.0003       |
|    loss                  | 97.5         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00673     |
|    std                   | 0.939        |
|    value_loss            | 712          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.45222822] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 25            |
|    time_elapsed          | 405           |
|    total_timesteps       | 51200         |
| train/                   |               |
|    approx_kl             | 0.0062625417  |
|    clip_fraction         | 0.0604        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 145           |
|    entropy_loss          | -2.87         |
|    explained_variance    | 0.00719       |
|    lagrangian_multiplier | 0.0774        |
|    learning_rate         | 0.0003        |
|    loss                  | 114           |
|    n_updates             | 240           |
|    policy_gradient_loss  | -0.00938      |
|    std                   | 1.01          |
|    value_loss            | 1.16e+03      |
--------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.6168269] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 25           |
|    time_elapsed          | 399          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.006163019  |
|    clip_fraction         | 0.071        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 246          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.00169      |
|    lagrangian_multiplier | 0.0711       |
|    learning_rate         | 0.0003       |
|    loss                  | 117          |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00897     |
|    std                   | 0.97         |
|    value_loss            | 1.02e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8517703] |
| time/                    |              |
|    fps                   | 95           |
|    iterations            | 18           |
|    time_elapsed          | 387          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.005664983  |
|    clip_fraction         | 0.0488       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 239          |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0031       |
|    lagrangian_multiplier | 0.0508       |
|    learning_rate         | 0.0003       |
|    loss                  | 109          |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00733     |
|    std                   | 0.945        |
|    value_loss            | 663          |
-------------------------------------------
------------------------------------------
| reward                   | [-2.912462] |
| time/                    |             |
|    fps                   | 112         |
|    iterations            | 19          |
|    time_elapsed          | 346         |
|    total_timesteps       | 38912       |
| train/                   |             |
|    approx_kl             | 0.008378355 |
|    clip_fraction         | 0.0833      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 267         |
|    entropy_loss          | -2.85       |
|    explained_variance    | -0.0314     |
|    lagrangian_multiplier | 0.0689      |
|    learning_rate         | 0.0003      |
|    loss                  | 92.5        |
|    n_updates             | 180         |
|    policy_gradient_loss  | -0.0102     |
|    std                   | 1           |
|    value_loss            | 864         |
------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.7767511] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 26           |
|    time_elapsed          | 421          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.007167667  |
|    clip_fraction         | 0.0761       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 73.8         |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0148       |
|    lagrangian_multiplier | 0.0599       |
|    learning_rate         | 0.0003       |
|    loss                  | 135          |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.0118      |
|    std                   | 1.02         |
|    value_loss            | 1.13e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0291662] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 26           |
|    time_elapsed          | 415          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.007198618  |
|    clip_fraction         | 0.0606       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 209          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0029       |
|    lagrangian_multiplier | 0.0678       |
|    learning_rate         | 0.0003       |
|    loss                  | 111          |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00737     |
|    std                   | 0.969        |
|    value_loss            | 1.18e+03     |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.4302425] |
| time/                    |              |
|    fps                   | 96           |
|    iterations            | 19           |
|    time_elapsed          | 405          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0067461873 |
|    clip_fraction         | 0.0603       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 261          |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.00368     |
|    lagrangian_multiplier | 0.0552       |
|    learning_rate         | 0.0003       |
|    loss                  | 98.4         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00851     |
|    std                   | 0.952        |
|    value_loss            | 616          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.1890392] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 20           |
|    time_elapsed          | 365          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0061424193 |
|    clip_fraction         | 0.0458       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 241          |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0183      |
|    lagrangian_multiplier | 0.0681       |
|    learning_rate         | 0.0003       |
|    loss                  | 106          |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00725     |
|    std                   | 0.992        |
|    value_loss            | 955          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8470318] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 27           |
|    time_elapsed          | 437          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.00671647   |
|    clip_fraction         | 0.0523       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 105          |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0301       |
|    lagrangian_multiplier | 0.0634       |
|    learning_rate         | 0.0003       |
|    loss                  | 90.9         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00746     |
|    std                   | 1.02         |
|    value_loss            | 825          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6539459] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 27           |
|    time_elapsed          | 431          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.0058396216 |
|    clip_fraction         | 0.0762       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 243          |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.00408      |
|    lagrangian_multiplier | 0.0697       |
|    learning_rate         | 0.0003       |
|    loss                  | 160          |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00959     |
|    std                   | 0.982        |
|    value_loss            | 1.52e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4304664] |
| time/                    |              |
|    fps                   | 97           |
|    iterations            | 20           |
|    time_elapsed          | 421          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.007596483  |
|    clip_fraction         | 0.0648       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 251          |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.00227      |
|    lagrangian_multiplier | 0.0462       |
|    learning_rate         | 0.0003       |
|    loss                  | 115          |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00805     |
|    std                   | 0.945        |
|    value_loss            | 779          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5219835] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 21           |
|    time_elapsed          | 383          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0051703593 |
|    clip_fraction         | 0.0547       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 242          |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0222      |
|    lagrangian_multiplier | 0.052        |
|    learning_rate         | 0.0003       |
|    loss                  | 161          |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00831     |
|    std                   | 0.992        |
|    value_loss            | 1.17e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7865907] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 28           |
|    time_elapsed          | 453          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.00757023   |
|    clip_fraction         | 0.074        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 133          |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0197       |
|    lagrangian_multiplier | 0.072        |
|    learning_rate         | 0.0003       |
|    loss                  | 127          |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.0109      |
|    std                   | 1.02         |
|    value_loss            | 1.27e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7401188] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 28           |
|    time_elapsed          | 447          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.00968721   |
|    clip_fraction         | 0.0982       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 212          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.00125      |
|    lagrangian_multiplier | 0.063        |
|    learning_rate         | 0.0003       |
|    loss                  | 94           |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.0115      |
|    std                   | 0.968        |
|    value_loss            | 697          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.81818414] |
| time/                    |               |
|    fps                   | 98            |
|    iterations            | 21            |
|    time_elapsed          | 437           |
|    total_timesteps       | 43008         |
| train/                   |               |
|    approx_kl             | 0.006683775   |
|    clip_fraction         | 0.0518        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 213           |
|    entropy_loss          | -2.74         |
|    explained_variance    | 0.00305       |
|    lagrangian_multiplier | 0.0543        |
|    learning_rate         | 0.0003        |
|    loss                  | 56.8          |
|    n_updates             | 200           |
|    policy_gradient_loss  | -0.00915      |
|    std                   | 0.958         |
|    value_loss            | 372           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.5512373] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 22           |
|    time_elapsed          | 401          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.0075650807 |
|    clip_fraction         | 0.0813       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 248          |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0327      |
|    lagrangian_multiplier | 0.0668       |
|    learning_rate         | 0.0003       |
|    loss                  | 101          |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.0123      |
|    std                   | 0.995        |
|    value_loss            | 759          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.4270918] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 29           |
|    time_elapsed          | 470          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.0064316946 |
|    clip_fraction         | 0.0655       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 140          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0112      |
|    lagrangian_multiplier | 0.062        |
|    learning_rate         | 0.0003       |
|    loss                  | 194          |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00863     |
|    std                   | 1.01         |
|    value_loss            | 1.6e+03      |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2598977] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 29           |
|    time_elapsed          | 463          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.009313909  |
|    clip_fraction         | 0.0867       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 240          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0175       |
|    lagrangian_multiplier | 0.0736       |
|    learning_rate         | 0.0003       |
|    loss                  | 108          |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.0122      |
|    std                   | 0.976        |
|    value_loss            | 1e+03        |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5686013] |
| time/                    |              |
|    fps                   | 99           |
|    iterations            | 22           |
|    time_elapsed          | 454          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.006412291  |
|    clip_fraction         | 0.06         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 180          |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0282      |
|    lagrangian_multiplier | 0.0484       |
|    learning_rate         | 0.0003       |
|    loss                  | 77.4         |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00873     |
|    std                   | 0.966        |
|    value_loss            | 386          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5478082] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 23           |
|    time_elapsed          | 420          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.0053046844 |
|    clip_fraction         | 0.0482       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 240          |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0385      |
|    lagrangian_multiplier | 0.0572       |
|    learning_rate         | 0.0003       |
|    loss                  | 80.3         |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.00768     |
|    std                   | 0.997        |
|    value_loss            | 483          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8905185] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 30           |
|    time_elapsed          | 486          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.00621022   |
|    clip_fraction         | 0.0571       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 127          |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.00529     |
|    lagrangian_multiplier | 0.0778       |
|    learning_rate         | 0.0003       |
|    loss                  | 90           |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00818     |
|    std                   | 1.01         |
|    value_loss            | 806          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4479274] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 30           |
|    time_elapsed          | 480          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.007507343  |
|    clip_fraction         | 0.0666       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 255          |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0127       |
|    lagrangian_multiplier | 0.0642       |
|    learning_rate         | 0.0003       |
|    loss                  | 107          |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00976     |
|    std                   | 0.979        |
|    value_loss            | 826          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8557826] |
| time/                    |              |
|    fps                   | 100          |
|    iterations            | 23           |
|    time_elapsed          | 470          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.007142962  |
|    clip_fraction         | 0.0728       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 203          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.00279      |
|    lagrangian_multiplier | 0.0428       |
|    learning_rate         | 0.0003       |
|    loss                  | 163          |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.00952     |
|    std                   | 0.972        |
|    value_loss            | 986          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8119292] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 24           |
|    time_elapsed          | 438          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.0069013834 |
|    clip_fraction         | 0.0718       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 260          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0428       |
|    lagrangian_multiplier | 0.0692       |
|    learning_rate         | 0.0003       |
|    loss                  | 68.9         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.0132      |
|    std                   | 0.998        |
|    value_loss            | 592          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.4374406] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 31           |
|    time_elapsed          | 502          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0056997575 |
|    clip_fraction         | 0.0527       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 151          |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.00157      |
|    lagrangian_multiplier | 0.0755       |
|    learning_rate         | 0.0003       |
|    loss                  | 152          |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00763     |
|    std                   | 1.01         |
|    value_loss            | 1.76e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5658997] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 31           |
|    time_elapsed          | 496          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0073531643 |
|    clip_fraction         | 0.08         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 256          |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0155       |
|    lagrangian_multiplier | 0.0739       |
|    learning_rate         | 0.0003       |
|    loss                  | 108          |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00929     |
|    std                   | 0.989        |
|    value_loss            | 858          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7978176] |
| time/                    |              |
|    fps                   | 100          |
|    iterations            | 24           |
|    time_elapsed          | 487          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.005250018  |
|    clip_fraction         | 0.0473       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 259          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.00453      |
|    lagrangian_multiplier | 0.0523       |
|    learning_rate         | 0.0003       |
|    loss                  | 83.6         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00595     |
|    std                   | 0.973        |
|    value_loss            | 489          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8668741] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 32           |
|    time_elapsed          | 518          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.006491008  |
|    clip_fraction         | 0.0714       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 178          |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0232       |
|    lagrangian_multiplier | 0.0553       |
|    learning_rate         | 0.0003       |
|    loss                  | 145          |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00986     |
|    std                   | 1.01         |
|    value_loss            | 1.1e+03      |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5714281] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 32           |
|    time_elapsed          | 512          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.007818036  |
|    clip_fraction         | 0.0775       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 249          |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.00825      |
|    lagrangian_multiplier | 0.0686       |
|    learning_rate         | 0.0003       |
|    loss                  | 103          |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00908     |
|    std                   | 0.978        |
|    value_loss            | 899          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.84983355] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 25            |
|    time_elapsed          | 456           |
|    total_timesteps       | 51200         |
| train/                   |               |
|    approx_kl             | 0.007659844   |
|    clip_fraction         | 0.0646        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 249           |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.0312       |
|    lagrangian_multiplier | 0.0611        |
|    learning_rate         | 0.0003        |
|    loss                  | 60.2          |
|    n_updates             | 240           |
|    policy_gradient_loss  | -0.00885      |
|    std                   | 0.994         |
|    value_loss            | 353           |
--------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1576742] |
| time/                    |              |
|    fps                   | 101          |
|    iterations            | 25           |
|    time_elapsed          | 503          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.006909959  |
|    clip_fraction         | 0.0666       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 246          |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0271       |
|    lagrangian_multiplier | 0.0633       |
|    learning_rate         | 0.0003       |
|    loss                  | 75.8         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00902     |
|    std                   | 0.978        |
|    value_loss            | 576          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.9621862] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 33           |
|    time_elapsed          | 535          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.006487431  |
|    clip_fraction         | 0.0686       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 153          |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0196       |
|    lagrangian_multiplier | 0.0781       |
|    learning_rate         | 0.0003       |
|    loss                  | 120          |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.01        |
|    std                   | 1.02         |
|    value_loss            | 1.32e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9525209] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 33           |
|    time_elapsed          | 528          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.010537173  |
|    clip_fraction         | 0.102        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 264          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0253       |
|    lagrangian_multiplier | 0.0687       |
|    learning_rate         | 0.0003       |
|    loss                  | 78.1         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.0114      |
|    std                   | 0.972        |
|    value_loss            | 681          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5002846] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 26           |
|    time_elapsed          | 475          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0073223915 |
|    clip_fraction         | 0.0574       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 258          |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.00674      |
|    lagrangian_multiplier | 0.0668       |
|    learning_rate         | 0.0003       |
|    loss                  | 69.1         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00908     |
|    std                   | 1.01         |
|    value_loss            | 558          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.3340358] |
| time/                    |              |
|    fps                   | 102          |
|    iterations            | 26           |
|    time_elapsed          | 519          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0068316264 |
|    clip_fraction         | 0.0528       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 179          |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0309      |
|    lagrangian_multiplier | 0.0519       |
|    learning_rate         | 0.0003       |
|    loss                  | 66           |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00638     |
|    std                   | 0.984        |
|    value_loss            | 385          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.3805683] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 34           |
|    time_elapsed          | 544          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.008972374  |
|    clip_fraction         | 0.0848       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 192          |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.0104       |
|    lagrangian_multiplier | 0.0725       |
|    learning_rate         | 0.0003       |
|    loss                  | 75.8         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.96         |
|    value_loss            | 574          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.9785185] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 34           |
|    time_elapsed          | 551          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0068901377 |
|    clip_fraction         | 0.0606       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 149          |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.000696     |
|    lagrangian_multiplier | 0.0631       |
|    learning_rate         | 0.0003       |
|    loss                  | 118          |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00971     |
|    std                   | 1.02         |
|    value_loss            | 1.13e+03     |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8092394] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 27           |
|    time_elapsed          | 493          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.006951018  |
|    clip_fraction         | 0.0808       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 262          |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.0258      |
|    lagrangian_multiplier | 0.0648       |
|    learning_rate         | 0.0003       |
|    loss                  | 59.3         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00986     |
|    std                   | 1.02         |
|    value_loss            | 362          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4349645] |
| time/                    |              |
|    fps                   | 103          |
|    iterations            | 27           |
|    time_elapsed          | 536          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.0054613734 |
|    clip_fraction         | 0.052        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 131          |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.00232      |
|    lagrangian_multiplier | 0.0958       |
|    learning_rate         | 0.0003       |
|    loss                  | 40.3         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00873     |
|    std                   | 0.984        |
|    value_loss            | 459          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2037151] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 35           |
|    time_elapsed          | 560          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.008380432  |
|    clip_fraction         | 0.101        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 256          |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.00166     |
|    lagrangian_multiplier | 0.0647       |
|    learning_rate         | 0.0003       |
|    loss                  | 84.8         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.0146      |
|    std                   | 0.961        |
|    value_loss            | 829          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5199077] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 35           |
|    time_elapsed          | 567          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.007814001  |
|    clip_fraction         | 0.06         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 145          |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0252       |
|    lagrangian_multiplier | 0.0623       |
|    learning_rate         | 0.0003       |
|    loss                  | 110          |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 1.01         |
|    value_loss            | 866          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.26173413] |
| time/                    |               |
|    fps                   | 103           |
|    iterations            | 28            |
|    time_elapsed          | 552           |
|    total_timesteps       | 57344         |
| train/                   |               |
|    approx_kl             | 0.00859647    |
|    clip_fraction         | 0.0773        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 203           |
|    entropy_loss          | -2.8          |
|    explained_variance    | -0.0258       |
|    lagrangian_multiplier | 0.0553        |
|    learning_rate         | 0.0003        |
|    loss                  | 51.6          |
|    n_updates             | 270           |
|    policy_gradient_loss  | -0.00968      |
|    std                   | 0.984         |
|    value_loss            | 280           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.0606139] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 28           |
|    time_elapsed          | 511          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.008900611  |
|    clip_fraction         | 0.0717       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 262          |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.0331      |
|    lagrangian_multiplier | 0.0578       |
|    learning_rate         | 0.0003       |
|    loss                  | 83.3         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00918     |
|    std                   | 1.02         |
|    value_loss            | 538          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8375816] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 36           |
|    time_elapsed          | 576          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.008827571  |
|    clip_fraction         | 0.0939       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 222          |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.0132       |
|    lagrangian_multiplier | 0.0655       |
|    learning_rate         | 0.0003       |
|    loss                  | 77.6         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0129      |
|    std                   | 0.949        |
|    value_loss            | 521          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8597848] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 36           |
|    time_elapsed          | 584          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0069203223 |
|    clip_fraction         | 0.0568       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 155          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.00973      |
|    lagrangian_multiplier | 0.064        |
|    learning_rate         | 0.0003       |
|    loss                  | 154          |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0103      |
|    std                   | 0.997        |
|    value_loss            | 1.34e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8243945] |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 29           |
|    time_elapsed          | 569          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.006874078  |
|    clip_fraction         | 0.0708       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 167          |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0223      |
|    lagrangian_multiplier | 0.0548       |
|    learning_rate         | 0.0003       |
|    loss                  | 70.5         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00881     |
|    std                   | 0.988        |
|    value_loss            | 468          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.56559753] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 29            |
|    time_elapsed          | 530           |
|    total_timesteps       | 59392         |
| train/                   |               |
|    approx_kl             | 0.0057065403  |
|    clip_fraction         | 0.052         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 235           |
|    entropy_loss          | -2.88         |
|    explained_variance    | -0.000405     |
|    lagrangian_multiplier | 0.0658        |
|    learning_rate         | 0.0003        |
|    loss                  | 131           |
|    n_updates             | 280           |
|    policy_gradient_loss  | -0.0075       |
|    std                   | 1.03          |
|    value_loss            | 1.05e+03      |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.4589242] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 37           |
|    time_elapsed          | 592          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.0094957035 |
|    clip_fraction         | 0.0959       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 247          |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.0042      |
|    lagrangian_multiplier | 0.0584       |
|    learning_rate         | 0.0003       |
|    loss                  | 88.7         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.0116      |
|    std                   | 0.938        |
|    value_loss            | 618          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.5590672] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 37           |
|    time_elapsed          | 600          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.007816268  |
|    clip_fraction         | 0.08         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 148          |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0143       |
|    lagrangian_multiplier | 0.0757       |
|    learning_rate         | 0.0003       |
|    loss                  | 94.4         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.0119      |
|    std                   | 0.996        |
|    value_loss            | 974          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9355868] |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 30           |
|    time_elapsed          | 585          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0057321996 |
|    clip_fraction         | 0.0517       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 189          |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0192      |
|    lagrangian_multiplier | 0.0743       |
|    learning_rate         | 0.0003       |
|    loss                  | 49.7         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.0084      |
|    std                   | 0.985        |
|    value_loss            | 347          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0563182] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 30           |
|    time_elapsed          | 548          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.00839363   |
|    clip_fraction         | 0.0706       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 228          |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0142       |
|    lagrangian_multiplier | 0.0514       |
|    learning_rate         | 0.0003       |
|    loss                  | 124          |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.01        |
|    std                   | 1.03         |
|    value_loss            | 727          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2882382] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 38           |
|    time_elapsed          | 608          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0077207917 |
|    clip_fraction         | 0.0644       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 189          |
|    entropy_loss          | -2.69        |
|    explained_variance    | 0.00106      |
|    lagrangian_multiplier | 0.0565       |
|    learning_rate         | 0.0003       |
|    loss                  | 76.8         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00752     |
|    std                   | 0.923        |
|    value_loss            | 539          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0108392] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 38           |
|    time_elapsed          | 616          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.007579402  |
|    clip_fraction         | 0.0679       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 168          |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0281       |
|    lagrangian_multiplier | 0.0658       |
|    learning_rate         | 0.0003       |
|    loss                  | 85.9         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.0105      |
|    std                   | 1            |
|    value_loss            | 928          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.87586516] |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 31            |
|    time_elapsed          | 601           |
|    total_timesteps       | 63488         |
| train/                   |               |
|    approx_kl             | 0.0063885897  |
|    clip_fraction         | 0.0489        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 218           |
|    entropy_loss          | -2.78         |
|    explained_variance    | -0.0186       |
|    lagrangian_multiplier | 0.0636        |
|    learning_rate         | 0.0003        |
|    loss                  | 62            |
|    n_updates             | 300           |
|    policy_gradient_loss  | -0.00811      |
|    std                   | 0.963         |
|    value_loss            | 392           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.7924592] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 31           |
|    time_elapsed          | 566          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0074099433 |
|    clip_fraction         | 0.0694       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 202          |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.0217      |
|    lagrangian_multiplier | 0.065        |
|    learning_rate         | 0.0003       |
|    loss                  | 39.3         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00982     |
|    std                   | 1.03         |
|    value_loss            | 228          |
-------------------------------------------
------------------------------------------
| reward                   | [-2.071324] |
| time/                    |             |
|    fps                   | 127         |
|    iterations            | 39          |
|    time_elapsed          | 624         |
|    total_timesteps       | 79872       |
| train/                   |             |
|    approx_kl             | 0.009346308 |
|    clip_fraction         | 0.103       |
|    clip_range            | 0.2         |
|    cost_value_loss       | 201         |
|    entropy_loss          | -2.7        |
|    explained_variance    | -0.000945   |
|    lagrangian_multiplier | 0.059       |
|    learning_rate         | 0.0003      |
|    loss                  | 75.2        |
|    n_updates             | 380         |
|    policy_gradient_loss  | -0.0142     |
|    std                   | 0.945       |
|    value_loss            | 478         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.6848577] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 39           |
|    time_elapsed          | 632          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.010955299  |
|    clip_fraction         | 0.111        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 143          |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0239       |
|    lagrangian_multiplier | 0.0572       |
|    learning_rate         | 0.0003       |
|    loss                  | 130          |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.0154      |
|    std                   | 1.01         |
|    value_loss            | 1.15e+03     |
-------------------------------------------
------------------------------------------
| reward                   | [-1.232548] |
| time/                    |             |
|    fps                   | 106         |
|    iterations            | 32          |
|    time_elapsed          | 618         |
|    total_timesteps       | 65536       |
| train/                   |             |
|    approx_kl             | 0.008287851 |
|    clip_fraction         | 0.076       |
|    clip_range            | 0.2         |
|    cost_value_loss       | 235         |
|    entropy_loss          | -2.76       |
|    explained_variance    | -0.0248     |
|    lagrangian_multiplier | 0.0518      |
|    learning_rate         | 0.0003      |
|    loss                  | 59.1        |
|    n_updates             | 310         |
|    policy_gradient_loss  | -0.00916    |
|    std                   | 0.96        |
|    value_loss            | 297         |
------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2085849] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 40           |
|    time_elapsed          | 640          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.008937489  |
|    clip_fraction         | 0.0937       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 269          |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.00914      |
|    lagrangian_multiplier | 0.0675       |
|    learning_rate         | 0.0003       |
|    loss                  | 75.6         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.0121      |
|    std                   | 0.942        |
|    value_loss            | 572          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1950518] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 32           |
|    time_elapsed          | 585          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.0068778098 |
|    clip_fraction         | 0.0631       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 215          |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.00801      |
|    lagrangian_multiplier | 0.0737       |
|    learning_rate         | 0.0003       |
|    loss                  | 65.4         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00975     |
|    std                   | 1.02         |
|    value_loss            | 515          |
-------------------------------------------
------------------------------------------
| reward                   | [-3.5882]   |
| time/                    |             |
|    fps                   | 126         |
|    iterations            | 40          |
|    time_elapsed          | 649         |
|    total_timesteps       | 81920       |
| train/                   |             |
|    approx_kl             | 0.006851478 |
|    clip_fraction         | 0.0597      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 88.6        |
|    entropy_loss          | -2.85       |
|    explained_variance    | 0.0346      |
|    lagrangian_multiplier | 0.0712      |
|    learning_rate         | 0.0003      |
|    loss                  | 63.4        |
|    n_updates             | 390         |
|    policy_gradient_loss  | -0.0094     |
|    std                   | 1.01        |
|    value_loss            | 783         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.4284903] |
| time/                    |              |
|    fps                   | 106          |
|    iterations            | 33           |
|    time_elapsed          | 634          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.008015124  |
|    clip_fraction         | 0.0917       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 200          |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.00394      |
|    lagrangian_multiplier | 0.0594       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.8         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00994     |
|    std                   | 0.964        |
|    value_loss            | 365          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.6575089] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 41           |
|    time_elapsed          | 656          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0073664533 |
|    clip_fraction         | 0.0674       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 243          |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0145       |
|    lagrangian_multiplier | 0.0592       |
|    learning_rate         | 0.0003       |
|    loss                  | 76.1         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00811     |
|    std                   | 0.945        |
|    value_loss            | 528          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.7925124] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 41           |
|    time_elapsed          | 665          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0075967936 |
|    clip_fraction         | 0.0652       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 209          |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.022        |
|    lagrangian_multiplier | 0.0666       |
|    learning_rate         | 0.0003       |
|    loss                  | 159          |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 1.01         |
|    value_loss            | 1.47e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5117152] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 33           |
|    time_elapsed          | 603          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.008021154  |
|    clip_fraction         | 0.0681       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 215          |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.0183      |
|    lagrangian_multiplier | 0.0682       |
|    learning_rate         | 0.0003       |
|    loss                  | 135          |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.0103      |
|    std                   | 1.03         |
|    value_loss            | 1.23e+03     |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1888484] |
| time/                    |              |
|    fps                   | 107          |
|    iterations            | 34           |
|    time_elapsed          | 650          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0065736137 |
|    clip_fraction         | 0.0606       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 202          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.00651     |
|    lagrangian_multiplier | 0.0617       |
|    learning_rate         | 0.0003       |
|    loss                  | 51           |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00804     |
|    std                   | 0.976        |
|    value_loss            | 341          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.671585]  |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 42           |
|    time_elapsed          | 672          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0071614906 |
|    clip_fraction         | 0.0767       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 199          |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.00712      |
|    lagrangian_multiplier | 0.0668       |
|    learning_rate         | 0.0003       |
|    loss                  | 53.2         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00789     |
|    std                   | 0.952        |
|    value_loss            | 357          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.1144357] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 42           |
|    time_elapsed          | 681          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.008171435  |
|    clip_fraction         | 0.0899       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 142          |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0104       |
|    lagrangian_multiplier | 0.0567       |
|    learning_rate         | 0.0003       |
|    loss                  | 133          |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.0121      |
|    std                   | 1.04         |
|    value_loss            | 1.13e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6975038] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 34           |
|    time_elapsed          | 621          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0059044454 |
|    clip_fraction         | 0.0579       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 232          |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.00848      |
|    lagrangian_multiplier | 0.0657       |
|    learning_rate         | 0.0003       |
|    loss                  | 84.4         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00931     |
|    std                   | 1.03         |
|    value_loss            | 583          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0982323] |
| time/                    |              |
|    fps                   | 107          |
|    iterations            | 35           |
|    time_elapsed          | 666          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0070559257 |
|    clip_fraction         | 0.0715       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 267          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0324       |
|    lagrangian_multiplier | 0.0619       |
|    learning_rate         | 0.0003       |
|    loss                  | 67.9         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00912     |
|    std                   | 0.968        |
|    value_loss            | 382          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2900722] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 43           |
|    time_elapsed          | 688          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.009320056  |
|    clip_fraction         | 0.0709       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 208          |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.00142      |
|    lagrangian_multiplier | 0.0672       |
|    learning_rate         | 0.0003       |
|    loss                  | 74.6         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00892     |
|    std                   | 0.944        |
|    value_loss            | 569          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.0602787] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 43           |
|    time_elapsed          | 697          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0058085187 |
|    clip_fraction         | 0.0578       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 141          |
|    entropy_loss          | -2.9         |
|    explained_variance    | -0.0168      |
|    lagrangian_multiplier | 0.0545       |
|    learning_rate         | 0.0003       |
|    loss                  | 91.9         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00882     |
|    std                   | 1.04         |
|    value_loss            | 643          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.5878792] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 35           |
|    time_elapsed          | 640          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.004084166  |
|    clip_fraction         | 0.0419       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 144          |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.0223       |
|    lagrangian_multiplier | 0.0624       |
|    learning_rate         | 0.0003       |
|    loss                  | 71.3         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00758     |
|    std                   | 1.05         |
|    value_loss            | 527          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2820284] |
| time/                    |              |
|    fps                   | 107          |
|    iterations            | 36           |
|    time_elapsed          | 682          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0069790105 |
|    clip_fraction         | 0.0868       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 183          |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0267      |
|    lagrangian_multiplier | 0.0597       |
|    learning_rate         | 0.0003       |
|    loss                  | 45.4         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.963        |
|    value_loss            | 297          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.51638114] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 44            |
|    time_elapsed          | 704           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.011237161   |
|    clip_fraction         | 0.11          |
|    clip_range            | 0.2           |
|    cost_value_loss       | 231           |
|    entropy_loss          | -2.71         |
|    explained_variance    | 0.00938       |
|    lagrangian_multiplier | 0.0663        |
|    learning_rate         | 0.0003        |
|    loss                  | 80.2          |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.0152       |
|    std                   | 0.935         |
|    value_loss            | 551           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.5462411] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 44           |
|    time_elapsed          | 714          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.0079371445 |
|    clip_fraction         | 0.0629       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 115          |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.0221       |
|    lagrangian_multiplier | 0.0616       |
|    learning_rate         | 0.0003       |
|    loss                  | 39.8         |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 1.04         |
|    value_loss            | 468          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7745942] |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 37           |
|    time_elapsed          | 698          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.0085217375 |
|    clip_fraction         | 0.0832       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 255          |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.0116       |
|    lagrangian_multiplier | 0.055        |
|    learning_rate         | 0.0003       |
|    loss                  | 54.4         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00989     |
|    std                   | 0.961        |
|    value_loss            | 244          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.3574305] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 36           |
|    time_elapsed          | 658          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0070696883 |
|    clip_fraction         | 0.0541       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 151          |
|    entropy_loss          | -2.96        |
|    explained_variance    | -0.0105      |
|    lagrangian_multiplier | 0.0717       |
|    learning_rate         | 0.0003       |
|    loss                  | 71           |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0094      |
|    std                   | 1.08         |
|    value_loss            | 490          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.37032253] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 45            |
|    time_elapsed          | 720           |
|    total_timesteps       | 92160         |
| train/                   |               |
|    approx_kl             | 0.009359932   |
|    clip_fraction         | 0.0911        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 260           |
|    entropy_loss          | -2.7          |
|    explained_variance    | -0.000987     |
|    lagrangian_multiplier | 0.0736        |
|    learning_rate         | 0.0003        |
|    loss                  | 64.2          |
|    n_updates             | 440           |
|    policy_gradient_loss  | -0.0115       |
|    std                   | 0.936         |
|    value_loss            | 496           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.52796674] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 45            |
|    time_elapsed          | 730           |
|    total_timesteps       | 92160         |
| train/                   |               |
|    approx_kl             | 0.006437512   |
|    clip_fraction         | 0.0633        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 93.1          |
|    entropy_loss          | -2.91         |
|    explained_variance    | 0.0331        |
|    lagrangian_multiplier | 0.0735        |
|    learning_rate         | 0.0003        |
|    loss                  | 90.8          |
|    n_updates             | 440           |
|    policy_gradient_loss  | -0.00829      |
|    std                   | 1.05          |
|    value_loss            | 866           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.8275862] |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 38           |
|    time_elapsed          | 715          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0071700206 |
|    clip_fraction         | 0.0813       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 269          |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.00241     |
|    lagrangian_multiplier | 0.0555       |
|    learning_rate         | 0.0003       |
|    loss                  | 71.1         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 0.959        |
|    value_loss            | 406          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0235716] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 37           |
|    time_elapsed          | 677          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.007892416  |
|    clip_fraction         | 0.0866       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 263          |
|    entropy_loss          | -2.99        |
|    explained_variance    | 0.0168       |
|    lagrangian_multiplier | 0.0708       |
|    learning_rate         | 0.0003       |
|    loss                  | 111          |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.0136      |
|    std                   | 1.09         |
|    value_loss            | 887          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.44079334] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 46            |
|    time_elapsed          | 736           |
|    total_timesteps       | 94208         |
| train/                   |               |
|    approx_kl             | 0.008053209   |
|    clip_fraction         | 0.0985        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 267           |
|    entropy_loss          | -2.71         |
|    explained_variance    | 0.00215       |
|    lagrangian_multiplier | 0.0529        |
|    learning_rate         | 0.0003        |
|    loss                  | 84.7          |
|    n_updates             | 450           |
|    policy_gradient_loss  | -0.0103       |
|    std                   | 0.938         |
|    value_loss            | 547           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.64699703] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 46            |
|    time_elapsed          | 746           |
|    total_timesteps       | 94208         |
| train/                   |               |
|    approx_kl             | 0.0063827136  |
|    clip_fraction         | 0.0605        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 45.8          |
|    entropy_loss          | -2.93         |
|    explained_variance    | 0.0233        |
|    lagrangian_multiplier | 0.0749        |
|    learning_rate         | 0.0003        |
|    loss                  | 39.7          |
|    n_updates             | 450           |
|    policy_gradient_loss  | -0.00863      |
|    std                   | 1.05          |
|    value_loss            | 420           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3515021] |
| time/                    |              |
|    fps                   | 109          |
|    iterations            | 39           |
|    time_elapsed          | 731          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0077486867 |
|    clip_fraction         | 0.0722       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 241          |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.00414     |
|    lagrangian_multiplier | 0.0663       |
|    learning_rate         | 0.0003       |
|    loss                  | 116          |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 0.964        |
|    value_loss            | 909          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7881577] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 38           |
|    time_elapsed          | 695          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0061583193 |
|    clip_fraction         | 0.0655       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 223          |
|    entropy_loss          | -3           |
|    explained_variance    | 0.0402       |
|    lagrangian_multiplier | 0.0612       |
|    learning_rate         | 0.0003       |
|    loss                  | 84.8         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.0106      |
|    std                   | 1.09         |
|    value_loss            | 622          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.51603585] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 47            |
|    time_elapsed          | 752           |
|    total_timesteps       | 96256         |
| train/                   |               |
|    approx_kl             | 0.008729044   |
|    clip_fraction         | 0.0778        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 216           |
|    entropy_loss          | -2.7          |
|    explained_variance    | -0.00901      |
|    lagrangian_multiplier | 0.0649        |
|    learning_rate         | 0.0003        |
|    loss                  | 43.1          |
|    n_updates             | 460           |
|    policy_gradient_loss  | -0.00555      |
|    std                   | 0.931         |
|    value_loss            | 228           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.4233199] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 47           |
|    time_elapsed          | 762          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.0052667498 |
|    clip_fraction         | 0.0474       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 129          |
|    entropy_loss          | -2.94        |
|    explained_variance    | 0.0476       |
|    lagrangian_multiplier | 0.0745       |
|    learning_rate         | 0.0003       |
|    loss                  | 77.2         |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.00761     |
|    std                   | 1.06         |
|    value_loss            | 897          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.0162132] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 48           |
|    time_elapsed          | 768          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.007694487  |
|    clip_fraction         | 0.0858       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 188          |
|    entropy_loss          | -2.68        |
|    explained_variance    | -0.00778     |
|    lagrangian_multiplier | 0.048        |
|    learning_rate         | 0.0003       |
|    loss                  | 131          |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00913     |
|    std                   | 0.922        |
|    value_loss            | 902          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4795585] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 39           |
|    time_elapsed          | 713          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.004576082  |
|    clip_fraction         | 0.048        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 261          |
|    entropy_loss          | -3           |
|    explained_variance    | 0.0464       |
|    lagrangian_multiplier | 0.0682       |
|    learning_rate         | 0.0003       |
|    loss                  | 78.3         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00651     |
|    std                   | 1.09         |
|    value_loss            | 566          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.54872495] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 48            |
|    time_elapsed          | 779           |
|    total_timesteps       | 98304         |
| train/                   |               |
|    approx_kl             | 0.004504195   |
|    clip_fraction         | 0.0591        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0292        |
|    entropy_loss          | -2.94         |
|    explained_variance    | 0.0404        |
|    lagrangian_multiplier | 0.0865        |
|    learning_rate         | 0.0003        |
|    loss                  | 23.8          |
|    n_updates             | 470           |
|    policy_gradient_loss  | -0.00465      |
|    std                   | 1.05          |
|    value_loss            | 317           |
--------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.4262234] |
| time/                    |              |
|    fps                   | 107          |
|    iterations            | 40           |
|    time_elapsed          | 764          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.006464916  |
|    clip_fraction         | 0.0566       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 270          |
|    entropy_loss          | -2.75        |
|    explained_variance    | 0.0078       |
|    lagrangian_multiplier | 0.0655       |
|    learning_rate         | 0.0003       |
|    loss                  | 85.5         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00825     |
|    std                   | 0.952        |
|    value_loss            | 591          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5669967] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 49           |
|    time_elapsed          | 784          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.0076886043 |
|    clip_fraction         | 0.0708       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 241          |
|    entropy_loss          | -2.68        |
|    explained_variance    | -0.0034      |
|    lagrangian_multiplier | 0.0493       |
|    learning_rate         | 0.0003       |
|    loss                  | 137          |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.00693     |
|    std                   | 0.929        |
|    value_loss            | 887          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1719595] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 49           |
|    time_elapsed          | 795          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.00910517   |
|    clip_fraction         | 0.076        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 151          |
|    entropy_loss          | -2.93        |
|    explained_variance    | 0.026        |
|    lagrangian_multiplier | 0.0649       |
|    learning_rate         | 0.0003       |
|    loss                  | 73.8         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.0115      |
|    std                   | 1.04         |
|    value_loss            | 585          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1247994] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 40           |
|    time_elapsed          | 732          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.006579276  |
|    clip_fraction         | 0.0801       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 226          |
|    entropy_loss          | -2.99        |
|    explained_variance    | -0.00587     |
|    lagrangian_multiplier | 0.0679       |
|    learning_rate         | 0.0003       |
|    loss                  | 56.5         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00996     |
|    std                   | 1.09         |
|    value_loss            | 438          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñà‚ñá‚ñá‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñà‚ñà‚ñá‚ñá‚ñà
wandb:             train/approx_kl ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ
wandb:         train/clip_fraction ‚ñÉ‚ñÅ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñá‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñÑ‚ñá‚ñá‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÖ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÑ‚ñÅ‚ñÜ‚ñÅ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÉ‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñà‚ñá‚ñÖ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñÑ‚ñÖ‚ñá‚ñÉ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÉ‚ñÜ
wandb:          train/entropy_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:    train/explained_variance ‚ñÑ‚ñà‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ
wandb: train/lagrangian_multiplier ‚ñÇ‚ñÑ‚ñà‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÅ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÉ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÇ‚ñÇ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÜ‚ñÜ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÜ‚ñá‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñá‚ñá‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñÅ‚ñÑ‚ñà‚ñÖ‚ñá
wandb:                   train/std ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ
wandb:            train/value_loss ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñà‚ñÖ‚ñÜ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb:                      reward -0.567
wandb:             train/approx_kl 0.00769
wandb:         train/clip_fraction 0.0708
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 240.76714
wandb:          train/entropy_loss -2.68262
wandb:    train/explained_variance -0.0034
wandb: train/lagrangian_multiplier 0.04927
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 136.53667
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00693
wandb:                   train/std 0.92886
wandb:            train/value_loss 887.03236
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2/runs/x6uwljc0
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_191158-x6uwljc0/logs
srun: error: gail.ist.berkeley.edu: task 0: Exited with exit code 1
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñá‚ñÜ‚ñá‚ñà‚ñÉ‚ñá‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñÑ‚ñá‚ñà‚ñÖ‚ñÇ‚ñÖ‚ñá‚ñÇ‚ñÖ‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñÜ
wandb:             train/approx_kl ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñà‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÜ
wandb:         train/clip_fraction ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÜ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñà‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÖ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÇ‚ñÑ‚ñà‚ñÇ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÑ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÅ‚ñá
wandb:          train/entropy_loss ‚ñá‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ
wandb:    train/explained_variance ‚ñÜ‚ñà‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ
wandb: train/lagrangian_multiplier ‚ñÇ‚ñÉ‚ñÖ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñà‚ñÖ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñà‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÉ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñà‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñÉ
wandb:                   train/std ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá
wandb:            train/value_loss ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñá‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb:                      reward -1.17196
wandb:             train/approx_kl 0.00911
wandb:         train/clip_fraction 0.07603
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 150.55292
wandb:          train/entropy_loss -2.92648
wandb:    train/explained_variance 0.02602
wandb: train/lagrangian_multiplier 0.06487
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 73.80835
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.01149
wandb:                   train/std 1.04472
wandb:            train/value_loss 585.47806
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/6en5g80o
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_191151-6en5g80o/logs
srun: error: dqn.ist.berkeley.edu: task 0: Exited with exit code 1
--------------------------------------------
| reward                   | [-0.92237747] |
| time/                    |               |
|    fps                   | 107           |
|    iterations            | 41            |
|    time_elapsed          | 781           |
|    total_timesteps       | 83968         |
| train/                   |               |
|    approx_kl             | 0.0069149244  |
|    clip_fraction         | 0.0484        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 257           |
|    entropy_loss          | -2.74         |
|    explained_variance    | -0.0175       |
|    lagrangian_multiplier | 0.0636        |
|    learning_rate         | 0.0003        |
|    loss                  | 60.2          |
|    n_updates             | 400           |
|    policy_gradient_loss  | -0.00921      |
|    std                   | 0.953         |
|    value_loss            | 363           |
--------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114547.8
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_192523-ugeywulf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/ugeywulf
-------------------------------------------
| reward                   | [-1.5802989] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 41           |
|    time_elapsed          | 750          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.007052241  |
|    clip_fraction         | 0.0622       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 226          |
|    entropy_loss          | -2.99        |
|    explained_variance    | 0.00541      |
|    lagrangian_multiplier | 0.0526       |
|    learning_rate         | 0.0003       |
|    loss                  | 70.9         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00907     |
|    std                   | 1.08         |
|    value_loss            | 386          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5652207] |
| time/                    |              |
|    fps                   | 107          |
|    iterations            | 42           |
|    time_elapsed          | 797          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0074542444 |
|    clip_fraction         | 0.0771       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 225          |
|    entropy_loss          | -2.74        |
|    explained_variance    | -0.00665     |
|    lagrangian_multiplier | 0.0644       |
|    learning_rate         | 0.0003       |
|    loss                  | 60.1         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.0114      |
|    std                   | 0.954        |
|    value_loss            | 383          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114547.9
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_192536-0ghartmw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16/runs/0ghartmw
Using cpu device
-------------------------------------
| reward             | [-0.4190346] |
| time/              |              |
|    fps             | 132          |
|    iterations      | 1            |
|    time_elapsed    | 15           |
|    total_timesteps | 2048         |
-------------------------------------
-------------------------------------------
| reward                   | [-2.0444145] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 42           |
|    time_elapsed          | 769          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.011294511  |
|    clip_fraction         | 0.109        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 220          |
|    entropy_loss          | -2.99        |
|    explained_variance    | 0.00611      |
|    lagrangian_multiplier | 0.0522       |
|    learning_rate         | 0.0003       |
|    loss                  | 70           |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.0117      |
|    std                   | 1.08         |
|    value_loss            | 436          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.4736613] |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 43           |
|    time_elapsed          | 813          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.008695771  |
|    clip_fraction         | 0.0719       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 245          |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.00417      |
|    lagrangian_multiplier | 0.0783       |
|    learning_rate         | 0.0003       |
|    loss                  | 46           |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 0.937        |
|    value_loss            | 330          |
-------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.64954025] |
| time/              |               |
|    fps             | 135           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
--------------------------------------------
| reward                   | [-0.60678524] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 2             |
|    time_elapsed          | 31            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.006391775   |
|    clip_fraction         | 0.0589        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0829        |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.0381        |
|    lagrangian_multiplier | 0.0681        |
|    learning_rate         | 0.0003        |
|    loss                  | 40.4          |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00694      |
|    std                   | 1             |
|    value_loss            | 442           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.5715103] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 43           |
|    time_elapsed          | 787          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.009320284  |
|    clip_fraction         | 0.0779       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 182          |
|    entropy_loss          | -2.98        |
|    explained_variance    | -0.000806    |
|    lagrangian_multiplier | 0.0689       |
|    learning_rate         | 0.0003       |
|    loss                  | 72           |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00809     |
|    std                   | 1.08         |
|    value_loss            | 603          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.45941105] |
| time/                    |               |
|    fps                   | 108           |
|    iterations            | 44            |
|    time_elapsed          | 830           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.0077560395  |
|    clip_fraction         | 0.06          |
|    clip_range            | 0.2           |
|    cost_value_loss       | 268           |
|    entropy_loss          | -2.7          |
|    explained_variance    | -0.000654     |
|    lagrangian_multiplier | 0.063         |
|    learning_rate         | 0.0003        |
|    loss                  | 59.6          |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.00791      |
|    std                   | 0.937         |
|    value_loss            | 366           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.4780956] |
| time/                    |              |
|    fps                   | 131          |
|    iterations            | 2            |
|    time_elapsed          | 31           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.005942258  |
|    clip_fraction         | 0.0441       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0904       |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0369       |
|    lagrangian_multiplier | 0.0766       |
|    learning_rate         | 0.0003       |
|    loss                  | 42.1         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00525     |
|    std                   | 0.99         |
|    value_loss            | 531          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.36441582] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 3             |
|    time_elapsed          | 48            |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.005502376   |
|    clip_fraction         | 0.06          |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0945        |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.0161       |
|    lagrangian_multiplier | 0.0662        |
|    learning_rate         | 0.0003        |
|    loss                  | 44.5          |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00554      |
|    std                   | 1             |
|    value_loss            | 410           |
--------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.73998845] |
| time/                    |               |
|    fps                   | 108           |
|    iterations            | 45            |
|    time_elapsed          | 846           |
|    total_timesteps       | 92160         |
| train/                   |               |
|    approx_kl             | 0.0072777895  |
|    clip_fraction         | 0.0644        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 254           |
|    entropy_loss          | -2.69         |
|    explained_variance    | -0.00579      |
|    lagrangian_multiplier | 0.0596        |
|    learning_rate         | 0.0003        |
|    loss                  | 67.8          |
|    n_updates             | 440           |
|    policy_gradient_loss  | -0.0109       |
|    std                   | 0.923         |
|    value_loss            | 435           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.9264026] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 44           |
|    time_elapsed          | 805          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.0062647383 |
|    clip_fraction         | 0.0562       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 209          |
|    entropy_loss          | -2.97        |
|    explained_variance    | -0.001       |
|    lagrangian_multiplier | 0.0622       |
|    learning_rate         | 0.0003       |
|    loss                  | 49           |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.00895     |
|    std                   | 1.07         |
|    value_loss            | 349          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5107546] |
| time/                    |              |
|    fps                   | 130          |
|    iterations            | 3            |
|    time_elapsed          | 47           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0051920433 |
|    clip_fraction         | 0.0344       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.146        |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0859      |
|    lagrangian_multiplier | 0.0599       |
|    learning_rate         | 0.0003       |
|    loss                  | 51.3         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.0046      |
|    std                   | 0.991        |
|    value_loss            | 468          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6882317] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 4            |
|    time_elapsed          | 64           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.006743796  |
|    clip_fraction         | 0.0481       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0864       |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0353       |
|    lagrangian_multiplier | 0.0617       |
|    learning_rate         | 0.0003       |
|    loss                  | 57.2         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00734     |
|    std                   | 1.02         |
|    value_loss            | 541          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.45151857] |
| time/                    |               |
|    fps                   | 109           |
|    iterations            | 46            |
|    time_elapsed          | 862           |
|    total_timesteps       | 94208         |
| train/                   |               |
|    approx_kl             | 0.005756193   |
|    clip_fraction         | 0.0778        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 258           |
|    entropy_loss          | -2.67         |
|    explained_variance    | 0.0192        |
|    lagrangian_multiplier | 0.0613        |
|    learning_rate         | 0.0003        |
|    loss                  | 59.1          |
|    n_updates             | 450           |
|    policy_gradient_loss  | -0.0091       |
|    std                   | 0.922         |
|    value_loss            | 261           |
--------------------------------------------
-------------------------------------------
| reward                   | [-3.4106538] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 45           |
|    time_elapsed          | 823          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.0070377034 |
|    clip_fraction         | 0.083        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 215          |
|    entropy_loss          | -2.96        |
|    explained_variance    | -0.0308      |
|    lagrangian_multiplier | 0.0558       |
|    learning_rate         | 0.0003       |
|    loss                  | 66.1         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00966     |
|    std                   | 1.07         |
|    value_loss            | 454          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5634927] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 4            |
|    time_elapsed          | 63           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.004654124  |
|    clip_fraction         | 0.0385       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.144        |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0233      |
|    lagrangian_multiplier | 0.057        |
|    learning_rate         | 0.0003       |
|    loss                  | 36.9         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00511     |
|    std                   | 0.994        |
|    value_loss            | 448          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.6033639] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 5            |
|    time_elapsed          | 81           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0072122533 |
|    clip_fraction         | 0.07         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0945       |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0063       |
|    lagrangian_multiplier | 0.06         |
|    learning_rate         | 0.0003       |
|    loss                  | 55.3         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00779     |
|    std                   | 1.02         |
|    value_loss            | 525          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.47442475] |
| time/                    |               |
|    fps                   | 109           |
|    iterations            | 47            |
|    time_elapsed          | 878           |
|    total_timesteps       | 96256         |
| train/                   |               |
|    approx_kl             | 0.0064475453  |
|    clip_fraction         | 0.0513        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 257           |
|    entropy_loss          | -2.67         |
|    explained_variance    | 0.00638       |
|    lagrangian_multiplier | 0.0664        |
|    learning_rate         | 0.0003        |
|    loss                  | 94.4          |
|    n_updates             | 460           |
|    policy_gradient_loss  | -0.00939      |
|    std                   | 0.918         |
|    value_loss            | 710           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.6449834] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 5            |
|    time_elapsed          | 79           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0046127597 |
|    clip_fraction         | 0.044        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.118        |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0661      |
|    lagrangian_multiplier | 0.058        |
|    learning_rate         | 0.0003       |
|    loss                  | 41.5         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00356     |
|    std                   | 0.996        |
|    value_loss            | 409          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.3558361] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 46           |
|    time_elapsed          | 842          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0067427387 |
|    clip_fraction         | 0.0725       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 160          |
|    entropy_loss          | -2.99        |
|    explained_variance    | 0.0248       |
|    lagrangian_multiplier | 0.0579       |
|    learning_rate         | 0.0003       |
|    loss                  | 118          |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 1.09         |
|    value_loss            | 837          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.63870543] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 6             |
|    time_elapsed          | 97            |
|    total_timesteps       | 12288         |
| train/                   |               |
|    approx_kl             | 0.0047705183  |
|    clip_fraction         | 0.0417        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.899         |
|    entropy_loss          | -2.9          |
|    explained_variance    | -0.000617     |
|    lagrangian_multiplier | 0.068         |
|    learning_rate         | 0.0003        |
|    loss                  | 53.8          |
|    n_updates             | 50            |
|    policy_gradient_loss  | -0.00691      |
|    std                   | 1.04          |
|    value_loss            | 633           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.0325403] |
| time/                    |              |
|    fps                   | 109          |
|    iterations            | 48           |
|    time_elapsed          | 895          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.0083755385 |
|    clip_fraction         | 0.085        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 266          |
|    entropy_loss          | -2.68        |
|    explained_variance    | -0.0195      |
|    lagrangian_multiplier | 0.0495       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.3         |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.0108      |
|    std                   | 0.928        |
|    value_loss            | 240          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2072634] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 6            |
|    time_elapsed          | 95           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0058631487 |
|    clip_fraction         | 0.0595       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.251        |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0117      |
|    lagrangian_multiplier | 0.0452       |
|    learning_rate         | 0.0003       |
|    loss                  | 84.6         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00735     |
|    std                   | 0.992        |
|    value_loss            | 588          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.49267402] |
| time/                    |               |
|    fps                   | 111           |
|    iterations            | 47            |
|    time_elapsed          | 860           |
|    total_timesteps       | 96256         |
| train/                   |               |
|    approx_kl             | 0.0062562665  |
|    clip_fraction         | 0.0614        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 117           |
|    entropy_loss          | -3.02         |
|    explained_variance    | -0.0101       |
|    lagrangian_multiplier | 0.0708        |
|    learning_rate         | 0.0003        |
|    loss                  | 42.6          |
|    n_updates             | 460           |
|    policy_gradient_loss  | -0.00753      |
|    std                   | 1.11          |
|    value_loss            | 431           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.97444147] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 7             |
|    time_elapsed          | 114           |
|    total_timesteps       | 14336         |
| train/                   |               |
|    approx_kl             | 0.0042036846  |
|    clip_fraction         | 0.0345        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0398        |
|    entropy_loss          | -2.93         |
|    explained_variance    | 0.036         |
|    lagrangian_multiplier | 0.0873        |
|    learning_rate         | 0.0003        |
|    loss                  | 21.4          |
|    n_updates             | 60            |
|    policy_gradient_loss  | -0.00518      |
|    std                   | 1.05          |
|    value_loss            | 271           |
--------------------------------------------
------------------------------------------
| reward                   | [-0.572673] |
| time/                    |             |
|    fps                   | 110         |
|    iterations            | 49          |
|    time_elapsed          | 911         |
|    total_timesteps       | 100352      |
| train/                   |             |
|    approx_kl             | 0.008481571 |
|    clip_fraction         | 0.0715      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 246         |
|    entropy_loss          | -2.7        |
|    explained_variance    | -0.000435   |
|    lagrangian_multiplier | 0.0636      |
|    learning_rate         | 0.0003      |
|    loss                  | 123         |
|    n_updates             | 480         |
|    policy_gradient_loss  | -0.00947    |
|    std                   | 0.937       |
|    value_loss            | 1.06e+03    |
------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
-------------------------------------------
| reward                   | [-0.9659067] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 7            |
|    time_elapsed          | 111          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.00377888   |
|    clip_fraction         | 0.0244       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.175        |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0406      |
|    lagrangian_multiplier | 0.0525       |
|    learning_rate         | 0.0003       |
|    loss                  | 77.6         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00569     |
|    std                   | 0.988        |
|    value_loss            | 747          |
-------------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñá‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñá‚ñá‚ñà‚ñà
wandb:             train/approx_kl ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñà‚ñÉ‚ñÑ‚ñá‚ñá‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñà
wandb:         train/clip_fraction ‚ñÇ‚ñÇ‚ñÜ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÅ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÜ‚ñà‚ñÑ‚ñá‚ñá‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñá‚ñÜ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñÑ‚ñÇ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:          train/entropy_loss ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñá
wandb:    train/explained_variance ‚ñÉ‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñÇ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÅ‚ñÖ‚ñá‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÑ
wandb: train/lagrangian_multiplier ‚ñá‚ñÑ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñà‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÑ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÜ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÖ‚ñá‚ñÜ‚ñà‚ñÜ‚ñà‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÉ
wandb:                   train/std ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ
wandb:            train/value_loss ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÇ‚ñÇ‚ñá‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñá‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÅ‚ñà
wandb: 
wandb: Run summary:
wandb:                      reward -0.57267
wandb:             train/approx_kl 0.00848
wandb:         train/clip_fraction 0.07153
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 245.67823
wandb:          train/entropy_loss -2.69822
wandb:    train/explained_variance -0.00043
wandb: train/lagrangian_multiplier 0.06355
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 122.89514
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00947
wandb:                   train/std 0.93704
wandb:            train/value_loss 1063.70702
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2/runs/l79bu81z
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231227_031212-l79bu81z/logs
srun: error: airl.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-1.1501529] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 8            |
|    time_elapsed          | 130          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.005363298  |
|    clip_fraction         | 0.0483       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.864        |
|    entropy_loss          | -2.95        |
|    explained_variance    | 0.0389       |
|    lagrangian_multiplier | 0.0734       |
|    learning_rate         | 0.0003       |
|    loss                  | 29.6         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00701     |
|    std                   | 1.06         |
|    value_loss            | 368          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8767687] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 48           |
|    time_elapsed          | 879          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.0057858042 |
|    clip_fraction         | 0.0568       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 107          |
|    entropy_loss          | -3.05        |
|    explained_variance    | -0.0669      |
|    lagrangian_multiplier | 0.0678       |
|    learning_rate         | 0.0003       |
|    loss                  | 33.4         |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00667     |
|    std                   | 1.12         |
|    value_loss            | 310          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3901391] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 8            |
|    time_elapsed          | 127          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.005462101  |
|    clip_fraction         | 0.0564       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.414        |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0489      |
|    lagrangian_multiplier | 0.0555       |
|    learning_rate         | 0.0003       |
|    loss                  | 80.3         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00681     |
|    std                   | 0.98         |
|    value_loss            | 712          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8264503] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 9            |
|    time_elapsed          | 146          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.004552029  |
|    clip_fraction         | 0.0442       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.33         |
|    entropy_loss          | -2.95        |
|    explained_variance    | 0.0312       |
|    lagrangian_multiplier | 0.0773       |
|    learning_rate         | 0.0003       |
|    loss                  | 54.5         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.007       |
|    std                   | 1.06         |
|    value_loss            | 702          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3002788] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 49           |
|    time_elapsed          | 897          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.0054862946 |
|    clip_fraction         | 0.0458       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 130          |
|    entropy_loss          | -3.07        |
|    explained_variance    | 0.0185       |
|    lagrangian_multiplier | 0.0616       |
|    learning_rate         | 0.0003       |
|    loss                  | 68.7         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.00782     |
|    std                   | 1.14         |
|    value_loss            | 440          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                      reward ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñá‚ñÑ‚ñÇ‚ñÅ‚ñÑ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÉ‚ñÜ‚ñá‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÅ‚ñá‚ñá‚ñÜ
wandb:             train/approx_kl ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñÅ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ
wandb:         train/clip_fraction ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñá‚ñÉ‚ñÉ‚ñÖ‚ñÅ‚ñÇ‚ñÜ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñà‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÇ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÇ‚ñÅ‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÑ
wandb:          train/entropy_loss ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:    train/explained_variance ‚ñÑ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñà‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñÜ
wandb: train/lagrangian_multiplier ‚ñÑ‚ñà‚ñà‚ñÅ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñá‚ñÜ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñá‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÖ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñà‚ñÑ‚ñá‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñÅ‚ñÉ‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñá‚ñÜ‚ñÑ‚ñà‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ
wandb:                   train/std ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà
wandb:            train/value_loss ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:                      reward -1.30028
wandb:             train/approx_kl 0.00549
wandb:         train/clip_fraction 0.0458
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 130.35471
wandb:          train/entropy_loss -3.07377
wandb:    train/explained_variance 0.01853
wandb: train/lagrangian_multiplier 0.06165
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 68.69303
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00782
wandb:                   train/std 1.13806
wandb:            train/value_loss 439.93439
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2/runs/xzjrcf8f
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_191254-xzjrcf8f/logs
srun: error: ddpg.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-0.9700979] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 9            |
|    time_elapsed          | 143          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0045998087 |
|    clip_fraction         | 0.0434       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.286        |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.00121     |
|    lagrangian_multiplier | 0.059        |
|    learning_rate         | 0.0003       |
|    loss                  | 41.3         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00725     |
|    std                   | 0.973        |
|    value_loss            | 450          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.61257285] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 10            |
|    time_elapsed          | 163           |
|    total_timesteps       | 20480         |
| train/                   |               |
|    approx_kl             | 0.005291893   |
|    clip_fraction         | 0.0402        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0373        |
|    entropy_loss          | -2.95         |
|    explained_variance    | 0.0105        |
|    lagrangian_multiplier | 0.0702        |
|    learning_rate         | 0.0003        |
|    loss                  | 16.7          |
|    n_updates             | 90            |
|    policy_gradient_loss  | -0.00626      |
|    std                   | 1.06          |
|    value_loss            | 195           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.5248863] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 10           |
|    time_elapsed          | 159          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0066849976 |
|    clip_fraction         | 0.0752       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.344        |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0239      |
|    lagrangian_multiplier | 0.0565       |
|    learning_rate         | 0.0003       |
|    loss                  | 51.9         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00885     |
|    std                   | 0.971        |
|    value_loss            | 510          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114547.10
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231227_032822-n7rh8fjm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16/runs/n7rh8fjm
-------------------------------------------
| reward                   | [-0.747402]  |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 11           |
|    time_elapsed          | 179          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0068517462 |
|    clip_fraction         | 0.0646       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0227       |
|    entropy_loss          | -2.94        |
|    explained_variance    | 0.0509       |
|    lagrangian_multiplier | 0.0687       |
|    learning_rate         | 0.0003       |
|    loss                  | 19.7         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00665     |
|    std                   | 1.05         |
|    value_loss            | 191          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4623492] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 11           |
|    time_elapsed          | 175          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0053432398 |
|    clip_fraction         | 0.0461       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.353        |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0029       |
|    lagrangian_multiplier | 0.0593       |
|    learning_rate         | 0.0003       |
|    loss                  | 90.4         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00737     |
|    std                   | 0.97         |
|    value_loss            | 873          |
-------------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.5162878] |
| time/              |              |
|    fps             | 136          |
|    iterations      | 1            |
|    time_elapsed    | 14           |
|    total_timesteps | 2048         |
-------------------------------------
--------------------------------------------
| reward                   | [-0.51335937] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 12            |
|    time_elapsed          | 196           |
|    total_timesteps       | 24576         |
| train/                   |               |
|    approx_kl             | 0.006202885   |
|    clip_fraction         | 0.0521        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0137        |
|    entropy_loss          | -2.94         |
|    explained_variance    | -0.0274       |
|    lagrangian_multiplier | 0.0881        |
|    learning_rate         | 0.0003        |
|    loss                  | 8.98          |
|    n_updates             | 110           |
|    policy_gradient_loss  | -0.00585      |
|    std                   | 1.06          |
|    value_loss            | 121           |
--------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114547.11
-------------------------------------------
| reward                   | [-2.1542706] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 12           |
|    time_elapsed          | 191          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0058965683 |
|    clip_fraction         | 0.0471       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.314        |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.023       |
|    lagrangian_multiplier | 0.0601       |
|    learning_rate         | 0.0003       |
|    loss                  | 41.6         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.008       |
|    std                   | 0.955        |
|    value_loss            | 424          |
-------------------------------------------
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_192852-ak6l5y2z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16/runs/ak6l5y2z
-------------------------------------------
| reward                   | [-1.4986002] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 13           |
|    time_elapsed          | 212          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.006836712  |
|    clip_fraction         | 0.0681       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0507       |
|    entropy_loss          | -2.96        |
|    explained_variance    | -0.00364     |
|    lagrangian_multiplier | 0.0513       |
|    learning_rate         | 0.0003       |
|    loss                  | 77.3         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00721     |
|    std                   | 1.07         |
|    value_loss            | 586          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.5003775] |
| time/                    |              |
|    fps                   | 99           |
|    iterations            | 2            |
|    time_elapsed          | 41           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0041045537 |
|    clip_fraction         | 0.0215       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0329       |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.00816      |
|    lagrangian_multiplier | 0.0657       |
|    learning_rate         | 0.0003       |
|    loss                  | 57.7         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00269     |
|    std                   | 1.01         |
|    value_loss            | 638          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.8860254] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 13           |
|    time_elapsed          | 208          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0053615514 |
|    clip_fraction         | 0.048        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.381        |
|    entropy_loss          | -2.74        |
|    explained_variance    | -0.0441      |
|    lagrangian_multiplier | 0.0614       |
|    learning_rate         | 0.0003       |
|    loss                  | 76.8         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00543     |
|    std                   | 0.952        |
|    value_loss            | 758          |
-------------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.6961303] |
| time/              |              |
|    fps             | 119          |
|    iterations      | 1            |
|    time_elapsed    | 17           |
|    total_timesteps | 2048         |
-------------------------------------
--------------------------------------------
| reward                   | [-0.46200666] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 14            |
|    time_elapsed          | 228           |
|    total_timesteps       | 28672         |
| train/                   |               |
|    approx_kl             | 0.0059591327  |
|    clip_fraction         | 0.0506        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0217        |
|    entropy_loss          | -2.96         |
|    explained_variance    | -0.00339      |
|    lagrangian_multiplier | 0.0541        |
|    learning_rate         | 0.0003        |
|    loss                  | 33.7          |
|    n_updates             | 130           |
|    policy_gradient_loss  | -0.00512      |
|    std                   | 1.06          |
|    value_loss            | 380           |
--------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.7687428] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 14           |
|    time_elapsed          | 224          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0049344837 |
|    clip_fraction         | 0.0381       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.311        |
|    entropy_loss          | -2.74        |
|    explained_variance    | -0.0153      |
|    lagrangian_multiplier | 0.0602       |
|    learning_rate         | 0.0003       |
|    loss                  | 152          |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00595     |
|    std                   | 0.951        |
|    value_loss            | 1.34e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5902191] |
| time/                    |              |
|    fps                   | 106          |
|    iterations            | 3            |
|    time_elapsed          | 57           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0052381805 |
|    clip_fraction         | 0.056        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0519       |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0166       |
|    lagrangian_multiplier | 0.0598       |
|    learning_rate         | 0.0003       |
|    loss                  | 92.6         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00758     |
|    std                   | 1.01         |
|    value_loss            | 938          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.66144913] |
| time/                    |               |
|    fps                   | 115           |
|    iterations            | 2             |
|    time_elapsed          | 35            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.00540565    |
|    clip_fraction         | 0.0436        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0673        |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.111         |
|    lagrangian_multiplier | 0.0542        |
|    learning_rate         | 0.0003        |
|    loss                  | 50.7          |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00596      |
|    std                   | 1.01          |
|    value_loss            | 491           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.6889506] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 15           |
|    time_elapsed          | 245          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0080180485 |
|    clip_fraction         | 0.069        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0346       |
|    entropy_loss          | -2.94        |
|    explained_variance    | -0.000723    |
|    lagrangian_multiplier | 0.055        |
|    learning_rate         | 0.0003       |
|    loss                  | 32.8         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00871     |
|    std                   | 1.06         |
|    value_loss            | 266          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.325599]  |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 15           |
|    time_elapsed          | 240          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0054532047 |
|    clip_fraction         | 0.0486       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.271        |
|    entropy_loss          | -2.74        |
|    explained_variance    | -0.0133      |
|    lagrangian_multiplier | 0.0537       |
|    learning_rate         | 0.0003       |
|    loss                  | 192          |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00589     |
|    std                   | 0.951        |
|    value_loss            | 1.63e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.40757528] |
| time/                    |               |
|    fps                   | 110           |
|    iterations            | 4             |
|    time_elapsed          | 73            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.0055043013  |
|    clip_fraction         | 0.0499        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.104         |
|    entropy_loss          | -2.86         |
|    explained_variance    | 0.0307        |
|    lagrangian_multiplier | 0.0531        |
|    learning_rate         | 0.0003        |
|    loss                  | 43.7          |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.0076       |
|    std                   | 1.01          |
|    value_loss            | 490           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.5011346] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 16           |
|    time_elapsed          | 261          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0039347266 |
|    clip_fraction         | 0.0196       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0125       |
|    entropy_loss          | -2.94        |
|    explained_variance    | 0.0424       |
|    lagrangian_multiplier | 0.0909       |
|    learning_rate         | 0.0003       |
|    loss                  | 8.89         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00245     |
|    std                   | 1.06         |
|    value_loss            | 115          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7678068] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 3            |
|    time_elapsed          | 53           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0060270834 |
|    clip_fraction         | 0.0657       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.142        |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0414       |
|    lagrangian_multiplier | 0.0532       |
|    learning_rate         | 0.0003       |
|    loss                  | 74.7         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00829     |
|    std                   | 1.01         |
|    value_loss            | 676          |
-------------------------------------------
------------------------------------------
| reward                   | [-2.970211] |
| time/                    |             |
|    fps                   | 127         |
|    iterations            | 16          |
|    time_elapsed          | 256         |
|    total_timesteps       | 32768       |
| train/                   |             |
|    approx_kl             | 0.005446031 |
|    clip_fraction         | 0.0416      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.362       |
|    entropy_loss          | -2.74       |
|    explained_variance    | -0.0259     |
|    lagrangian_multiplier | 0.0599      |
|    learning_rate         | 0.0003      |
|    loss                  | 145         |
|    n_updates             | 150         |
|    policy_gradient_loss  | -0.00856    |
|    std                   | 0.95        |
|    value_loss            | 1.25e+03    |
------------------------------------------
--------------------------------------------
| reward                   | [-0.86343557] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 5             |
|    time_elapsed          | 90            |
|    total_timesteps       | 10240         |
| train/                   |               |
|    approx_kl             | 0.0059372727  |
|    clip_fraction         | 0.0485        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.105         |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.0112       |
|    lagrangian_multiplier | 0.0582        |
|    learning_rate         | 0.0003        |
|    loss                  | 61.9          |
|    n_updates             | 40            |
|    policy_gradient_loss  | -0.00566      |
|    std                   | 1             |
|    value_loss            | 518           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.1710227] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 17           |
|    time_elapsed          | 278          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0042830445 |
|    clip_fraction         | 0.0363       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.886        |
|    entropy_loss          | -2.95        |
|    explained_variance    | 0.00156      |
|    lagrangian_multiplier | 0.073        |
|    learning_rate         | 0.0003       |
|    loss                  | 57.9         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00693     |
|    std                   | 1.07         |
|    value_loss            | 499          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4586937] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 4            |
|    time_elapsed          | 71           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.007226724  |
|    clip_fraction         | 0.0765       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0539       |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.112        |
|    lagrangian_multiplier | 0.0581       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.9         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00586     |
|    std                   | 1.01         |
|    value_loss            | 426          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.4250636] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 17           |
|    time_elapsed          | 272          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.005860636  |
|    clip_fraction         | 0.0504       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.361        |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.0226      |
|    lagrangian_multiplier | 0.0638       |
|    learning_rate         | 0.0003       |
|    loss                  | 136          |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.0092      |
|    std                   | 0.944        |
|    value_loss            | 1.35e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.66835403] |
| time/                    |               |
|    fps                   | 115           |
|    iterations            | 6             |
|    time_elapsed          | 106           |
|    total_timesteps       | 12288         |
| train/                   |               |
|    approx_kl             | 0.005269322   |
|    clip_fraction         | 0.0487        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.043         |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.0278        |
|    lagrangian_multiplier | 0.0573        |
|    learning_rate         | 0.0003        |
|    loss                  | 41            |
|    n_updates             | 50            |
|    policy_gradient_loss  | -0.00534      |
|    std                   | 1             |
|    value_loss            | 324           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.5626757] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 18           |
|    time_elapsed          | 294          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0067016203 |
|    clip_fraction         | 0.0571       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 2.84         |
|    entropy_loss          | -2.96        |
|    explained_variance    | 0.00028      |
|    lagrangian_multiplier | 0.064        |
|    learning_rate         | 0.0003       |
|    loss                  | 95.1         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.01        |
|    std                   | 1.06         |
|    value_loss            | 865          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7521584] |
| time/                    |              |
|    fps                   | 114          |
|    iterations            | 5            |
|    time_elapsed          | 89           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.006116697  |
|    clip_fraction         | 0.0557       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.129        |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.00297     |
|    lagrangian_multiplier | 0.0545       |
|    learning_rate         | 0.0003       |
|    loss                  | 64.4         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00846     |
|    std                   | 1.01         |
|    value_loss            | 655          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2756436] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 18           |
|    time_elapsed          | 288          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0060975216 |
|    clip_fraction         | 0.0535       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.342        |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.0536      |
|    lagrangian_multiplier | 0.0622       |
|    learning_rate         | 0.0003       |
|    loss                  | 113          |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00873     |
|    std                   | 0.943        |
|    value_loss            | 1.06e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.98190904] |
| time/                    |               |
|    fps                   | 116           |
|    iterations            | 7             |
|    time_elapsed          | 122           |
|    total_timesteps       | 14336         |
| train/                   |               |
|    approx_kl             | 0.0061055315  |
|    clip_fraction         | 0.05          |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0594        |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.0367        |
|    lagrangian_multiplier | 0.0561        |
|    learning_rate         | 0.0003        |
|    loss                  | 25.3          |
|    n_updates             | 60            |
|    policy_gradient_loss  | -0.00651      |
|    std                   | 1.01          |
|    value_loss            | 263           |
--------------------------------------------
------------------------------------------
| reward                   | [-2.523615] |
| time/                    |             |
|    fps                   | 125         |
|    iterations            | 19          |
|    time_elapsed          | 311         |
|    total_timesteps       | 38912       |
| train/                   |             |
|    approx_kl             | 0.00769097  |
|    clip_fraction         | 0.0652      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.379       |
|    entropy_loss          | -2.94       |
|    explained_variance    | -0.00707    |
|    lagrangian_multiplier | 0.042       |
|    learning_rate         | 0.0003      |
|    loss                  | 103         |
|    n_updates             | 180         |
|    policy_gradient_loss  | -0.00916    |
|    std                   | 1.05        |
|    value_loss            | 946         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.0684406] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 6            |
|    time_elapsed          | 107          |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0053906697 |
|    clip_fraction         | 0.0442       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0878       |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0417       |
|    lagrangian_multiplier | 0.0652       |
|    learning_rate         | 0.0003       |
|    loss                  | 104          |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.0065      |
|    std                   | 0.996        |
|    value_loss            | 1.15e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6019763] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 19           |
|    time_elapsed          | 304          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.008344391  |
|    clip_fraction         | 0.0714       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.314        |
|    entropy_loss          | -2.74        |
|    explained_variance    | -0.0647      |
|    lagrangian_multiplier | 0.0651       |
|    learning_rate         | 0.0003       |
|    loss                  | 68.5         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.959        |
|    value_loss            | 708          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.568621] |
| time/                    |             |
|    fps                   | 117         |
|    iterations            | 8           |
|    time_elapsed          | 139         |
|    total_timesteps       | 16384       |
| train/                   |             |
|    approx_kl             | 0.00502414  |
|    clip_fraction         | 0.0382      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.103       |
|    entropy_loss          | -2.84       |
|    explained_variance    | 0.024       |
|    lagrangian_multiplier | 0.0455      |
|    learning_rate         | 0.0003      |
|    loss                  | 34.8        |
|    n_updates             | 70          |
|    policy_gradient_loss  | -0.00595    |
|    std                   | 0.999       |
|    value_loss            | 269         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.9107968] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 20           |
|    time_elapsed          | 327          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.004743435  |
|    clip_fraction         | 0.0392       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.697        |
|    entropy_loss          | -2.92        |
|    explained_variance    | -9.66e-05    |
|    lagrangian_multiplier | 0.0396       |
|    learning_rate         | 0.0003       |
|    loss                  | 140          |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00408     |
|    std                   | 1.04         |
|    value_loss            | 1e+03        |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4531474] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 20           |
|    time_elapsed          | 320          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.008250796  |
|    clip_fraction         | 0.0661       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.278        |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0262      |
|    lagrangian_multiplier | 0.0565       |
|    learning_rate         | 0.0003       |
|    loss                  | 42.5         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 0.961        |
|    value_loss            | 392          |
-------------------------------------------
------------------------------------------
| reward                   | [-2.054316] |
| time/                    |             |
|    fps                   | 118         |
|    iterations            | 9           |
|    time_elapsed          | 155         |
|    total_timesteps       | 18432       |
| train/                   |             |
|    approx_kl             | 0.00657056  |
|    clip_fraction         | 0.0591      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0776      |
|    entropy_loss          | -2.84       |
|    explained_variance    | 0.0335      |
|    lagrangian_multiplier | 0.0503      |
|    learning_rate         | 0.0003      |
|    loss                  | 46.3        |
|    n_updates             | 80          |
|    policy_gradient_loss  | -0.00798    |
|    std                   | 1           |
|    value_loss            | 346         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.39173618] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 7             |
|    time_elapsed          | 126           |
|    total_timesteps       | 14336         |
| train/                   |               |
|    approx_kl             | 0.0076441066  |
|    clip_fraction         | 0.065         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.151         |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.059         |
|    lagrangian_multiplier | 0.0584        |
|    learning_rate         | 0.0003        |
|    loss                  | 56.1          |
|    n_updates             | 60            |
|    policy_gradient_loss  | -0.0099       |
|    std                   | 0.986         |
|    value_loss            | 487           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.9346924] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 21           |
|    time_elapsed          | 344          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0056079356 |
|    clip_fraction         | 0.0562       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.25         |
|    entropy_loss          | -2.92        |
|    explained_variance    | -0.0207      |
|    lagrangian_multiplier | 0.0508       |
|    learning_rate         | 0.0003       |
|    loss                  | 73.6         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00637     |
|    std                   | 1.05         |
|    value_loss            | 588          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.6598617] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 21           |
|    time_elapsed          | 337          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0060210833 |
|    clip_fraction         | 0.0567       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.189        |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0242      |
|    lagrangian_multiplier | 0.0615       |
|    learning_rate         | 0.0003       |
|    loss                  | 80.8         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.0107      |
|    std                   | 0.977        |
|    value_loss            | 779          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0337248] |
| time/                    |              |
|    fps                   | 119          |
|    iterations            | 10           |
|    time_elapsed          | 171          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0058180355 |
|    clip_fraction         | 0.0362       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.113        |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.00347      |
|    lagrangian_multiplier | 0.0582       |
|    learning_rate         | 0.0003       |
|    loss                  | 125          |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.0056      |
|    std                   | 1            |
|    value_loss            | 1.16e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.98335177] |
| time/                    |               |
|    fps                   | 113           |
|    iterations            | 8             |
|    time_elapsed          | 144           |
|    total_timesteps       | 16384         |
| train/                   |               |
|    approx_kl             | 0.0060256105  |
|    clip_fraction         | 0.0501        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.148         |
|    entropy_loss          | -2.79         |
|    explained_variance    | 0.0154        |
|    lagrangian_multiplier | 0.0554        |
|    learning_rate         | 0.0003        |
|    loss                  | 37            |
|    n_updates             | 70            |
|    policy_gradient_loss  | -0.00665      |
|    std                   | 0.97          |
|    value_loss            | 397           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.7072987] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 22           |
|    time_elapsed          | 360          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.0041204803 |
|    clip_fraction         | 0.0353       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0516       |
|    entropy_loss          | -2.92        |
|    explained_variance    | -0.0392      |
|    lagrangian_multiplier | 0.079        |
|    learning_rate         | 0.0003       |
|    loss                  | 30.4         |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00842     |
|    std                   | 1.04         |
|    value_loss            | 297          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.37078103] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 22            |
|    time_elapsed          | 353           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.006213211   |
|    clip_fraction         | 0.0668        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.157         |
|    entropy_loss          | -2.79         |
|    explained_variance    | 0.0199        |
|    lagrangian_multiplier | 0.0573        |
|    learning_rate         | 0.0003        |
|    loss                  | 34.9          |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.0119       |
|    std                   | 0.974         |
|    value_loss            | 278           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.4167187] |
| time/                    |              |
|    fps                   | 119          |
|    iterations            | 11           |
|    time_elapsed          | 188          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0065376423 |
|    clip_fraction         | 0.071        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.137        |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.00271      |
|    lagrangian_multiplier | 0.0567       |
|    learning_rate         | 0.0003       |
|    loss                  | 87.9         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00943     |
|    std                   | 1            |
|    value_loss            | 930          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3480875] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 9            |
|    time_elapsed          | 162          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.006212122  |
|    clip_fraction         | 0.0446       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.227        |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0242      |
|    lagrangian_multiplier | 0.0482       |
|    learning_rate         | 0.0003       |
|    loss                  | 58           |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00709     |
|    std                   | 0.967        |
|    value_loss            | 479          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.6001396] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 23           |
|    time_elapsed          | 377          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.0056561986 |
|    clip_fraction         | 0.0488       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0386       |
|    entropy_loss          | -2.92        |
|    explained_variance    | 0.031        |
|    lagrangian_multiplier | 0.0765       |
|    learning_rate         | 0.0003       |
|    loss                  | 14.9         |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.00679     |
|    std                   | 1.04         |
|    value_loss            | 156          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.71380395] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 23            |
|    time_elapsed          | 369           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.00284972    |
|    clip_fraction         | 0.0338        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.12          |
|    entropy_loss          | -2.79         |
|    explained_variance    | -0.0216       |
|    lagrangian_multiplier | 0.0618        |
|    learning_rate         | 0.0003        |
|    loss                  | 38.3          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00561      |
|    std                   | 0.974         |
|    value_loss            | 362           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.6713178] |
| time/                    |              |
|    fps                   | 120          |
|    iterations            | 12           |
|    time_elapsed          | 204          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0047795456 |
|    clip_fraction         | 0.0391       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0605       |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0284       |
|    lagrangian_multiplier | 0.0529       |
|    learning_rate         | 0.0003       |
|    loss                  | 66.4         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00597     |
|    std                   | 1.01         |
|    value_loss            | 576          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1523852] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 10           |
|    time_elapsed          | 180          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.006644576  |
|    clip_fraction         | 0.0617       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.239        |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.021       |
|    lagrangian_multiplier | 0.0434       |
|    learning_rate         | 0.0003       |
|    loss                  | 112          |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00908     |
|    std                   | 0.973        |
|    value_loss            | 823          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.0882626] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 24           |
|    time_elapsed          | 393          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.007239583  |
|    clip_fraction         | 0.0748       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 9.9          |
|    entropy_loss          | -2.9         |
|    explained_variance    | -0.0183      |
|    lagrangian_multiplier | 0.0453       |
|    learning_rate         | 0.0003       |
|    loss                  | 135          |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00907     |
|    std                   | 1.03         |
|    value_loss            | 1.07e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.64642715] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 24            |
|    time_elapsed          | 385           |
|    total_timesteps       | 49152         |
| train/                   |               |
|    approx_kl             | 0.005671515   |
|    clip_fraction         | 0.0368        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0302        |
|    entropy_loss          | -2.77         |
|    explained_variance    | 0.0687        |
|    lagrangian_multiplier | 0.06          |
|    learning_rate         | 0.0003        |
|    loss                  | 65.5          |
|    n_updates             | 230           |
|    policy_gradient_loss  | -0.00378      |
|    std                   | 0.957         |
|    value_loss            | 589           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.4499085] |
| time/                    |              |
|    fps                   | 120          |
|    iterations            | 13           |
|    time_elapsed          | 220          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.005181363  |
|    clip_fraction         | 0.0341       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0949       |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0103       |
|    lagrangian_multiplier | 0.0496       |
|    learning_rate         | 0.0003       |
|    loss                  | 63.4         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00485     |
|    std                   | 1.01         |
|    value_loss            | 624          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4629664] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 11           |
|    time_elapsed          | 198          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0068806894 |
|    clip_fraction         | 0.071        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.237        |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.102       |
|    lagrangian_multiplier | 0.055        |
|    learning_rate         | 0.0003       |
|    loss                  | 98.5         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 0.976        |
|    value_loss            | 810          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2059573] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 25           |
|    time_elapsed          | 410          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.007679992  |
|    clip_fraction         | 0.084        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.91         |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.0217      |
|    lagrangian_multiplier | 0.0521       |
|    learning_rate         | 0.0003       |
|    loss                  | 67.4         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.0129      |
|    std                   | 1.03         |
|    value_loss            | 569          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.69978195] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 25            |
|    time_elapsed          | 401           |
|    total_timesteps       | 51200         |
| train/                   |               |
|    approx_kl             | 0.0050485344  |
|    clip_fraction         | 0.055         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0605        |
|    entropy_loss          | -2.76         |
|    explained_variance    | -0.0018       |
|    lagrangian_multiplier | 0.0535        |
|    learning_rate         | 0.0003        |
|    loss                  | 93.7          |
|    n_updates             | 240           |
|    policy_gradient_loss  | -0.00761      |
|    std                   | 0.968         |
|    value_loss            | 748           |
--------------------------------------------
------------------------------------------
| reward                   | [-1.96298]  |
| time/                    |             |
|    fps                   | 121         |
|    iterations            | 14          |
|    time_elapsed          | 236         |
|    total_timesteps       | 28672       |
| train/                   |             |
|    approx_kl             | 0.006110528 |
|    clip_fraction         | 0.0468      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.154       |
|    entropy_loss          | -2.87       |
|    explained_variance    | 0.0286      |
|    lagrangian_multiplier | 0.0509      |
|    learning_rate         | 0.0003      |
|    loss                  | 59.1        |
|    n_updates             | 130         |
|    policy_gradient_loss  | -0.00672    |
|    std                   | 1.02        |
|    value_loss            | 490         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.0960687] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 12           |
|    time_elapsed          | 217          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.005474109  |
|    clip_fraction         | 0.0336       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.216        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.094        |
|    lagrangian_multiplier | 0.0455       |
|    learning_rate         | 0.0003       |
|    loss                  | 72           |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00573     |
|    std                   | 0.994        |
|    value_loss            | 532          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2867968] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 26           |
|    time_elapsed          | 426          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0063174134 |
|    clip_fraction         | 0.0484       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 10.7         |
|    entropy_loss          | -2.9         |
|    explained_variance    | -0.0258      |
|    lagrangian_multiplier | 0.0639       |
|    learning_rate         | 0.0003       |
|    loss                  | 78.9         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.0106      |
|    std                   | 1.04         |
|    value_loss            | 733          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.94380754] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 26            |
|    time_elapsed          | 417           |
|    total_timesteps       | 53248         |
| train/                   |               |
|    approx_kl             | 0.008077584   |
|    clip_fraction         | 0.0586        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.02          |
|    entropy_loss          | -2.76         |
|    explained_variance    | 0.0205        |
|    lagrangian_multiplier | 0.0575        |
|    learning_rate         | 0.0003        |
|    loss                  | 53.8          |
|    n_updates             | 250           |
|    policy_gradient_loss  | -0.00437      |
|    std                   | 0.958         |
|    value_loss            | 475           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.61172515] |
| time/                    |               |
|    fps                   | 121           |
|    iterations            | 15            |
|    time_elapsed          | 252           |
|    total_timesteps       | 30720         |
| train/                   |               |
|    approx_kl             | 0.0055950815  |
|    clip_fraction         | 0.0389        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.296         |
|    entropy_loss          | -2.87         |
|    explained_variance    | 0.0169        |
|    lagrangian_multiplier | 0.0611        |
|    learning_rate         | 0.0003        |
|    loss                  | 54.2          |
|    n_updates             | 140           |
|    policy_gradient_loss  | -0.00822      |
|    std                   | 1.02          |
|    value_loss            | 589           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.2713573] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 27           |
|    time_elapsed          | 443          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.007313091  |
|    clip_fraction         | 0.0794       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.34         |
|    entropy_loss          | -2.91        |
|    explained_variance    | -0.0131      |
|    lagrangian_multiplier | 0.0554       |
|    learning_rate         | 0.0003       |
|    loss                  | 91           |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.0126      |
|    std                   | 1.04         |
|    value_loss            | 820          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9152086] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 13           |
|    time_elapsed          | 235          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.006326261  |
|    clip_fraction         | 0.0502       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.309        |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0292       |
|    lagrangian_multiplier | 0.0581       |
|    learning_rate         | 0.0003       |
|    loss                  | 43           |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00729     |
|    std                   | 0.987        |
|    value_loss            | 391          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5016706] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 27           |
|    time_elapsed          | 434          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.007364765  |
|    clip_fraction         | 0.0646       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0522       |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.0227       |
|    lagrangian_multiplier | 0.0694       |
|    learning_rate         | 0.0003       |
|    loss                  | 41.7         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.0078      |
|    std                   | 0.949        |
|    value_loss            | 415          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3117343] |
| time/                    |              |
|    fps                   | 121          |
|    iterations            | 16           |
|    time_elapsed          | 269          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0045531997 |
|    clip_fraction         | 0.0387       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.323        |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.00607     |
|    lagrangian_multiplier | 0.0506       |
|    learning_rate         | 0.0003       |
|    loss                  | 44.7         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00579     |
|    std                   | 1.02         |
|    value_loss            | 381          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.1998863] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 28           |
|    time_elapsed          | 459          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.0055138064 |
|    clip_fraction         | 0.0458       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 5.73         |
|    entropy_loss          | -2.93        |
|    explained_variance    | -0.00298     |
|    lagrangian_multiplier | 0.0549       |
|    learning_rate         | 0.0003       |
|    loss                  | 96.8         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00668     |
|    std                   | 1.05         |
|    value_loss            | 817          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.113033]  |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 14           |
|    time_elapsed          | 253          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0066656037 |
|    clip_fraction         | 0.039        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.296        |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0449      |
|    lagrangian_multiplier | 0.0545       |
|    learning_rate         | 0.0003       |
|    loss                  | 74.7         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00668     |
|    std                   | 0.98         |
|    value_loss            | 650          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1260781] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 28           |
|    time_elapsed          | 450          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.0072485358 |
|    clip_fraction         | 0.0544       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0816       |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0442       |
|    lagrangian_multiplier | 0.0615       |
|    learning_rate         | 0.0003       |
|    loss                  | 39.4         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00638     |
|    std                   | 0.938        |
|    value_loss            | 394          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.224436] |
| time/                    |             |
|    fps                   | 121         |
|    iterations            | 17          |
|    time_elapsed          | 285         |
|    total_timesteps       | 34816       |
| train/                   |             |
|    approx_kl             | 0.005048905 |
|    clip_fraction         | 0.0268      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.11        |
|    entropy_loss          | -2.89       |
|    explained_variance    | 0.0196      |
|    lagrangian_multiplier | 0.0521      |
|    learning_rate         | 0.0003      |
|    loss                  | 22.4        |
|    n_updates             | 160         |
|    policy_gradient_loss  | -0.004      |
|    std                   | 1.03        |
|    value_loss            | 210         |
------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.47233188] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 29            |
|    time_elapsed          | 476           |
|    total_timesteps       | 59392         |
| train/                   |               |
|    approx_kl             | 0.007690968   |
|    clip_fraction         | 0.0727        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 4.49          |
|    entropy_loss          | -2.94         |
|    explained_variance    | -0.0184       |
|    lagrangian_multiplier | 0.0654        |
|    learning_rate         | 0.0003        |
|    loss                  | 82.5          |
|    n_updates             | 280           |
|    policy_gradient_loss  | -0.0115       |
|    std                   | 1.06          |
|    value_loss            | 906           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.1622398] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 29           |
|    time_elapsed          | 466          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.0074439715 |
|    clip_fraction         | 0.0809       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.181        |
|    entropy_loss          | -2.71        |
|    explained_variance    | 0.0116       |
|    lagrangian_multiplier | 0.0494       |
|    learning_rate         | 0.0003       |
|    loss                  | 49.4         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.0109      |
|    std                   | 0.935        |
|    value_loss            | 413          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6216267] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 15           |
|    time_elapsed          | 271          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.006560747  |
|    clip_fraction         | 0.0621       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.21         |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0551      |
|    lagrangian_multiplier | 0.0541       |
|    learning_rate         | 0.0003       |
|    loss                  | 122          |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00799     |
|    std                   | 0.985        |
|    value_loss            | 1.03e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5092629] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 18           |
|    time_elapsed          | 301          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0057011764 |
|    clip_fraction         | 0.0501       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.244        |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.0177       |
|    lagrangian_multiplier | 0.049        |
|    learning_rate         | 0.0003       |
|    loss                  | 45.8         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00653     |
|    std                   | 1.03         |
|    value_loss            | 397          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.47364905] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 30            |
|    time_elapsed          | 492           |
|    total_timesteps       | 61440         |
| train/                   |               |
|    approx_kl             | 0.0066285497  |
|    clip_fraction         | 0.0626        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 7.79          |
|    entropy_loss          | -2.94         |
|    explained_variance    | -0.0265       |
|    lagrangian_multiplier | 0.0514        |
|    learning_rate         | 0.0003        |
|    loss                  | 65.3          |
|    n_updates             | 290           |
|    policy_gradient_loss  | -0.0107       |
|    std                   | 1.05          |
|    value_loss            | 628           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.9928802] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 30           |
|    time_elapsed          | 482          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0063259862 |
|    clip_fraction         | 0.0576       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0531       |
|    entropy_loss          | -2.7         |
|    explained_variance    | 0.0176       |
|    lagrangian_multiplier | 0.0643       |
|    learning_rate         | 0.0003       |
|    loss                  | 27.5         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00534     |
|    std                   | 0.935        |
|    value_loss            | 279          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9683875] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 19           |
|    time_elapsed          | 318          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.00527434   |
|    clip_fraction         | 0.0442       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.244        |
|    entropy_loss          | -2.9         |
|    explained_variance    | -0.00434     |
|    lagrangian_multiplier | 0.049        |
|    learning_rate         | 0.0003       |
|    loss                  | 81.4         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00864     |
|    std                   | 1.03         |
|    value_loss            | 649          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7309535] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 16           |
|    time_elapsed          | 289          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0074147885 |
|    clip_fraction         | 0.0705       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.305        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0688       |
|    lagrangian_multiplier | 0.0575       |
|    learning_rate         | 0.0003       |
|    loss                  | 72.3         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.0118      |
|    std                   | 0.986        |
|    value_loss            | 700          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.43719482] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 31            |
|    time_elapsed          | 509           |
|    total_timesteps       | 63488         |
| train/                   |               |
|    approx_kl             | 0.0075264843  |
|    clip_fraction         | 0.0584        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 17.9          |
|    entropy_loss          | -2.91         |
|    explained_variance    | 0.00642       |
|    lagrangian_multiplier | 0.0593        |
|    learning_rate         | 0.0003        |
|    loss                  | 51.3          |
|    n_updates             | 300           |
|    policy_gradient_loss  | -0.00846      |
|    std                   | 1.03          |
|    value_loss            | 537           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2738451] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 31           |
|    time_elapsed          | 498          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.006394496  |
|    clip_fraction         | 0.0636       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0607       |
|    entropy_loss          | -2.71        |
|    explained_variance    | 0.0653       |
|    lagrangian_multiplier | 0.0639       |
|    learning_rate         | 0.0003       |
|    loss                  | 29.5         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00489     |
|    std                   | 0.941        |
|    value_loss            | 333          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6963216] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 20           |
|    time_elapsed          | 334          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0052514155 |
|    clip_fraction         | 0.0429       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0892       |
|    entropy_loss          | -2.9         |
|    explained_variance    | -0.00599     |
|    lagrangian_multiplier | 0.0569       |
|    learning_rate         | 0.0003       |
|    loss                  | 59.5         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00705     |
|    std                   | 1.03         |
|    value_loss            | 650          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.1012886] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 17           |
|    time_elapsed          | 307          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0059093726 |
|    clip_fraction         | 0.0544       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.342        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0438       |
|    lagrangian_multiplier | 0.0605       |
|    learning_rate         | 0.0003       |
|    loss                  | 45.3         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00904     |
|    std                   | 0.985        |
|    value_loss            | 496          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8102406] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 32           |
|    time_elapsed          | 525          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.0075367084 |
|    clip_fraction         | 0.0566       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 13.4         |
|    entropy_loss          | -2.9         |
|    explained_variance    | 5.26e-05     |
|    lagrangian_multiplier | 0.064        |
|    learning_rate         | 0.0003       |
|    loss                  | 57.5         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00845     |
|    std                   | 1.03         |
|    value_loss            | 576          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9394678] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 32           |
|    time_elapsed          | 514          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.006252705  |
|    clip_fraction         | 0.0684       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.156        |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.00275     |
|    lagrangian_multiplier | 0.0587       |
|    learning_rate         | 0.0003       |
|    loss                  | 54.5         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.0105      |
|    std                   | 0.952        |
|    value_loss            | 440          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7463713] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 21           |
|    time_elapsed          | 351          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.005424313  |
|    clip_fraction         | 0.0624       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0927       |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.00305     |
|    lagrangian_multiplier | 0.0562       |
|    learning_rate         | 0.0003       |
|    loss                  | 56.5         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00846     |
|    std                   | 1.02         |
|    value_loss            | 585          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.3136938] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 18           |
|    time_elapsed          | 326          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0049341284 |
|    clip_fraction         | 0.0351       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.279        |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0334      |
|    lagrangian_multiplier | 0.0482       |
|    learning_rate         | 0.0003       |
|    loss                  | 67.8         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00545     |
|    std                   | 0.985        |
|    value_loss            | 494          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.63173914] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 33            |
|    time_elapsed          | 542           |
|    total_timesteps       | 67584         |
| train/                   |               |
|    approx_kl             | 0.009864809   |
|    clip_fraction         | 0.0885        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 17.9          |
|    entropy_loss          | -2.88         |
|    explained_variance    | 0.00824       |
|    lagrangian_multiplier | 0.0645        |
|    learning_rate         | 0.0003        |
|    loss                  | 83.2          |
|    n_updates             | 320           |
|    policy_gradient_loss  | -0.00964      |
|    std                   | 1.02          |
|    value_loss            | 762           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3606279] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 33           |
|    time_elapsed          | 530          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0062352577 |
|    clip_fraction         | 0.0515       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.244        |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.00012     |
|    lagrangian_multiplier | 0.0619       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.8         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00773     |
|    std                   | 0.945        |
|    value_loss            | 523          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.3732528] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 22           |
|    time_elapsed          | 367          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.0060364767 |
|    clip_fraction         | 0.0557       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.131        |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.00202      |
|    lagrangian_multiplier | 0.0483       |
|    learning_rate         | 0.0003       |
|    loss                  | 116          |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.0063      |
|    std                   | 1.03         |
|    value_loss            | 849          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.4597092] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 19           |
|    time_elapsed          | 344          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.007287981  |
|    clip_fraction         | 0.0885       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.153        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0696       |
|    lagrangian_multiplier | 0.0517       |
|    learning_rate         | 0.0003       |
|    loss                  | 83.8         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00949     |
|    std                   | 0.991        |
|    value_loss            | 744          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.206762] |
| time/                    |             |
|    fps                   | 124         |
|    iterations            | 34          |
|    time_elapsed          | 558         |
|    total_timesteps       | 69632       |
| train/                   |             |
|    approx_kl             | 0.009470716 |
|    clip_fraction         | 0.0935      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 10.9        |
|    entropy_loss          | -2.87       |
|    explained_variance    | 0.00127     |
|    lagrangian_multiplier | 0.0535      |
|    learning_rate         | 0.0003      |
|    loss                  | 72.9        |
|    n_updates             | 330         |
|    policy_gradient_loss  | -0.0134     |
|    std                   | 1.02        |
|    value_loss            | 706         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.76965064] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 34            |
|    time_elapsed          | 546           |
|    total_timesteps       | 69632         |
| train/                   |               |
|    approx_kl             | 0.0061953655  |
|    clip_fraction         | 0.0692        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.142         |
|    entropy_loss          | -2.71         |
|    explained_variance    | 0.0191        |
|    lagrangian_multiplier | 0.0594        |
|    learning_rate         | 0.0003        |
|    loss                  | 26.3          |
|    n_updates             | 330           |
|    policy_gradient_loss  | -0.00865      |
|    std                   | 0.935         |
|    value_loss            | 255           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.7199074] |
| time/                    |              |
|    fps                   | 120          |
|    iterations            | 23           |
|    time_elapsed          | 389          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.0059637493 |
|    clip_fraction         | 0.069        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0859       |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.00471      |
|    lagrangian_multiplier | 0.0479       |
|    learning_rate         | 0.0003       |
|    loss                  | 40.5         |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.00968     |
|    std                   | 1.04         |
|    value_loss            | 364          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.049276]  |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 20           |
|    time_elapsed          | 362          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0077227578 |
|    clip_fraction         | 0.0735       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.284        |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0788       |
|    lagrangian_multiplier | 0.0536       |
|    learning_rate         | 0.0003       |
|    loss                  | 90.3         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00887     |
|    std                   | 0.99         |
|    value_loss            | 806          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.89964795] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 35            |
|    time_elapsed          | 575           |
|    total_timesteps       | 71680         |
| train/                   |               |
|    approx_kl             | 0.007888398   |
|    clip_fraction         | 0.0716        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 6.33          |
|    entropy_loss          | -2.88         |
|    explained_variance    | 0.0176        |
|    lagrangian_multiplier | 0.0534        |
|    learning_rate         | 0.0003        |
|    loss                  | 58.2          |
|    n_updates             | 340           |
|    policy_gradient_loss  | -0.0102       |
|    std                   | 1.03          |
|    value_loss            | 507           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.6082917] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 35           |
|    time_elapsed          | 563          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0057986947 |
|    clip_fraction         | 0.0431       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0747       |
|    entropy_loss          | -2.7         |
|    explained_variance    | 0.0303       |
|    lagrangian_multiplier | 0.0618       |
|    learning_rate         | 0.0003       |
|    loss                  | 22.2         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00337     |
|    std                   | 0.936        |
|    value_loss            | 231          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8557315] |
| time/                    |              |
|    fps                   | 121          |
|    iterations            | 24           |
|    time_elapsed          | 406          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.0075672516 |
|    clip_fraction         | 0.0564       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.143        |
|    entropy_loss          | -2.92        |
|    explained_variance    | 0.02         |
|    lagrangian_multiplier | 0.0558       |
|    learning_rate         | 0.0003       |
|    loss                  | 75.5         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.0114      |
|    std                   | 1.04         |
|    value_loss            | 716          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5025822] |
| time/                    |              |
|    fps                   | 113          |
|    iterations            | 21           |
|    time_elapsed          | 380          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0064252615 |
|    clip_fraction         | 0.035        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.293        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0673       |
|    lagrangian_multiplier | 0.0521       |
|    learning_rate         | 0.0003       |
|    loss                  | 64.7         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00652     |
|    std                   | 0.987        |
|    value_loss            | 506          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.469338] |
| time/                    |             |
|    fps                   | 124         |
|    iterations            | 36          |
|    time_elapsed          | 591         |
|    total_timesteps       | 73728       |
| train/                   |             |
|    approx_kl             | 0.008627613 |
|    clip_fraction         | 0.0729      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 6.17        |
|    entropy_loss          | -2.88       |
|    explained_variance    | -0.0125     |
|    lagrangian_multiplier | 0.0454      |
|    learning_rate         | 0.0003      |
|    loss                  | 101         |
|    n_updates             | 350         |
|    policy_gradient_loss  | -0.0105     |
|    std                   | 1.02        |
|    value_loss            | 730         |
------------------------------------------
-------------------------------------------
| reward                   | [-2.0645304] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 36           |
|    time_elapsed          | 579          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0073688067 |
|    clip_fraction         | 0.0754       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.145        |
|    entropy_loss          | -2.71        |
|    explained_variance    | -0.02        |
|    lagrangian_multiplier | 0.05         |
|    learning_rate         | 0.0003       |
|    loss                  | 50.3         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0122      |
|    std                   | 0.944        |
|    value_loss            | 437          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.5028942] |
| time/                    |              |
|    fps                   | 121          |
|    iterations            | 25           |
|    time_elapsed          | 422          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.006229327  |
|    clip_fraction         | 0.0488       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.131        |
|    entropy_loss          | -2.92        |
|    explained_variance    | -0.000392    |
|    lagrangian_multiplier | 0.0557       |
|    learning_rate         | 0.0003       |
|    loss                  | 33.6         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.0085      |
|    std                   | 1.04         |
|    value_loss            | 295          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.30872703] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 22            |
|    time_elapsed          | 398           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.0061836145  |
|    clip_fraction         | 0.0546        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.148         |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.0504        |
|    lagrangian_multiplier | 0.0497        |
|    learning_rate         | 0.0003        |
|    loss                  | 78.9          |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.00884      |
|    std                   | 0.998         |
|    value_loss            | 676           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.8437989] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 37           |
|    time_elapsed          | 608          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.006999242  |
|    clip_fraction         | 0.0756       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.31         |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.0158      |
|    lagrangian_multiplier | 0.0586       |
|    learning_rate         | 0.0003       |
|    loss                  | 44.1         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00979     |
|    std                   | 1.02         |
|    value_loss            | 379          |
-------------------------------------------
------------------------------------------
| reward                   | [-2.076176] |
| time/                    |             |
|    fps                   | 127         |
|    iterations            | 37          |
|    time_elapsed          | 595         |
|    total_timesteps       | 75776       |
| train/                   |             |
|    approx_kl             | 0.006073053 |
|    clip_fraction         | 0.0645      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.14        |
|    entropy_loss          | -2.72       |
|    explained_variance    | -0.0151     |
|    lagrangian_multiplier | 0.0602      |
|    learning_rate         | 0.0003      |
|    loss                  | 35.2        |
|    n_updates             | 360         |
|    policy_gradient_loss  | -0.0114     |
|    std                   | 0.949       |
|    value_loss            | 391         |
------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.0722252] |
| time/                    |              |
|    fps                   | 121          |
|    iterations            | 26           |
|    time_elapsed          | 438          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0051452187 |
|    clip_fraction         | 0.0418       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.117        |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.0125       |
|    lagrangian_multiplier | 0.0677       |
|    learning_rate         | 0.0003       |
|    loss                  | 85.4         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00708     |
|    std                   | 1.04         |
|    value_loss            | 951          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0568459] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 38           |
|    time_elapsed          | 624          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.006894587  |
|    clip_fraction         | 0.0629       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 7.98         |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0101       |
|    lagrangian_multiplier | 0.0554       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.4         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.0106      |
|    std                   | 1.02         |
|    value_loss            | 512          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5693176] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 38           |
|    time_elapsed          | 611          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0073562497 |
|    clip_fraction         | 0.0575       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.148        |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.0192      |
|    lagrangian_multiplier | 0.0534       |
|    learning_rate         | 0.0003       |
|    loss                  | 61.3         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.0078      |
|    std                   | 0.946        |
|    value_loss            | 575          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.36508697] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 23            |
|    time_elapsed          | 416           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.0056760684  |
|    clip_fraction         | 0.0411        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.149         |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.033        |
|    lagrangian_multiplier | 0.0517        |
|    learning_rate         | 0.0003        |
|    loss                  | 191           |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00724      |
|    std                   | 0.999         |
|    value_loss            | 1.29e+03      |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2498469] |
| time/                    |              |
|    fps                   | 121          |
|    iterations            | 27           |
|    time_elapsed          | 455          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.007372753  |
|    clip_fraction         | 0.0852       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.177        |
|    entropy_loss          | -2.92        |
|    explained_variance    | 0.0146       |
|    lagrangian_multiplier | 0.0484       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.4         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.0104      |
|    std                   | 1.04         |
|    value_loss            | 493          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.28397]   |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 39           |
|    time_elapsed          | 627          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0069138217 |
|    clip_fraction         | 0.0534       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.111        |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.00296      |
|    lagrangian_multiplier | 0.0564       |
|    learning_rate         | 0.0003       |
|    loss                  | 53.2         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00642     |
|    std                   | 0.96         |
|    value_loss            | 451          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4282173] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 39           |
|    time_elapsed          | 640          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.007475746  |
|    clip_fraction         | 0.0663       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.77         |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.000156    |
|    lagrangian_multiplier | 0.0596       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.4         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.0099      |
|    std                   | 1.03         |
|    value_loss            | 483          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7936111] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 24           |
|    time_elapsed          | 435          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.0072298683 |
|    clip_fraction         | 0.0721       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.178        |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.083        |
|    lagrangian_multiplier | 0.0536       |
|    learning_rate         | 0.0003       |
|    loss                  | 78.4         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00819     |
|    std                   | 1            |
|    value_loss            | 655          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3168544] |
| time/                    |              |
|    fps                   | 121          |
|    iterations            | 28           |
|    time_elapsed          | 471          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.006541325  |
|    clip_fraction         | 0.0672       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0891       |
|    entropy_loss          | -2.92        |
|    explained_variance    | -0.0195      |
|    lagrangian_multiplier | 0.0531       |
|    learning_rate         | 0.0003       |
|    loss                  | 37.7         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00718     |
|    std                   | 1.04         |
|    value_loss            | 346          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2824086] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 40           |
|    time_elapsed          | 643          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.0065944064 |
|    clip_fraction         | 0.074        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.11         |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0363       |
|    lagrangian_multiplier | 0.0623       |
|    learning_rate         | 0.0003       |
|    loss                  | 39.8         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00854     |
|    std                   | 0.981        |
|    value_loss            | 380          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1018741] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 40           |
|    time_elapsed          | 657          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.008079863  |
|    clip_fraction         | 0.0713       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 14.8         |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.01         |
|    lagrangian_multiplier | 0.0553       |
|    learning_rate         | 0.0003       |
|    loss                  | 54.7         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.0099      |
|    std                   | 1.02         |
|    value_loss            | 503          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3970561] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 25           |
|    time_elapsed          | 453          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.007802822  |
|    clip_fraction         | 0.0721       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.207        |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0589       |
|    lagrangian_multiplier | 0.0504       |
|    learning_rate         | 0.0003       |
|    loss                  | 113          |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.0099      |
|    std                   | 1            |
|    value_loss            | 1.02e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6425663] |
| time/                    |              |
|    fps                   | 121          |
|    iterations            | 29           |
|    time_elapsed          | 487          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.0060611805 |
|    clip_fraction         | 0.0505       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.129        |
|    entropy_loss          | -2.92        |
|    explained_variance    | -0.0214      |
|    lagrangian_multiplier | 0.0544       |
|    learning_rate         | 0.0003       |
|    loss                  | 121          |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00993     |
|    std                   | 1.04         |
|    value_loss            | 1.31e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.1406877] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 41           |
|    time_elapsed          | 660          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.005823115  |
|    clip_fraction         | 0.0427       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.121        |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0359       |
|    lagrangian_multiplier | 0.0597       |
|    learning_rate         | 0.0003       |
|    loss                  | 25.7         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00533     |
|    std                   | 0.975        |
|    value_loss            | 284          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2428477] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 41           |
|    time_elapsed          | 674          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.008823287  |
|    clip_fraction         | 0.0843       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 9            |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.00196     |
|    lagrangian_multiplier | 0.0499       |
|    learning_rate         | 0.0003       |
|    loss                  | 50.6         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 1.02         |
|    value_loss            | 422          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3857627] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 26           |
|    time_elapsed          | 471          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0056055943 |
|    clip_fraction         | 0.0504       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.185        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0447       |
|    lagrangian_multiplier | 0.0512       |
|    learning_rate         | 0.0003       |
|    loss                  | 174          |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00812     |
|    std                   | 0.994        |
|    value_loss            | 1.42e+03     |
-------------------------------------------
------------------------------------------
| reward                   | [-1.643877] |
| time/                    |             |
|    fps                   | 127         |
|    iterations            | 42          |
|    time_elapsed          | 676         |
|    total_timesteps       | 86016       |
| train/                   |             |
|    approx_kl             | 0.009429827 |
|    clip_fraction         | 0.095       |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.118       |
|    entropy_loss          | -2.79       |
|    explained_variance    | 0.0278      |
|    lagrangian_multiplier | 0.0649      |
|    learning_rate         | 0.0003      |
|    loss                  | 69.7        |
|    n_updates             | 410         |
|    policy_gradient_loss  | -0.00964    |
|    std                   | 0.976       |
|    value_loss            | 695         |
------------------------------------------
-------------------------------------------
| reward                   | [-2.4076197] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 42           |
|    time_elapsed          | 690          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.007436315  |
|    clip_fraction         | 0.0634       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 12           |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.00164      |
|    lagrangian_multiplier | 0.0574       |
|    learning_rate         | 0.0003       |
|    loss                  | 48.9         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00822     |
|    std                   | 1.01         |
|    value_loss            | 526          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8510119] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 27           |
|    time_elapsed          | 489          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.005305758  |
|    clip_fraction         | 0.0571       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.231        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0605       |
|    lagrangian_multiplier | 0.05         |
|    learning_rate         | 0.0003       |
|    loss                  | 91.7         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00708     |
|    std                   | 0.985        |
|    value_loss            | 865          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0618286] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 43           |
|    time_elapsed          | 692          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.005845624  |
|    clip_fraction         | 0.0551       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.198        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0239       |
|    lagrangian_multiplier | 0.0638       |
|    learning_rate         | 0.0003       |
|    loss                  | 41.7         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00677     |
|    std                   | 0.992        |
|    value_loss            | 433          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1556805] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 43           |
|    time_elapsed          | 707          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.00582656   |
|    clip_fraction         | 0.0653       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 14.5         |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.00827     |
|    lagrangian_multiplier | 0.0544       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.3         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00923     |
|    std                   | 1.02         |
|    value_loss            | 440          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3622792] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 28           |
|    time_elapsed          | 507          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.0051278053 |
|    clip_fraction         | 0.0386       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.186        |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.058        |
|    lagrangian_multiplier | 0.0502       |
|    learning_rate         | 0.0003       |
|    loss                  | 97.5         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00742     |
|    std                   | 0.977        |
|    value_loss            | 754          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.57600206] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 44            |
|    time_elapsed          | 708           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.006583207   |
|    clip_fraction         | 0.0551        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.271         |
|    entropy_loss          | -2.82         |
|    explained_variance    | -0.00898      |
|    lagrangian_multiplier | 0.0654        |
|    learning_rate         | 0.0003        |
|    loss                  | 52.7          |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.00763      |
|    std                   | 0.992         |
|    value_loss            | 494           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.4041634] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 44           |
|    time_elapsed          | 723          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.007338756  |
|    clip_fraction         | 0.0683       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 16.7         |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.0114      |
|    lagrangian_multiplier | 0.0532       |
|    learning_rate         | 0.0003       |
|    loss                  | 189          |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.00859     |
|    std                   | 1.03         |
|    value_loss            | 1.37e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3913785] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 30           |
|    time_elapsed          | 549          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.007358283  |
|    clip_fraction         | 0.0712       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.138        |
|    entropy_loss          | -2.94        |
|    explained_variance    | -0.00656     |
|    lagrangian_multiplier | 0.0584       |
|    learning_rate         | 0.0003       |
|    loss                  | 130          |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.0128      |
|    std                   | 1.06         |
|    value_loss            | 1.3e+03      |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6523387] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 29           |
|    time_elapsed          | 526          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.006055313  |
|    clip_fraction         | 0.0573       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.18         |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.055        |
|    lagrangian_multiplier | 0.0519       |
|    learning_rate         | 0.0003       |
|    loss                  | 92.6         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00877     |
|    std                   | 0.976        |
|    value_loss            | 796          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5791487] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 45           |
|    time_elapsed          | 725          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.0070913    |
|    clip_fraction         | 0.0553       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.197        |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.033        |
|    lagrangian_multiplier | 0.0652       |
|    learning_rate         | 0.0003       |
|    loss                  | 48.7         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00778     |
|    std                   | 0.989        |
|    value_loss            | 488          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1240323] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 45           |
|    time_elapsed          | 740          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.008389719  |
|    clip_fraction         | 0.086        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 20.1         |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.0286      |
|    lagrangian_multiplier | 0.0547       |
|    learning_rate         | 0.0003       |
|    loss                  | 87.9         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.0128      |
|    std                   | 1.03         |
|    value_loss            | 607          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3326331] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 30           |
|    time_elapsed          | 544          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.007967798  |
|    clip_fraction         | 0.0822       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.2          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0534       |
|    lagrangian_multiplier | 0.051        |
|    learning_rate         | 0.0003       |
|    loss                  | 80.5         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.0069      |
|    std                   | 0.972        |
|    value_loss            | 691          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.49239224] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 46            |
|    time_elapsed          | 741           |
|    total_timesteps       | 94208         |
| train/                   |               |
|    approx_kl             | 0.0072269216  |
|    clip_fraction         | 0.0675        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0957        |
|    entropy_loss          | -2.81         |
|    explained_variance    | 0.0264        |
|    lagrangian_multiplier | 0.0728        |
|    learning_rate         | 0.0003        |
|    loss                  | 23.6          |
|    n_updates             | 450           |
|    policy_gradient_loss  | -0.00607      |
|    std                   | 0.984         |
|    value_loss            | 281           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.6244867] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 46           |
|    time_elapsed          | 756          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.007190445  |
|    clip_fraction         | 0.0758       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 25.2         |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.0125      |
|    lagrangian_multiplier | 0.0506       |
|    learning_rate         | 0.0003       |
|    loss                  | 92.4         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.0107      |
|    std                   | 1.02         |
|    value_loss            | 765          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6090696] |
| time/                    |              |
|    fps                   | 107          |
|    iterations            | 31           |
|    time_elapsed          | 587          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.007584678  |
|    clip_fraction         | 0.0652       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.145        |
|    entropy_loss          | -2.97        |
|    explained_variance    | -0.00506     |
|    lagrangian_multiplier | 0.0464       |
|    learning_rate         | 0.0003       |
|    loss                  | 133          |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.013       |
|    std                   | 1.08         |
|    value_loss            | 980          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.50463504] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 47            |
|    time_elapsed          | 757           |
|    total_timesteps       | 96256         |
| train/                   |               |
|    approx_kl             | 0.007483432   |
|    clip_fraction         | 0.0604        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.186         |
|    entropy_loss          | -2.8          |
|    explained_variance    | 0.00787       |
|    lagrangian_multiplier | 0.0625        |
|    learning_rate         | 0.0003        |
|    loss                  | 40.7          |
|    n_updates             | 460           |
|    policy_gradient_loss  | -0.0088       |
|    std                   | 0.982         |
|    value_loss            | 391           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.4060798] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 31           |
|    time_elapsed          | 562          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.004945341  |
|    clip_fraction         | 0.0434       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.291        |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0614       |
|    lagrangian_multiplier | 0.0498       |
|    learning_rate         | 0.0003       |
|    loss                  | 72.8         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00512     |
|    std                   | 0.963        |
|    value_loss            | 648          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9519527] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 47           |
|    time_elapsed          | 773          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.008317001  |
|    clip_fraction         | 0.0875       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 19           |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0214       |
|    lagrangian_multiplier | 0.0546       |
|    learning_rate         | 0.0003       |
|    loss                  | 70.1         |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 1.03         |
|    value_loss            | 672          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7488128] |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 32           |
|    time_elapsed          | 604          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.007006295  |
|    clip_fraction         | 0.0538       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.124        |
|    entropy_loss          | -3           |
|    explained_variance    | -0.0211      |
|    lagrangian_multiplier | 0.0541       |
|    learning_rate         | 0.0003       |
|    loss                  | 85           |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 1.09         |
|    value_loss            | 797          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5618893] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 48           |
|    time_elapsed          | 773          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.006139832  |
|    clip_fraction         | 0.0517       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.228        |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0296       |
|    lagrangian_multiplier | 0.0632       |
|    learning_rate         | 0.0003       |
|    loss                  | 38.2         |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00947     |
|    std                   | 0.984        |
|    value_loss            | 352          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8875678] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 48           |
|    time_elapsed          | 789          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.009196345  |
|    clip_fraction         | 0.0997       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 20.3         |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.0253       |
|    lagrangian_multiplier | 0.059        |
|    learning_rate         | 0.0003       |
|    loss                  | 87.6         |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.0139      |
|    std                   | 1.05         |
|    value_loss            | 668          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9422632] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 32           |
|    time_elapsed          | 580          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.0067449724 |
|    clip_fraction         | 0.0598       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.24         |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.0855       |
|    lagrangian_multiplier | 0.0529       |
|    learning_rate         | 0.0003       |
|    loss                  | 60.4         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00703     |
|    std                   | 0.965        |
|    value_loss            | 513          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6201069] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 49           |
|    time_elapsed          | 789          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.005495432  |
|    clip_fraction         | 0.043        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0709       |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0358       |
|    lagrangian_multiplier | 0.0631       |
|    learning_rate         | 0.0003       |
|    loss                  | 27.8         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.00762     |
|    std                   | 0.988        |
|    value_loss            | 229          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
-------------------------------------------
| reward                   | [-2.0614052] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 49           |
|    time_elapsed          | 806          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.0066963173 |
|    clip_fraction         | 0.0698       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 18.9         |
|    entropy_loss          | -2.93        |
|    explained_variance    | -0.00984     |
|    lagrangian_multiplier | 0.0481       |
|    learning_rate         | 0.0003       |
|    loss                  | 71.8         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.00998     |
|    std                   | 1.05         |
|    value_loss            | 601          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 109, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 96, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
-------------------------------------------
| reward                   | [-1.5662103] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 33           |
|    time_elapsed          | 598          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0056693275 |
|    clip_fraction         | 0.0561       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.104        |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0621       |
|    lagrangian_multiplier | 0.053        |
|    learning_rate         | 0.0003       |
|    loss                  | 101          |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00612     |
|    std                   | 0.97         |
|    value_loss            | 960          |
-------------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñá
wandb:             train/approx_kl ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñá‚ñÑ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ
wandb:         train/clip_fraction ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÇ
wandb:          train/entropy_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ
wandb:    train/explained_variance ‚ñá‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñá
wandb: train/lagrangian_multiplier ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñá‚ñá‚ñá‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñá‚ñá‚ñÇ‚ñÖ‚ñÑ‚ñÅ‚ñÇ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÖ
wandb:                   train/std ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñá
wandb:            train/value_loss ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñá‚ñà‚ñÜ‚ñá‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                      reward -0.62011
wandb:             train/approx_kl 0.0055
wandb:         train/clip_fraction 0.04302
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 0.0709
wandb:          train/entropy_loss -2.80766
wandb:    train/explained_variance 0.03581
wandb: train/lagrangian_multiplier 0.06305
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 27.84812
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00762
wandb:                   train/std 0.98798
wandb:            train/value_loss 229.21674
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D16/runs/0ghartmw
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_192536-0ghartmw/logs
srun: error: gail.ist.berkeley.edu: task 0: Exited with exit code 1
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÑ‚ñÑ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñá‚ñÉ‚ñÅ‚ñÜ‚ñÉ‚ñÜ‚ñà‚ñà‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÖ‚ñÜ‚ñá‚ñÇ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÑ
wandb:             train/approx_kl ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñà‚ñá‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÑ
wandb:         train/clip_fraction ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÅ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñÖ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:          train/entropy_loss ‚ñà‚ñà‚ñá‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ
wandb:    train/explained_variance ‚ñá‚ñÉ‚ñá‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñà‚ñÇ‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÜ‚ñÜ‚ñÉ
wandb: train/lagrangian_multiplier ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñÉ‚ñÉ‚ñà‚ñÜ‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÉ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñà‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÉ
wandb:                   train/std ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÜ
wandb:            train/value_loss ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb:                      reward -2.06141
wandb:             train/approx_kl 0.0067
wandb:         train/clip_fraction 0.06978
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 18.93983
wandb:          train/entropy_loss -2.92961
wandb:    train/explained_variance -0.00984
wandb: train/lagrangian_multiplier 0.0481
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 71.82877
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00998
wandb:                   train/std 1.0488
wandb:            train/value_loss 600.57268
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/ugeywulf
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231226_192523-ugeywulf/logs
srun: error: dqn.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-1.0842112] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 34           |
|    time_elapsed          | 616          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0063709063 |
|    clip_fraction         | 0.0555       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.199        |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0895      |
|    lagrangian_multiplier | 0.0424       |
|    learning_rate         | 0.0003       |
|    loss                  | 128          |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00731     |
|    std                   | 0.982        |
|    value_loss            | 872          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6947513] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 35           |
|    time_elapsed          | 635          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0069883578 |
|    clip_fraction         | 0.0615       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.236        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0343       |
|    lagrangian_multiplier | 0.054        |
|    learning_rate         | 0.0003       |
|    loss                  | 74.6         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00884     |
|    std                   | 0.993        |
|    value_loss            | 659          |
-------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114547.12
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114547.13
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_193932-526cto7l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D8/runs/526cto7l
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231226_193932-itki7359
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D2/runs/itki7359
-------------------------------------------
| reward                   | [-1.8178234] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 36           |
|    time_elapsed          | 653          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0061962707 |
|    clip_fraction         | 0.0863       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.179        |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0561       |
|    lagrangian_multiplier | 0.0516       |
|    learning_rate         | 0.0003       |
|    loss                  | 89.8         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0114      |
|    std                   | 0.993        |
|    value_loss            | 801          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4283507] |
| time/                    |              |
|    fps                   | 98           |
|    iterations            | 33           |
|    time_elapsed          | 683          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.007161473  |
|    clip_fraction         | 0.0632       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.142        |
|    entropy_loss          | -3.02        |
|    explained_variance    | 0.00174      |
|    lagrangian_multiplier | 0.0587       |
|    learning_rate         | 0.0003       |
|    loss                  | 69.1         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00978     |
|    std                   | 1.1          |
|    value_loss            | 696          |
-------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.11524871] |
| time/              |               |
|    fps             | 133           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.44111267] |
| time/              |               |
|    fps             | 131           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-2.422724] |
| time/                    |             |
|    fps                   | 112         |
|    iterations            | 37          |
|    time_elapsed          | 671         |
|    total_timesteps       | 75776       |
| train/                   |             |
|    approx_kl             | 0.008268476 |
|    clip_fraction         | 0.0589      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.264       |
|    entropy_loss          | -2.83       |
|    explained_variance    | 0.0647      |
|    lagrangian_multiplier | 0.0515      |
|    learning_rate         | 0.0003      |
|    loss                  | 52.6        |
|    n_updates             | 360         |
|    policy_gradient_loss  | -0.006      |
|    std                   | 1.01        |
|    value_loss            | 434         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.43689325] |
| time/                    |               |
|    fps                   | 129           |
|    iterations            | 2             |
|    time_elapsed          | 31            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.0071642264  |
|    clip_fraction         | 0.0598        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.113         |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.0398        |
|    lagrangian_multiplier | 0.0767        |
|    learning_rate         | 0.0003        |
|    loss                  | 36            |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00618      |
|    std                   | 0.985         |
|    value_loss            | 454           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.52897674] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 2             |
|    time_elapsed          | 31            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.0030591167  |
|    clip_fraction         | 0.0154        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 102           |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.0479       |
|    lagrangian_multiplier | 0.046         |
|    learning_rate         | 0.0003        |
|    loss                  | 53.4          |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00293      |
|    std                   | 0.998         |
|    value_loss            | 267           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.8454172] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 3            |
|    time_elapsed          | 47           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.006414275  |
|    clip_fraction         | 0.0669       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.407        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.00734      |
|    lagrangian_multiplier | 0.0532       |
|    learning_rate         | 0.0003       |
|    loss                  | 61.1         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00733     |
|    std                   | 0.99         |
|    value_loss            | 581          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.90794367] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 3             |
|    time_elapsed          | 48            |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.0057630017  |
|    clip_fraction         | 0.0439        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 112           |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.0461       |
|    lagrangian_multiplier | 0.0465        |
|    learning_rate         | 0.0003        |
|    loss                  | 62.3          |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00696      |
|    std                   | 1.01          |
|    value_loss            | 356           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.9639398] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 38           |
|    time_elapsed          | 689          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.011824163  |
|    clip_fraction         | 0.109        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.157        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0384       |
|    lagrangian_multiplier | 0.0498       |
|    learning_rate         | 0.0003       |
|    loss                  | 126          |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.0133      |
|    std                   | 0.99         |
|    value_loss            | 1.12e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5715604] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 4            |
|    time_elapsed          | 64           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.0047047576 |
|    clip_fraction         | 0.0321       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0814       |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0526       |
|    lagrangian_multiplier | 0.0672       |
|    learning_rate         | 0.0003       |
|    loss                  | 54.8         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.0049      |
|    std                   | 0.981        |
|    value_loss            | 572          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7859812] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 4            |
|    time_elapsed          | 64           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.004103755  |
|    clip_fraction         | 0.032        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 139          |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.0178      |
|    lagrangian_multiplier | 0.0523       |
|    learning_rate         | 0.0003       |
|    loss                  | 54.3         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00536     |
|    std                   | 1.02         |
|    value_loss            | 328          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0009058] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 39           |
|    time_elapsed          | 708          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0074086823 |
|    clip_fraction         | 0.0704       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.221        |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0606       |
|    lagrangian_multiplier | 0.0467       |
|    learning_rate         | 0.0003       |
|    loss                  | 84.5         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.0107      |
|    std                   | 1.01         |
|    value_loss            | 675          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0546162] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 5            |
|    time_elapsed          | 80           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0045578387 |
|    clip_fraction         | 0.0486       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.152        |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.028        |
|    lagrangian_multiplier | 0.055        |
|    learning_rate         | 0.0003       |
|    loss                  | 62.2         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00443     |
|    std                   | 0.98         |
|    value_loss            | 530          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.45049447] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 5             |
|    time_elapsed          | 80            |
|    total_timesteps       | 10240         |
| train/                   |               |
|    approx_kl             | 0.005465044   |
|    clip_fraction         | 0.0448        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 133           |
|    entropy_loss          | -2.88         |
|    explained_variance    | 0.0559        |
|    lagrangian_multiplier | 0.0497        |
|    learning_rate         | 0.0003        |
|    loss                  | 55.7          |
|    n_updates             | 40            |
|    policy_gradient_loss  | -0.00632      |
|    std                   | 1.02          |
|    value_loss            | 306           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.442941]  |
| time/                    |              |
|    fps                   | 92           |
|    iterations            | 34           |
|    time_elapsed          | 753          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0075731743 |
|    clip_fraction         | 0.0647       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.146        |
|    entropy_loss          | -3           |
|    explained_variance    | 0.0152       |
|    lagrangian_multiplier | 0.0553       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.3         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00929     |
|    std                   | 1.07         |
|    value_loss            | 464          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2511282] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 40           |
|    time_elapsed          | 726          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.007705163  |
|    clip_fraction         | 0.0738       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.229        |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0334       |
|    lagrangian_multiplier | 0.0598       |
|    learning_rate         | 0.0003       |
|    loss                  | 104          |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00876     |
|    std                   | 0.996        |
|    value_loss            | 1.06e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5293485] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 6            |
|    time_elapsed          | 96           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0049536293 |
|    clip_fraction         | 0.0233       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.143        |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0104       |
|    lagrangian_multiplier | 0.0634       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.4         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00461     |
|    std                   | 0.955        |
|    value_loss            | 552          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7876883] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 6            |
|    time_elapsed          | 97           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0077433335 |
|    clip_fraction         | 0.0759       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 118          |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.0311      |
|    lagrangian_multiplier | 0.0753       |
|    learning_rate         | 0.0003       |
|    loss                  | 38.1         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00651     |
|    std                   | 1.02         |
|    value_loss            | 301          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4566184] |
| time/                    |              |
|    fps                   | 93           |
|    iterations            | 35           |
|    time_elapsed          | 769          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0060459925 |
|    clip_fraction         | 0.0613       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.126        |
|    entropy_loss          | -2.96        |
|    explained_variance    | 0.00575      |
|    lagrangian_multiplier | 0.0649       |
|    learning_rate         | 0.0003       |
|    loss                  | 94.3         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.0076      |
|    std                   | 1.06         |
|    value_loss            | 858          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.7713106] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 41           |
|    time_elapsed          | 744          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0056553185 |
|    clip_fraction         | 0.0464       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.262        |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0696       |
|    lagrangian_multiplier | 0.0519       |
|    learning_rate         | 0.0003       |
|    loss                  | 68.2         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.0079      |
|    std                   | 0.991        |
|    value_loss            | 567          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6230483] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 7            |
|    time_elapsed          | 112          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.00641902   |
|    clip_fraction         | 0.0534       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0741       |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0127      |
|    lagrangian_multiplier | 0.0696       |
|    learning_rate         | 0.0003       |
|    loss                  | 67.3         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00631     |
|    std                   | 0.955        |
|    value_loss            | 784          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1331866] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 7            |
|    time_elapsed          | 113          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.005966503  |
|    clip_fraction         | 0.0577       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 116          |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.00381     |
|    lagrangian_multiplier | 0.0621       |
|    learning_rate         | 0.0003       |
|    loss                  | 56.9         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00665     |
|    std                   | 1.03         |
|    value_loss            | 429          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5342536] |
| time/                    |              |
|    fps                   | 93           |
|    iterations            | 36           |
|    time_elapsed          | 786          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0071868636 |
|    clip_fraction         | 0.0644       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.214        |
|    entropy_loss          | -2.95        |
|    explained_variance    | -0.0101      |
|    lagrangian_multiplier | 0.0562       |
|    learning_rate         | 0.0003       |
|    loss                  | 90.6         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 1.06         |
|    value_loss            | 1.02e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.7479892] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 42           |
|    time_elapsed          | 762          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.007854377  |
|    clip_fraction         | 0.0735       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.234        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0537       |
|    lagrangian_multiplier | 0.0496       |
|    learning_rate         | 0.0003       |
|    loss                  | 76.3         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00982     |
|    std                   | 0.986        |
|    value_loss            | 706          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.299032]  |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 8            |
|    time_elapsed          | 129          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0058446974 |
|    clip_fraction         | 0.0562       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.571        |
|    entropy_loss          | -2.74        |
|    explained_variance    | -0.0383      |
|    lagrangian_multiplier | 0.0658       |
|    learning_rate         | 0.0003       |
|    loss                  | 44.7         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.0072      |
|    std                   | 0.947        |
|    value_loss            | 469          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.25854]   |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 8            |
|    time_elapsed          | 130          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0053257127 |
|    clip_fraction         | 0.0537       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 115          |
|    entropy_loss          | -2.91        |
|    explained_variance    | -0.0238      |
|    lagrangian_multiplier | 0.0534       |
|    learning_rate         | 0.0003       |
|    loss                  | 40           |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00779     |
|    std                   | 1.04         |
|    value_loss            | 276          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.9468935] |
| time/                    |              |
|    fps                   | 94           |
|    iterations            | 37           |
|    time_elapsed          | 802          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.007667408  |
|    clip_fraction         | 0.0747       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.192        |
|    entropy_loss          | -2.96        |
|    explained_variance    | -0.00949     |
|    lagrangian_multiplier | 0.0533       |
|    learning_rate         | 0.0003       |
|    loss                  | 82.7         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.013       |
|    std                   | 1.07         |
|    value_loss            | 652          |
-------------------------------------------
------------------------------------------
| reward                   | [-4.077426] |
| time/                    |             |
|    fps                   | 112         |
|    iterations            | 43          |
|    time_elapsed          | 780         |
|    total_timesteps       | 88064       |
| train/                   |             |
|    approx_kl             | 0.009694106 |
|    clip_fraction         | 0.0877      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.23        |
|    entropy_loss          | -2.8        |
|    explained_variance    | 0.063       |
|    lagrangian_multiplier | 0.0488      |
|    learning_rate         | 0.0003      |
|    loss                  | 101         |
|    n_updates             | 420         |
|    policy_gradient_loss  | -0.0114     |
|    std                   | 0.982       |
|    value_loss            | 756         |
------------------------------------------
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114547 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.81489563] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 9             |
|    time_elapsed          | 145           |
|    total_timesteps       | 18432         |
| train/                   |               |
|    approx_kl             | 0.0049950043  |
|    clip_fraction         | 0.0527        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 3.78          |
|    entropy_loss          | -2.72         |
|    explained_variance    | -0.0233       |
|    lagrangian_multiplier | 0.049         |
|    learning_rate         | 0.0003        |
|    loss                  | 88.2          |
|    n_updates             | 80            |
|    policy_gradient_loss  | -0.00672      |
|    std                   | 0.94          |
|    value_loss            | 719           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.9632141] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 9            |
|    time_elapsed          | 146          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0044101346 |
|    clip_fraction         | 0.0373       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 162          |
|    entropy_loss          | -2.92        |
|    explained_variance    | -0.0306      |
|    lagrangian_multiplier | 0.0822       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.8         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00457     |
|    std                   | 1.04         |
|    value_loss            | 513          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2122085] |
| time/                    |              |
|    fps                   | 95           |
|    iterations            | 38           |
|    time_elapsed          | 818          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.007193843  |
|    clip_fraction         | 0.0717       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.147        |
|    entropy_loss          | -2.98        |
|    explained_variance    | -0.0224      |
|    lagrangian_multiplier | 0.059        |
|    learning_rate         | 0.0003       |
|    loss                  | 73.8         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.0119      |
|    std                   | 1.08         |
|    value_loss            | 739          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6030462] |
| time/                    |              |
|    fps                   | 112          |
|    iterations            | 44           |
|    time_elapsed          | 798          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.0057273293 |
|    clip_fraction         | 0.0515       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.123        |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0485       |
|    lagrangian_multiplier | 0.0554       |
|    learning_rate         | 0.0003       |
|    loss                  | 142          |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 0.974        |
|    value_loss            | 1.29e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9869381] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 10           |
|    time_elapsed          | 161          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.005043203  |
|    clip_fraction         | 0.037        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.22         |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.0437      |
|    lagrangian_multiplier | 0.0598       |
|    learning_rate         | 0.0003       |
|    loss                  | 73.6         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.0062      |
|    std                   | 0.947        |
|    value_loss            | 690          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2065185] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 10           |
|    time_elapsed          | 162          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.005792893  |
|    clip_fraction         | 0.0524       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 204          |
|    entropy_loss          | -2.93        |
|    explained_variance    | 0.00132      |
|    lagrangian_multiplier | 0.0665       |
|    learning_rate         | 0.0003       |
|    loss                  | 97.8         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00765     |
|    std                   | 1.05         |
|    value_loss            | 954          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5002702] |
| time/                    |              |
|    fps                   | 95           |
|    iterations            | 39           |
|    time_elapsed          | 835          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0052798595 |
|    clip_fraction         | 0.055        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.2          |
|    entropy_loss          | -2.99        |
|    explained_variance    | 0.0018       |
|    lagrangian_multiplier | 0.0531       |
|    learning_rate         | 0.0003       |
|    loss                  | 90           |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00868     |
|    std                   | 1.08         |
|    value_loss            | 761          |
-------------------------------------------
slurmstepd: error: *** STEP 114547.13 ON gail.ist.berkeley.edu CANCELLED AT 2023-12-26T19:42:24 ***
slurmstepd: error: *** STEP 114547.11 ON ddpg.ist.berkeley.edu CANCELLED AT 2023-12-26T19:42:24 ***
slurmstepd: error: *** STEP 114547.12 ON dqn.ist.berkeley.edu CANCELLED AT 2023-12-26T19:42:24 ***
slurmstepd: error: *** STEP 114547.10 ON airl.ist.berkeley.edu CANCELLED AT 2023-12-27T03:42:24 ***
slurmstepd: error: *** JOB 114547 ON airl.ist.berkeley.edu CANCELLED AT 2023-12-27T03:42:24 ***
