Running script with speed limit: 2
Running script with speed limit: 4
Running script with speed limit: 8
Running script with speed limit: 16
Running script with speed limit: 2
Running script with speed limit: 4
Running script with speed limit: 8
Running script with speed limit: 16
Running script with speed limit: 2
Running script with speed limit: 4
Running script with speed limit: 8
Running script with speed limit: 16
Running script with speed limit: 2
Running script with speed limit: 4
Running script with speed limit: 8
Running script with speed limit: 16
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231223_000944-5mdjt0ln
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/5mdjt0ln
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231223_000944-tjc7kab3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/tjc7kab3
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231223_080944-nd3knsyr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/nd3knsyr
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231223_000944-fanvdd9w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/fanvdd9w
Using cpu device
------------------------------------
| reward             | [-0.533787] |
| time/              |             |
|    fps             | 131         |
|    iterations      | 1           |
|    time_elapsed    | 15          |
|    total_timesteps | 2048        |
------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.36515853] |
| time/              |               |
|    fps             | 136           |
|    iterations      | 1             |
|    time_elapsed    | 14            |
|    total_timesteps | 2048          |
--------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.5954924] |
| time/              |              |
|    fps             | 122          |
|    iterations      | 1            |
|    time_elapsed    | 16           |
|    total_timesteps | 2048         |
-------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.5712862] |
| time/              |              |
|    fps             | 116          |
|    iterations      | 1            |
|    time_elapsed    | 17           |
|    total_timesteps | 2048         |
-------------------------------------
--------------------------------------------
| reward                   | [-0.37597868] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 2             |
|    time_elapsed          | 31            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.0053587183  |
|    clip_fraction         | 0.0453        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.119         |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.0104        |
|    lagrangian_multiplier | 0.0518        |
|    learning_rate         | 0.0003        |
|    loss                  | 73.6          |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00634      |
|    std                   | 1             |
|    value_loss            | 575           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.53639364] |
| time/                    |               |
|    fps                   | 132           |
|    iterations            | 2             |
|    time_elapsed          | 30            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.0062705795  |
|    clip_fraction         | 0.0421        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0579        |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.0433        |
|    lagrangian_multiplier | 0.0681        |
|    learning_rate         | 0.0003        |
|    loss                  | 63            |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00425      |
|    std                   | 0.997         |
|    value_loss            | 680           |
--------------------------------------------
------------------------------------------
| reward                   | [-1.037407] |
| time/                    |             |
|    fps                   | 131         |
|    iterations            | 3           |
|    time_elapsed          | 46          |
|    total_timesteps       | 6144        |
| train/                   |             |
|    approx_kl             | 0.005203825 |
|    clip_fraction         | 0.0385      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0615      |
|    entropy_loss          | -2.83       |
|    explained_variance    | 0.0246      |
|    lagrangian_multiplier | 0.0655      |
|    learning_rate         | 0.0003      |
|    loss                  | 33.7        |
|    n_updates             | 20          |
|    policy_gradient_loss  | -0.00428    |
|    std                   | 0.997       |
|    value_loss            | 344         |
------------------------------------------
-------------------------------------------
| reward                   | [-0.971536]  |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 3            |
|    time_elapsed          | 48           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0058627822 |
|    clip_fraction         | 0.0367       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.116        |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0136      |
|    lagrangian_multiplier | 0.0646       |
|    learning_rate         | 0.0003       |
|    loss                  | 26.3         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00451     |
|    std                   | 0.986        |
|    value_loss            | 248          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.91186655] |
| time/                    |               |
|    fps                   | 130           |
|    iterations            | 4             |
|    time_elapsed          | 62            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.005745101   |
|    clip_fraction         | 0.0463        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0146        |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.00971      |
|    lagrangian_multiplier | 0.0674        |
|    learning_rate         | 0.0003        |
|    loss                  | 54.1          |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00569      |
|    std                   | 0.994         |
|    value_loss            | 551           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.9701247] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 4            |
|    time_elapsed          | 64           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.0042986423 |
|    clip_fraction         | 0.0315       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.162        |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.00418     |
|    lagrangian_multiplier | 0.0438       |
|    learning_rate         | 0.0003       |
|    loss                  | 113          |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00526     |
|    std                   | 0.998        |
|    value_loss            | 729          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.99743074] |
| time/                    |               |
|    fps                   | 130           |
|    iterations            | 5             |
|    time_elapsed          | 78            |
|    total_timesteps       | 10240         |
| train/                   |               |
|    approx_kl             | 0.0051396093  |
|    clip_fraction         | 0.045         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0954        |
|    entropy_loss          | -2.83         |
|    explained_variance    | 0.0605        |
|    lagrangian_multiplier | 0.0672        |
|    learning_rate         | 0.0003        |
|    loss                  | 49.6          |
|    n_updates             | 40            |
|    policy_gradient_loss  | -0.00548      |
|    std                   | 1             |
|    value_loss            | 491           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.9962328] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 5            |
|    time_elapsed          | 81           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.007842017  |
|    clip_fraction         | 0.081        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.125        |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0113      |
|    lagrangian_multiplier | 0.0568       |
|    learning_rate         | 0.0003       |
|    loss                  | 54.9         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00943     |
|    std                   | 0.98         |
|    value_loss            | 432          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8861323] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 6            |
|    time_elapsed          | 94           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0050502494 |
|    clip_fraction         | 0.0486       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.11         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0526      |
|    lagrangian_multiplier | 0.0501       |
|    learning_rate         | 0.0003       |
|    loss                  | 85.7         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.0061      |
|    std                   | 1            |
|    value_loss            | 677          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5754466] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 6            |
|    time_elapsed          | 97           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.005730882  |
|    clip_fraction         | 0.0398       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.186        |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.000735    |
|    lagrangian_multiplier | 0.0479       |
|    learning_rate         | 0.0003       |
|    loss                  | 60.2         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00675     |
|    std                   | 0.973        |
|    value_loss            | 571          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.99577063] |
| time/                    |               |
|    fps                   | 40            |
|    iterations            | 2             |
|    time_elapsed          | 99            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.0044461917  |
|    clip_fraction         | 0.0304        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 139           |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.0596       |
|    lagrangian_multiplier | 0.0683        |
|    learning_rate         | 0.0003        |
|    loss                  | 57.1          |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00561      |
|    std                   | 1.01          |
|    value_loss            | 433           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.6251594] |
| time/                    |              |
|    fps                   | 37           |
|    iterations            | 2            |
|    time_elapsed          | 110          |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0054540234 |
|    clip_fraction         | 0.0463       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00914      |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.00478      |
|    lagrangian_multiplier | 0.0792       |
|    learning_rate         | 0.0003       |
|    loss                  | 63.4         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00696     |
|    std                   | 1.02         |
|    value_loss            | 787          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6167874] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 7            |
|    time_elapsed          | 110          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.005419592  |
|    clip_fraction         | 0.0483       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.138        |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0184      |
|    lagrangian_multiplier | 0.0652       |
|    learning_rate         | 0.0003       |
|    loss                  | 87.6         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00751     |
|    std                   | 1.01         |
|    value_loss            | 918          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.849565]  |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 7            |
|    time_elapsed          | 113          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.0064426064 |
|    clip_fraction         | 0.0524       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.204        |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0276      |
|    lagrangian_multiplier | 0.0515       |
|    learning_rate         | 0.0003       |
|    loss                  | 78           |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00779     |
|    std                   | 0.97         |
|    value_loss            | 679          |
-------------------------------------------
srun: Job 114474 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation temporarily disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.9890556] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 8            |
|    time_elapsed          | 127          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0055298395 |
|    clip_fraction         | 0.0434       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.127        |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.0444      |
|    lagrangian_multiplier | 0.0514       |
|    learning_rate         | 0.0003       |
|    loss                  | 57.4         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00665     |
|    std                   | 1.02         |
|    value_loss            | 487          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4573379] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 8            |
|    time_elapsed          | 130          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0050611594 |
|    clip_fraction         | 0.0371       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.184        |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0156      |
|    lagrangian_multiplier | 0.0508       |
|    learning_rate         | 0.0003       |
|    loss                  | 101          |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00592     |
|    std                   | 0.96         |
|    value_loss            | 922          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.39240882] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 9             |
|    time_elapsed          | 143           |
|    total_timesteps       | 18432         |
| train/                   |               |
|    approx_kl             | 0.0057675894  |
|    clip_fraction         | 0.0512        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0795        |
|    entropy_loss          | -2.89         |
|    explained_variance    | -0.0411       |
|    lagrangian_multiplier | 0.0619        |
|    learning_rate         | 0.0003        |
|    loss                  | 79.6          |
|    n_updates             | 80            |
|    policy_gradient_loss  | -0.00645      |
|    std                   | 1.03          |
|    value_loss            | 748           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.0859361] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 9            |
|    time_elapsed          | 146          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.005502262  |
|    clip_fraction         | 0.047        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.122        |
|    entropy_loss          | -2.75        |
|    explained_variance    | 0.00899      |
|    lagrangian_multiplier | 0.0587       |
|    learning_rate         | 0.0003       |
|    loss                  | 40.5         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00667     |
|    std                   | 0.957        |
|    value_loss            | 399          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2285503] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 10           |
|    time_elapsed          | 159          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0051828437 |
|    clip_fraction         | 0.0337       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.101        |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.0489      |
|    lagrangian_multiplier | 0.0638       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.3         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00533     |
|    std                   | 1.02         |
|    value_loss            | 693          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4920181] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 10           |
|    time_elapsed          | 162          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.006609821  |
|    clip_fraction         | 0.062        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.198        |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.0237       |
|    lagrangian_multiplier | 0.0489       |
|    learning_rate         | 0.0003       |
|    loss                  | 68.7         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00916     |
|    std                   | 0.967        |
|    value_loss            | 655          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.47573218] |
| time/                    |               |
|    fps                   | 35            |
|    iterations            | 3             |
|    time_elapsed          | 175           |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.00489547    |
|    clip_fraction         | 0.0371        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 132           |
|    entropy_loss          | -2.86         |
|    explained_variance    | -0.094        |
|    lagrangian_multiplier | 0.0891        |
|    learning_rate         | 0.0003        |
|    loss                  | 45.5          |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00513      |
|    std                   | 1.01          |
|    value_loss            | 458           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.7200307] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 11           |
|    time_elapsed          | 175          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.005694922  |
|    clip_fraction         | 0.0389       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0176       |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.0523      |
|    lagrangian_multiplier | 0.0803       |
|    learning_rate         | 0.0003       |
|    loss                  | 29.6         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.0042      |
|    std                   | 1.02         |
|    value_loss            | 413          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5242491] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 11           |
|    time_elapsed          | 179          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.005489545  |
|    clip_fraction         | 0.0619       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.181        |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0104      |
|    lagrangian_multiplier | 0.05         |
|    learning_rate         | 0.0003       |
|    loss                  | 64.5         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.0103      |
|    std                   | 0.979        |
|    value_loss            | 581          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6119821] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 12           |
|    time_elapsed          | 191          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0051093    |
|    clip_fraction         | 0.0405       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0674       |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.00111     |
|    lagrangian_multiplier | 0.0629       |
|    learning_rate         | 0.0003       |
|    loss                  | 26.3         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00656     |
|    std                   | 1.01         |
|    value_loss            | 265          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3785161] |
| time/                    |              |
|    fps                   | 42           |
|    iterations            | 4            |
|    time_elapsed          | 193          |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.005764339  |
|    clip_fraction         | 0.0456       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 192          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0417      |
|    lagrangian_multiplier | 0.0679       |
|    learning_rate         | 0.0003       |
|    loss                  | 54.2         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.0061      |
|    std                   | 1.01         |
|    value_loss            | 466          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5497737] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 12           |
|    time_elapsed          | 195          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.006754648  |
|    clip_fraction         | 0.0519       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.19         |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.00427     |
|    lagrangian_multiplier | 0.0601       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.3         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00738     |
|    std                   | 0.974        |
|    value_loss            | 395          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.44219837] |
| time/                    |               |
|    fps                   | 31            |
|    iterations            | 3             |
|    time_elapsed          | 196           |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.004853002   |
|    clip_fraction         | 0.0382        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0147        |
|    entropy_loss          | -2.89         |
|    explained_variance    | 0.0394        |
|    lagrangian_multiplier | 0.0781        |
|    learning_rate         | 0.0003        |
|    loss                  | 20.3          |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00531      |
|    std                   | 1.03          |
|    value_loss            | 283           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2461772] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 13           |
|    time_elapsed          | 206          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0057163583 |
|    clip_fraction         | 0.0528       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0732       |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0109      |
|    lagrangian_multiplier | 0.0639       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.5         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00649     |
|    std                   | 1.01         |
|    value_loss            | 417          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2380791] |
| time/                    |              |
|    fps                   | 48           |
|    iterations            | 5            |
|    time_elapsed          | 210          |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.005061229  |
|    clip_fraction         | 0.0439       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 82.5         |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.0523      |
|    lagrangian_multiplier | 0.0826       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.2         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00572     |
|    std                   | 1.02         |
|    value_loss            | 681          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.5472753] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 13           |
|    time_elapsed          | 212          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0053286646 |
|    clip_fraction         | 0.0542       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.185        |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.00629      |
|    lagrangian_multiplier | 0.0577       |
|    learning_rate         | 0.0003       |
|    loss                  | 68.4         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00938     |
|    std                   | 0.972        |
|    value_loss            | 620          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.1571136] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 14           |
|    time_elapsed          | 223          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.005711415  |
|    clip_fraction         | 0.046        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0887       |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.000737    |
|    lagrangian_multiplier | 0.0711       |
|    learning_rate         | 0.0003       |
|    loss                  | 59.4         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.0058      |
|    std                   | 0.998        |
|    value_loss            | 537          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2184763] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 14           |
|    time_elapsed          | 228          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.008949315  |
|    clip_fraction         | 0.0915       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.197        |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0139       |
|    lagrangian_multiplier | 0.0561       |
|    learning_rate         | 0.0003       |
|    loss                  | 78.1         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.0133      |
|    std                   | 0.978        |
|    value_loss            | 781          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7815946] |
| time/                    |              |
|    fps                   | 52           |
|    iterations            | 6            |
|    time_elapsed          | 232          |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0063432986 |
|    clip_fraction         | 0.0469       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 121          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.019       |
|    lagrangian_multiplier | 0.0794       |
|    learning_rate         | 0.0003       |
|    loss                  | 116          |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00647     |
|    std                   | 1.01         |
|    value_loss            | 1.25e+03     |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.6969128] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 15           |
|    time_elapsed          | 239          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.005829758  |
|    clip_fraction         | 0.0492       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.095        |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.00117     |
|    lagrangian_multiplier | 0.07         |
|    learning_rate         | 0.0003       |
|    loss                  | 40.1         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00786     |
|    std                   | 0.994        |
|    value_loss            | 417          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.1514826] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 15           |
|    time_elapsed          | 244          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.005417646  |
|    clip_fraction         | 0.0469       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.138        |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.013        |
|    lagrangian_multiplier | 0.0599       |
|    learning_rate         | 0.0003       |
|    loss                  | 71.9         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00587     |
|    std                   | 0.978        |
|    value_loss            | 672          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.7884972] |
| time/                    |              |
|    fps                   | 57           |
|    iterations            | 7            |
|    time_elapsed          | 250          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.0054961164 |
|    clip_fraction         | 0.037        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 162          |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0148      |
|    lagrangian_multiplier | 0.0855       |
|    learning_rate         | 0.0003       |
|    loss                  | 122          |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00493     |
|    std                   | 1.01         |
|    value_loss            | 1.37e+03     |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.5815538] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 16           |
|    time_elapsed          | 255          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0064531853 |
|    clip_fraction         | 0.0648       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0882       |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0594      |
|    lagrangian_multiplier | 0.0619       |
|    learning_rate         | 0.0003       |
|    loss                  | 42.9         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.0109      |
|    std                   | 1            |
|    value_loss            | 417          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9502877] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 16           |
|    time_elapsed          | 261          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.006288319  |
|    clip_fraction         | 0.0567       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.153        |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0115      |
|    lagrangian_multiplier | 0.0612       |
|    learning_rate         | 0.0003       |
|    loss                  | 96.3         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00786     |
|    std                   | 0.976        |
|    value_loss            | 848          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7090307] |
| time/                    |              |
|    fps                   | 61           |
|    iterations            | 8            |
|    time_elapsed          | 268          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0068074693 |
|    clip_fraction         | 0.0698       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 118          |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.00202      |
|    lagrangian_multiplier | 0.0922       |
|    learning_rate         | 0.0003       |
|    loss                  | 72.5         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.0114      |
|    std                   | 1            |
|    value_loss            | 850          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1466782] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 17           |
|    time_elapsed          | 270          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.005121683  |
|    clip_fraction         | 0.043        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0685       |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0368      |
|    lagrangian_multiplier | 0.061        |
|    learning_rate         | 0.0003       |
|    loss                  | 39.4         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00801     |
|    std                   | 1            |
|    value_loss            | 437          |
-------------------------------------------
------------------------------------------
| reward                   | [-2.11225]  |
| time/                    |             |
|    fps                   | 125         |
|    iterations            | 17          |
|    time_elapsed          | 278         |
|    total_timesteps       | 34816       |
| train/                   |             |
|    approx_kl             | 0.004383938 |
|    clip_fraction         | 0.0491      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.209       |
|    entropy_loss          | -2.79       |
|    explained_variance    | -0.0118     |
|    lagrangian_multiplier | 0.0561      |
|    learning_rate         | 0.0003      |
|    loss                  | 65.5        |
|    n_updates             | 160         |
|    policy_gradient_loss  | -0.00776    |
|    std                   | 0.974       |
|    value_loss            | 598         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.0696557] |
| time/                    |              |
|    fps                   | 28           |
|    iterations            | 4            |
|    time_elapsed          | 282          |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.005082546  |
|    clip_fraction         | 0.0389       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0179       |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.0147       |
|    lagrangian_multiplier | 0.0583       |
|    learning_rate         | 0.0003       |
|    loss                  | 45.7         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.0042      |
|    std                   | 1.04         |
|    value_loss            | 402          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7816967] |
| time/                    |              |
|    fps                   | 64           |
|    iterations            | 9            |
|    time_elapsed          | 286          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.00542537   |
|    clip_fraction         | 0.0462       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 102          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.000985    |
|    lagrangian_multiplier | 0.0857       |
|    learning_rate         | 0.0003       |
|    loss                  | 67.3         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00553     |
|    std                   | 1            |
|    value_loss            | 648          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6977293] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 18           |
|    time_elapsed          | 287          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.006638064  |
|    clip_fraction         | 0.0608       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0788       |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0241      |
|    lagrangian_multiplier | 0.064        |
|    learning_rate         | 0.0003       |
|    loss                  | 64.4         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.0084      |
|    std                   | 1.01         |
|    value_loss            | 583          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6109954] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 18           |
|    time_elapsed          | 294          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.006418689  |
|    clip_fraction         | 0.0661       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.191        |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0266      |
|    lagrangian_multiplier | 0.0523       |
|    learning_rate         | 0.0003       |
|    loss                  | 57.7         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00896     |
|    std                   | 0.966        |
|    value_loss            | 496          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.891816]  |
| time/                    |              |
|    fps                   | 67           |
|    iterations            | 10           |
|    time_elapsed          | 303          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0072425324 |
|    clip_fraction         | 0.0737       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 103          |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.00492      |
|    lagrangian_multiplier | 0.0777       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.8         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.0098      |
|    std                   | 1.01         |
|    value_loss            | 594          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2862066] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 19           |
|    time_elapsed          | 303          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.004532098  |
|    clip_fraction         | 0.0424       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0451       |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0125      |
|    lagrangian_multiplier | 0.068        |
|    learning_rate         | 0.0003       |
|    loss                  | 33.7         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00529     |
|    std                   | 1.01         |
|    value_loss            | 359          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5204946] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 19           |
|    time_elapsed          | 311          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0058497684 |
|    clip_fraction         | 0.0563       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.167        |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.00213      |
|    lagrangian_multiplier | 0.0544       |
|    learning_rate         | 0.0003       |
|    loss                  | 50.4         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00985     |
|    std                   | 0.967        |
|    value_loss            | 492          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4010869] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 20           |
|    time_elapsed          | 319          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0052571306 |
|    clip_fraction         | 0.0488       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0548       |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0551      |
|    lagrangian_multiplier | 0.0675       |
|    learning_rate         | 0.0003       |
|    loss                  | 44.9         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00717     |
|    std                   | 1.02         |
|    value_loss            | 513          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3364756] |
| time/                    |              |
|    fps                   | 70           |
|    iterations            | 11           |
|    time_elapsed          | 321          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0038599977 |
|    clip_fraction         | 0.0182       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 120          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0348      |
|    lagrangian_multiplier | 0.0703       |
|    learning_rate         | 0.0003       |
|    loss                  | 53           |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00355     |
|    std                   | 0.999        |
|    value_loss            | 476          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1623439] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 20           |
|    time_elapsed          | 327          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0053264005 |
|    clip_fraction         | 0.0517       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.184        |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0405       |
|    lagrangian_multiplier | 0.0562       |
|    learning_rate         | 0.0003       |
|    loss                  | 45.3         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00672     |
|    std                   | 0.965        |
|    value_loss            | 440          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.4619188] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 21           |
|    time_elapsed          | 335          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.008351335  |
|    clip_fraction         | 0.0709       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.139        |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0116      |
|    lagrangian_multiplier | 0.064        |
|    learning_rate         | 0.0003       |
|    loss                  | 58.2         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 1.01         |
|    value_loss            | 560          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6446075] |
| time/                    |              |
|    fps                   | 72           |
|    iterations            | 12           |
|    time_elapsed          | 339          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.005821673  |
|    clip_fraction         | 0.0448       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.68         |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0619      |
|    lagrangian_multiplier | 0.0945       |
|    learning_rate         | 0.0003       |
|    loss                  | 22.2         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00641     |
|    std                   | 0.998        |
|    value_loss            | 294          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4398712] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 21           |
|    time_elapsed          | 344          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0055443007 |
|    clip_fraction         | 0.0413       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.123        |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0604      |
|    lagrangian_multiplier | 0.0574       |
|    learning_rate         | 0.0003       |
|    loss                  | 40           |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.0084      |
|    std                   | 0.968        |
|    value_loss            | 356          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.4647794] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 22           |
|    time_elapsed          | 351          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.0050661825 |
|    clip_fraction         | 0.0468       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.095        |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.012       |
|    lagrangian_multiplier | 0.0634       |
|    learning_rate         | 0.0003       |
|    loss                  | 66.6         |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00698     |
|    std                   | 1.01         |
|    value_loss            | 694          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.24603075] |
| time/                    |               |
|    fps                   | 74            |
|    iterations            | 13            |
|    time_elapsed          | 357           |
|    total_timesteps       | 26624         |
| train/                   |               |
|    approx_kl             | 0.0048785736  |
|    clip_fraction         | 0.0471        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0311        |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.0507       |
|    lagrangian_multiplier | 0.107         |
|    learning_rate         | 0.0003        |
|    loss                  | 20            |
|    n_updates             | 120           |
|    policy_gradient_loss  | -0.0043       |
|    std                   | 0.993         |
|    value_loss            | 326           |
--------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.3217564] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 22           |
|    time_elapsed          | 360          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.0060769496 |
|    clip_fraction         | 0.076        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.163        |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0364      |
|    lagrangian_multiplier | 0.0553       |
|    learning_rate         | 0.0003       |
|    loss                  | 39.5         |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.0106      |
|    std                   | 0.967        |
|    value_loss            | 354          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.85420275] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 23            |
|    time_elapsed          | 367           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.007832121   |
|    clip_fraction         | 0.0691        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.111         |
|    entropy_loss          | -2.86         |
|    explained_variance    | 0.0132        |
|    lagrangian_multiplier | 0.0636        |
|    learning_rate         | 0.0003        |
|    loss                  | 43.5          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.0101       |
|    std                   | 1.02          |
|    value_loss            | 449           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.6155684] |
| time/                    |              |
|    fps                   | 27           |
|    iterations            | 5            |
|    time_elapsed          | 369          |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.006136543  |
|    clip_fraction         | 0.0493       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0278       |
|    entropy_loss          | -2.91        |
|    explained_variance    | -0.0121      |
|    lagrangian_multiplier | 0.0727       |
|    learning_rate         | 0.0003       |
|    loss                  | 39.3         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00788     |
|    std                   | 1.03         |
|    value_loss            | 408          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.96325016] |
| time/                    |               |
|    fps                   | 76            |
|    iterations            | 14            |
|    time_elapsed          | 374           |
|    total_timesteps       | 28672         |
| train/                   |               |
|    approx_kl             | 0.0048072226  |
|    clip_fraction         | 0.0404        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 3.97          |
|    entropy_loss          | -2.81         |
|    explained_variance    | -0.0352       |
|    lagrangian_multiplier | 0.0915        |
|    learning_rate         | 0.0003        |
|    loss                  | 20.1          |
|    n_updates             | 130           |
|    policy_gradient_loss  | -0.00585      |
|    std                   | 0.982         |
|    value_loss            | 278           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.8981864] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 23           |
|    time_elapsed          | 377          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.005693632  |
|    clip_fraction         | 0.0436       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.134        |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0056      |
|    lagrangian_multiplier | 0.0615       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.6         |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.00754     |
|    std                   | 0.966        |
|    value_loss            | 556          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.9937125] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 24           |
|    time_elapsed          | 383          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.0061853183 |
|    clip_fraction         | 0.0646       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.101        |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.0516      |
|    lagrangian_multiplier | 0.0624       |
|    learning_rate         | 0.0003       |
|    loss                  | 87.8         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.0112      |
|    std                   | 1.02         |
|    value_loss            | 737          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.54101384] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 24            |
|    time_elapsed          | 393           |
|    total_timesteps       | 49152         |
| train/                   |               |
|    approx_kl             | 0.0056387503  |
|    clip_fraction         | 0.0487        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.132         |
|    entropy_loss          | -2.77         |
|    explained_variance    | -0.0138       |
|    lagrangian_multiplier | 0.0541        |
|    learning_rate         | 0.0003        |
|    loss                  | 90.9          |
|    n_updates             | 230           |
|    policy_gradient_loss  | -0.0074       |
|    std                   | 0.971         |
|    value_loss            | 750           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.8619738] |
| time/                    |              |
|    fps                   | 77           |
|    iterations            | 15           |
|    time_elapsed          | 394          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0052425642 |
|    clip_fraction         | 0.0341       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0162       |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0766      |
|    lagrangian_multiplier | 0.0921       |
|    learning_rate         | 0.0003       |
|    loss                  | 16.2         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00277     |
|    std                   | 0.975        |
|    value_loss            | 210          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8222843] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 25           |
|    time_elapsed          | 399          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.0059903986 |
|    clip_fraction         | 0.0513       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0815       |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.0189      |
|    lagrangian_multiplier | 0.0719       |
|    learning_rate         | 0.0003       |
|    loss                  | 31.4         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00854     |
|    std                   | 1.02         |
|    value_loss            | 360          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0254155] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 25           |
|    time_elapsed          | 410          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.005640608  |
|    clip_fraction         | 0.0488       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.164        |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.005       |
|    lagrangian_multiplier | 0.0583       |
|    learning_rate         | 0.0003       |
|    loss                  | 87.4         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.0082      |
|    std                   | 0.96         |
|    value_loss            | 816          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.64447224] |
| time/                    |               |
|    fps                   | 79            |
|    iterations            | 16            |
|    time_elapsed          | 412           |
|    total_timesteps       | 32768         |
| train/                   |               |
|    approx_kl             | 0.007849103   |
|    clip_fraction         | 0.0645        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0132        |
|    entropy_loss          | -2.79         |
|    explained_variance    | -0.0328       |
|    lagrangian_multiplier | 0.0901        |
|    learning_rate         | 0.0003        |
|    loss                  | 10.5          |
|    n_updates             | 150           |
|    policy_gradient_loss  | -0.0078       |
|    std                   | 0.979         |
|    value_loss            | 126           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.1635259] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 26           |
|    time_elapsed          | 415          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.005343821  |
|    clip_fraction         | 0.0568       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.105        |
|    entropy_loss          | -2.9         |
|    explained_variance    | -0.00559     |
|    lagrangian_multiplier | 0.064        |
|    learning_rate         | 0.0003       |
|    loss                  | 41.1         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00528     |
|    std                   | 1.03         |
|    value_loss            | 435          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.90947753] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 26            |
|    time_elapsed          | 426           |
|    total_timesteps       | 53248         |
| train/                   |               |
|    approx_kl             | 0.008289067   |
|    clip_fraction         | 0.0782        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.207         |
|    entropy_loss          | -2.75         |
|    explained_variance    | -0.00599      |
|    lagrangian_multiplier | 0.0545        |
|    learning_rate         | 0.0003        |
|    loss                  | 73.7          |
|    n_updates             | 250           |
|    policy_gradient_loss  | -0.0103       |
|    std                   | 0.953         |
|    value_loss            | 666           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.95380425] |
| time/                    |               |
|    fps                   | 80            |
|    iterations            | 17            |
|    time_elapsed          | 430           |
|    total_timesteps       | 34816         |
| train/                   |               |
|    approx_kl             | 0.0043831477  |
|    clip_fraction         | 0.0271        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.015         |
|    entropy_loss          | -2.77         |
|    explained_variance    | 0.00639       |
|    lagrangian_multiplier | 0.0933        |
|    learning_rate         | 0.0003        |
|    loss                  | 13            |
|    n_updates             | 160           |
|    policy_gradient_loss  | -0.00256      |
|    std                   | 0.961         |
|    value_loss            | 161           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3094113] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 27           |
|    time_elapsed          | 431          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.0056580068 |
|    clip_fraction         | 0.0648       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.133        |
|    entropy_loss          | -2.9         |
|    explained_variance    | -0.0187      |
|    lagrangian_multiplier | 0.0626       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.8         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00947     |
|    std                   | 1.02         |
|    value_loss            | 615          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4593592] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 27           |
|    time_elapsed          | 442          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.006525389  |
|    clip_fraction         | 0.0673       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.185        |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.00553      |
|    lagrangian_multiplier | 0.0586       |
|    learning_rate         | 0.0003       |
|    loss                  | 102          |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00873     |
|    std                   | 0.95         |
|    value_loss            | 1.04e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6726366] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 28           |
|    time_elapsed          | 447          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.006029824  |
|    clip_fraction         | 0.0468       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.109        |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0151       |
|    lagrangian_multiplier | 0.0611       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.7         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00735     |
|    std                   | 1.03         |
|    value_loss            | 446          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.46789837] |
| time/                    |               |
|    fps                   | 82            |
|    iterations            | 18            |
|    time_elapsed          | 447           |
|    total_timesteps       | 36864         |
| train/                   |               |
|    approx_kl             | 0.00449114    |
|    clip_fraction         | 0.0323        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0126        |
|    entropy_loss          | -2.74         |
|    explained_variance    | 0.0153        |
|    lagrangian_multiplier | 0.106         |
|    learning_rate         | 0.0003        |
|    loss                  | 13.4          |
|    n_updates             | 170           |
|    policy_gradient_loss  | -0.00351      |
|    std                   | 0.947         |
|    value_loss            | 192           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.58853805] |
| time/                    |               |
|    fps                   | 26            |
|    iterations            | 6             |
|    time_elapsed          | 455           |
|    total_timesteps       | 12288         |
| train/                   |               |
|    approx_kl             | 0.005728597   |
|    clip_fraction         | 0.0503        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 1.53          |
|    entropy_loss          | -2.93         |
|    explained_variance    | 0.0189        |
|    lagrangian_multiplier | 0.0493        |
|    learning_rate         | 0.0003        |
|    loss                  | 34            |
|    n_updates             | 50            |
|    policy_gradient_loss  | -0.00604      |
|    std                   | 1.05          |
|    value_loss            | 336           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.9406301] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 28           |
|    time_elapsed          | 459          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.0062612277 |
|    clip_fraction         | 0.0781       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.173        |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.00433      |
|    lagrangian_multiplier | 0.0639       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.7         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.0112      |
|    std                   | 0.95         |
|    value_loss            | 684          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.98903835] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 29            |
|    time_elapsed          | 463           |
|    total_timesteps       | 59392         |
| train/                   |               |
|    approx_kl             | 0.0058365725  |
|    clip_fraction         | 0.0474        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0841        |
|    entropy_loss          | -2.9          |
|    explained_variance    | -0.0211       |
|    lagrangian_multiplier | 0.0637        |
|    learning_rate         | 0.0003        |
|    loss                  | 32.7          |
|    n_updates             | 280           |
|    policy_gradient_loss  | -0.00678      |
|    std                   | 1.03          |
|    value_loss            | 324           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.2676241] |
| time/                    |              |
|    fps                   | 83           |
|    iterations            | 19           |
|    time_elapsed          | 465          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0038591186 |
|    clip_fraction         | 0.0382       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0113       |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.07        |
|    lagrangian_multiplier | 0.0889       |
|    learning_rate         | 0.0003       |
|    loss                  | 10           |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.002       |
|    std                   | 0.946        |
|    value_loss            | 132          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.375579] |
| time/                    |             |
|    fps                   | 124         |
|    iterations            | 29          |
|    time_elapsed          | 475         |
|    total_timesteps       | 59392       |
| train/                   |             |
|    approx_kl             | 0.006039857 |
|    clip_fraction         | 0.0458      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.16        |
|    entropy_loss          | -2.73       |
|    explained_variance    | 0.0181      |
|    lagrangian_multiplier | 0.0636      |
|    learning_rate         | 0.0003      |
|    loss                  | 79.7        |
|    n_updates             | 280         |
|    policy_gradient_loss  | -0.00625    |
|    std                   | 0.948       |
|    value_loss            | 746         |
------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.9419497] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 30           |
|    time_elapsed          | 479          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0053962427 |
|    clip_fraction         | 0.0396       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.12         |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.000554    |
|    lagrangian_multiplier | 0.062        |
|    learning_rate         | 0.0003       |
|    loss                  | 41.7         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.007       |
|    std                   | 1.02         |
|    value_loss            | 403          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.6549054] |
| time/                    |              |
|    fps                   | 84           |
|    iterations            | 20           |
|    time_elapsed          | 483          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0078225285 |
|    clip_fraction         | 0.0521       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00229      |
|    entropy_loss          | -2.71        |
|    explained_variance    | -0.0795      |
|    lagrangian_multiplier | 0.0963       |
|    learning_rate         | 0.0003       |
|    loss                  | 3.2          |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00581     |
|    std                   | 0.928        |
|    value_loss            | 45           |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.3896087] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 30           |
|    time_elapsed          | 492          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0069168285 |
|    clip_fraction         | 0.0744       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.165        |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0219      |
|    lagrangian_multiplier | 0.0623       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.1         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.0115      |
|    std                   | 0.967        |
|    value_loss            | 542          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.6141445] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 31           |
|    time_elapsed          | 495          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0061946847 |
|    clip_fraction         | 0.0517       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.12         |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0486      |
|    lagrangian_multiplier | 0.0576       |
|    learning_rate         | 0.0003       |
|    loss                  | 68.1         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00776     |
|    std                   | 1            |
|    value_loss            | 600          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8145886] |
| time/                    |              |
|    fps                   | 85           |
|    iterations            | 21           |
|    time_elapsed          | 501          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0059835236 |
|    clip_fraction         | 0.0445       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00671      |
|    entropy_loss          | -2.69        |
|    explained_variance    | -0.0678      |
|    lagrangian_multiplier | 0.0983       |
|    learning_rate         | 0.0003       |
|    loss                  | 4.65         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.0052      |
|    std                   | 0.926        |
|    value_loss            | 68.2         |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1534034] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 31           |
|    time_elapsed          | 508          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.006783762  |
|    clip_fraction         | 0.0622       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0959       |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0241       |
|    lagrangian_multiplier | 0.0711       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.7         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00664     |
|    std                   | 0.971        |
|    value_loss            | 537          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.096544]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 32           |
|    time_elapsed          | 511          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.0058002677 |
|    clip_fraction         | 0.0491       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.126        |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.00145      |
|    lagrangian_multiplier | 0.0601       |
|    learning_rate         | 0.0003       |
|    loss                  | 50.9         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.0061      |
|    std                   | 0.999        |
|    value_loss            | 477          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8900061] |
| time/                    |              |
|    fps                   | 86           |
|    iterations            | 22           |
|    time_elapsed          | 519          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.002371313  |
|    clip_fraction         | 0.0234       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0184       |
|    entropy_loss          | -2.68        |
|    explained_variance    | -0.103       |
|    lagrangian_multiplier | 0.117        |
|    learning_rate         | 0.0003       |
|    loss                  | 14.4         |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00134     |
|    std                   | 0.918        |
|    value_loss            | 228          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.91266537] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 32            |
|    time_elapsed          | 525           |
|    total_timesteps       | 65536         |
| train/                   |               |
|    approx_kl             | 0.0065651014  |
|    clip_fraction         | 0.0561        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.155         |
|    entropy_loss          | -2.76         |
|    explained_variance    | 0.00204       |
|    lagrangian_multiplier | 0.0592        |
|    learning_rate         | 0.0003        |
|    loss                  | 64.8          |
|    n_updates             | 310           |
|    policy_gradient_loss  | -0.00964      |
|    std                   | 0.954         |
|    value_loss            | 537           |
--------------------------------------------
------------------------------------------
| reward                   | [-1.02452]  |
| time/                    |             |
|    fps                   | 128         |
|    iterations            | 33          |
|    time_elapsed          | 527         |
|    total_timesteps       | 67584       |
| train/                   |             |
|    approx_kl             | 0.007397099 |
|    clip_fraction         | 0.0687      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0903      |
|    entropy_loss          | -2.84       |
|    explained_variance    | 0.0263      |
|    lagrangian_multiplier | 0.0624      |
|    learning_rate         | 0.0003      |
|    loss                  | 36          |
|    n_updates             | 320         |
|    policy_gradient_loss  | -0.00991    |
|    std                   | 1           |
|    value_loss            | 389         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.50806946] |
| time/                    |               |
|    fps                   | 87            |
|    iterations            | 23            |
|    time_elapsed          | 537           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.0037599597  |
|    clip_fraction         | 0.0312        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0108        |
|    entropy_loss          | -2.67         |
|    explained_variance    | -0.0663       |
|    lagrangian_multiplier | 0.0917        |
|    learning_rate         | 0.0003        |
|    loss                  | 12.5          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00139      |
|    std                   | 0.92          |
|    value_loss            | 163           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.5508041] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 33           |
|    time_elapsed          | 541          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0063515585 |
|    clip_fraction         | 0.048        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0893       |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.0104      |
|    lagrangian_multiplier | 0.0623       |
|    learning_rate         | 0.0003       |
|    loss                  | 49           |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00872     |
|    std                   | 0.941        |
|    value_loss            | 473          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.51935774] |
| time/                    |               |
|    fps                   | 26            |
|    iterations            | 7             |
|    time_elapsed          | 541           |
|    total_timesteps       | 14336         |
| train/                   |               |
|    approx_kl             | 0.005940796   |
|    clip_fraction         | 0.0497        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0152        |
|    entropy_loss          | -2.96         |
|    explained_variance    | 0.00371       |
|    lagrangian_multiplier | 0.0656        |
|    learning_rate         | 0.0003        |
|    loss                  | 29.6          |
|    n_updates             | 60            |
|    policy_gradient_loss  | -0.00712      |
|    std                   | 1.07          |
|    value_loss            | 284           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.07128]   |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 34           |
|    time_elapsed          | 543          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0058226055 |
|    clip_fraction         | 0.0458       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0988       |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0506       |
|    lagrangian_multiplier | 0.0656       |
|    learning_rate         | 0.0003       |
|    loss                  | 53           |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00908     |
|    std                   | 1.01         |
|    value_loss            | 500          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5025219] |
| time/                    |              |
|    fps                   | 88           |
|    iterations            | 24           |
|    time_elapsed          | 554          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.0053104395 |
|    clip_fraction         | 0.0386       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0145       |
|    entropy_loss          | -2.68        |
|    explained_variance    | -0.00384     |
|    lagrangian_multiplier | 0.0911       |
|    learning_rate         | 0.0003       |
|    loss                  | 8.8          |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00238     |
|    std                   | 0.927        |
|    value_loss            | 110          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0016111] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 34           |
|    time_elapsed          | 558          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0066226795 |
|    clip_fraction         | 0.0709       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.187        |
|    entropy_loss          | -2.71        |
|    explained_variance    | 0.0015       |
|    lagrangian_multiplier | 0.0496       |
|    learning_rate         | 0.0003       |
|    loss                  | 53.5         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00816     |
|    std                   | 0.937        |
|    value_loss            | 430          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.613491]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 35           |
|    time_elapsed          | 559          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0077361017 |
|    clip_fraction         | 0.0726       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.101        |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.023        |
|    lagrangian_multiplier | 0.0575       |
|    learning_rate         | 0.0003       |
|    loss                  | 55.6         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00907     |
|    std                   | 1.02         |
|    value_loss            | 524          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.46751314] |
| time/                    |               |
|    fps                   | 89            |
|    iterations            | 25            |
|    time_elapsed          | 572           |
|    total_timesteps       | 51200         |
| train/                   |               |
|    approx_kl             | 0.005783804   |
|    clip_fraction         | 0.0496        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00232       |
|    entropy_loss          | -2.69         |
|    explained_variance    | -0.036        |
|    lagrangian_multiplier | 0.0957        |
|    learning_rate         | 0.0003        |
|    loss                  | 5.91          |
|    n_updates             | 240           |
|    policy_gradient_loss  | -0.00346      |
|    std                   | 0.927         |
|    value_loss            | 84.4          |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3504888] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 35           |
|    time_elapsed          | 575          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0064216014 |
|    clip_fraction         | 0.065        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.137        |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.00243      |
|    lagrangian_multiplier | 0.0612       |
|    learning_rate         | 0.0003       |
|    loss                  | 70.1         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.0104      |
|    std                   | 0.948        |
|    value_loss            | 688          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8064234] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 36           |
|    time_elapsed          | 575          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.004226922  |
|    clip_fraction         | 0.0273       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.119        |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.0039      |
|    lagrangian_multiplier | 0.0647       |
|    learning_rate         | 0.0003       |
|    loss                  | 44.4         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0068      |
|    std                   | 1.02         |
|    value_loss            | 439          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.4436096] |
| time/                    |              |
|    fps                   | 90           |
|    iterations            | 26           |
|    time_elapsed          | 590          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0074486053 |
|    clip_fraction         | 0.0739       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.36         |
|    entropy_loss          | -2.67        |
|    explained_variance    | -0.0076      |
|    lagrangian_multiplier | 0.0813       |
|    learning_rate         | 0.0003       |
|    loss                  | 10.8         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00623     |
|    std                   | 0.917        |
|    value_loss            | 117          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5181686] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 36           |
|    time_elapsed          | 591          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0051395837 |
|    clip_fraction         | 0.0488       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0771       |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.0145      |
|    lagrangian_multiplier | 0.0729       |
|    learning_rate         | 0.0003       |
|    loss                  | 65.4         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00777     |
|    std                   | 0.946        |
|    value_loss            | 697          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6000671] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 37           |
|    time_elapsed          | 591          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.0077565154 |
|    clip_fraction         | 0.0735       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.151        |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.00467      |
|    lagrangian_multiplier | 0.0636       |
|    learning_rate         | 0.0003       |
|    loss                  | 36.9         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.0112      |
|    std                   | 1.01         |
|    value_loss            | 360          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.6090076] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 37           |
|    time_elapsed          | 608          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.0079871025 |
|    clip_fraction         | 0.0681       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.126        |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.0386       |
|    lagrangian_multiplier | 0.0578       |
|    learning_rate         | 0.0003       |
|    loss                  | 44.1         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00871     |
|    std                   | 0.947        |
|    value_loss            | 457          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.7347183] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 38           |
|    time_elapsed          | 607          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0062890304 |
|    clip_fraction         | 0.0571       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.166        |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0201       |
|    lagrangian_multiplier | 0.0676       |
|    learning_rate         | 0.0003       |
|    loss                  | 38.9         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00855     |
|    std                   | 1.01         |
|    value_loss            | 444          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.35289136] |
| time/                    |               |
|    fps                   | 90            |
|    iterations            | 27            |
|    time_elapsed          | 608           |
|    total_timesteps       | 55296         |
| train/                   |               |
|    approx_kl             | 0.007500273   |
|    clip_fraction         | 0.0588        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0153        |
|    entropy_loss          | -2.64         |
|    explained_variance    | -0.0139       |
|    lagrangian_multiplier | 0.0675        |
|    learning_rate         | 0.0003        |
|    loss                  | 19.3          |
|    n_updates             | 260           |
|    policy_gradient_loss  | -0.0066       |
|    std                   | 0.896         |
|    value_loss            | 196           |
--------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.0001578] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 39           |
|    time_elapsed          | 623          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0073972344 |
|    clip_fraction         | 0.0567       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.14         |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0396       |
|    lagrangian_multiplier | 0.0714       |
|    learning_rate         | 0.0003       |
|    loss                  | 65.8         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00971     |
|    std                   | 1.01         |
|    value_loss            | 750          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0767913] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 38           |
|    time_elapsed          | 624          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0078013465 |
|    clip_fraction         | 0.0821       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.155        |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.0486      |
|    lagrangian_multiplier | 0.0484       |
|    learning_rate         | 0.0003       |
|    loss                  | 61.3         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00853     |
|    std                   | 0.943        |
|    value_loss            | 504          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.64379513] |
| time/                    |               |
|    fps                   | 91            |
|    iterations            | 28            |
|    time_elapsed          | 625           |
|    total_timesteps       | 57344         |
| train/                   |               |
|    approx_kl             | 0.0063162046  |
|    clip_fraction         | 0.0664        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 34.4          |
|    entropy_loss          | -2.63         |
|    explained_variance    | -0.0351       |
|    lagrangian_multiplier | 0.0797        |
|    learning_rate         | 0.0003        |
|    loss                  | 29.4          |
|    n_updates             | 270           |
|    policy_gradient_loss  | -0.00767      |
|    std                   | 0.903         |
|    value_loss            | 323           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.8120586] |
| time/                    |              |
|    fps                   | 26           |
|    iterations            | 8            |
|    time_elapsed          | 628          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.005774028  |
|    clip_fraction         | 0.0584       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.56         |
|    entropy_loss          | -2.99        |
|    explained_variance    | 0.0692       |
|    lagrangian_multiplier | 0.0692       |
|    learning_rate         | 0.0003       |
|    loss                  | 49.6         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00916     |
|    std                   | 1.09         |
|    value_loss            | 528          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.1479375] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 40           |
|    time_elapsed          | 639          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.0053639133 |
|    clip_fraction         | 0.04         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.124        |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0351       |
|    lagrangian_multiplier | 0.0756       |
|    learning_rate         | 0.0003       |
|    loss                  | 81.4         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00662     |
|    std                   | 1            |
|    value_loss            | 1.01e+03     |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.4540817] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 39           |
|    time_elapsed          | 641          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.006553885  |
|    clip_fraction         | 0.0768       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.093        |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.00509     |
|    lagrangian_multiplier | 0.0611       |
|    learning_rate         | 0.0003       |
|    loss                  | 45.9         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00985     |
|    std                   | 0.946        |
|    value_loss            | 353          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6844036] |
| time/                    |              |
|    fps                   | 92           |
|    iterations            | 29           |
|    time_elapsed          | 643          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.0062758615 |
|    clip_fraction         | 0.0418       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 51.8         |
|    entropy_loss          | -2.62        |
|    explained_variance    | -0.0263      |
|    lagrangian_multiplier | 0.0848       |
|    learning_rate         | 0.0003       |
|    loss                  | 32.3         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00628     |
|    std                   | 0.892        |
|    value_loss            | 308          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.3097422] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 41           |
|    time_elapsed          | 655          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0052184216 |
|    clip_fraction         | 0.0612       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.132        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0328       |
|    lagrangian_multiplier | 0.0726       |
|    learning_rate         | 0.0003       |
|    loss                  | 86.9         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00795     |
|    std                   | 0.992        |
|    value_loss            | 970          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1125287] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 40           |
|    time_elapsed          | 657          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.007861655  |
|    clip_fraction         | 0.0828       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0966       |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.0236      |
|    lagrangian_multiplier | 0.0659       |
|    learning_rate         | 0.0003       |
|    loss                  | 48.4         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.951        |
|    value_loss            | 530          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.4249986] |
| time/                    |              |
|    fps                   | 92           |
|    iterations            | 30           |
|    time_elapsed          | 661          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.006625972  |
|    clip_fraction         | 0.0531       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0097       |
|    entropy_loss          | -2.61        |
|    explained_variance    | -0.0709      |
|    lagrangian_multiplier | 0.0882       |
|    learning_rate         | 0.0003       |
|    loss                  | 7.72         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00411     |
|    std                   | 0.895        |
|    value_loss            | 104          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.614187]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 42           |
|    time_elapsed          | 671          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0062767854 |
|    clip_fraction         | 0.0514       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.13         |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0389       |
|    lagrangian_multiplier | 0.0685       |
|    learning_rate         | 0.0003       |
|    loss                  | 95.9         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.0104      |
|    std                   | 0.986        |
|    value_loss            | 1.03e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7099196] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 41           |
|    time_elapsed          | 674          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.006445517  |
|    clip_fraction         | 0.0518       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0436       |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.0102      |
|    lagrangian_multiplier | 0.0697       |
|    learning_rate         | 0.0003       |
|    loss                  | 32.6         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00565     |
|    std                   | 0.95         |
|    value_loss            | 355          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.60125107] |
| time/                    |               |
|    fps                   | 93            |
|    iterations            | 31            |
|    time_elapsed          | 679           |
|    total_timesteps       | 63488         |
| train/                   |               |
|    approx_kl             | 0.004171078   |
|    clip_fraction         | 0.037         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 67.6          |
|    entropy_loss          | -2.63         |
|    explained_variance    | -0.0216       |
|    lagrangian_multiplier | 0.0822        |
|    learning_rate         | 0.0003        |
|    loss                  | 75.6          |
|    n_updates             | 300           |
|    policy_gradient_loss  | -0.00653      |
|    std                   | 0.904         |
|    value_loss            | 697           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.6942325] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 43           |
|    time_elapsed          | 687          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.008859031  |
|    clip_fraction         | 0.0812       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.139        |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0351       |
|    lagrangian_multiplier | 0.0748       |
|    learning_rate         | 0.0003       |
|    loss                  | 57.4         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.0141      |
|    std                   | 0.972        |
|    value_loss            | 697          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4368817] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 42           |
|    time_elapsed          | 690          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.010031136  |
|    clip_fraction         | 0.0856       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.117        |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.0154      |
|    lagrangian_multiplier | 0.0661       |
|    learning_rate         | 0.0003       |
|    loss                  | 35.1         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.0126      |
|    std                   | 0.941        |
|    value_loss            | 442          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.65191597] |
| time/                    |               |
|    fps                   | 93            |
|    iterations            | 32            |
|    time_elapsed          | 698           |
|    total_timesteps       | 65536         |
| train/                   |               |
|    approx_kl             | 0.0059108557  |
|    clip_fraction         | 0.0559        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 86.2          |
|    entropy_loss          | -2.63         |
|    explained_variance    | -0.0135       |
|    lagrangian_multiplier | 0.0728        |
|    learning_rate         | 0.0003        |
|    loss                  | 49            |
|    n_updates             | 310           |
|    policy_gradient_loss  | -0.00676      |
|    std                   | 0.895         |
|    value_loss            | 443           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.5730823] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 44           |
|    time_elapsed          | 703          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.0079600345 |
|    clip_fraction         | 0.075        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.117        |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0367       |
|    lagrangian_multiplier | 0.0641       |
|    learning_rate         | 0.0003       |
|    loss                  | 64.9         |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.0108      |
|    std                   | 0.966        |
|    value_loss            | 668          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.88485765] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 43            |
|    time_elapsed          | 706           |
|    total_timesteps       | 88064         |
| train/                   |               |
|    approx_kl             | 0.007290376   |
|    clip_fraction         | 0.0828        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.111         |
|    entropy_loss          | -2.72         |
|    explained_variance    | -0.0325       |
|    lagrangian_multiplier | 0.0592        |
|    learning_rate         | 0.0003        |
|    loss                  | 70            |
|    n_updates             | 420           |
|    policy_gradient_loss  | -0.00831      |
|    std                   | 0.945         |
|    value_loss            | 676           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3997099] |
| time/                    |              |
|    fps                   | 25           |
|    iterations            | 9            |
|    time_elapsed          | 714          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.003441998  |
|    clip_fraction         | 0.0241       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00763      |
|    entropy_loss          | -3.02        |
|    explained_variance    | -0.0128      |
|    lagrangian_multiplier | 0.0744       |
|    learning_rate         | 0.0003       |
|    loss                  | 15.7         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.0042      |
|    std                   | 1.1          |
|    value_loss            | 193          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.4420926] |
| time/                    |              |
|    fps                   | 94           |
|    iterations            | 33           |
|    time_elapsed          | 715          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0052897725 |
|    clip_fraction         | 0.0418       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 72.3         |
|    entropy_loss          | -2.63        |
|    explained_variance    | -0.0231      |
|    lagrangian_multiplier | 0.084        |
|    learning_rate         | 0.0003       |
|    loss                  | 63.6         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00921     |
|    std                   | 0.907        |
|    value_loss            | 649          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.036702]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 45           |
|    time_elapsed          | 719          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.0075245625 |
|    clip_fraction         | 0.073        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.13         |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0111       |
|    lagrangian_multiplier | 0.0667       |
|    learning_rate         | 0.0003       |
|    loss                  | 39           |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.964        |
|    value_loss            | 395          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-1.238597] |
| time/                    |             |
|    fps                   | 124         |
|    iterations            | 44          |
|    time_elapsed          | 723         |
|    total_timesteps       | 90112       |
| train/                   |             |
|    approx_kl             | 0.009646292 |
|    clip_fraction         | 0.0844      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0638      |
|    entropy_loss          | -2.72       |
|    explained_variance    | -0.0192     |
|    lagrangian_multiplier | 0.0702      |
|    learning_rate         | 0.0003      |
|    loss                  | 57          |
|    n_updates             | 430         |
|    policy_gradient_loss  | -0.0123     |
|    std                   | 0.941       |
|    value_loss            | 566         |
------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-3.7268708] |
| time/                    |              |
|    fps                   | 94           |
|    iterations            | 34           |
|    time_elapsed          | 733          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.007599364  |
|    clip_fraction         | 0.0518       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 110          |
|    entropy_loss          | -2.64        |
|    explained_variance    | -0.013       |
|    lagrangian_multiplier | 0.0865       |
|    learning_rate         | 0.0003       |
|    loss                  | 99.7         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.0089      |
|    std                   | 0.906        |
|    value_loss            | 1.08e+03     |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.0739435] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 46           |
|    time_elapsed          | 735          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.00793395   |
|    clip_fraction         | 0.0693       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0757       |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.0812       |
|    lagrangian_multiplier | 0.0656       |
|    learning_rate         | 0.0003       |
|    loss                  | 36.6         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.0111      |
|    std                   | 0.958        |
|    value_loss            | 372          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1064862] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 45           |
|    time_elapsed          | 739          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.007791376  |
|    clip_fraction         | 0.0751       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.143        |
|    entropy_loss          | -2.71        |
|    explained_variance    | -0.00281     |
|    lagrangian_multiplier | 0.0639       |
|    learning_rate         | 0.0003       |
|    loss                  | 71.4         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.0141      |
|    std                   | 0.942        |
|    value_loss            | 711          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.1688054] |
| time/                    |              |
|    fps                   | 95           |
|    iterations            | 35           |
|    time_elapsed          | 751          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.006178259  |
|    clip_fraction         | 0.0581       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 42           |
|    entropy_loss          | -2.64        |
|    explained_variance    | -0.0262      |
|    lagrangian_multiplier | 0.0838       |
|    learning_rate         | 0.0003       |
|    loss                  | 41.4         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.0105      |
|    std                   | 0.911        |
|    value_loss            | 427          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.5458577] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 47           |
|    time_elapsed          | 751          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.007158048  |
|    clip_fraction         | 0.0668       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.132        |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.0328       |
|    lagrangian_multiplier | 0.0685       |
|    learning_rate         | 0.0003       |
|    loss                  | 36.2         |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.0094      |
|    std                   | 0.949        |
|    value_loss            | 408          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.3692666] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 46           |
|    time_elapsed          | 756          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.008011812  |
|    clip_fraction         | 0.0639       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.204        |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.00085     |
|    lagrangian_multiplier | 0.049        |
|    learning_rate         | 0.0003       |
|    loss                  | 45.1         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.0082      |
|    std                   | 0.95         |
|    value_loss            | 366          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.4801421] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 48           |
|    time_elapsed          | 767          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.0084143635 |
|    clip_fraction         | 0.0684       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.121        |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.0285       |
|    lagrangian_multiplier | 0.0684       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.8         |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00894     |
|    std                   | 0.946        |
|    value_loss            | 525          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7486687] |
| time/                    |              |
|    fps                   | 95           |
|    iterations            | 36           |
|    time_elapsed          | 768          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.007697991  |
|    clip_fraction         | 0.0739       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 142          |
|    entropy_loss          | -2.67        |
|    explained_variance    | -0.00853     |
|    lagrangian_multiplier | 0.0795       |
|    learning_rate         | 0.0003       |
|    loss                  | 113          |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0117      |
|    std                   | 0.93         |
|    value_loss            | 1.23e+03     |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8314824] |
| time/                    |              |
|    fps                   | 124          |
|    iterations            | 47           |
|    time_elapsed          | 773          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.0065115336 |
|    clip_fraction         | 0.0548       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.091        |
|    entropy_loss          | -2.73        |
|    explained_variance    | -4.07e-05    |
|    lagrangian_multiplier | 0.0755       |
|    learning_rate         | 0.0003       |
|    loss                  | 88.4         |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.00948     |
|    std                   | 0.951        |
|    value_loss            | 959          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3109171] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 49           |
|    time_elapsed          | 783          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.007184313  |
|    clip_fraction         | 0.0727       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0782       |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0436       |
|    lagrangian_multiplier | 0.0722       |
|    learning_rate         | 0.0003       |
|    loss                  | 51.5         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.944        |
|    value_loss            | 573          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 103, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 90, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
------------------------------------------
| reward                   | [-3.146002] |
| time/                    |             |
|    fps                   | 96          |
|    iterations            | 37          |
|    time_elapsed          | 786         |
|    total_timesteps       | 75776       |
| train/                   |             |
|    approx_kl             | 0.004661427 |
|    clip_fraction         | 0.0444      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 77          |
|    entropy_loss          | -2.71       |
|    explained_variance    | -0.0208     |
|    lagrangian_multiplier | 0.08        |
|    learning_rate         | 0.0003      |
|    loss                  | 97.6        |
|    n_updates             | 360         |
|    policy_gradient_loss  | -0.0103     |
|    std                   | 0.948       |
|    value_loss            | 1.05e+03    |
------------------------------------------
--------------------------------------------
| reward                   | [-0.82840484] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 48            |
|    time_elapsed          | 789           |
|    total_timesteps       | 98304         |
| train/                   |               |
|    approx_kl             | 0.0071823103  |
|    clip_fraction         | 0.0685        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.125         |
|    entropy_loss          | -2.74         |
|    explained_variance    | -0.064        |
|    lagrangian_multiplier | 0.0695        |
|    learning_rate         | 0.0003        |
|    loss                  | 40            |
|    n_updates             | 470           |
|    policy_gradient_loss  | -0.00748      |
|    std                   | 0.956         |
|    value_loss            | 418           |
--------------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñá‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÇ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÖ‚ñá‚ñá‚ñÑ
wandb:             train/approx_kl ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñá‚ñÇ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÅ‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñà‚ñá‚ñÜ‚ñÖ‚ñá‚ñÖ
wandb:         train/clip_fraction ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÅ‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñá
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ
wandb:          train/entropy_loss ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà
wandb:    train/explained_variance ‚ñá‚ñÜ‚ñÑ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá
wandb: train/lagrangian_multiplier ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñÑ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñá‚ñà‚ñÑ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÑ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñá‚ñÖ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ
wandb:                   train/std ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ
wandb:            train/value_loss ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñà‚ñÖ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÑ
wandb: 
wandb: Run summary:
wandb:                      reward -1.31092
wandb:             train/approx_kl 0.00718
wandb:         train/clip_fraction 0.07266
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 0.07818
wandb:          train/entropy_loss -2.72389
wandb:    train/explained_variance 0.04362
wandb: train/lagrangian_multiplier 0.07218
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 51.45464
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.01023
wandb:                   train/std 0.94389
wandb:            train/value_loss 573.3146
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/fanvdd9w
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231223_000944-fanvdd9w/logs
srun: error: dqn.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-3.6696842] |
| time/                    |              |
|    fps                   | 96           |
|    iterations            | 38           |
|    time_elapsed          | 804          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.006398827  |
|    clip_fraction         | 0.0597       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 87.7         |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.00239      |
|    lagrangian_multiplier | 0.0646       |
|    learning_rate         | 0.0003       |
|    loss                  | 90.2         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.0098      |
|    std                   | 0.957        |
|    value_loss            | 742          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.437121] |
| time/                    |             |
|    fps                   | 124         |
|    iterations            | 49          |
|    time_elapsed          | 805         |
|    total_timesteps       | 100352      |
| train/                   |             |
|    approx_kl             | 0.008075913 |
|    clip_fraction         | 0.0799      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.147       |
|    entropy_loss          | -2.74       |
|    explained_variance    | -0.038      |
|    lagrangian_multiplier | 0.0606      |
|    learning_rate         | 0.0003      |
|    loss                  | 41.8        |
|    n_updates             | 480         |
|    policy_gradient_loss  | -0.0102     |
|    std                   | 0.951       |
|    value_loss            | 417         |
------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 103, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 90, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÅ‚ñà‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ
wandb:             train/approx_kl ‚ñÇ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÜ
wandb:         train/clip_fraction ‚ñÉ‚ñÇ‚ñÅ‚ñá‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñá
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñá‚ñá‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñÇ‚ñá‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñÖ
wandb:          train/entropy_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÜ
wandb:    train/explained_variance ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñà‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÅ‚ñÉ
wandb: train/lagrangian_multiplier ‚ñÉ‚ñÜ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÇ‚ñá‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñà‚ñá‚ñÖ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÖ‚ñÅ‚ñà‚ñÉ‚ñÑ‚ñá‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÇ‚ñÇ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñá‚ñà‚ñá‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÇ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÑ‚ñÜ‚ñÑ
wandb:                   train/std ‚ñà‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ
wandb:            train/value_loss ‚ñÑ‚ñÅ‚ñÖ‚ñÉ‚ñÑ‚ñá‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÜ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:                      reward -1.43712
wandb:             train/approx_kl 0.00808
wandb:         train/clip_fraction 0.07993
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 0.14706
wandb:          train/entropy_loss -2.73751
wandb:    train/explained_variance -0.03802
wandb: train/lagrangian_multiplier 0.06055
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 41.8045
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.01021
wandb:                   train/std 0.95078
wandb:            train/value_loss 416.52881
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/5mdjt0ln
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231223_000944-5mdjt0ln/logs
-------------------------------------------
| reward                   | [-1.5433251] |
| time/                    |              |
|    fps                   | 25           |
|    iterations            | 10           |
|    time_elapsed          | 811          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0073349793 |
|    clip_fraction         | 0.0699       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.74         |
|    entropy_loss          | -3.03        |
|    explained_variance    | 0.0196       |
|    lagrangian_multiplier | 0.0712       |
|    learning_rate         | 0.0003       |
|    loss                  | 45.1         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00908     |
|    std                   | 1.1          |
|    value_loss            | 482          |
-------------------------------------------
srun: error: ddpg.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-4.2332845] |
| time/                    |              |
|    fps                   | 97           |
|    iterations            | 39           |
|    time_elapsed          | 822          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0067315144 |
|    clip_fraction         | 0.0717       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 138          |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0231      |
|    lagrangian_multiplier | 0.0938       |
|    learning_rate         | 0.0003       |
|    loss                  | 129          |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00942     |
|    std                   | 0.957        |
|    value_loss            | 1.97e+03     |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114474.4
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114474.5
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231223_002344-pk5hyaxd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/pk5hyaxd
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231223_002344-b8ocxbji
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/b8ocxbji
------------------------------------------
| reward                   | [-4.671812] |
| time/                    |             |
|    fps                   | 97          |
|    iterations            | 40          |
|    time_elapsed          | 840         |
|    total_timesteps       | 81920       |
| train/                   |             |
|    approx_kl             | 0.00505579  |
|    clip_fraction         | 0.0357      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 197         |
|    entropy_loss          | -2.75       |
|    explained_variance    | -0.0183     |
|    lagrangian_multiplier | 0.097       |
|    learning_rate         | 0.0003      |
|    loss                  | 243         |
|    n_updates             | 390         |
|    policy_gradient_loss  | -0.00436    |
|    std                   | 0.955       |
|    value_loss            | 3.06e+03    |
------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
Using cpu device
-------------------------------------
| reward             | [-0.4311558] |
| time/              |              |
|    fps             | 136          |
|    iterations      | 1            |
|    time_elapsed    | 14           |
|    total_timesteps | 2048         |
-------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.6627132] |
| time/              |              |
|    fps             | 130          |
|    iterations      | 1            |
|    time_elapsed    | 15           |
|    total_timesteps | 2048         |
-------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-4.683803] |
| time/                    |             |
|    fps                   | 97          |
|    iterations            | 41          |
|    time_elapsed          | 857         |
|    total_timesteps       | 83968       |
| train/                   |             |
|    approx_kl             | 0.006357114 |
|    clip_fraction         | 0.0592      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 211         |
|    entropy_loss          | -2.74       |
|    explained_variance    | -0.0061     |
|    lagrangian_multiplier | 0.0862      |
|    learning_rate         | 0.0003      |
|    loss                  | 288         |
|    n_updates             | 400         |
|    policy_gradient_loss  | -0.0105     |
|    std                   | 0.947       |
|    value_loss            | 3.3e+03     |
------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.6432243] |
| time/                    |              |
|    fps                   | 132          |
|    iterations            | 2            |
|    time_elapsed          | 30           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0053895954 |
|    clip_fraction         | 0.037        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.188        |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0428       |
|    lagrangian_multiplier | 0.0539       |
|    learning_rate         | 0.0003       |
|    loss                  | 43           |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00496     |
|    std                   | 1            |
|    value_loss            | 368          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.46633977] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 2             |
|    time_elapsed          | 32            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.0073711015  |
|    clip_fraction         | 0.0612        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 28.7          |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.00294       |
|    lagrangian_multiplier | 0.0415        |
|    learning_rate         | 0.0003        |
|    loss                  | 32            |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00732      |
|    std                   | 1             |
|    value_loss            | 226           |
--------------------------------------------
-------------------------------------------
| reward                   | [-4.619318]  |
| time/                    |              |
|    fps                   | 98           |
|    iterations            | 42           |
|    time_elapsed          | 875          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0075131115 |
|    clip_fraction         | 0.0684       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 185          |
|    entropy_loss          | -2.74        |
|    explained_variance    | -0.00607     |
|    lagrangian_multiplier | 0.0861       |
|    learning_rate         | 0.0003       |
|    loss                  | 286          |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00921     |
|    std                   | 0.953        |
|    value_loss            | 3.62e+03     |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.69739026] |
| time/                    |               |
|    fps                   | 131           |
|    iterations            | 3             |
|    time_elapsed          | 46            |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.0047858926  |
|    clip_fraction         | 0.0357        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 3.16          |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.0171        |
|    lagrangian_multiplier | 0.0614        |
|    learning_rate         | 0.0003        |
|    loss                  | 81.3          |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00444      |
|    std                   | 0.997         |
|    value_loss            | 761           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.53751355] |
| time/                    |               |
|    fps                   | 124           |
|    iterations            | 3             |
|    time_elapsed          | 49            |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.0060405666  |
|    clip_fraction         | 0.0473        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 114           |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.0232       |
|    lagrangian_multiplier | 0.0449        |
|    learning_rate         | 0.0003        |
|    loss                  | 61.7          |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00567      |
|    std                   | 0.99          |
|    value_loss            | 372           |
--------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-5.704126] |
| time/                    |             |
|    fps                   | 98          |
|    iterations            | 43          |
|    time_elapsed          | 892         |
|    total_timesteps       | 88064       |
| train/                   |             |
|    approx_kl             | 0.008231962 |
|    clip_fraction         | 0.0813      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 201         |
|    entropy_loss          | -2.74       |
|    explained_variance    | -0.00543    |
|    lagrangian_multiplier | 0.0858      |
|    learning_rate         | 0.0003      |
|    loss                  | 213         |
|    n_updates             | 420         |
|    policy_gradient_loss  | -0.011      |
|    std                   | 0.953       |
|    value_loss            | 3e+03       |
------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.0622118] |
| time/                    |              |
|    fps                   | 130          |
|    iterations            | 4            |
|    time_elapsed          | 62           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.004967426  |
|    clip_fraction         | 0.0368       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.187        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.00848      |
|    lagrangian_multiplier | 0.0687       |
|    learning_rate         | 0.0003       |
|    loss                  | 63.6         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.0049      |
|    std                   | 0.999        |
|    value_loss            | 705          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.58031434] |
| time/                    |               |
|    fps                   | 123           |
|    iterations            | 4             |
|    time_elapsed          | 66            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.0032139411  |
|    clip_fraction         | 0.0198        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 15            |
|    entropy_loss          | -2.81         |
|    explained_variance    | 0.0369        |
|    lagrangian_multiplier | 0.075         |
|    learning_rate         | 0.0003        |
|    loss                  | 23.7          |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00375      |
|    std                   | 0.98          |
|    value_loss            | 359           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.435622]  |
| time/                    |              |
|    fps                   | 24           |
|    iterations            | 11           |
|    time_elapsed          | 908          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0041494463 |
|    clip_fraction         | 0.0305       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.267        |
|    entropy_loss          | -3.03        |
|    explained_variance    | 0.00974      |
|    lagrangian_multiplier | 0.0677       |
|    learning_rate         | 0.0003       |
|    loss                  | 31           |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00491     |
|    std                   | 1.1          |
|    value_loss            | 336          |
-------------------------------------------
------------------------------------------
| reward                   | [-5.929204] |
| time/                    |             |
|    fps                   | 98          |
|    iterations            | 44          |
|    time_elapsed          | 910         |
|    total_timesteps       | 90112       |
| train/                   |             |
|    approx_kl             | 0.008488525 |
|    clip_fraction         | 0.0835      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 227         |
|    entropy_loss          | -2.73       |
|    explained_variance    | -0.00208    |
|    lagrangian_multiplier | 0.0797      |
|    learning_rate         | 0.0003      |
|    loss                  | 323         |
|    n_updates             | 430         |
|    policy_gradient_loss  | -0.0104     |
|    std                   | 0.942       |
|    value_loss            | 3.52e+03    |
------------------------------------------
--------------------------------------------
| reward                   | [-0.54673886] |
| time/                    |               |
|    fps                   | 130           |
|    iterations            | 5             |
|    time_elapsed          | 78            |
|    total_timesteps       | 10240         |
| train/                   |               |
|    approx_kl             | 0.005794501   |
|    clip_fraction         | 0.0621        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.21          |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.012         |
|    lagrangian_multiplier | 0.0829        |
|    learning_rate         | 0.0003        |
|    loss                  | 34.3          |
|    n_updates             | 40            |
|    policy_gradient_loss  | -0.0085       |
|    std                   | 1             |
|    value_loss            | 416           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2064027] |
| time/                    |              |
|    fps                   | 123          |
|    iterations            | 5            |
|    time_elapsed          | 82           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0066564353 |
|    clip_fraction         | 0.0499       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 77.2         |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.00149     |
|    lagrangian_multiplier | 0.0831       |
|    learning_rate         | 0.0003       |
|    loss                  | 54.9         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00821     |
|    std                   | 0.959        |
|    value_loss            | 568          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.49716246] |
| time/                    |               |
|    fps                   | 99            |
|    iterations            | 45            |
|    time_elapsed          | 927           |
|    total_timesteps       | 92160         |
| train/                   |               |
|    approx_kl             | 0.0072629955  |
|    clip_fraction         | 0.0559        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 222           |
|    entropy_loss          | -2.71         |
|    explained_variance    | -0.00212      |
|    lagrangian_multiplier | 0.0892        |
|    learning_rate         | 0.0003        |
|    loss                  | 312           |
|    n_updates             | 440           |
|    policy_gradient_loss  | -0.00963      |
|    std                   | 0.932         |
|    value_loss            | 4.24e+03      |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.0481894] |
| time/                    |              |
|    fps                   | 130          |
|    iterations            | 6            |
|    time_elapsed          | 94           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.003962044  |
|    clip_fraction         | 0.0243       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.237        |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0292      |
|    lagrangian_multiplier | 0.0664       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.4         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00415     |
|    std                   | 1            |
|    value_loss            | 503          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6883162] |
| time/                    |              |
|    fps                   | 123          |
|    iterations            | 6            |
|    time_elapsed          | 99           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.004238425  |
|    clip_fraction         | 0.0377       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 63.2         |
|    entropy_loss          | -2.74        |
|    explained_variance    | -0.00219     |
|    lagrangian_multiplier | 0.0752       |
|    learning_rate         | 0.0003       |
|    loss                  | 39.8         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00449     |
|    std                   | 0.949        |
|    value_loss            | 370          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6081929] |
| time/                    |              |
|    fps                   | 99           |
|    iterations            | 46           |
|    time_elapsed          | 945          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.009001986  |
|    clip_fraction         | 0.0952       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 209          |
|    entropy_loss          | -2.69        |
|    explained_variance    | -0.0141      |
|    lagrangian_multiplier | 0.082        |
|    learning_rate         | 0.0003       |
|    loss                  | 268          |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.0108      |
|    std                   | 0.93         |
|    value_loss            | 3.01e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0737484] |
| time/                    |              |
|    fps                   | 130          |
|    iterations            | 7            |
|    time_elapsed          | 110          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.005798389  |
|    clip_fraction         | 0.0451       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.26         |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0133       |
|    lagrangian_multiplier | 0.0805       |
|    learning_rate         | 0.0003       |
|    loss                  | 54.5         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00759     |
|    std                   | 0.994        |
|    value_loss            | 630          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.94441634] |
| time/                    |               |
|    fps                   | 122           |
|    iterations            | 7             |
|    time_elapsed          | 116           |
|    total_timesteps       | 14336         |
| train/                   |               |
|    approx_kl             | 0.0062670065  |
|    clip_fraction         | 0.0557        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 95.9          |
|    entropy_loss          | -2.73         |
|    explained_variance    | 0.0285        |
|    lagrangian_multiplier | 0.0744        |
|    learning_rate         | 0.0003        |
|    loss                  | 47.6          |
|    n_updates             | 60            |
|    policy_gradient_loss  | -0.00683      |
|    std                   | 0.947         |
|    value_loss            | 442           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.84897274] |
| time/                    |               |
|    fps                   | 99            |
|    iterations            | 47            |
|    time_elapsed          | 962           |
|    total_timesteps       | 96256         |
| train/                   |               |
|    approx_kl             | 0.008493035   |
|    clip_fraction         | 0.0974        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 232           |
|    entropy_loss          | -2.69         |
|    explained_variance    | -0.00839      |
|    lagrangian_multiplier | 0.0902        |
|    learning_rate         | 0.0003        |
|    loss                  | 249           |
|    n_updates             | 460           |
|    policy_gradient_loss  | -0.0121       |
|    std                   | 0.926         |
|    value_loss            | 3.24e+03      |
--------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.9780056] |
| time/                    |              |
|    fps                   | 130          |
|    iterations            | 8            |
|    time_elapsed          | 125          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.006236316  |
|    clip_fraction         | 0.059        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.99         |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0156       |
|    lagrangian_multiplier | 0.0639       |
|    learning_rate         | 0.0003       |
|    loss                  | 57.6         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00754     |
|    std                   | 1.01         |
|    value_loss            | 722          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1569953] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 8            |
|    time_elapsed          | 133          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.005365714  |
|    clip_fraction         | 0.0465       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 140          |
|    entropy_loss          | -2.71        |
|    explained_variance    | 0.0261       |
|    lagrangian_multiplier | 0.0634       |
|    learning_rate         | 0.0003       |
|    loss                  | 43.5         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.0066      |
|    std                   | 0.934        |
|    value_loss            | 320          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.65170026] |
| time/                    |               |
|    fps                   | 100           |
|    iterations            | 48            |
|    time_elapsed          | 980           |
|    total_timesteps       | 98304         |
| train/                   |               |
|    approx_kl             | 0.008524499   |
|    clip_fraction         | 0.0801        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 207           |
|    entropy_loss          | -2.68         |
|    explained_variance    | -0.00651      |
|    lagrangian_multiplier | 0.0823        |
|    learning_rate         | 0.0003        |
|    loss                  | 194           |
|    n_updates             | 470           |
|    policy_gradient_loss  | -0.00994      |
|    std                   | 0.925         |
|    value_loss            | 2.14e+03      |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.98960024] |
| time/                    |               |
|    fps                   | 130           |
|    iterations            | 9             |
|    time_elapsed          | 141           |
|    total_timesteps       | 18432         |
| train/                   |               |
|    approx_kl             | 0.006542079   |
|    clip_fraction         | 0.0635        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.491         |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.0146       |
|    lagrangian_multiplier | 0.0662        |
|    learning_rate         | 0.0003        |
|    loss                  | 59.4          |
|    n_updates             | 80            |
|    policy_gradient_loss  | -0.0073       |
|    std                   | 1.01          |
|    value_loss            | 602           |
--------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1246048] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 9            |
|    time_elapsed          | 150          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0045661256 |
|    clip_fraction         | 0.0475       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 135          |
|    entropy_loss          | -2.7         |
|    explained_variance    | 0.0224       |
|    lagrangian_multiplier | 0.0525       |
|    learning_rate         | 0.0003       |
|    loss                  | 56.3         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00731     |
|    std                   | 0.934        |
|    value_loss            | 351          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2579576] |
| time/                    |              |
|    fps                   | 130          |
|    iterations            | 10           |
|    time_elapsed          | 157          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.004117308  |
|    clip_fraction         | 0.0322       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.45         |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.00735     |
|    lagrangian_multiplier | 0.058        |
|    learning_rate         | 0.0003       |
|    loss                  | 57.4         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00637     |
|    std                   | 1.02         |
|    value_loss            | 438          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8009408] |
| time/                    |              |
|    fps                   | 100          |
|    iterations            | 49           |
|    time_elapsed          | 997          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.0070933476 |
|    clip_fraction         | 0.0692       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 225          |
|    entropy_loss          | -2.68        |
|    explained_variance    | -0.00526     |
|    lagrangian_multiplier | 0.0869       |
|    learning_rate         | 0.0003       |
|    loss                  | 303          |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.922        |
|    value_loss            | 3.42e+03     |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 103, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 90, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñÖ‚ñà‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñà‚ñá‚ñá‚ñá
wandb:             train/approx_kl ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÉ‚ñá‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñá‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñÜ
wandb:         train/clip_fraction ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÜ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñá‚ñá‚ñÑ‚ñà‚ñÜ‚ñÜ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÖ‚ñÖ‚ñá‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà
wandb:          train/entropy_loss ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ
wandb:    train/explained_variance ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÜ‚ñà‚ñà‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñá‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: train/lagrangian_multiplier ‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñà‚ñà‚ñÜ‚ñÖ‚ñà
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñá‚ñà‚ñÖ‚ñÖ‚ñà‚ñà‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ
wandb:                   train/std ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ
wandb:            train/value_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñÜ‚ñÖ‚ñá
wandb: 
wandb: Run summary:
wandb:                      reward -0.80094
wandb:             train/approx_kl 0.00709
wandb:         train/clip_fraction 0.06924
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 224.91416
wandb:          train/entropy_loss -2.67876
wandb:    train/explained_variance -0.00526
wandb: train/lagrangian_multiplier 0.08695
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 303.35843
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.01018
wandb:                   train/std 0.92166
wandb:            train/value_loss 3416.2367
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/nd3knsyr
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231223_080944-nd3knsyr/logs
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/usr/lib/python3.9/threading.py", line 954, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.9/threading.py", line 892, in run
    self._target(*self._args, **self._kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 259, in check_network_status
    self._loop_check_status(
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 215, in _loop_check_status
    local_handle = request()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface.py", line 795, in deliver_network_status
    return self._deliver_network_status(status)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py", line 601, in _deliver_network_status
    return self._deliver_record(record)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py", line 560, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
srun: error: airl.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-0.6523184] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 10           |
|    time_elapsed          | 167          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.005313902  |
|    clip_fraction         | 0.0418       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 108          |
|    entropy_loss          | -2.7         |
|    explained_variance    | -0.0424      |
|    lagrangian_multiplier | 0.0673       |
|    learning_rate         | 0.0003       |
|    loss                  | 33           |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00563     |
|    std                   | 0.928        |
|    value_loss            | 304          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9171739] |
| time/                    |              |
|    fps                   | 24           |
|    iterations            | 12           |
|    time_elapsed          | 1008         |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0067635328 |
|    clip_fraction         | 0.0504       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.37         |
|    entropy_loss          | -3.02        |
|    explained_variance    | 0.0591       |
|    lagrangian_multiplier | 0.0768       |
|    learning_rate         | 0.0003       |
|    loss                  | 103          |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00558     |
|    std                   | 1.09         |
|    value_loss            | 1.12e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.117517]  |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 11           |
|    time_elapsed          | 173          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0053979587 |
|    clip_fraction         | 0.0483       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.232        |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0426      |
|    lagrangian_multiplier | 0.0529       |
|    learning_rate         | 0.0003       |
|    loss                  | 69           |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00654     |
|    std                   | 1            |
|    value_loss            | 797          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114474.6
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
-------------------------------------------
| reward                   | [-0.8491479] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 11           |
|    time_elapsed          | 183          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0047458652 |
|    clip_fraction         | 0.0339       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 74.9         |
|    entropy_loss          | -2.69        |
|    explained_variance    | -0.0332      |
|    lagrangian_multiplier | 0.0622       |
|    learning_rate         | 0.0003       |
|    loss                  | 34.4         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00401     |
|    std                   | 0.931        |
|    value_loss            | 241          |
-------------------------------------------
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231223_082648-ax6e3eqq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/ax6e3eqq
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.011091]  |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 12           |
|    time_elapsed          | 189          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0052651297 |
|    clip_fraction         | 0.0408       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0973       |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0344      |
|    lagrangian_multiplier | 0.0662       |
|    learning_rate         | 0.0003       |
|    loss                  | 36.6         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00426     |
|    std                   | 0.985        |
|    value_loss            | 337          |
-------------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.4978125] |
| time/              |              |
|    fps             | 126          |
|    iterations      | 1            |
|    time_elapsed    | 16           |
|    total_timesteps | 2048         |
-------------------------------------
-------------------------------------------
| reward                   | [-1.1058315] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 12           |
|    time_elapsed          | 200          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0072079813 |
|    clip_fraction         | 0.0748       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 87.2         |
|    entropy_loss          | -2.68        |
|    explained_variance    | 0.0552       |
|    lagrangian_multiplier | 0.0561       |
|    learning_rate         | 0.0003       |
|    loss                  | 33           |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.92         |
|    value_loss            | 214          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.94733083] |
| time/                    |               |
|    fps                   | 129           |
|    iterations            | 13            |
|    time_elapsed          | 205           |
|    total_timesteps       | 26624         |
| train/                   |               |
|    approx_kl             | 0.005022134   |
|    clip_fraction         | 0.0497        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.416         |
|    entropy_loss          | -2.8          |
|    explained_variance    | -0.0377       |
|    lagrangian_multiplier | 0.0674        |
|    learning_rate         | 0.0003        |
|    loss                  | 51.5          |
|    n_updates             | 120           |
|    policy_gradient_loss  | -0.0068       |
|    std                   | 0.979         |
|    value_loss            | 559           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.98449296] |
| time/                    |               |
|    fps                   | 122           |
|    iterations            | 13            |
|    time_elapsed          | 217           |
|    total_timesteps       | 26624         |
| train/                   |               |
|    approx_kl             | 0.0062884362  |
|    clip_fraction         | 0.0577        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 174           |
|    entropy_loss          | -2.67         |
|    explained_variance    | 0.0351        |
|    lagrangian_multiplier | 0.0469        |
|    learning_rate         | 0.0003        |
|    loss                  | 72.8          |
|    n_updates             | 120           |
|    policy_gradient_loss  | -0.00682      |
|    std                   | 0.92          |
|    value_loss            | 378           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.7555887] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 14           |
|    time_elapsed          | 221          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.00665757   |
|    clip_fraction         | 0.0667       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.493        |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.036       |
|    lagrangian_multiplier | 0.0761       |
|    learning_rate         | 0.0003       |
|    loss                  | 49.9         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00857     |
|    std                   | 0.984        |
|    value_loss            | 619          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8284192] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 14           |
|    time_elapsed          | 234          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.006153405  |
|    clip_fraction         | 0.0529       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 182          |
|    entropy_loss          | -2.67        |
|    explained_variance    | -0.00522     |
|    lagrangian_multiplier | 0.0504       |
|    learning_rate         | 0.0003       |
|    loss                  | 64.6         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00649     |
|    std                   | 0.918        |
|    value_loss            | 331          |
-------------------------------------------
------------------------------------------
| reward                   | [-0.964184] |
| time/                    |             |
|    fps                   | 129         |
|    iterations            | 15          |
|    time_elapsed          | 237         |
|    total_timesteps       | 30720       |
| train/                   |             |
|    approx_kl             | 0.003299198 |
|    clip_fraction         | 0.0302      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 2.02        |
|    entropy_loss          | -2.8        |
|    explained_variance    | 0.0363      |
|    lagrangian_multiplier | 0.0818      |
|    learning_rate         | 0.0003      |
|    loss                  | 29.2        |
|    n_updates             | 140         |
|    policy_gradient_loss  | -0.0045     |
|    std                   | 0.979       |
|    value_loss            | 338         |
------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.0080535] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 15           |
|    time_elapsed          | 251          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0046799253 |
|    clip_fraction         | 0.0394       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 192          |
|    entropy_loss          | -2.66        |
|    explained_variance    | -0.0271      |
|    lagrangian_multiplier | 0.0552       |
|    learning_rate         | 0.0003       |
|    loss                  | 64.2         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.006       |
|    std                   | 0.914        |
|    value_loss            | 332          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7518644] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 16           |
|    time_elapsed          | 253          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0053428924 |
|    clip_fraction         | 0.0481       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.46         |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0249       |
|    lagrangian_multiplier | 0.0665       |
|    learning_rate         | 0.0003       |
|    loss                  | 40.2         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00676     |
|    std                   | 0.974        |
|    value_loss            | 403          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-1.572997] |
| time/                    |             |
|    fps                   | 24          |
|    iterations            | 13          |
|    time_elapsed          | 1108        |
|    total_timesteps       | 26624       |
| train/                   |             |
|    approx_kl             | 0.004469651 |
|    clip_fraction         | 0.0309      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0235      |
|    entropy_loss          | -3          |
|    explained_variance    | 0.0404      |
|    lagrangian_multiplier | 0.079       |
|    learning_rate         | 0.0003      |
|    loss                  | 37.3        |
|    n_updates             | 120         |
|    policy_gradient_loss  | -0.00405    |
|    std                   | 1.08        |
|    value_loss            | 475         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.1232035] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 16           |
|    time_elapsed          | 268          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0066720834 |
|    clip_fraction         | 0.0541       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 175          |
|    entropy_loss          | -2.66        |
|    explained_variance    | 0.00956      |
|    lagrangian_multiplier | 0.0541       |
|    learning_rate         | 0.0003       |
|    loss                  | 60.7         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00786     |
|    std                   | 0.919        |
|    value_loss            | 321          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.894192] |
| time/                    |             |
|    fps                   | 129         |
|    iterations            | 17          |
|    time_elapsed          | 269         |
|    total_timesteps       | 34816       |
| train/                   |             |
|    approx_kl             | 0.006581154 |
|    clip_fraction         | 0.0575      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 5.75        |
|    entropy_loss          | -2.79       |
|    explained_variance    | -0.00646    |
|    lagrangian_multiplier | 0.0747      |
|    learning_rate         | 0.0003      |
|    loss                  | 24.6        |
|    n_updates             | 160         |
|    policy_gradient_loss  | -0.00812    |
|    std                   | 0.981       |
|    value_loss            | 265         |
------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.9954896] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 17           |
|    time_elapsed          | 285          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.006243121  |
|    clip_fraction         | 0.0634       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 192          |
|    entropy_loss          | -2.68        |
|    explained_variance    | 0.0159       |
|    lagrangian_multiplier | 0.0573       |
|    learning_rate         | 0.0003       |
|    loss                  | 61.8         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00878     |
|    std                   | 0.925        |
|    value_loss            | 358          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4527587] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 18           |
|    time_elapsed          | 285          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.005939245  |
|    clip_fraction         | 0.055        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 10           |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0431       |
|    lagrangian_multiplier | 0.0643       |
|    learning_rate         | 0.0003       |
|    loss                  | 28.3         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00681     |
|    std                   | 0.974        |
|    value_loss            | 339          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6323749] |
| time/                    |              |
|    fps                   | 39           |
|    iterations            | 2            |
|    time_elapsed          | 102          |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0065913456 |
|    clip_fraction         | 0.0568       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 175          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0136       |
|    lagrangian_multiplier | 0.0455       |
|    learning_rate         | 0.0003       |
|    loss                  | 105          |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.007       |
|    std                   | 0.987        |
|    value_loss            | 588          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-2.306377] |
| time/                    |             |
|    fps                   | 129         |
|    iterations            | 19          |
|    time_elapsed          | 301         |
|    total_timesteps       | 38912       |
| train/                   |             |
|    approx_kl             | 0.008014113 |
|    clip_fraction         | 0.0791      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 13.8        |
|    entropy_loss          | -2.79       |
|    explained_variance    | 0.0061      |
|    lagrangian_multiplier | 0.0813      |
|    learning_rate         | 0.0003      |
|    loss                  | 34.4        |
|    n_updates             | 180         |
|    policy_gradient_loss  | -0.00976    |
|    std                   | 0.974       |
|    value_loss            | 399         |
------------------------------------------
-------------------------------------------
| reward                   | [-0.9307961] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 18           |
|    time_elapsed          | 302          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.006972102  |
|    clip_fraction         | 0.066        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 198          |
|    entropy_loss          | -2.7         |
|    explained_variance    | -0.0122      |
|    lagrangian_multiplier | 0.0627       |
|    learning_rate         | 0.0003       |
|    loss                  | 53.7         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00953     |
|    std                   | 0.94         |
|    value_loss            | 374          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.7540209] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 20           |
|    time_elapsed          | 317          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.006675479  |
|    clip_fraction         | 0.0583       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 35.4         |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.00633     |
|    lagrangian_multiplier | 0.0672       |
|    learning_rate         | 0.0003       |
|    loss                  | 56.5         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00906     |
|    std                   | 0.98         |
|    value_loss            | 615          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9142281] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 19           |
|    time_elapsed          | 318          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0054900153 |
|    clip_fraction         | 0.0544       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 186          |
|    entropy_loss          | -2.71        |
|    explained_variance    | -0.0275      |
|    lagrangian_multiplier | 0.0512       |
|    learning_rate         | 0.0003       |
|    loss                  | 66           |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00966     |
|    std                   | 0.941        |
|    value_loss            | 346          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7240062] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 21           |
|    time_elapsed          | 333          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.004208263  |
|    clip_fraction         | 0.0321       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 15           |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0227       |
|    lagrangian_multiplier | 0.0716       |
|    learning_rate         | 0.0003       |
|    loss                  | 43.1         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00558     |
|    std                   | 0.985        |
|    value_loss            | 429          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.313542] |
| time/                    |             |
|    fps                   | 122         |
|    iterations            | 20          |
|    time_elapsed          | 335         |
|    total_timesteps       | 40960       |
| train/                   |             |
|    approx_kl             | 0.007226008 |
|    clip_fraction         | 0.0691      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 212         |
|    entropy_loss          | -2.7        |
|    explained_variance    | 0.0186      |
|    lagrangian_multiplier | 0.0505      |
|    learning_rate         | 0.0003      |
|    loss                  | 80.5        |
|    n_updates             | 190         |
|    policy_gradient_loss  | -0.0108     |
|    std                   | 0.932       |
|    value_loss            | 438         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.1921132] |
| time/                    |              |
|    fps                   | 38           |
|    iterations            | 3            |
|    time_elapsed          | 161          |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.005616546  |
|    clip_fraction         | 0.048        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 96.4         |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0352       |
|    lagrangian_multiplier | 0.0821       |
|    learning_rate         | 0.0003       |
|    loss                  | 61.3         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00596     |
|    std                   | 0.993        |
|    value_loss            | 590          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6766778] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 22           |
|    time_elapsed          | 349          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.005659555  |
|    clip_fraction         | 0.0468       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 7.87         |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.00166      |
|    lagrangian_multiplier | 0.0586       |
|    learning_rate         | 0.0003       |
|    loss                  | 39.7         |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00677     |
|    std                   | 0.975        |
|    value_loss            | 380          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0643376] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 21           |
|    time_elapsed          | 352          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0062047876 |
|    clip_fraction         | 0.0418       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 193          |
|    entropy_loss          | -2.68        |
|    explained_variance    | 0.0513       |
|    lagrangian_multiplier | 0.0597       |
|    learning_rate         | 0.0003       |
|    loss                  | 48.6         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00763     |
|    std                   | 0.916        |
|    value_loss            | 300          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2924061] |
| time/                    |              |
|    fps                   | 45           |
|    iterations            | 4            |
|    time_elapsed          | 178          |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.0054444456 |
|    clip_fraction         | 0.0355       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 168          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.00987      |
|    lagrangian_multiplier | 0.0846       |
|    learning_rate         | 0.0003       |
|    loss                  | 96.8         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00686     |
|    std                   | 0.998        |
|    value_loss            | 1.09e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.47825414] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 23            |
|    time_elapsed          | 365           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.00790134    |
|    clip_fraction         | 0.0729        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 35.6          |
|    entropy_loss          | -2.78         |
|    explained_variance    | -0.00437      |
|    lagrangian_multiplier | 0.0818        |
|    learning_rate         | 0.0003        |
|    loss                  | 46.7          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.011        |
|    std                   | 0.968         |
|    value_loss            | 562           |
--------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.4693344] |
| time/                    |              |
|    fps                   | 23           |
|    iterations            | 14           |
|    time_elapsed          | 1207         |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0053716255 |
|    clip_fraction         | 0.0339       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.12         |
|    entropy_loss          | -2.98        |
|    explained_variance    | 0.024        |
|    lagrangian_multiplier | 0.079        |
|    learning_rate         | 0.0003       |
|    loss                  | 29.3         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00638     |
|    std                   | 1.07         |
|    value_loss            | 353          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.27889243] |
| time/                    |               |
|    fps                   | 122           |
|    iterations            | 22            |
|    time_elapsed          | 369           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.007893919   |
|    clip_fraction         | 0.0679        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 215           |
|    entropy_loss          | -2.67         |
|    explained_variance    | 0.0199        |
|    lagrangian_multiplier | 0.0552        |
|    learning_rate         | 0.0003        |
|    loss                  | 59            |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.0108       |
|    std                   | 0.922         |
|    value_loss            | 342           |
--------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.5020559] |
| time/                    |              |
|    fps                   | 52           |
|    iterations            | 5            |
|    time_elapsed          | 194          |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.004020986  |
|    clip_fraction         | 0.0337       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 183          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0413       |
|    lagrangian_multiplier | 0.0697       |
|    learning_rate         | 0.0003       |
|    loss                  | 140          |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00515     |
|    std                   | 0.988        |
|    value_loss            | 1.54e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.84257454] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 24            |
|    time_elapsed          | 381           |
|    total_timesteps       | 49152         |
| train/                   |               |
|    approx_kl             | 0.0074582687  |
|    clip_fraction         | 0.0791        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 42            |
|    entropy_loss          | -2.77         |
|    explained_variance    | -0.0298       |
|    lagrangian_multiplier | 0.0831        |
|    learning_rate         | 0.0003        |
|    loss                  | 41.3          |
|    n_updates             | 230           |
|    policy_gradient_loss  | -0.01         |
|    std                   | 0.966         |
|    value_loss            | 480           |
--------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8036486] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 23           |
|    time_elapsed          | 385          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.007095906  |
|    clip_fraction         | 0.0571       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 195          |
|    entropy_loss          | -2.66        |
|    explained_variance    | 0.0123       |
|    lagrangian_multiplier | 0.052        |
|    learning_rate         | 0.0003       |
|    loss                  | 77.1         |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 0.908        |
|    value_loss            | 470          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.57004184] |
| time/                    |               |
|    fps                   | 58            |
|    iterations            | 6             |
|    time_elapsed          | 211           |
|    total_timesteps       | 12288         |
| train/                   |               |
|    approx_kl             | 0.0058010714  |
|    clip_fraction         | 0.0493        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 172           |
|    entropy_loss          | -2.83         |
|    explained_variance    | 0.0267        |
|    lagrangian_multiplier | 0.0787        |
|    learning_rate         | 0.0003        |
|    loss                  | 100           |
|    n_updates             | 50            |
|    policy_gradient_loss  | -0.00652      |
|    std                   | 1.01          |
|    value_loss            | 958           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.88352674] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 25            |
|    time_elapsed          | 397           |
|    total_timesteps       | 51200         |
| train/                   |               |
|    approx_kl             | 0.005422959   |
|    clip_fraction         | 0.0409        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 17.5          |
|    entropy_loss          | -2.77         |
|    explained_variance    | 0.0313        |
|    lagrangian_multiplier | 0.0944        |
|    learning_rate         | 0.0003        |
|    loss                  | 67.8          |
|    n_updates             | 240           |
|    policy_gradient_loss  | -0.00688      |
|    std                   | 0.966         |
|    value_loss            | 1.09e+03      |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2350762] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 24           |
|    time_elapsed          | 402          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.003715415  |
|    clip_fraction         | 0.0329       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 188          |
|    entropy_loss          | -2.64        |
|    explained_variance    | 0.0276       |
|    lagrangian_multiplier | 0.0556       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.5         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00599     |
|    std                   | 0.911        |
|    value_loss            | 281          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.0436404] |
| time/                    |              |
|    fps                   | 62           |
|    iterations            | 7            |
|    time_elapsed          | 227          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.006443094  |
|    clip_fraction         | 0.0622       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 156          |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0211       |
|    lagrangian_multiplier | 0.0819       |
|    learning_rate         | 0.0003       |
|    loss                  | 105          |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00791     |
|    std                   | 1.02         |
|    value_loss            | 985          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6260914] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 26           |
|    time_elapsed          | 413          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0066228365 |
|    clip_fraction         | 0.0726       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 25.1         |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0129      |
|    lagrangian_multiplier | 0.0753       |
|    learning_rate         | 0.0003       |
|    loss                  | 81.8         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00779     |
|    std                   | 0.965        |
|    value_loss            | 986          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2062984] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 25           |
|    time_elapsed          | 419          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.0074828975 |
|    clip_fraction         | 0.0684       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 207          |
|    entropy_loss          | -2.63        |
|    explained_variance    | 0.0113       |
|    lagrangian_multiplier | 0.053        |
|    learning_rate         | 0.0003       |
|    loss                  | 67.8         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00907     |
|    std                   | 0.894        |
|    value_loss            | 376          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7975376] |
| time/                    |              |
|    fps                   | 67           |
|    iterations            | 8            |
|    time_elapsed          | 244          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0045894766 |
|    clip_fraction         | 0.0377       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 77.9         |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0348       |
|    lagrangian_multiplier | 0.0652       |
|    learning_rate         | 0.0003       |
|    loss                  | 83.5         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00482     |
|    std                   | 1.03         |
|    value_loss            | 684          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7178422] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 27           |
|    time_elapsed          | 429          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.0069715558 |
|    clip_fraction         | 0.059        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 42.3         |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.00665     |
|    lagrangian_multiplier | 0.0839       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.5         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00888     |
|    std                   | 0.967        |
|    value_loss            | 743          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0471709] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 26           |
|    time_elapsed          | 435          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0076893987 |
|    clip_fraction         | 0.0833       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 201          |
|    entropy_loss          | -2.62        |
|    explained_variance    | -0.00997     |
|    lagrangian_multiplier | 0.0529       |
|    learning_rate         | 0.0003       |
|    loss                  | 82.7         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.012       |
|    std                   | 0.897        |
|    value_loss            | 537          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.32765412] |
| time/                    |               |
|    fps                   | 70            |
|    iterations            | 9             |
|    time_elapsed          | 260           |
|    total_timesteps       | 18432         |
| train/                   |               |
|    approx_kl             | 0.007181434   |
|    clip_fraction         | 0.0637        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 134           |
|    entropy_loss          | -2.92         |
|    explained_variance    | 0.0536        |
|    lagrangian_multiplier | 0.0619        |
|    learning_rate         | 0.0003        |
|    loss                  | 97.2          |
|    n_updates             | 80            |
|    policy_gradient_loss  | -0.00701      |
|    std                   | 1.05          |
|    value_loss            | 760           |
--------------------------------------------
------------------------------------------
| reward                   | [-2.098325] |
| time/                    |             |
|    fps                   | 128         |
|    iterations            | 28          |
|    time_elapsed          | 445         |
|    total_timesteps       | 57344       |
| train/                   |             |
|    approx_kl             | 0.006625943 |
|    clip_fraction         | 0.0583      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 53.5        |
|    entropy_loss          | -2.75       |
|    explained_variance    | -0.0267     |
|    lagrangian_multiplier | 0.0665      |
|    learning_rate         | 0.0003      |
|    loss                  | 120         |
|    n_updates             | 270         |
|    policy_gradient_loss  | -0.0104     |
|    std                   | 0.952       |
|    value_loss            | 1.16e+03    |
------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.3913635] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 27           |
|    time_elapsed          | 452          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.0051154373 |
|    clip_fraction         | 0.0501       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 228          |
|    entropy_loss          | -2.62        |
|    explained_variance    | -0.0417      |
|    lagrangian_multiplier | 0.0573       |
|    learning_rate         | 0.0003       |
|    loss                  | 73.1         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00767     |
|    std                   | 0.897        |
|    value_loss            | 440          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.132887] |
| time/                    |             |
|    fps                   | 128         |
|    iterations            | 29          |
|    time_elapsed          | 461         |
|    total_timesteps       | 59392       |
| train/                   |             |
|    approx_kl             | 0.007483409 |
|    clip_fraction         | 0.0817      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 59.8        |
|    entropy_loss          | -2.74       |
|    explained_variance    | -0.0212     |
|    lagrangian_multiplier | 0.0595      |
|    learning_rate         | 0.0003      |
|    loss                  | 103         |
|    n_updates             | 280         |
|    policy_gradient_loss  | -0.00919    |
|    std                   | 0.954       |
|    value_loss            | 939         |
------------------------------------------
-------------------------------------------
| reward                   | [-2.2872958] |
| time/                    |              |
|    fps                   | 73           |
|    iterations            | 10           |
|    time_elapsed          | 276          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.006301807  |
|    clip_fraction         | 0.0663       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 211          |
|    entropy_loss          | -2.93        |
|    explained_variance    | 0.0293       |
|    lagrangian_multiplier | 0.0644       |
|    learning_rate         | 0.0003       |
|    loss                  | 172          |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00896     |
|    std                   | 1.04         |
|    value_loss            | 1.46e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4005481] |
| time/                    |              |
|    fps                   | 23           |
|    iterations            | 15           |
|    time_elapsed          | 1307         |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0067479163 |
|    clip_fraction         | 0.0644       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0208       |
|    entropy_loss          | -2.96        |
|    explained_variance    | 0.0434       |
|    lagrangian_multiplier | 0.0629       |
|    learning_rate         | 0.0003       |
|    loss                  | 28.3         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00674     |
|    std                   | 1.06         |
|    value_loss            | 316          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.007703] |
| time/                    |             |
|    fps                   | 122         |
|    iterations            | 28          |
|    time_elapsed          | 469         |
|    total_timesteps       | 57344       |
| train/                   |             |
|    approx_kl             | 0.007518985 |
|    clip_fraction         | 0.0729      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 226         |
|    entropy_loss          | -2.6        |
|    explained_variance    | -0.0184     |
|    lagrangian_multiplier | 0.0477      |
|    learning_rate         | 0.0003      |
|    loss                  | 99.5        |
|    n_updates             | 270         |
|    policy_gradient_loss  | -0.0098     |
|    std                   | 0.886       |
|    value_loss            | 557         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.5469873] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 30           |
|    time_elapsed          | 477          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0066741053 |
|    clip_fraction         | 0.0768       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 39.2         |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0275      |
|    lagrangian_multiplier | 0.0626       |
|    learning_rate         | 0.0003       |
|    loss                  | 94.7         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.0121      |
|    std                   | 0.967        |
|    value_loss            | 983          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9584547] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 29           |
|    time_elapsed          | 485          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.006129252  |
|    clip_fraction         | 0.0541       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 222          |
|    entropy_loss          | -2.58        |
|    explained_variance    | -0.0588      |
|    lagrangian_multiplier | 0.0466       |
|    learning_rate         | 0.0003       |
|    loss                  | 70.3         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00817     |
|    std                   | 0.877        |
|    value_loss            | 346          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.0702341] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 31           |
|    time_elapsed          | 493          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.007089197  |
|    clip_fraction         | 0.0952       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 27.9         |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0107      |
|    lagrangian_multiplier | 0.0807       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.9         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.0103      |
|    std                   | 0.972        |
|    value_loss            | 579          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.057595]  |
| time/                    |              |
|    fps                   | 72           |
|    iterations            | 11           |
|    time_elapsed          | 308          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0066075064 |
|    clip_fraction         | 0.0791       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 209          |
|    entropy_loss          | -2.93        |
|    explained_variance    | 0.0463       |
|    lagrangian_multiplier | 0.0703       |
|    learning_rate         | 0.0003       |
|    loss                  | 88.2         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00914     |
|    std                   | 1.04         |
|    value_loss            | 729          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1696097] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 30           |
|    time_elapsed          | 502          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0077918754 |
|    clip_fraction         | 0.0607       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 225          |
|    entropy_loss          | -2.57        |
|    explained_variance    | 0.0186       |
|    lagrangian_multiplier | 0.0439       |
|    learning_rate         | 0.0003       |
|    loss                  | 99.9         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00645     |
|    std                   | 0.874        |
|    value_loss            | 550          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.192183]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 32           |
|    time_elapsed          | 509          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.0067370264 |
|    clip_fraction         | 0.0688       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 33.7         |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0107      |
|    lagrangian_multiplier | 0.0822       |
|    learning_rate         | 0.0003       |
|    loss                  | 70.5         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00894     |
|    std                   | 0.967        |
|    value_loss            | 818          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.3811865] |
| time/                    |              |
|    fps                   | 75           |
|    iterations            | 12           |
|    time_elapsed          | 325          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0058008432 |
|    clip_fraction         | 0.0517       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 220          |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.0311       |
|    lagrangian_multiplier | 0.0748       |
|    learning_rate         | 0.0003       |
|    loss                  | 142          |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00757     |
|    std                   | 1.03         |
|    value_loss            | 1.53e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0645382] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 31           |
|    time_elapsed          | 519          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0064899093 |
|    clip_fraction         | 0.0631       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 239          |
|    entropy_loss          | -2.56        |
|    explained_variance    | 0.00393      |
|    lagrangian_multiplier | 0.0523       |
|    learning_rate         | 0.0003       |
|    loss                  | 92           |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00894     |
|    std                   | 0.871        |
|    value_loss            | 501          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2766622] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 33           |
|    time_elapsed          | 525          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.004722732  |
|    clip_fraction         | 0.0458       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 19.6         |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0248      |
|    lagrangian_multiplier | 0.0672       |
|    learning_rate         | 0.0003       |
|    loss                  | 79           |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00583     |
|    std                   | 0.96         |
|    value_loss            | 766          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.4543674] |
| time/                    |              |
|    fps                   | 77           |
|    iterations            | 13           |
|    time_elapsed          | 341          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.007106096  |
|    clip_fraction         | 0.073        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 263          |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.0283       |
|    lagrangian_multiplier | 0.0826       |
|    learning_rate         | 0.0003       |
|    loss                  | 206          |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00947     |
|    std                   | 1.04         |
|    value_loss            | 2.24e+03     |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-3.21448]  |
| time/                    |             |
|    fps                   | 122         |
|    iterations            | 32          |
|    time_elapsed          | 536         |
|    total_timesteps       | 65536       |
| train/                   |             |
|    approx_kl             | 0.006477926 |
|    clip_fraction         | 0.0688      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 236         |
|    entropy_loss          | -2.55       |
|    explained_variance    | 0.0239      |
|    lagrangian_multiplier | 0.0419      |
|    learning_rate         | 0.0003      |
|    loss                  | 141         |
|    n_updates             | 310         |
|    policy_gradient_loss  | -0.00892    |
|    std                   | 0.868       |
|    value_loss            | 865         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.8679845] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 34           |
|    time_elapsed          | 541          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.00868329   |
|    clip_fraction         | 0.0946       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 20.2         |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0226      |
|    lagrangian_multiplier | 0.0706       |
|    learning_rate         | 0.0003       |
|    loss                  | 60.7         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.0121      |
|    std                   | 0.958        |
|    value_loss            | 657          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.6790783] |
| time/                    |              |
|    fps                   | 80           |
|    iterations            | 14           |
|    time_elapsed          | 358          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0069247507 |
|    clip_fraction         | 0.0811       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 261          |
|    entropy_loss          | -2.93        |
|    explained_variance    | 0.0326       |
|    lagrangian_multiplier | 0.0807       |
|    learning_rate         | 0.0003       |
|    loss                  | 215          |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00947     |
|    std                   | 1.05         |
|    value_loss            | 2.54e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.6040952] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 33           |
|    time_elapsed          | 553          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.006791601  |
|    clip_fraction         | 0.085        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 252          |
|    entropy_loss          | -2.55        |
|    explained_variance    | -0.00242     |
|    lagrangian_multiplier | 0.0533       |
|    learning_rate         | 0.0003       |
|    loss                  | 137          |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00943     |
|    std                   | 0.865        |
|    value_loss            | 897          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2876091] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 35           |
|    time_elapsed          | 556          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.008273322  |
|    clip_fraction         | 0.0686       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 11.7         |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.00381     |
|    lagrangian_multiplier | 0.0681       |
|    learning_rate         | 0.0003       |
|    loss                  | 63.9         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00979     |
|    std                   | 0.96         |
|    value_loss            | 626          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.9055848] |
| time/                    |              |
|    fps                   | 82           |
|    iterations            | 15           |
|    time_elapsed          | 374          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0076371017 |
|    clip_fraction         | 0.0627       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 246          |
|    entropy_loss          | -2.92        |
|    explained_variance    | 0.0184       |
|    lagrangian_multiplier | 0.0703       |
|    learning_rate         | 0.0003       |
|    loss                  | 215          |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00974     |
|    std                   | 1.03         |
|    value_loss            | 2.23e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.88800913] |
| time/                    |               |
|    fps                   | 23            |
|    iterations            | 16            |
|    time_elapsed          | 1407          |
|    total_timesteps       | 32768         |
| train/                   |               |
|    approx_kl             | 0.0076725674  |
|    clip_fraction         | 0.0773        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0145        |
|    entropy_loss          | -2.96         |
|    explained_variance    | 0.0577        |
|    lagrangian_multiplier | 0.0663        |
|    learning_rate         | 0.0003        |
|    loss                  | 27.5          |
|    n_updates             | 150           |
|    policy_gradient_loss  | -0.0075       |
|    std                   | 1.06          |
|    value_loss            | 246           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3359398] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 34           |
|    time_elapsed          | 570          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0070712045 |
|    clip_fraction         | 0.0615       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 247          |
|    entropy_loss          | -2.56        |
|    explained_variance    | 0.00814      |
|    lagrangian_multiplier | 0.05         |
|    learning_rate         | 0.0003       |
|    loss                  | 216          |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00849     |
|    std                   | 0.876        |
|    value_loss            | 1.34e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6968473] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 36           |
|    time_elapsed          | 572          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0065954104 |
|    clip_fraction         | 0.0554       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 26.2         |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0177      |
|    lagrangian_multiplier | 0.07         |
|    learning_rate         | 0.0003       |
|    loss                  | 69.5         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00753     |
|    std                   | 0.954        |
|    value_loss            | 762          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-3.3480713] |
| time/                    |              |
|    fps                   | 83           |
|    iterations            | 16           |
|    time_elapsed          | 390          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.006731917  |
|    clip_fraction         | 0.0549       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 255          |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.0207       |
|    lagrangian_multiplier | 0.0795       |
|    learning_rate         | 0.0003       |
|    loss                  | 216          |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00761     |
|    std                   | 1.03         |
|    value_loss            | 2.27e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6162081] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 35           |
|    time_elapsed          | 587          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.007343074  |
|    clip_fraction         | 0.0754       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 219          |
|    entropy_loss          | -2.56        |
|    explained_variance    | 0.0162       |
|    lagrangian_multiplier | 0.0531       |
|    learning_rate         | 0.0003       |
|    loss                  | 117          |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00983     |
|    std                   | 0.867        |
|    value_loss            | 1e+03        |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2221498] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 37           |
|    time_elapsed          | 588          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.008152736  |
|    clip_fraction         | 0.0848       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 32.2         |
|    entropy_loss          | -2.75        |
|    explained_variance    | 0.0104       |
|    lagrangian_multiplier | 0.0708       |
|    learning_rate         | 0.0003       |
|    loss                  | 50.6         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.96         |
|    value_loss            | 518          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.72051]   |
| time/                    |              |
|    fps                   | 85           |
|    iterations            | 17           |
|    time_elapsed          | 406          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0062655974 |
|    clip_fraction         | 0.0554       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 228          |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.0366       |
|    lagrangian_multiplier | 0.0745       |
|    learning_rate         | 0.0003       |
|    loss                  | 226          |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.0074      |
|    std                   | 1.05         |
|    value_loss            | 2.16e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0371506] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 36           |
|    time_elapsed          | 603          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0074896934 |
|    clip_fraction         | 0.0789       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 227          |
|    entropy_loss          | -2.55        |
|    explained_variance    | 0.00604      |
|    lagrangian_multiplier | 0.0595       |
|    learning_rate         | 0.0003       |
|    loss                  | 75.2         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0103      |
|    std                   | 0.871        |
|    value_loss            | 493          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2676495] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 38           |
|    time_elapsed          | 604          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0085807545 |
|    clip_fraction         | 0.0946       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 42           |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.00224     |
|    lagrangian_multiplier | 0.08         |
|    learning_rate         | 0.0003       |
|    loss                  | 44.4         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.0131      |
|    std                   | 0.966        |
|    value_loss            | 599          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.3461745] |
| time/                    |              |
|    fps                   | 87           |
|    iterations            | 18           |
|    time_elapsed          | 423          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.00526237   |
|    clip_fraction         | 0.0555       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 241          |
|    entropy_loss          | -2.93        |
|    explained_variance    | 0.0336       |
|    lagrangian_multiplier | 0.0787       |
|    learning_rate         | 0.0003       |
|    loss                  | 215          |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00739     |
|    std                   | 1.05         |
|    value_loss            | 2.33e+03     |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-3.1985204] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 37           |
|    time_elapsed          | 620          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.0070328647 |
|    clip_fraction         | 0.0645       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 248          |
|    entropy_loss          | -2.54        |
|    explained_variance    | -0.0279      |
|    lagrangian_multiplier | 0.0456       |
|    learning_rate         | 0.0003       |
|    loss                  | 110          |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.0099      |
|    std                   | 0.857        |
|    value_loss            | 591          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1730486] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 39           |
|    time_elapsed          | 620          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0060296236 |
|    clip_fraction         | 0.0473       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 39           |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0143      |
|    lagrangian_multiplier | 0.0754       |
|    learning_rate         | 0.0003       |
|    loss                  | 43.4         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00621     |
|    std                   | 0.961        |
|    value_loss            | 499          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.8320441] |
| time/                    |              |
|    fps                   | 88           |
|    iterations            | 19           |
|    time_elapsed          | 439          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0075761843 |
|    clip_fraction         | 0.0597       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 268          |
|    entropy_loss          | -2.94        |
|    explained_variance    | 0.0327       |
|    lagrangian_multiplier | 0.0771       |
|    learning_rate         | 0.0003       |
|    loss                  | 190          |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00732     |
|    std                   | 1.05         |
|    value_loss            | 2.18e+03     |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1166878] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 40           |
|    time_elapsed          | 636          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.005378506  |
|    clip_fraction         | 0.0545       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 46.4         |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.00117     |
|    lagrangian_multiplier | 0.0906       |
|    learning_rate         | 0.0003       |
|    loss                  | 27.1         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00582     |
|    std                   | 0.954        |
|    value_loss            | 338          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.0532935] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 38           |
|    time_elapsed          | 637          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.008086775  |
|    clip_fraction         | 0.0633       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 255          |
|    entropy_loss          | -2.51        |
|    explained_variance    | 0.0153       |
|    lagrangian_multiplier | 0.0401       |
|    learning_rate         | 0.0003       |
|    loss                  | 227          |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00997     |
|    std                   | 0.841        |
|    value_loss            | 1.3e+03      |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.6667094] |
| time/                    |              |
|    fps                   | 89           |
|    iterations            | 20           |
|    time_elapsed          | 456          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.007257081  |
|    clip_fraction         | 0.0894       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 233          |
|    entropy_loss          | -2.94        |
|    explained_variance    | 0.0126       |
|    lagrangian_multiplier | 0.0781       |
|    learning_rate         | 0.0003       |
|    loss                  | 170          |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.0103      |
|    std                   | 1.05         |
|    value_loss            | 1.68e+03     |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2614759] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 41           |
|    time_elapsed          | 652          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.008283243  |
|    clip_fraction         | 0.0918       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 19.3         |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.0438       |
|    lagrangian_multiplier | 0.104        |
|    learning_rate         | 0.0003       |
|    loss                  | 24.5         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.0104      |
|    std                   | 0.953        |
|    value_loss            | 402          |
-------------------------------------------
------------------------------------------
| reward                   | [-3.161404] |
| time/                    |             |
|    fps                   | 122         |
|    iterations            | 39          |
|    time_elapsed          | 653         |
|    total_timesteps       | 79872       |
| train/                   |             |
|    approx_kl             | 0.008073255 |
|    clip_fraction         | 0.099       |
|    clip_range            | 0.2         |
|    cost_value_loss       | 243         |
|    entropy_loss          | -2.49       |
|    explained_variance    | -0.00479    |
|    lagrangian_multiplier | 0.046       |
|    learning_rate         | 0.0003      |
|    loss                  | 161         |
|    n_updates             | 380         |
|    policy_gradient_loss  | -0.0128     |
|    std                   | 0.839       |
|    value_loss            | 976         |
------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2191029] |
| time/                    |              |
|    fps                   | 23           |
|    iterations            | 17           |
|    time_elapsed          | 1496         |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0050172447 |
|    clip_fraction         | 0.0518       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00599      |
|    entropy_loss          | -2.98        |
|    explained_variance    | 0.0881       |
|    lagrangian_multiplier | 0.0667       |
|    learning_rate         | 0.0003       |
|    loss                  | 10.3         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00768     |
|    std                   | 1.08         |
|    value_loss            | 110          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.4490483] |
| time/                    |              |
|    fps                   | 91           |
|    iterations            | 21           |
|    time_elapsed          | 472          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.00705271   |
|    clip_fraction         | 0.0552       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 179          |
|    entropy_loss          | -2.92        |
|    explained_variance    | 0.0474       |
|    lagrangian_multiplier | 0.0817       |
|    learning_rate         | 0.0003       |
|    loss                  | 145          |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00849     |
|    std                   | 1.03         |
|    value_loss            | 1.69e+03     |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.8214086] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 42           |
|    time_elapsed          | 668          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.006104027  |
|    clip_fraction         | 0.053        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 22.3         |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.00419      |
|    lagrangian_multiplier | 0.0763       |
|    learning_rate         | 0.0003       |
|    loss                  | 29.3         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00646     |
|    std                   | 0.956        |
|    value_loss            | 333          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.1255689] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 40           |
|    time_elapsed          | 670          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.0068023014 |
|    clip_fraction         | 0.0709       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 228          |
|    entropy_loss          | -2.49        |
|    explained_variance    | 7.34e-05     |
|    lagrangian_multiplier | 0.0533       |
|    learning_rate         | 0.0003       |
|    loss                  | 123          |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.0103      |
|    std                   | 0.842        |
|    value_loss            | 760          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.55471355] |
| time/                    |               |
|    fps                   | 92            |
|    iterations            | 22            |
|    time_elapsed          | 488           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.008593071   |
|    clip_fraction         | 0.081         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 234           |
|    entropy_loss          | -2.9          |
|    explained_variance    | 0.0309        |
|    lagrangian_multiplier | 0.0798        |
|    learning_rate         | 0.0003        |
|    loss                  | 203           |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.0105       |
|    std                   | 1.03          |
|    value_loss            | 2.17e+03      |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2442422] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 43           |
|    time_elapsed          | 684          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.009180391  |
|    clip_fraction         | 0.105        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 37.7         |
|    entropy_loss          | -2.74        |
|    explained_variance    | -0.00235     |
|    lagrangian_multiplier | 0.0849       |
|    learning_rate         | 0.0003       |
|    loss                  | 31.8         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.0083      |
|    std                   | 0.952        |
|    value_loss            | 374          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.84511167] |
| time/                    |               |
|    fps                   | 122           |
|    iterations            | 41            |
|    time_elapsed          | 687           |
|    total_timesteps       | 83968         |
| train/                   |               |
|    approx_kl             | 0.0067008417  |
|    clip_fraction         | 0.0586        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 235           |
|    entropy_loss          | -2.49         |
|    explained_variance    | -0.0196       |
|    lagrangian_multiplier | 0.0547        |
|    learning_rate         | 0.0003        |
|    loss                  | 104           |
|    n_updates             | 400           |
|    policy_gradient_loss  | -0.00919      |
|    std                   | 0.844         |
|    value_loss            | 714           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.6690021] |
| time/                    |              |
|    fps                   | 93           |
|    iterations            | 23           |
|    time_elapsed          | 505          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.0064805862 |
|    clip_fraction         | 0.0555       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 256          |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.0282       |
|    lagrangian_multiplier | 0.0841       |
|    learning_rate         | 0.0003       |
|    loss                  | 195          |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.00826     |
|    std                   | 1.03         |
|    value_loss            | 2.23e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.128309]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 44           |
|    time_elapsed          | 700          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.0064493786 |
|    clip_fraction         | 0.075        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 11.9         |
|    entropy_loss          | -2.74        |
|    explained_variance    | -0.00653     |
|    lagrangian_multiplier | 0.0841       |
|    learning_rate         | 0.0003       |
|    loss                  | 23.1         |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.0085      |
|    std                   | 0.957        |
|    value_loss            | 302          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.1097445] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 42           |
|    time_elapsed          | 704          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0070572915 |
|    clip_fraction         | 0.0682       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 247          |
|    entropy_loss          | -2.5         |
|    explained_variance    | 0.01         |
|    lagrangian_multiplier | 0.0378       |
|    learning_rate         | 0.0003       |
|    loss                  | 222          |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00985     |
|    std                   | 0.846        |
|    value_loss            | 1.16e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0979844] |
| time/                    |              |
|    fps                   | 94           |
|    iterations            | 24           |
|    time_elapsed          | 521          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.0057643075 |
|    clip_fraction         | 0.0598       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 255          |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.024        |
|    lagrangian_multiplier | 0.0757       |
|    learning_rate         | 0.0003       |
|    loss                  | 202          |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00589     |
|    std                   | 1.03         |
|    value_loss            | 1.95e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.63729626] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 45            |
|    time_elapsed          | 716           |
|    total_timesteps       | 92160         |
| train/                   |               |
|    approx_kl             | 0.007987287   |
|    clip_fraction         | 0.0748        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 20.2          |
|    entropy_loss          | -2.74         |
|    explained_variance    | 0.00772       |
|    lagrangian_multiplier | 0.0729        |
|    learning_rate         | 0.0003        |
|    loss                  | 22.4          |
|    n_updates             | 440           |
|    policy_gradient_loss  | -0.00851      |
|    std                   | 0.947         |
|    value_loss            | 230           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.6878414] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 43           |
|    time_elapsed          | 720          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.008675748  |
|    clip_fraction         | 0.0798       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 258          |
|    entropy_loss          | -2.5         |
|    explained_variance    | 0.00838      |
|    lagrangian_multiplier | 0.044        |
|    learning_rate         | 0.0003       |
|    loss                  | 211          |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.0114      |
|    std                   | 0.842        |
|    value_loss            | 1.31e+03     |
-------------------------------------------
------------------------------------------
| reward                   | [-1.574808] |
| time/                    |             |
|    fps                   | 95          |
|    iterations            | 25          |
|    time_elapsed          | 537         |
|    total_timesteps       | 51200       |
| train/                   |             |
|    approx_kl             | 0.008284376 |
|    clip_fraction         | 0.0912      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 252         |
|    entropy_loss          | -2.9        |
|    explained_variance    | 0.0405      |
|    lagrangian_multiplier | 0.0763      |
|    learning_rate         | 0.0003      |
|    loss                  | 170         |
|    n_updates             | 240         |
|    policy_gradient_loss  | -0.00818    |
|    std                   | 1.03        |
|    value_loss            | 1.96e+03    |
------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.635817]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 46           |
|    time_elapsed          | 732          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0077203442 |
|    clip_fraction         | 0.102        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 23           |
|    entropy_loss          | -2.73        |
|    explained_variance    | 0.0115       |
|    lagrangian_multiplier | 0.0898       |
|    learning_rate         | 0.0003       |
|    loss                  | 23.1         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 0.946        |
|    value_loss            | 301          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5298546] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 44           |
|    time_elapsed          | 737          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.0087081455 |
|    clip_fraction         | 0.0918       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 239          |
|    entropy_loss          | -2.49        |
|    explained_variance    | 0.025        |
|    lagrangian_multiplier | 0.0428       |
|    learning_rate         | 0.0003       |
|    loss                  | 117          |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.0121      |
|    std                   | 0.837        |
|    value_loss            | 638          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.145731]  |
| time/                    |              |
|    fps                   | 23           |
|    iterations            | 18           |
|    time_elapsed          | 1584         |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0052579846 |
|    clip_fraction         | 0.0506       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0051       |
|    entropy_loss          | -2.99        |
|    explained_variance    | -0.00412     |
|    lagrangian_multiplier | 0.0805       |
|    learning_rate         | 0.0003       |
|    loss                  | 14.1         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00586     |
|    std                   | 1.08         |
|    value_loss            | 170          |
-------------------------------------------
------------------------------------------
| reward                   | [-0.937364] |
| time/                    |             |
|    fps                   | 128         |
|    iterations            | 47          |
|    time_elapsed          | 747         |
|    total_timesteps       | 96256       |
| train/                   |             |
|    approx_kl             | 0.006160655 |
|    clip_fraction         | 0.0666      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 30.6        |
|    entropy_loss          | -2.75       |
|    explained_variance    | -0.0192     |
|    lagrangian_multiplier | 0.0818      |
|    learning_rate         | 0.0003      |
|    loss                  | 57.8        |
|    n_updates             | 460         |
|    policy_gradient_loss  | -0.0121     |
|    std                   | 0.964       |
|    value_loss            | 640         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.4645617] |
| time/                    |              |
|    fps                   | 93           |
|    iterations            | 26           |
|    time_elapsed          | 568          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0062037776 |
|    clip_fraction         | 0.0543       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 250          |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.0428       |
|    lagrangian_multiplier | 0.0755       |
|    learning_rate         | 0.0003       |
|    loss                  | 210          |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00595     |
|    std                   | 1.03         |
|    value_loss            | 2.25e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.80251426] |
| time/                    |               |
|    fps                   | 122           |
|    iterations            | 45            |
|    time_elapsed          | 754           |
|    total_timesteps       | 92160         |
| train/                   |               |
|    approx_kl             | 0.008424754   |
|    clip_fraction         | 0.0801        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 262           |
|    entropy_loss          | -2.47         |
|    explained_variance    | 0.00665       |
|    lagrangian_multiplier | 0.0369        |
|    learning_rate         | 0.0003        |
|    loss                  | 307           |
|    n_updates             | 440           |
|    policy_gradient_loss  | -0.0116       |
|    std                   | 0.833         |
|    value_loss            | 1.78e+03      |
--------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.8379825] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 48           |
|    time_elapsed          | 764          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.007638611  |
|    clip_fraction         | 0.0818       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 52.8         |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0162      |
|    lagrangian_multiplier | 0.074        |
|    learning_rate         | 0.0003       |
|    loss                  | 101          |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.0108      |
|    std                   | 0.968        |
|    value_loss            | 885          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-2.359373] |
| time/                    |             |
|    fps                   | 94          |
|    iterations            | 27          |
|    time_elapsed          | 585         |
|    total_timesteps       | 55296       |
| train/                   |             |
|    approx_kl             | 0.007213408 |
|    clip_fraction         | 0.0554      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 235         |
|    entropy_loss          | -2.9        |
|    explained_variance    | 0.0373      |
|    lagrangian_multiplier | 0.0823      |
|    learning_rate         | 0.0003      |
|    loss                  | 214         |
|    n_updates             | 260         |
|    policy_gradient_loss  | -0.0068     |
|    std                   | 1.03        |
|    value_loss            | 2.52e+03    |
------------------------------------------
-------------------------------------------
| reward                   | [-1.0542474] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 46           |
|    time_elapsed          | 770          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.007392872  |
|    clip_fraction         | 0.0973       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 246          |
|    entropy_loss          | -2.46        |
|    explained_variance    | 0.0112       |
|    lagrangian_multiplier | 0.0462       |
|    learning_rate         | 0.0003       |
|    loss                  | 96.6         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.0119      |
|    std                   | 0.822        |
|    value_loss            | 536          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8920625] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 49           |
|    time_elapsed          | 780          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.008256993  |
|    clip_fraction         | 0.0752       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 29.4         |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.00945     |
|    lagrangian_multiplier | 0.079        |
|    learning_rate         | 0.0003       |
|    loss                  | 22.6         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.00874     |
|    std                   | 0.973        |
|    value_loss            | 266          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 103, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 90, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñà‚ñÖ‚ñÖ‚ñá‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÖ‚ñÜ
wandb:             train/approx_kl ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñÇ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñá‚ñÖ‚ñá‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñà‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñá
wandb:         train/clip_fraction ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñá‚ñÉ‚ñÑ‚ñÉ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÜ‚ñÉ‚ñÑ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñà‚ñÖ
wandb:          train/entropy_loss ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ
wandb:    train/explained_variance ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÇ‚ñÜ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñá‚ñá‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñá‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÑ
wandb: train/lagrangian_multiplier ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÇ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÖ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñà‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñá‚ñÅ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñá‚ñà‚ñá‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÜ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñá‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñÑ
wandb:                   train/std ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÑ
wandb:            train/value_loss ‚ñÇ‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñà‚ñá‚ñÖ‚ñà‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÜ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                      reward -0.89206
wandb:             train/approx_kl 0.00826
wandb:         train/clip_fraction 0.07524
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 29.39954
wandb:          train/entropy_loss -2.77717
wandb:    train/explained_variance -0.00945
wandb: train/lagrangian_multiplier 0.07901
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 22.57004
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00874
wandb:                   train/std 0.97287
wandb:            train/value_loss 266.31929
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/pk5hyaxd
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231223_002344-pk5hyaxd/logs
-------------------------------------------
| reward                   | [-2.4296746] |
| time/                    |              |
|    fps                   | 95           |
|    iterations            | 28           |
|    time_elapsed          | 601          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.0072920364 |
|    clip_fraction         | 0.0779       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 240          |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.0227       |
|    lagrangian_multiplier | 0.076        |
|    learning_rate         | 0.0003       |
|    loss                  | 190          |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00852     |
|    std                   | 1.03         |
|    value_loss            | 2.1e+03      |
-------------------------------------------
srun: error: dqn.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-1.4012018] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 47           |
|    time_elapsed          | 787          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.008960124  |
|    clip_fraction         | 0.0977       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 265          |
|    entropy_loss          | -2.43        |
|    explained_variance    | 0.000268     |
|    lagrangian_multiplier | 0.0347       |
|    learning_rate         | 0.0003       |
|    loss                  | 430          |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.00925     |
|    std                   | 0.815        |
|    value_loss            | 2.13e+03     |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114474.7
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231223_003703-1o43c241
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/1o43c241
-------------------------------------------
| reward                   | [-2.7233167] |
| time/                    |              |
|    fps                   | 96           |
|    iterations            | 29           |
|    time_elapsed          | 617          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.007012459  |
|    clip_fraction         | 0.0602       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 236          |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.0424       |
|    lagrangian_multiplier | 0.0743       |
|    learning_rate         | 0.0003       |
|    loss                  | 239          |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00723     |
|    std                   | 1.04         |
|    value_loss            | 2.61e+03     |
-------------------------------------------
------------------------------------------
| reward                   | [-1.635166] |
| time/                    |             |
|    fps                   | 122         |
|    iterations            | 48          |
|    time_elapsed          | 803         |
|    total_timesteps       | 98304       |
| train/                   |             |
|    approx_kl             | 0.010150325 |
|    clip_fraction         | 0.0958      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 266         |
|    entropy_loss          | -2.44       |
|    explained_variance    | -0.00017    |
|    lagrangian_multiplier | 0.0365      |
|    learning_rate         | 0.0003      |
|    loss                  | 307         |
|    n_updates             | 470         |
|    policy_gradient_loss  | -0.00937    |
|    std                   | 0.823       |
|    value_loss            | 1.9e+03     |
------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.58009666] |
| time/              |               |
|    fps             | 137           |
|    iterations      | 1             |
|    time_elapsed    | 14            |
|    total_timesteps | 2048          |
--------------------------------------
-------------------------------------------
| reward                   | [-2.6302438] |
| time/                    |              |
|    fps                   | 96           |
|    iterations            | 30           |
|    time_elapsed          | 634          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.007336097  |
|    clip_fraction         | 0.0634       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 250          |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.0293       |
|    lagrangian_multiplier | 0.0761       |
|    learning_rate         | 0.0003       |
|    loss                  | 200          |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00802     |
|    std                   | 1.04         |
|    value_loss            | 2.12e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7230784] |
| time/                    |              |
|    fps                   | 122          |
|    iterations            | 49           |
|    time_elapsed          | 820          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.0099318465 |
|    clip_fraction         | 0.0961       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 266          |
|    entropy_loss          | -2.45        |
|    explained_variance    | -8.52e-05    |
|    lagrangian_multiplier | 0.0398       |
|    learning_rate         | 0.0003       |
|    loss                  | 252          |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 0.828        |
|    value_loss            | 1.75e+03     |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 103, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 90, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñÅ‚ñÖ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ
wandb:             train/approx_kl ‚ñÖ‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñà
wandb:         train/clip_fraction ‚ñÖ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñà‚ñà‚ñà
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:          train/entropy_loss ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:    train/explained_variance ‚ñÑ‚ñÇ‚ñá‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÅ‚ñÇ‚ñà‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÇ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ
wandb: train/lagrangian_multiplier ‚ñÇ‚ñÇ‚ñá‚ñà‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñà‚ñÜ‚ñÖ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÖ‚ñá‚ñà‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñá‚ñà‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÉ
wandb:                   train/std ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:            train/value_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñá‚ñà‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:                      reward -1.72308
wandb:             train/approx_kl 0.00993
wandb:         train/clip_fraction 0.09609
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 266.0869
wandb:          train/entropy_loss -2.45325
wandb:    train/explained_variance -9e-05
wandb: train/lagrangian_multiplier 0.03977
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 252.34261
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.01013
wandb:                   train/std 0.82836
wandb:            train/value_loss 1750.15046
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/b8ocxbji
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231223_002344-b8ocxbji/logs
srun: error: ddpg.ist.berkeley.edu: task 0: Exited with exit code 1
--------------------------------------------
| reward                   | [-0.52578706] |
| time/                    |               |
|    fps                   | 133           |
|    iterations            | 2             |
|    time_elapsed          | 30            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.0057649603  |
|    clip_fraction         | 0.0406        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0296        |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.0299       |
|    lagrangian_multiplier | 0.0555        |
|    learning_rate         | 0.0003        |
|    loss                  | 79.2          |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00583      |
|    std                   | 1.01          |
|    value_loss            | 625           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.9039557] |
| time/                    |              |
|    fps                   | 23           |
|    iterations            | 19           |
|    time_elapsed          | 1671         |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.004090063  |
|    clip_fraction         | 0.0421       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00515      |
|    entropy_loss          | -2.99        |
|    explained_variance    | 0.0809       |
|    lagrangian_multiplier | 0.0803       |
|    learning_rate         | 0.0003       |
|    loss                  | 30.7         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00326     |
|    std                   | 1.07         |
|    value_loss            | 386          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114474.8
-------------------------------------------
| reward                   | [-2.8982882] |
| time/                    |              |
|    fps                   | 97           |
|    iterations            | 31           |
|    time_elapsed          | 650          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0077464217 |
|    clip_fraction         | 0.0826       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 220          |
|    entropy_loss          | -2.92        |
|    explained_variance    | 0.057        |
|    lagrangian_multiplier | 0.0748       |
|    learning_rate         | 0.0003       |
|    loss                  | 193          |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00851     |
|    std                   | 1.04         |
|    value_loss            | 1.97e+03     |
-------------------------------------------
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231223_003742-387kdty3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/387kdty3
-------------------------------------------
| reward                   | [-0.3440384] |
| time/                    |              |
|    fps                   | 132          |
|    iterations            | 3            |
|    time_elapsed          | 46           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0030015558 |
|    clip_fraction         | 0.0223       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.12         |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.019        |
|    lagrangian_multiplier | 0.0416       |
|    learning_rate         | 0.0003       |
|    loss                  | 106          |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00302     |
|    std                   | 1.02         |
|    value_loss            | 829          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.399411]  |
| time/                    |              |
|    fps                   | 98           |
|    iterations            | 32           |
|    time_elapsed          | 667          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.0064675827 |
|    clip_fraction         | 0.0678       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 250          |
|    entropy_loss          | -2.92        |
|    explained_variance    | 0.0526       |
|    lagrangian_multiplier | 0.0789       |
|    learning_rate         | 0.0003       |
|    loss                  | 185          |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.0103      |
|    std                   | 1.04         |
|    value_loss            | 1.9e+03      |
-------------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.4431752] |
| time/              |              |
|    fps             | 133          |
|    iterations      | 1            |
|    time_elapsed    | 15           |
|    total_timesteps | 2048         |
-------------------------------------
--------------------------------------------
| reward                   | [-0.49442747] |
| time/                    |               |
|    fps                   | 131           |
|    iterations            | 4             |
|    time_elapsed          | 62            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.005758869   |
|    clip_fraction         | 0.0507        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.985         |
|    entropy_loss          | -2.87         |
|    explained_variance    | -0.0411       |
|    lagrangian_multiplier | 0.0603        |
|    learning_rate         | 0.0003        |
|    loss                  | 42.5          |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.0054       |
|    std                   | 1.01          |
|    value_loss            | 434           |
--------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.6537554] |
| time/                    |              |
|    fps                   | 98           |
|    iterations            | 33           |
|    time_elapsed          | 683          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.005511472  |
|    clip_fraction         | 0.0656       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 225          |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.0298       |
|    lagrangian_multiplier | 0.0733       |
|    learning_rate         | 0.0003       |
|    loss                  | 175          |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00754     |
|    std                   | 1.03         |
|    value_loss            | 1.75e+03     |
-------------------------------------------
------------------------------------------
| reward                   | [-0.759931] |
| time/                    |             |
|    fps                   | 129         |
|    iterations            | 2           |
|    time_elapsed          | 31          |
|    total_timesteps       | 4096        |
| train/                   |             |
|    approx_kl             | 0.005369781 |
|    clip_fraction         | 0.0383      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 56.8        |
|    entropy_loss          | -2.86       |
|    explained_variance    | 0.0415      |
|    lagrangian_multiplier | 0.085       |
|    learning_rate         | 0.0003      |
|    loss                  | 34.1        |
|    n_updates             | 10          |
|    policy_gradient_loss  | -0.00541    |
|    std                   | 1.02        |
|    value_loss            | 355         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.90048796] |
| time/                    |               |
|    fps                   | 131           |
|    iterations            | 5             |
|    time_elapsed          | 78            |
|    total_timesteps       | 10240         |
| train/                   |               |
|    approx_kl             | 0.005426323   |
|    clip_fraction         | 0.0447        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.119         |
|    entropy_loss          | -2.86         |
|    explained_variance    | -0.00272      |
|    lagrangian_multiplier | 0.0525        |
|    learning_rate         | 0.0003        |
|    loss                  | 60.3          |
|    n_updates             | 40            |
|    policy_gradient_loss  | -0.00503      |
|    std                   | 1.01          |
|    value_loss            | 644           |
--------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.4555938] |
| time/                    |              |
|    fps                   | 99           |
|    iterations            | 34           |
|    time_elapsed          | 699          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0061853277 |
|    clip_fraction         | 0.0618       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 223          |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.0417       |
|    lagrangian_multiplier | 0.0624       |
|    learning_rate         | 0.0003       |
|    loss                  | 226          |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00697     |
|    std                   | 1.03         |
|    value_loss            | 1.87e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.81121755] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 3             |
|    time_elapsed          | 48            |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.00741649    |
|    clip_fraction         | 0.0681        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 47.3          |
|    entropy_loss          | -2.87         |
|    explained_variance    | 0.0261        |
|    lagrangian_multiplier | 0.0479        |
|    learning_rate         | 0.0003        |
|    loss                  | 76.4          |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00868      |
|    std                   | 1.01          |
|    value_loss            | 560           |
--------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-1.490613] |
| time/                    |             |
|    fps                   | 130         |
|    iterations            | 6           |
|    time_elapsed          | 93          |
|    total_timesteps       | 12288       |
| train/                   |             |
|    approx_kl             | 0.004653777 |
|    clip_fraction         | 0.0363      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 3.5         |
|    entropy_loss          | -2.85       |
|    explained_variance    | 0.0221      |
|    lagrangian_multiplier | 0.0574      |
|    learning_rate         | 0.0003      |
|    loss                  | 75.1        |
|    n_updates             | 50          |
|    policy_gradient_loss  | -0.00584    |
|    std                   | 1           |
|    value_loss            | 589         |
------------------------------------------
-------------------------------------------
| reward                   | [-2.5260558] |
| time/                    |              |
|    fps                   | 100          |
|    iterations            | 35           |
|    time_elapsed          | 716          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.008368645  |
|    clip_fraction         | 0.0667       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 233          |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.0417       |
|    lagrangian_multiplier | 0.0634       |
|    learning_rate         | 0.0003       |
|    loss                  | 193          |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 1.05         |
|    value_loss            | 2.11e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.74913925] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 4             |
|    time_elapsed          | 64            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.0054952083  |
|    clip_fraction         | 0.0482        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 2.76          |
|    entropy_loss          | -2.86         |
|    explained_variance    | -0.0146       |
|    lagrangian_multiplier | 0.0425        |
|    learning_rate         | 0.0003        |
|    loss                  | 56.1          |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00622      |
|    std                   | 1.01          |
|    value_loss            | 425           |
--------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.98677784] |
| time/                    |               |
|    fps                   | 130           |
|    iterations            | 7             |
|    time_elapsed          | 109           |
|    total_timesteps       | 14336         |
| train/                   |               |
|    approx_kl             | 0.00506158    |
|    clip_fraction         | 0.0441        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 5.51          |
|    entropy_loss          | -2.85         |
|    explained_variance    | 6.47e-05      |
|    lagrangian_multiplier | 0.0721        |
|    learning_rate         | 0.0003        |
|    loss                  | 43.6          |
|    n_updates             | 60            |
|    policy_gradient_loss  | -0.00725      |
|    std                   | 1.01          |
|    value_loss            | 546           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.4027356] |
| time/                    |              |
|    fps                   | 23           |
|    iterations            | 20           |
|    time_elapsed          | 1757         |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0043986393 |
|    clip_fraction         | 0.0375       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00542      |
|    entropy_loss          | -2.97        |
|    explained_variance    | 0.043        |
|    lagrangian_multiplier | 0.0773       |
|    learning_rate         | 0.0003       |
|    loss                  | 19.7         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00514     |
|    std                   | 1.06         |
|    value_loss            | 213          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.6056316] |
| time/                    |              |
|    fps                   | 100          |
|    iterations            | 36           |
|    time_elapsed          | 732          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0055547813 |
|    clip_fraction         | 0.0557       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 230          |
|    entropy_loss          | -2.94        |
|    explained_variance    | 0.0341       |
|    lagrangian_multiplier | 0.0672       |
|    learning_rate         | 0.0003       |
|    loss                  | 193          |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 1.06         |
|    value_loss            | 1.83e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.93042517] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 5             |
|    time_elapsed          | 80            |
|    total_timesteps       | 10240         |
| train/                   |               |
|    approx_kl             | 0.005059909   |
|    clip_fraction         | 0.0322        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0538        |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.0869       |
|    lagrangian_multiplier | 0.0401        |
|    learning_rate         | 0.0003        |
|    loss                  | 31.9          |
|    n_updates             | 40            |
|    policy_gradient_loss  | -0.00259      |
|    std                   | 1             |
|    value_loss            | 230           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.0882478] |
| time/                    |              |
|    fps                   | 130          |
|    iterations            | 8            |
|    time_elapsed          | 125          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.00695706   |
|    clip_fraction         | 0.0627       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 10.5         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.00847      |
|    lagrangian_multiplier | 0.0753       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.9         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00861     |
|    std                   | 1            |
|    value_loss            | 782          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.5699797] |
| time/                    |              |
|    fps                   | 101          |
|    iterations            | 37           |
|    time_elapsed          | 748          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.005982372  |
|    clip_fraction         | 0.0615       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 217          |
|    entropy_loss          | -2.96        |
|    explained_variance    | 0.0355       |
|    lagrangian_multiplier | 0.0621       |
|    learning_rate         | 0.0003       |
|    loss                  | 213          |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.0086      |
|    std                   | 1.07         |
|    value_loss            | 1.88e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2569026] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 6            |
|    time_elapsed          | 97           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0055889124 |
|    clip_fraction         | 0.0501       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 101          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0202      |
|    lagrangian_multiplier | 0.0533       |
|    learning_rate         | 0.0003       |
|    loss                  | 76.1         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00644     |
|    std                   | 1            |
|    value_loss            | 520          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.1497006] |
| time/                    |              |
|    fps                   | 130          |
|    iterations            | 9            |
|    time_elapsed          | 141          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0065135225 |
|    clip_fraction         | 0.0602       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 8.55         |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.00112     |
|    lagrangian_multiplier | 0.0533       |
|    learning_rate         | 0.0003       |
|    loss                  | 173          |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00911     |
|    std                   | 1.01         |
|    value_loss            | 1.31e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.990866]  |
| time/                    |              |
|    fps                   | 101          |
|    iterations            | 38           |
|    time_elapsed          | 764          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0044859955 |
|    clip_fraction         | 0.0412       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 241          |
|    entropy_loss          | -2.98        |
|    explained_variance    | 0.0302       |
|    lagrangian_multiplier | 0.0753       |
|    learning_rate         | 0.0003       |
|    loss                  | 166          |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00588     |
|    std                   | 1.08         |
|    value_loss            | 1.7e+03      |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.1231318] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 7            |
|    time_elapsed          | 113          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.0072826836 |
|    clip_fraction         | 0.067        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.598        |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.000471    |
|    lagrangian_multiplier | 0.0471       |
|    learning_rate         | 0.0003       |
|    loss                  | 74.5         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00631     |
|    std                   | 0.993        |
|    value_loss            | 673          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9457278] |
| time/                    |              |
|    fps                   | 130          |
|    iterations            | 10           |
|    time_elapsed          | 157          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.005503887  |
|    clip_fraction         | 0.0489       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.71         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0076       |
|    lagrangian_multiplier | 0.0521       |
|    learning_rate         | 0.0003       |
|    loss                  | 130          |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00714     |
|    std                   | 1            |
|    value_loss            | 1.25e+03     |
-------------------------------------------
------------------------------------------
| reward                   | [-2.746405] |
| time/                    |             |
|    fps                   | 102         |
|    iterations            | 39          |
|    time_elapsed          | 781         |
|    total_timesteps       | 79872       |
| train/                   |             |
|    approx_kl             | 0.007111406 |
|    clip_fraction         | 0.0646      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 217         |
|    entropy_loss          | -2.99       |
|    explained_variance    | 0.027       |
|    lagrangian_multiplier | 0.0778      |
|    learning_rate         | 0.0003      |
|    loss                  | 135         |
|    n_updates             | 380         |
|    policy_gradient_loss  | -0.00989    |
|    std                   | 1.08        |
|    value_loss            | 1.47e+03    |
------------------------------------------
-------------------------------------------
| reward                   | [-1.4269552] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 8            |
|    time_elapsed          | 129          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0051093507 |
|    clip_fraction         | 0.035        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 169          |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0164      |
|    lagrangian_multiplier | 0.0367       |
|    learning_rate         | 0.0003       |
|    loss                  | 117          |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00686     |
|    std                   | 0.975        |
|    value_loss            | 692          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.6587276] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 11           |
|    time_elapsed          | 173          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0047245678 |
|    clip_fraction         | 0.0263       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.8          |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0657       |
|    lagrangian_multiplier | 0.0898       |
|    learning_rate         | 0.0003       |
|    loss                  | 78.1         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00603     |
|    std                   | 0.998        |
|    value_loss            | 997          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.3475657] |
| time/                    |              |
|    fps                   | 102          |
|    iterations            | 40           |
|    time_elapsed          | 797          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.008453434  |
|    clip_fraction         | 0.0674       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 155          |
|    entropy_loss          | -3           |
|    explained_variance    | 0.031        |
|    lagrangian_multiplier | 0.069        |
|    learning_rate         | 0.0003       |
|    loss                  | 195          |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.0124      |
|    std                   | 1.09         |
|    value_loss            | 1.8e+03      |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7499547] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 9            |
|    time_elapsed          | 146          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.007200788  |
|    clip_fraction         | 0.0624       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 201          |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.00238      |
|    lagrangian_multiplier | 0.0566       |
|    learning_rate         | 0.0003       |
|    loss                  | 143          |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00983     |
|    std                   | 0.975        |
|    value_loss            | 1.08e+03     |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.6330824] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 12           |
|    time_elapsed          | 189          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0044786762 |
|    clip_fraction         | 0.058        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 6.13         |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0268       |
|    lagrangian_multiplier | 0.0839       |
|    learning_rate         | 0.0003       |
|    loss                  | 55.1         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00827     |
|    std                   | 0.99         |
|    value_loss            | 782          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.565868]  |
| time/                    |              |
|    fps                   | 103          |
|    iterations            | 41           |
|    time_elapsed          | 813          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0077115414 |
|    clip_fraction         | 0.0959       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 201          |
|    entropy_loss          | -2.98        |
|    explained_variance    | 0.0347       |
|    lagrangian_multiplier | 0.0686       |
|    learning_rate         | 0.0003       |
|    loss                  | 190          |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.0121      |
|    std                   | 1.07         |
|    value_loss            | 1.68e+03     |
-------------------------------------------
------------------------------------------
| reward                   | [-1.188694] |
| time/                    |             |
|    fps                   | 125         |
|    iterations            | 10          |
|    time_elapsed          | 162         |
|    total_timesteps       | 20480       |
| train/                   |             |
|    approx_kl             | 0.007153224 |
|    clip_fraction         | 0.0602      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 194         |
|    entropy_loss          | -2.78       |
|    explained_variance    | 0.00855     |
|    lagrangian_multiplier | 0.0579      |
|    learning_rate         | 0.0003      |
|    loss                  | 80          |
|    n_updates             | 90          |
|    policy_gradient_loss  | -0.0109     |
|    std                   | 0.969       |
|    value_loss            | 504         |
------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.46463922] |
| time/                    |               |
|    fps                   | 23            |
|    iterations            | 21            |
|    time_elapsed          | 1843          |
|    total_timesteps       | 43008         |
| train/                   |               |
|    approx_kl             | 0.004387115   |
|    clip_fraction         | 0.0441        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0036        |
|    entropy_loss          | -2.95         |
|    explained_variance    | 0.0105        |
|    lagrangian_multiplier | 0.0843        |
|    learning_rate         | 0.0003        |
|    loss                  | 5.81          |
|    n_updates             | 200           |
|    policy_gradient_loss  | -0.00402      |
|    std                   | 1.06          |
|    value_loss            | 75.2          |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.5718454] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 13           |
|    time_elapsed          | 204          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.00980592   |
|    clip_fraction         | 0.0974       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.98         |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0143       |
|    lagrangian_multiplier | 0.0552       |
|    learning_rate         | 0.0003       |
|    loss                  | 148          |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 0.991        |
|    value_loss            | 1.12e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-4.415605]  |
| time/                    |              |
|    fps                   | 103          |
|    iterations            | 42           |
|    time_elapsed          | 829          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0075978795 |
|    clip_fraction         | 0.0691       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 195          |
|    entropy_loss          | -2.96        |
|    explained_variance    | 0.0361       |
|    lagrangian_multiplier | 0.0656       |
|    learning_rate         | 0.0003       |
|    loss                  | 205          |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00969     |
|    std                   | 1.06         |
|    value_loss            | 1.85e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.012468]  |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 11           |
|    time_elapsed          | 178          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0059832614 |
|    clip_fraction         | 0.0543       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 193          |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.00606     |
|    lagrangian_multiplier | 0.0691       |
|    learning_rate         | 0.0003       |
|    loss                  | 90.8         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00606     |
|    std                   | 0.962        |
|    value_loss            | 723          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.6515113] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 14           |
|    time_elapsed          | 220          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0062151393 |
|    clip_fraction         | 0.0707       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.96         |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0694       |
|    lagrangian_multiplier | 0.0568       |
|    learning_rate         | 0.0003       |
|    loss                  | 69.7         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00717     |
|    std                   | 0.995        |
|    value_loss            | 680          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.5875719] |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 43           |
|    time_elapsed          | 846          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0087697245 |
|    clip_fraction         | 0.0953       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 238          |
|    entropy_loss          | -2.93        |
|    explained_variance    | 0.0378       |
|    lagrangian_multiplier | 0.0664       |
|    learning_rate         | 0.0003       |
|    loss                  | 255          |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.0127      |
|    std                   | 1.05         |
|    value_loss            | 2.16e+03     |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.9853498] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 12           |
|    time_elapsed          | 195          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0049679223 |
|    clip_fraction         | 0.0528       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 193          |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.000835    |
|    lagrangian_multiplier | 0.0815       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.3         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00588     |
|    std                   | 0.963        |
|    value_loss            | 644          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6600194] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 15           |
|    time_elapsed          | 236          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0061550336 |
|    clip_fraction         | 0.0481       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.81         |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0119       |
|    lagrangian_multiplier | 0.082        |
|    learning_rate         | 0.0003       |
|    loss                  | 46.8         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00961     |
|    std                   | 0.999        |
|    value_loss            | 618          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5861913] |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 44           |
|    time_elapsed          | 862          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.011606576  |
|    clip_fraction         | 0.115        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 215          |
|    entropy_loss          | -2.95        |
|    explained_variance    | 0.0402       |
|    lagrangian_multiplier | 0.0681       |
|    learning_rate         | 0.0003       |
|    loss                  | 191          |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.0136      |
|    std                   | 1.07         |
|    value_loss            | 1.77e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4384513] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 13           |
|    time_elapsed          | 211          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0060854717 |
|    clip_fraction         | 0.0623       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 105          |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0039      |
|    lagrangian_multiplier | 0.0474       |
|    learning_rate         | 0.0003       |
|    loss                  | 86.4         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.0078      |
|    std                   | 0.973        |
|    value_loss            | 664          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4984952] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 16           |
|    time_elapsed          | 252          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.009704739  |
|    clip_fraction         | 0.0902       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 5.59         |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0571       |
|    lagrangian_multiplier | 0.0851       |
|    learning_rate         | 0.0003       |
|    loss                  | 55.5         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.0112      |
|    std                   | 1            |
|    value_loss            | 728          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.81005305] |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 45            |
|    time_elapsed          | 878           |
|    total_timesteps       | 92160         |
| train/                   |               |
|    approx_kl             | 0.0057372386  |
|    clip_fraction         | 0.0479        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 241           |
|    entropy_loss          | -2.98         |
|    explained_variance    | 0.0312        |
|    lagrangian_multiplier | 0.0746        |
|    learning_rate         | 0.0003        |
|    loss                  | 190           |
|    n_updates             | 440           |
|    policy_gradient_loss  | -0.00673      |
|    std                   | 1.08          |
|    value_loss            | 1.83e+03      |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.9461305] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 14           |
|    time_elapsed          | 228          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.006682408  |
|    clip_fraction         | 0.0494       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 111          |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.00386     |
|    lagrangian_multiplier | 0.0566       |
|    learning_rate         | 0.0003       |
|    loss                  | 71.9         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00911     |
|    std                   | 0.986        |
|    value_loss            | 613          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0393066] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 17           |
|    time_elapsed          | 268          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.007148356  |
|    clip_fraction         | 0.0727       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0775       |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.011        |
|    lagrangian_multiplier | 0.0582       |
|    learning_rate         | 0.0003       |
|    loss                  | 50.4         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.0116      |
|    std                   | 0.998        |
|    value_loss            | 502          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3369603] |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 46           |
|    time_elapsed          | 894          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0089270985 |
|    clip_fraction         | 0.0949       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 253          |
|    entropy_loss          | -2.98        |
|    explained_variance    | 0.0435       |
|    lagrangian_multiplier | 0.0771       |
|    learning_rate         | 0.0003       |
|    loss                  | 162          |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.01        |
|    std                   | 1.08         |
|    value_loss            | 1.96e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.905902]  |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 15           |
|    time_elapsed          | 244          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0045691393 |
|    clip_fraction         | 0.0392       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 146          |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.00317     |
|    lagrangian_multiplier | 0.0604       |
|    learning_rate         | 0.0003       |
|    loss                  | 84.1         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00725     |
|    std                   | 0.99         |
|    value_loss            | 560          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1111951] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 18           |
|    time_elapsed          | 284          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.00564945   |
|    clip_fraction         | 0.0584       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.154        |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0456      |
|    lagrangian_multiplier | 0.0555       |
|    learning_rate         | 0.0003       |
|    loss                  | 74.6         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00671     |
|    std                   | 1            |
|    value_loss            | 600          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2740196] |
| time/                    |              |
|    fps                   | 23           |
|    iterations            | 22           |
|    time_elapsed          | 1928         |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.0057752365 |
|    clip_fraction         | 0.0631       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0065       |
|    entropy_loss          | -2.97        |
|    explained_variance    | 0.0361       |
|    lagrangian_multiplier | 0.0772       |
|    learning_rate         | 0.0003       |
|    loss                  | 25           |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00767     |
|    std                   | 1.07         |
|    value_loss            | 282          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2493885] |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 47           |
|    time_elapsed          | 910          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.0061522643 |
|    clip_fraction         | 0.0542       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 214          |
|    entropy_loss          | -2.97        |
|    explained_variance    | 0.0505       |
|    lagrangian_multiplier | 0.0735       |
|    learning_rate         | 0.0003       |
|    loss                  | 180          |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.00869     |
|    std                   | 1.07         |
|    value_loss            | 1.73e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.169035]  |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 16           |
|    time_elapsed          | 261          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0051130466 |
|    clip_fraction         | 0.0478       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 217          |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0186       |
|    lagrangian_multiplier | 0.07         |
|    learning_rate         | 0.0003       |
|    loss                  | 85.4         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.0066      |
|    std                   | 0.986        |
|    value_loss            | 763          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2628224] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 19           |
|    time_elapsed          | 300          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0033689276 |
|    clip_fraction         | 0.0322       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0662       |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0597       |
|    lagrangian_multiplier | 0.0643       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.4         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00616     |
|    std                   | 1.01         |
|    value_loss            | 573          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.7564006] |
| time/                    |              |
|    fps                   | 106          |
|    iterations            | 48           |
|    time_elapsed          | 927          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.0064445087 |
|    clip_fraction         | 0.0553       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 221          |
|    entropy_loss          | -2.96        |
|    explained_variance    | 0.0554       |
|    lagrangian_multiplier | 0.074        |
|    learning_rate         | 0.0003       |
|    loss                  | 159          |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00909     |
|    std                   | 1.07         |
|    value_loss            | 1.63e+03     |
-------------------------------------------
------------------------------------------
| reward                   | [-1.706495] |
| time/                    |             |
|    fps                   | 129         |
|    iterations            | 20          |
|    time_elapsed          | 315         |
|    total_timesteps       | 40960       |
| train/                   |             |
|    approx_kl             | 0.005767993 |
|    clip_fraction         | 0.0458      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0432      |
|    entropy_loss          | -2.83       |
|    explained_variance    | -0.0638     |
|    lagrangian_multiplier | 0.0742      |
|    learning_rate         | 0.0003      |
|    loss                  | 28.2        |
|    n_updates             | 190         |
|    policy_gradient_loss  | -0.00654    |
|    std                   | 0.986       |
|    value_loss            | 290         |
------------------------------------------
------------------------------------------
| reward                   | [-1.561897] |
| time/                    |             |
|    fps                   | 125         |
|    iterations            | 17          |
|    time_elapsed          | 277         |
|    total_timesteps       | 34816       |
| train/                   |             |
|    approx_kl             | 0.004301914 |
|    clip_fraction         | 0.0311      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 237         |
|    entropy_loss          | -2.81       |
|    explained_variance    | 0.0168      |
|    lagrangian_multiplier | 0.0847      |
|    learning_rate         | 0.0003      |
|    loss                  | 86.5        |
|    n_updates             | 160         |
|    policy_gradient_loss  | -0.00579    |
|    std                   | 0.987       |
|    value_loss            | 1.01e+03    |
------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.9700978] |
| time/                    |              |
|    fps                   | 106          |
|    iterations            | 49           |
|    time_elapsed          | 943          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.00798932   |
|    clip_fraction         | 0.0734       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 233          |
|    entropy_loss          | -2.94        |
|    explained_variance    | 0.0372       |
|    lagrangian_multiplier | 0.0735       |
|    learning_rate         | 0.0003       |
|    loss                  | 158          |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.0112      |
|    std                   | 1.05         |
|    value_loss            | 1.67e+03     |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 103, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 90, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
-------------------------------------------
| reward                   | [-3.71508]   |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 21           |
|    time_elapsed          | 331          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0064988714 |
|    clip_fraction         | 0.0639       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 8.2          |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.014       |
|    lagrangian_multiplier | 0.05         |
|    learning_rate         | 0.0003       |
|    loss                  | 104          |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00971     |
|    std                   | 1            |
|    value_loss            | 929          |
-------------------------------------------
------------------------------------------
| reward                   | [-3.108391] |
| time/                    |             |
|    fps                   | 125         |
|    iterations            | 18          |
|    time_elapsed          | 293         |
|    total_timesteps       | 36864       |
| train/                   |             |
|    approx_kl             | 0.005425266 |
|    clip_fraction         | 0.0502      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 134         |
|    entropy_loss          | -2.81       |
|    explained_variance    | -0.00212    |
|    lagrangian_multiplier | 0.0741      |
|    learning_rate         | 0.0003      |
|    loss                  | 60.2        |
|    n_updates             | 170         |
|    policy_gradient_loss  | -0.0072     |
|    std                   | 0.979       |
|    value_loss            | 608         |
------------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñà‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ
wandb:             train/approx_kl ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÖ
wandb:         train/clip_fraction ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñÇ‚ñÉ‚ñÉ‚ñÑ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñÖ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá
wandb:          train/entropy_loss ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb:    train/explained_variance ‚ñÇ‚ñÖ‚ñÅ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÅ‚ñá‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñà‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñà‚ñÖ
wandb: train/lagrangian_multiplier ‚ñÅ‚ñà‚ñà‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÜ‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÜ‚ñÖ‚ñÖ‚ñÉ
wandb:                   train/std ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ
wandb:            train/value_loss ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb:                      reward -1.9701
wandb:             train/approx_kl 0.00799
wandb:         train/clip_fraction 0.07339
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 232.88996
wandb:          train/entropy_loss -2.94242
wandb:    train/explained_variance 0.03717
wandb: train/lagrangian_multiplier 0.07348
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 157.89671
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.01124
wandb:                   train/std 1.05422
wandb:            train/value_loss 1666.49897
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/ax6e3eqq
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231223_082648-ax6e3eqq/logs
srun: error: airl.ist.berkeley.edu: task 0: Exited with exit code 1
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114474.9
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231223_084249-nwc579oy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/nwc579oy
-------------------------------------------
| reward                   | [-1.6514947] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 22           |
|    time_elapsed          | 347          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.0063663614 |
|    clip_fraction         | 0.0758       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 5.33         |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.033        |
|    lagrangian_multiplier | 0.0685       |
|    learning_rate         | 0.0003       |
|    loss                  | 96.7         |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00952     |
|    std                   | 0.993        |
|    value_loss            | 769          |
-------------------------------------------
------------------------------------------
| reward                   | [-2.033287] |
| time/                    |             |
|    fps                   | 125         |
|    iterations            | 19          |
|    time_elapsed          | 310         |
|    total_timesteps       | 38912       |
| train/                   |             |
|    approx_kl             | 0.006320758 |
|    clip_fraction         | 0.0549      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 232         |
|    entropy_loss          | -2.79       |
|    explained_variance    | 0.014       |
|    lagrangian_multiplier | 0.0969      |
|    learning_rate         | 0.0003      |
|    loss                  | 103         |
|    n_updates             | 180         |
|    policy_gradient_loss  | -0.00675    |
|    std                   | 0.977       |
|    value_loss            | 1.29e+03    |
------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
Using cpu device
-------------------------------------
| reward             | [-0.5242104] |
| time/              |              |
|    fps             | 137          |
|    iterations      | 1            |
|    time_elapsed    | 14           |
|    total_timesteps | 2048         |
-------------------------------------
--------------------------------------------
| reward                   | [-0.29197836] |
| time/                    |               |
|    fps                   | 129           |
|    iterations            | 23            |
|    time_elapsed          | 363           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.0062505747  |
|    clip_fraction         | 0.0554        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 5.69          |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.02          |
|    lagrangian_multiplier | 0.0683        |
|    learning_rate         | 0.0003        |
|    loss                  | 84.3          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.0087       |
|    std                   | 1             |
|    value_loss            | 1.06e+03      |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.4722393] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 20           |
|    time_elapsed          | 326          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.006762987  |
|    clip_fraction         | 0.0542       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 166          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.00229      |
|    lagrangian_multiplier | 0.0901       |
|    learning_rate         | 0.0003       |
|    loss                  | 83           |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00772     |
|    std                   | 0.972        |
|    value_loss            | 991          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5402415] |
| time/                    |              |
|    fps                   | 23           |
|    iterations            | 23           |
|    time_elapsed          | 2014         |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.0069748787 |
|    clip_fraction         | 0.0755       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0159       |
|    entropy_loss          | -2.99        |
|    explained_variance    | 0.0228       |
|    lagrangian_multiplier | 0.0726       |
|    learning_rate         | 0.0003       |
|    loss                  | 18.2         |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.0098      |
|    std                   | 1.08         |
|    value_loss            | 221          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7214226] |
| time/                    |              |
|    fps                   | 132          |
|    iterations            | 2            |
|    time_elapsed          | 30           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0064950185 |
|    clip_fraction         | 0.0451       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 149          |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0195       |
|    lagrangian_multiplier | 0.0675       |
|    learning_rate         | 0.0003       |
|    loss                  | 54.2         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00613     |
|    std                   | 1            |
|    value_loss            | 435          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.88043714] |
| time/                    |               |
|    fps                   | 129           |
|    iterations            | 24            |
|    time_elapsed          | 379           |
|    total_timesteps       | 49152         |
| train/                   |               |
|    approx_kl             | 0.006877103   |
|    clip_fraction         | 0.0602        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 1.22          |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.0282        |
|    lagrangian_multiplier | 0.0701        |
|    learning_rate         | 0.0003        |
|    loss                  | 92.7          |
|    n_updates             | 230           |
|    policy_gradient_loss  | -0.00874      |
|    std                   | 1             |
|    value_loss            | 833           |
--------------------------------------------
------------------------------------------
| reward                   | [-3.113657] |
| time/                    |             |
|    fps                   | 125         |
|    iterations            | 21          |
|    time_elapsed          | 342         |
|    total_timesteps       | 43008       |
| train/                   |             |
|    approx_kl             | 0.006147285 |
|    clip_fraction         | 0.0585      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 179         |
|    entropy_loss          | -2.79       |
|    explained_variance    | -1.51e-05   |
|    lagrangian_multiplier | 0.0818      |
|    learning_rate         | 0.0003      |
|    loss                  | 82.4        |
|    n_updates             | 200         |
|    policy_gradient_loss  | -0.00749    |
|    std                   | 0.981       |
|    value_loss            | 759         |
------------------------------------------
------------------------------------------
| reward                   | [-0.845278] |
| time/                    |             |
|    fps                   | 130         |
|    iterations            | 3           |
|    time_elapsed          | 46          |
|    total_timesteps       | 6144        |
| train/                   |             |
|    approx_kl             | 0.005844661 |
|    clip_fraction         | 0.0472      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 189         |
|    entropy_loss          | -2.85       |
|    explained_variance    | -0.0234     |
|    lagrangian_multiplier | 0.074       |
|    learning_rate         | 0.0003      |
|    loss                  | 78.7        |
|    n_updates             | 20          |
|    policy_gradient_loss  | -0.00672    |
|    std                   | 1.01        |
|    value_loss            | 917         |
------------------------------------------
-------------------------------------------
| reward                   | [-0.9705572] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 25           |
|    time_elapsed          | 395          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.00470574   |
|    clip_fraction         | 0.0496       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0537       |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0101      |
|    lagrangian_multiplier | 0.0751       |
|    learning_rate         | 0.0003       |
|    loss                  | 26.9         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00517     |
|    std                   | 1            |
|    value_loss            | 309          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.34664267] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 22            |
|    time_elapsed          | 358           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.0062436257  |
|    clip_fraction         | 0.0577        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 155           |
|    entropy_loss          | -2.81         |
|    explained_variance    | 0.00558       |
|    lagrangian_multiplier | 0.0691        |
|    learning_rate         | 0.0003        |
|    loss                  | 93.3          |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.00707      |
|    std                   | 0.991         |
|    value_loss            | 851           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.56692463] |
| time/                    |               |
|    fps                   | 130           |
|    iterations            | 4             |
|    time_elapsed          | 62            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.005706268   |
|    clip_fraction         | 0.0449        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 142           |
|    entropy_loss          | -2.86         |
|    explained_variance    | -0.0212       |
|    lagrangian_multiplier | 0.0653        |
|    learning_rate         | 0.0003        |
|    loss                  | 114           |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00568      |
|    std                   | 1.01          |
|    value_loss            | 963           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2442814] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 26           |
|    time_elapsed          | 411          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.008814244  |
|    clip_fraction         | 0.0935       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.26         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0119      |
|    lagrangian_multiplier | 0.0611       |
|    learning_rate         | 0.0003       |
|    loss                  | 125          |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.0107      |
|    std                   | 0.999        |
|    value_loss            | 1.07e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.96219456] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 23            |
|    time_elapsed          | 375           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.0063565206  |
|    clip_fraction         | 0.0541        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 110           |
|    entropy_loss          | -2.81         |
|    explained_variance    | 0.00145       |
|    lagrangian_multiplier | 0.0792        |
|    learning_rate         | 0.0003        |
|    loss                  | 52.3          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00789      |
|    std                   | 0.986         |
|    value_loss            | 504           |
--------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1344874] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 5            |
|    time_elapsed          | 78           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0063747205 |
|    clip_fraction         | 0.0564       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 132          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0159      |
|    lagrangian_multiplier | 0.0593       |
|    learning_rate         | 0.0003       |
|    loss                  | 92.8         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00965     |
|    std                   | 1.01         |
|    value_loss            | 971          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4254748] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 27           |
|    time_elapsed          | 426          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.005974544  |
|    clip_fraction         | 0.0622       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0962       |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.00118     |
|    lagrangian_multiplier | 0.0574       |
|    learning_rate         | 0.0003       |
|    loss                  | 80.8         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00743     |
|    std                   | 1            |
|    value_loss            | 716          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3735749] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 24           |
|    time_elapsed          | 391          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.007003621  |
|    clip_fraction         | 0.0536       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 233          |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0153       |
|    lagrangian_multiplier | 0.0837       |
|    learning_rate         | 0.0003       |
|    loss                  | 143          |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00737     |
|    std                   | 0.976        |
|    value_loss            | 1.4e+03      |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.5999079] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 6            |
|    time_elapsed          | 95           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0045969607 |
|    clip_fraction         | 0.0393       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 147          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0234      |
|    lagrangian_multiplier | 0.0742       |
|    learning_rate         | 0.0003       |
|    loss                  | 87.5         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00572     |
|    std                   | 1.01         |
|    value_loss            | 928          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3584608] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 28           |
|    time_elapsed          | 442          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.006331267  |
|    clip_fraction         | 0.0596       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.97         |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0213       |
|    lagrangian_multiplier | 0.056        |
|    learning_rate         | 0.0003       |
|    loss                  | 106          |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00893     |
|    std                   | 1.02         |
|    value_loss            | 929          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5881395] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 25           |
|    time_elapsed          | 407          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.008429845  |
|    clip_fraction         | 0.0898       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 170          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.00266      |
|    lagrangian_multiplier | 0.0643       |
|    learning_rate         | 0.0003       |
|    loss                  | 106          |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.0106      |
|    std                   | 0.968        |
|    value_loss            | 1.02e+03     |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.4783611] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 7            |
|    time_elapsed          | 111          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.004605756  |
|    clip_fraction         | 0.0306       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 161          |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.018       |
|    lagrangian_multiplier | 0.0607       |
|    learning_rate         | 0.0003       |
|    loss                  | 101          |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00488     |
|    std                   | 1.03         |
|    value_loss            | 981          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.152646] |
| time/                    |             |
|    fps                   | 129         |
|    iterations            | 29          |
|    time_elapsed          | 458         |
|    total_timesteps       | 59392       |
| train/                   |             |
|    approx_kl             | 0.004637168 |
|    clip_fraction         | 0.0396      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0577      |
|    entropy_loss          | -2.88       |
|    explained_variance    | 0.0538      |
|    lagrangian_multiplier | 0.0647      |
|    learning_rate         | 0.0003      |
|    loss                  | 57.4        |
|    n_updates             | 280         |
|    policy_gradient_loss  | -0.00553    |
|    std                   | 1.02        |
|    value_loss            | 534         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.46423993] |
| time/                    |               |
|    fps                   | 23            |
|    iterations            | 24            |
|    time_elapsed          | 2099          |
|    total_timesteps       | 49152         |
| train/                   |               |
|    approx_kl             | 0.00650601    |
|    clip_fraction         | 0.0507        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00901       |
|    entropy_loss          | -2.99         |
|    explained_variance    | 0.0461        |
|    lagrangian_multiplier | 0.083         |
|    learning_rate         | 0.0003        |
|    loss                  | 32.3          |
|    n_updates             | 230           |
|    policy_gradient_loss  | -0.00708      |
|    std                   | 1.08          |
|    value_loss            | 398           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.7878604] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 26           |
|    time_elapsed          | 424          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.007248116  |
|    clip_fraction         | 0.0717       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 249          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0141       |
|    lagrangian_multiplier | 0.0856       |
|    learning_rate         | 0.0003       |
|    loss                  | 131          |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00802     |
|    std                   | 0.973        |
|    value_loss            | 1.33e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1968142] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 8            |
|    time_elapsed          | 127          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0048720697 |
|    clip_fraction         | 0.0362       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 107          |
|    entropy_loss          | -2.9         |
|    explained_variance    | -0.0108      |
|    lagrangian_multiplier | 0.0768       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.1         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00513     |
|    std                   | 1.03         |
|    value_loss            | 543          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8494271] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 30           |
|    time_elapsed          | 474          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.007257549  |
|    clip_fraction         | 0.0917       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0973       |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0379       |
|    lagrangian_multiplier | 0.0576       |
|    learning_rate         | 0.0003       |
|    loss                  | 84.7         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.0111      |
|    std                   | 1.03         |
|    value_loss            | 724          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2302557] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 27           |
|    time_elapsed          | 441          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.0046645124 |
|    clip_fraction         | 0.0309       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 115          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.00244     |
|    lagrangian_multiplier | 0.0951       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.2         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00623     |
|    std                   | 0.974        |
|    value_loss            | 797          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.5075907] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 9            |
|    time_elapsed          | 143          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.008068249  |
|    clip_fraction         | 0.0728       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 241          |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.00963     |
|    lagrangian_multiplier | 0.0605       |
|    learning_rate         | 0.0003       |
|    loss                  | 169          |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.0087      |
|    std                   | 1.01         |
|    value_loss            | 1.32e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2238541] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 31           |
|    time_elapsed          | 490          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0051947255 |
|    clip_fraction         | 0.0509       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.2          |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.077        |
|    lagrangian_multiplier | 0.0651       |
|    learning_rate         | 0.0003       |
|    loss                  | 63.5         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00784     |
|    std                   | 1.02         |
|    value_loss            | 662          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1198114] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 28           |
|    time_elapsed          | 457          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.0068389117 |
|    clip_fraction         | 0.0619       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 168          |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.016        |
|    lagrangian_multiplier | 0.0896       |
|    learning_rate         | 0.0003       |
|    loss                  | 102          |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00932     |
|    std                   | 0.976        |
|    value_loss            | 1.07e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2359354] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 10           |
|    time_elapsed          | 159          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0063639623 |
|    clip_fraction         | 0.0531       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 228          |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.0204      |
|    lagrangian_multiplier | 0.0684       |
|    learning_rate         | 0.0003       |
|    loss                  | 116          |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00789     |
|    std                   | 1.02         |
|    value_loss            | 1.02e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6418633] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 32           |
|    time_elapsed          | 506          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.0038572722 |
|    clip_fraction         | 0.045        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0309       |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.0242      |
|    lagrangian_multiplier | 0.0695       |
|    learning_rate         | 0.0003       |
|    loss                  | 18.1         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00476     |
|    std                   | 1.03         |
|    value_loss            | 190          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2614017] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 29           |
|    time_elapsed          | 473          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.007677789  |
|    clip_fraction         | 0.0603       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 153          |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.00216      |
|    lagrangian_multiplier | 0.0948       |
|    learning_rate         | 0.0003       |
|    loss                  | 74.3         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00767     |
|    std                   | 0.984        |
|    value_loss            | 928          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3068823] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 11           |
|    time_elapsed          | 175          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.007401655  |
|    clip_fraction         | 0.0766       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 249          |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.000673     |
|    lagrangian_multiplier | 0.0838       |
|    learning_rate         | 0.0003       |
|    loss                  | 76.8         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00979     |
|    std                   | 0.997        |
|    value_loss            | 774          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6943615] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 33           |
|    time_elapsed          | 522          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.006185157  |
|    clip_fraction         | 0.0452       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0561       |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.0584       |
|    lagrangian_multiplier | 0.0715       |
|    learning_rate         | 0.0003       |
|    loss                  | 20.9         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00655     |
|    std                   | 1.03         |
|    value_loss            | 270          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3375354] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 30           |
|    time_elapsed          | 490          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0073991935 |
|    clip_fraction         | 0.0612       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 138          |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0043      |
|    lagrangian_multiplier | 0.0846       |
|    learning_rate         | 0.0003       |
|    loss                  | 66.7         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00835     |
|    std                   | 0.975        |
|    value_loss            | 577          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.5918843] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 12           |
|    time_elapsed          | 191          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0073249517 |
|    clip_fraction         | 0.0659       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 217          |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0215      |
|    lagrangian_multiplier | 0.0719       |
|    learning_rate         | 0.0003       |
|    loss                  | 99.2         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00856     |
|    std                   | 0.986        |
|    value_loss            | 966          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.1924808] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 34           |
|    time_elapsed          | 538          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.006757312  |
|    clip_fraction         | 0.0725       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 2.26         |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.0373       |
|    lagrangian_multiplier | 0.0566       |
|    learning_rate         | 0.0003       |
|    loss                  | 71.7         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00779     |
|    std                   | 1.03         |
|    value_loss            | 696          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1554372] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 31           |
|    time_elapsed          | 506          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.008060966  |
|    clip_fraction         | 0.0635       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 104          |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.00291      |
|    lagrangian_multiplier | 0.0748       |
|    learning_rate         | 0.0003       |
|    loss                  | 55.7         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00946     |
|    std                   | 0.957        |
|    value_loss            | 533          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9481385] |
| time/                    |              |
|    fps                   | 23           |
|    iterations            | 25           |
|    time_elapsed          | 2185         |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.0051336866 |
|    clip_fraction         | 0.0451       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00306      |
|    entropy_loss          | -2.99        |
|    explained_variance    | -0.0117      |
|    lagrangian_multiplier | 0.0785       |
|    learning_rate         | 0.0003       |
|    loss                  | 5.86         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00346     |
|    std                   | 1.08         |
|    value_loss            | 72.6         |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.7371708] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 13           |
|    time_elapsed          | 207          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0060235406 |
|    clip_fraction         | 0.0507       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 208          |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.00171      |
|    lagrangian_multiplier | 0.0677       |
|    learning_rate         | 0.0003       |
|    loss                  | 138          |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00878     |
|    std                   | 1            |
|    value_loss            | 1.14e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7921548] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 35           |
|    time_elapsed          | 554          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.007903096  |
|    clip_fraction         | 0.0615       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.84         |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.029        |
|    lagrangian_multiplier | 0.0544       |
|    learning_rate         | 0.0003       |
|    loss                  | 69.5         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00792     |
|    std                   | 1.03         |
|    value_loss            | 643          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7354403] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 32           |
|    time_elapsed          | 522          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.0061247055 |
|    clip_fraction         | 0.0589       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 188          |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.00216      |
|    lagrangian_multiplier | 0.0889       |
|    learning_rate         | 0.0003       |
|    loss                  | 66.2         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00991     |
|    std                   | 0.969        |
|    value_loss            | 697          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3872406] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 14           |
|    time_elapsed          | 224          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.007305472  |
|    clip_fraction         | 0.0606       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 224          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0272       |
|    lagrangian_multiplier | 0.0693       |
|    learning_rate         | 0.0003       |
|    loss                  | 93.7         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00917     |
|    std                   | 0.996        |
|    value_loss            | 815          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0674707] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 36           |
|    time_elapsed          | 570          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0057912963 |
|    clip_fraction         | 0.0538       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 5.12         |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0395       |
|    lagrangian_multiplier | 0.0606       |
|    learning_rate         | 0.0003       |
|    loss                  | 98.8         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00754     |
|    std                   | 1.02         |
|    value_loss            | 894          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.96647584] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 33            |
|    time_elapsed          | 538           |
|    total_timesteps       | 67584         |
| train/                   |               |
|    approx_kl             | 0.009071348   |
|    clip_fraction         | 0.0759        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 35.7          |
|    entropy_loss          | -2.78         |
|    explained_variance    | 0.00503       |
|    lagrangian_multiplier | 0.068         |
|    learning_rate         | 0.0003        |
|    loss                  | 31.2          |
|    n_updates             | 320           |
|    policy_gradient_loss  | -0.00997      |
|    std                   | 0.974         |
|    value_loss            | 275           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.5937828] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 15           |
|    time_elapsed          | 240          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.00638406   |
|    clip_fraction         | 0.0534       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 187          |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0205       |
|    lagrangian_multiplier | 0.0634       |
|    learning_rate         | 0.0003       |
|    loss                  | 131          |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00893     |
|    std                   | 0.988        |
|    value_loss            | 947          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3818204] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 37           |
|    time_elapsed          | 586          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.006109313  |
|    clip_fraction         | 0.0664       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.281        |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0538       |
|    lagrangian_multiplier | 0.0579       |
|    learning_rate         | 0.0003       |
|    loss                  | 83.6         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00912     |
|    std                   | 1.02         |
|    value_loss            | 699          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.96938425] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 34            |
|    time_elapsed          | 554           |
|    total_timesteps       | 69632         |
| train/                   |               |
|    approx_kl             | 0.0070046377  |
|    clip_fraction         | 0.08          |
|    clip_range            | 0.2           |
|    cost_value_loss       | 159           |
|    entropy_loss          | -2.78         |
|    explained_variance    | 0.0174        |
|    lagrangian_multiplier | 0.0809        |
|    learning_rate         | 0.0003        |
|    loss                  | 76.5          |
|    n_updates             | 330           |
|    policy_gradient_loss  | -0.00969      |
|    std                   | 0.968         |
|    value_loss            | 764           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.5484506] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 16           |
|    time_elapsed          | 256          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.008723153  |
|    clip_fraction         | 0.087        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 226          |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0255      |
|    lagrangian_multiplier | 0.0732       |
|    learning_rate         | 0.0003       |
|    loss                  | 73.1         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.0094      |
|    std                   | 0.976        |
|    value_loss            | 677          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.1451948] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 38           |
|    time_elapsed          | 602          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.007147522  |
|    clip_fraction         | 0.0563       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.92         |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.0376       |
|    lagrangian_multiplier | 0.0549       |
|    learning_rate         | 0.0003       |
|    loss                  | 50.8         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00913     |
|    std                   | 1.05         |
|    value_loss            | 485          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.3126606] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 35           |
|    time_elapsed          | 571          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.004399133  |
|    clip_fraction         | 0.0332       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.2          |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.0274       |
|    lagrangian_multiplier | 0.0595       |
|    learning_rate         | 0.0003       |
|    loss                  | 29.5         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00392     |
|    std                   | 0.962        |
|    value_loss            | 261          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5710365] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 39           |
|    time_elapsed          | 618          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.007201866  |
|    clip_fraction         | 0.0837       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.425        |
|    entropy_loss          | -2.94        |
|    explained_variance    | 0.0648       |
|    lagrangian_multiplier | 0.0623       |
|    learning_rate         | 0.0003       |
|    loss                  | 53.7         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.0098      |
|    std                   | 1.06         |
|    value_loss            | 517          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0254657] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 17           |
|    time_elapsed          | 272          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.0052359854 |
|    clip_fraction         | 0.0489       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 220          |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.00957     |
|    lagrangian_multiplier | 0.0743       |
|    learning_rate         | 0.0003       |
|    loss                  | 98.1         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00867     |
|    std                   | 0.967        |
|    value_loss            | 902          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6460482] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 36           |
|    time_elapsed          | 587          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0049937023 |
|    clip_fraction         | 0.0424       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0835       |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0106      |
|    lagrangian_multiplier | 0.0501       |
|    learning_rate         | 0.0003       |
|    loss                  | 29           |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00317     |
|    std                   | 0.959        |
|    value_loss            | 232          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.38697296] |
| time/                    |               |
|    fps                   | 23            |
|    iterations            | 26            |
|    time_elapsed          | 2270          |
|    total_timesteps       | 53248         |
| train/                   |               |
|    approx_kl             | 0.005312031   |
|    clip_fraction         | 0.0554        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00203       |
|    entropy_loss          | -2.98         |
|    explained_variance    | 0.0122        |
|    lagrangian_multiplier | 0.0805        |
|    learning_rate         | 0.0003        |
|    loss                  | 7.48          |
|    n_updates             | 250           |
|    policy_gradient_loss  | -0.00497      |
|    std                   | 1.07          |
|    value_loss            | 96.6          |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.5196614] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 40           |
|    time_elapsed          | 634          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.0058761286 |
|    clip_fraction         | 0.0531       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.34         |
|    entropy_loss          | -2.95        |
|    explained_variance    | -0.0134      |
|    lagrangian_multiplier | 0.0562       |
|    learning_rate         | 0.0003       |
|    loss                  | 87.2         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00802     |
|    std                   | 1.06         |
|    value_loss            | 738          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0073366] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 18           |
|    time_elapsed          | 288          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0062711546 |
|    clip_fraction         | 0.065        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 233          |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0301      |
|    lagrangian_multiplier | 0.0686       |
|    learning_rate         | 0.0003       |
|    loss                  | 126          |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00983     |
|    std                   | 0.958        |
|    value_loss            | 1.07e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5835544] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 37           |
|    time_elapsed          | 603          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.0061590197 |
|    clip_fraction         | 0.0542       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 10.7         |
|    entropy_loss          | -2.76        |
|    explained_variance    | 0.00363      |
|    lagrangian_multiplier | 0.0842       |
|    learning_rate         | 0.0003       |
|    loss                  | 30.2         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00795     |
|    std                   | 0.97         |
|    value_loss            | 354          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.99973524] |
| time/                    |               |
|    fps                   | 129           |
|    iterations            | 41            |
|    time_elapsed          | 650           |
|    total_timesteps       | 83968         |
| train/                   |               |
|    approx_kl             | 0.0038302671  |
|    clip_fraction         | 0.036         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 1.57          |
|    entropy_loss          | -2.95         |
|    explained_variance    | 0.0372        |
|    lagrangian_multiplier | 0.0645        |
|    learning_rate         | 0.0003        |
|    loss                  | 31.5          |
|    n_updates             | 400           |
|    policy_gradient_loss  | -0.00587      |
|    std                   | 1.06          |
|    value_loss            | 285           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.4900992] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 19           |
|    time_elapsed          | 304          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.008455408  |
|    clip_fraction         | 0.108        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 248          |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0192      |
|    lagrangian_multiplier | 0.0797       |
|    learning_rate         | 0.0003       |
|    loss                  | 88.8         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 0.961        |
|    value_loss            | 819          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.90521413] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 38            |
|    time_elapsed          | 619           |
|    total_timesteps       | 77824         |
| train/                   |               |
|    approx_kl             | 0.005508773   |
|    clip_fraction         | 0.0598        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.278         |
|    entropy_loss          | -2.79         |
|    explained_variance    | -0.0166       |
|    lagrangian_multiplier | 0.0564        |
|    learning_rate         | 0.0003        |
|    loss                  | 32.6          |
|    n_updates             | 370           |
|    policy_gradient_loss  | -0.00755      |
|    std                   | 0.982         |
|    value_loss            | 294           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.79451007] |
| time/                    |               |
|    fps                   | 129           |
|    iterations            | 42            |
|    time_elapsed          | 666           |
|    total_timesteps       | 86016         |
| train/                   |               |
|    approx_kl             | 0.0046411576  |
|    clip_fraction         | 0.0401        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0581        |
|    entropy_loss          | -2.96         |
|    explained_variance    | 0.0758        |
|    lagrangian_multiplier | 0.0778        |
|    learning_rate         | 0.0003        |
|    loss                  | 26            |
|    n_updates             | 410           |
|    policy_gradient_loss  | -0.0141       |
|    std                   | 1.07          |
|    value_loss            | 319           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.1780838] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 39           |
|    time_elapsed          | 635          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.006228336  |
|    clip_fraction         | 0.0522       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0444       |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0153       |
|    lagrangian_multiplier | 0.0622       |
|    learning_rate         | 0.0003       |
|    loss                  | 17.9         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.0103      |
|    std                   | 0.987        |
|    value_loss            | 195          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.9335966] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 43           |
|    time_elapsed          | 682          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.004809356  |
|    clip_fraction         | 0.0361       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0177       |
|    entropy_loss          | -2.97        |
|    explained_variance    | -0.00754     |
|    lagrangian_multiplier | 0.0854       |
|    learning_rate         | 0.0003       |
|    loss                  | 16.4         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00354     |
|    std                   | 1.07         |
|    value_loss            | 206          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.306942] |
| time/                    |             |
|    fps                   | 125         |
|    iterations            | 40          |
|    time_elapsed          | 651         |
|    total_timesteps       | 81920       |
| train/                   |             |
|    approx_kl             | 0.008871392 |
|    clip_fraction         | 0.0666      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 1.36        |
|    entropy_loss          | -2.84       |
|    explained_variance    | -0.0623     |
|    lagrangian_multiplier | 0.0457      |
|    learning_rate         | 0.0003      |
|    loss                  | 26.3        |
|    n_updates             | 390         |
|    policy_gradient_loss  | -0.0056     |
|    std                   | 1.01        |
|    value_loss            | 209         |
------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.7772325] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 44           |
|    time_elapsed          | 698          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.0059280964 |
|    clip_fraction         | 0.0746       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0163       |
|    entropy_loss          | -2.96        |
|    explained_variance    | -0.0149      |
|    lagrangian_multiplier | 0.0825       |
|    learning_rate         | 0.0003       |
|    loss                  | 15           |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.00687     |
|    std                   | 1.07         |
|    value_loss            | 179          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1300865] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 41           |
|    time_elapsed          | 667          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0048514656 |
|    clip_fraction         | 0.045        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 96.5         |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0299      |
|    lagrangian_multiplier | 0.0405       |
|    learning_rate         | 0.0003       |
|    loss                  | 77           |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.0056      |
|    std                   | 1.02         |
|    value_loss            | 393          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5152136] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 45           |
|    time_elapsed          | 713          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.0060267756 |
|    clip_fraction         | 0.0564       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 14           |
|    entropy_loss          | -2.96        |
|    explained_variance    | 0.0259       |
|    lagrangian_multiplier | 0.0695       |
|    learning_rate         | 0.0003       |
|    loss                  | 45.8         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00846     |
|    std                   | 1.06         |
|    value_loss            | 473          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.538295]  |
| time/                    |              |
|    fps                   | 23           |
|    iterations            | 27           |
|    time_elapsed          | 2355         |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.0046897503 |
|    clip_fraction         | 0.0436       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0038       |
|    entropy_loss          | -2.96        |
|    explained_variance    | 0.0489       |
|    lagrangian_multiplier | 0.0805       |
|    learning_rate         | 0.0003       |
|    loss                  | 34.9         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00691     |
|    std                   | 1.06         |
|    value_loss            | 408          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.83504146] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 42            |
|    time_elapsed          | 683           |
|    total_timesteps       | 86016         |
| train/                   |               |
|    approx_kl             | 0.007074698   |
|    clip_fraction         | 0.0622        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 94.2          |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.0275       |
|    lagrangian_multiplier | 0.0539        |
|    learning_rate         | 0.0003        |
|    loss                  | 54.4          |
|    n_updates             | 410           |
|    policy_gradient_loss  | -0.00865      |
|    std                   | 1.01          |
|    value_loss            | 360           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.43945447] |
| time/                    |               |
|    fps                   | 129           |
|    iterations            | 46            |
|    time_elapsed          | 729           |
|    total_timesteps       | 94208         |
| train/                   |               |
|    approx_kl             | 0.004371211   |
|    clip_fraction         | 0.0322        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0476        |
|    entropy_loss          | -2.94         |
|    explained_variance    | -0.07         |
|    lagrangian_multiplier | 0.0852        |
|    learning_rate         | 0.0003        |
|    loss                  | 20.5          |
|    n_updates             | 450           |
|    policy_gradient_loss  | -0.00421      |
|    std                   | 1.05          |
|    value_loss            | 257           |
--------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.5902904] |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 20           |
|    time_elapsed          | 391          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0065517384 |
|    clip_fraction         | 0.0592       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 242          |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0245      |
|    lagrangian_multiplier | 0.0766       |
|    learning_rate         | 0.0003       |
|    loss                  | 96.1         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00808     |
|    std                   | 0.959        |
|    value_loss            | 832          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5875059] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 43           |
|    time_elapsed          | 700          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0030830714 |
|    clip_fraction         | 0.0238       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0297       |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0289      |
|    lagrangian_multiplier | 0.0639       |
|    learning_rate         | 0.0003       |
|    loss                  | 14.5         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00399     |
|    std                   | 1            |
|    value_loss            | 127          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.52894837] |
| time/                    |               |
|    fps                   | 129           |
|    iterations            | 47            |
|    time_elapsed          | 745           |
|    total_timesteps       | 96256         |
| train/                   |               |
|    approx_kl             | 0.005348319   |
|    clip_fraction         | 0.0511        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 52.6          |
|    entropy_loss          | -2.93         |
|    explained_variance    | -0.00634      |
|    lagrangian_multiplier | 0.0597        |
|    learning_rate         | 0.0003        |
|    loss                  | 155           |
|    n_updates             | 460           |
|    policy_gradient_loss  | -0.00703      |
|    std                   | 1.06          |
|    value_loss            | 1.08e+03      |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.42791972] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 44            |
|    time_elapsed          | 716           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.0062064384  |
|    clip_fraction         | 0.0538        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 122           |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.00831       |
|    lagrangian_multiplier | 0.0683        |
|    learning_rate         | 0.0003        |
|    loss                  | 65.9          |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.0071       |
|    std                   | 1.01          |
|    value_loss            | 556           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.49063507] |
| time/                    |               |
|    fps                   | 129           |
|    iterations            | 48            |
|    time_elapsed          | 761           |
|    total_timesteps       | 98304         |
| train/                   |               |
|    approx_kl             | 0.0052914205  |
|    clip_fraction         | 0.0657        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 1.13          |
|    entropy_loss          | -2.94         |
|    explained_variance    | 0.0136        |
|    lagrangian_multiplier | 0.0668        |
|    learning_rate         | 0.0003        |
|    loss                  | 50.7          |
|    n_updates             | 470           |
|    policy_gradient_loss  | -0.0136       |
|    std                   | 1.06          |
|    value_loss            | 474           |
--------------------------------------------
------------------------------------------
| reward                   | [-0.476333] |
| time/                    |             |
|    fps                   | 125         |
|    iterations            | 45          |
|    time_elapsed          | 732         |
|    total_timesteps       | 92160       |
| train/                   |             |
|    approx_kl             | 0.004892179 |
|    clip_fraction         | 0.0479      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 107         |
|    entropy_loss          | -2.84       |
|    explained_variance    | -0.0313     |
|    lagrangian_multiplier | 0.0504      |
|    learning_rate         | 0.0003      |
|    loss                  | 59.4        |
|    n_updates             | 440         |
|    policy_gradient_loss  | -0.00735    |
|    std                   | 1           |
|    value_loss            | 467         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.2615658] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 49           |
|    time_elapsed          | 777          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.0071413415 |
|    clip_fraction         | 0.0601       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.323        |
|    entropy_loss          | -2.92        |
|    explained_variance    | -0.022       |
|    lagrangian_multiplier | 0.0462       |
|    learning_rate         | 0.0003       |
|    loss                  | 23.3         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.00788     |
|    std                   | 1.04         |
|    value_loss            | 189          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 103, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 90, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñà‚ñÜ‚ñá‚ñÜ
wandb:             train/approx_kl ‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñà‚ñÖ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ
wandb:         train/clip_fraction ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÅ‚ñÖ‚ñÜ‚ñÑ‚ñà‚ñÜ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñà‚ñÖ‚ñÖ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÅ‚ñÅ
wandb:          train/entropy_loss ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÉ
wandb:    train/explained_variance ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñà‚ñÖ‚ñá‚ñÖ‚ñá‚ñÅ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñÉ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÑ‚ñà‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÉ
wandb: train/lagrangian_multiplier ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñà‚ñá‚ñÉ‚ñá‚ñá‚ñÉ‚ñÑ‚ñÜ‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÑ‚ñÖ‚ñÇ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñá‚ñÉ‚ñÅ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÅ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÅ‚ñÖ
wandb:                   train/std ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ
wandb:            train/value_loss ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÜ‚ñÖ‚ñÜ‚ñÇ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                      reward -1.26157
wandb:             train/approx_kl 0.00714
wandb:         train/clip_fraction 0.06006
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 0.32276
wandb:          train/entropy_loss -2.9245
wandb:    train/explained_variance -0.02203
wandb: train/lagrangian_multiplier 0.04616
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 23.28121
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00788
wandb:                   train/std 1.04187
wandb:            train/value_loss 189.36195
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/1o43c241
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231223_003703-1o43c241/logs
srun: error: dqn.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-0.831496]  |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 46           |
|    time_elapsed          | 748          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0054743085 |
|    clip_fraction         | 0.0402       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 20.7         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0172       |
|    lagrangian_multiplier | 0.0579       |
|    learning_rate         | 0.0003       |
|    loss                  | 32.9         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.00702     |
|    std                   | 1.01         |
|    value_loss            | 337          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114474.10
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
--------------------------------------------
| reward                   | [-0.65037936] |
| time/                    |               |
|    fps                   | 23            |
|    iterations            | 28            |
|    time_elapsed          | 2440          |
|    total_timesteps       | 57344         |
| train/                   |               |
|    approx_kl             | 0.005713024   |
|    clip_fraction         | 0.0766        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00545       |
|    entropy_loss          | -2.96         |
|    explained_variance    | 0.0384        |
|    lagrangian_multiplier | 0.0784        |
|    learning_rate         | 0.0003        |
|    loss                  | 7.02          |
|    n_updates             | 270           |
|    policy_gradient_loss  | -0.0101       |
|    std                   | 1.07          |
|    value_loss            | 88.4          |
--------------------------------------------
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231223_005025-7amm0ztn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/7amm0ztn
--------------------------------------------
| reward                   | [-0.85748094] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 47            |
|    time_elapsed          | 764           |
|    total_timesteps       | 96256         |
| train/                   |               |
|    approx_kl             | 0.006954139   |
|    clip_fraction         | 0.0488        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 85.3          |
|    entropy_loss          | -2.86         |
|    explained_variance    | -0.0123       |
|    lagrangian_multiplier | 0.0571        |
|    learning_rate         | 0.0003        |
|    loss                  | 51.1          |
|    n_updates             | 460           |
|    policy_gradient_loss  | -0.00995      |
|    std                   | 1.02          |
|    value_loss            | 536           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.0396857] |
| time/                    |              |
|    fps                   | 93           |
|    iterations            | 21           |
|    time_elapsed          | 460          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.007704478  |
|    clip_fraction         | 0.0786       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 191          |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0276      |
|    lagrangian_multiplier | 0.0659       |
|    learning_rate         | 0.0003       |
|    loss                  | 84.3         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00936     |
|    std                   | 0.961        |
|    value_loss            | 649          |
-------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.46364677] |
| time/              |               |
|    fps             | 136           |
|    iterations      | 1             |
|    time_elapsed    | 14            |
|    total_timesteps | 2048          |
--------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
------------------------------------------
| reward                   | [-0.716449] |
| time/                    |             |
|    fps                   | 125         |
|    iterations            | 48          |
|    time_elapsed          | 780         |
|    total_timesteps       | 98304       |
| train/                   |             |
|    approx_kl             | 0.005109059 |
|    clip_fraction         | 0.0463      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 46.9        |
|    entropy_loss          | -2.86       |
|    explained_variance    | 0.0107      |
|    lagrangian_multiplier | 0.0517      |
|    learning_rate         | 0.0003      |
|    loss                  | 64.8        |
|    n_updates             | 470         |
|    policy_gradient_loss  | -0.00489    |
|    std                   | 1.01        |
|    value_loss            | 518         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.36939555] |
| time/                    |               |
|    fps                   | 94            |
|    iterations            | 22            |
|    time_elapsed          | 476           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.006762695   |
|    clip_fraction         | 0.0609        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 245           |
|    entropy_loss          | -2.76         |
|    explained_variance    | -0.0305       |
|    lagrangian_multiplier | 0.0776        |
|    learning_rate         | 0.0003        |
|    loss                  | 78.3          |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.00856      |
|    std                   | 0.963         |
|    value_loss            | 683           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.6229895] |
| time/                    |              |
|    fps                   | 131          |
|    iterations            | 2            |
|    time_elapsed          | 31           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0069984132 |
|    clip_fraction         | 0.0605       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.067        |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0112       |
|    lagrangian_multiplier | 0.0609       |
|    learning_rate         | 0.0003       |
|    loss                  | 37.5         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00682     |
|    std                   | 1.01         |
|    value_loss            | 382          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9942659] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 49           |
|    time_elapsed          | 796          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.0058138277 |
|    clip_fraction         | 0.045        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 46.5         |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.00435      |
|    lagrangian_multiplier | 0.0584       |
|    learning_rate         | 0.0003       |
|    loss                  | 64.7         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.0034      |
|    std                   | 1.01         |
|    value_loss            | 590          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 103, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 90, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
------------------------------------------
| reward                   | [-0.841966] |
| time/                    |             |
|    fps                   | 95          |
|    iterations            | 23          |
|    time_elapsed          | 492         |
|    total_timesteps       | 47104       |
| train/                   |             |
|    approx_kl             | 0.009472141 |
|    clip_fraction         | 0.0998      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 236         |
|    entropy_loss          | -2.76       |
|    explained_variance    | -0.0128     |
|    lagrangian_multiplier | 0.0652      |
|    learning_rate         | 0.0003      |
|    loss                  | 95.8        |
|    n_updates             | 220         |
|    policy_gradient_loss  | -0.0141     |
|    std                   | 0.961       |
|    value_loss            | 727         |
------------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñÑ‚ñà‚ñà‚ñÖ‚ñÖ‚ñÜ
wandb:             train/approx_kl ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñà‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÜ‚ñÅ‚ñÖ‚ñÉ‚ñÜ‚ñÉ‚ñÑ
wandb:         train/clip_fraction ‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñà‚ñÜ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñà‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÇ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ
wandb:          train/entropy_loss ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:    train/explained_variance ‚ñà‚ñá‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ
wandb: train/lagrangian_multiplier ‚ñá‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñà‚ñá‚ñá‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÜ‚ñÉ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñá
wandb:                   train/std ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñá‚ñá
wandb:            train/value_loss ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñá‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñà‚ñÖ‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ
wandb: 
wandb: Run summary:
wandb:                      reward -0.99427
wandb:             train/approx_kl 0.00581
wandb:         train/clip_fraction 0.04497
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 46.45607
wandb:          train/entropy_loss -2.85633
wandb:    train/explained_variance 0.00435
wandb: train/lagrangian_multiplier 0.0584
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 64.65322
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.0034
wandb:                   train/std 1.01332
wandb:            train/value_loss 589.87103
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/387kdty3
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231223_003742-387kdty3/logs
srun: error: ddpg.ist.berkeley.edu: task 0: Exited with exit code 1
--------------------------------------------
| reward                   | [-0.40243924] |
| time/                    |               |
|    fps                   | 130           |
|    iterations            | 3             |
|    time_elapsed          | 47            |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.0061575174  |
|    clip_fraction         | 0.0486        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.373         |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.075        |
|    lagrangian_multiplier | 0.0509        |
|    learning_rate         | 0.0003        |
|    loss                  | 34.5          |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00558      |
|    std                   | 1.01          |
|    value_loss            | 300           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.4910021] |
| time/                    |              |
|    fps                   | 96           |
|    iterations            | 24           |
|    time_elapsed          | 508          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.007056984  |
|    clip_fraction         | 0.0702       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 240          |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.00992     |
|    lagrangian_multiplier | 0.0647       |
|    learning_rate         | 0.0003       |
|    loss                  | 102          |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00989     |
|    std                   | 0.953        |
|    value_loss            | 906          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114474.11
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231223_005126-u1qsee1p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/u1qsee1p
------------------------------------------
| reward                   | [-1.114191] |
| time/                    |             |
|    fps                   | 129         |
|    iterations            | 4           |
|    time_elapsed          | 63          |
|    total_timesteps       | 8192        |
| train/                   |             |
|    approx_kl             | 0.005109921 |
|    clip_fraction         | 0.0334      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0946      |
|    entropy_loss          | -2.86       |
|    explained_variance    | -0.0265     |
|    lagrangian_multiplier | 0.059       |
|    learning_rate         | 0.0003      |
|    loss                  | 28.1        |
|    n_updates             | 30          |
|    policy_gradient_loss  | -0.00469    |
|    std                   | 1.01        |
|    value_loss            | 256         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.0143961] |
| time/                    |              |
|    fps                   | 97           |
|    iterations            | 25           |
|    time_elapsed          | 524          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.00806855   |
|    clip_fraction         | 0.074        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 251          |
|    entropy_loss          | -2.74        |
|    explained_variance    | 0.00995      |
|    lagrangian_multiplier | 0.0621       |
|    learning_rate         | 0.0003       |
|    loss                  | 293          |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.0109      |
|    std                   | 0.949        |
|    value_loss            | 2.77e+03     |
-------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.31384775] |
| time/              |               |
|    fps             | 137           |
|    iterations      | 1             |
|    time_elapsed    | 14            |
|    total_timesteps | 2048          |
--------------------------------------
-------------------------------------------
| reward                   | [-1.0245544] |
| time/                    |              |
|    fps                   | 23           |
|    iterations            | 29           |
|    time_elapsed          | 2519         |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.0047057564 |
|    clip_fraction         | 0.0463       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00409      |
|    entropy_loss          | -2.97        |
|    explained_variance    | 0.00451      |
|    lagrangian_multiplier | 0.0743       |
|    learning_rate         | 0.0003       |
|    loss                  | 26.3         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00401     |
|    std                   | 1.07         |
|    value_loss            | 260          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.287904]  |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 5            |
|    time_elapsed          | 78           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0033223522 |
|    clip_fraction         | 0.0179       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.147        |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0408      |
|    lagrangian_multiplier | 0.0451       |
|    learning_rate         | 0.0003       |
|    loss                  | 57.2         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00297     |
|    std                   | 1            |
|    value_loss            | 446          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2581897] |
| time/                    |              |
|    fps                   | 98           |
|    iterations            | 26           |
|    time_elapsed          | 540          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.006732722  |
|    clip_fraction         | 0.0666       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 200          |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.0125      |
|    lagrangian_multiplier | 0.0684       |
|    learning_rate         | 0.0003       |
|    loss                  | 124          |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.0112      |
|    std                   | 0.949        |
|    value_loss            | 1.27e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.49480182] |
| time/                    |               |
|    fps                   | 133           |
|    iterations            | 2             |
|    time_elapsed          | 30            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.0059221745  |
|    clip_fraction         | 0.0571        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.19          |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.0138       |
|    lagrangian_multiplier | 0.0571        |
|    learning_rate         | 0.0003        |
|    loss                  | 32.7          |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00635      |
|    std                   | 0.999         |
|    value_loss            | 292           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.4184403] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 6            |
|    time_elapsed          | 94           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.004633948  |
|    clip_fraction         | 0.038        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.04         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0253      |
|    lagrangian_multiplier | 0.0447       |
|    learning_rate         | 0.0003       |
|    loss                  | 109          |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00524     |
|    std                   | 1            |
|    value_loss            | 752          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1780475] |
| time/                    |              |
|    fps                   | 99           |
|    iterations            | 27           |
|    time_elapsed          | 556          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.0057516564 |
|    clip_fraction         | 0.0634       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 256          |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.0111      |
|    lagrangian_multiplier | 0.0622       |
|    learning_rate         | 0.0003       |
|    loss                  | 135          |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00866     |
|    std                   | 0.948        |
|    value_loss            | 1.3e+03      |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6309363] |
| time/                    |              |
|    fps                   | 132          |
|    iterations            | 3            |
|    time_elapsed          | 46           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0049044034 |
|    clip_fraction         | 0.0354       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0953       |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.00414      |
|    lagrangian_multiplier | 0.0684       |
|    learning_rate         | 0.0003       |
|    loss                  | 48.6         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00736     |
|    std                   | 1.01         |
|    value_loss            | 698          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.45835534] |
| time/                    |               |
|    fps                   | 129           |
|    iterations            | 7             |
|    time_elapsed          | 110           |
|    total_timesteps       | 14336         |
| train/                   |               |
|    approx_kl             | 0.0055893157  |
|    clip_fraction         | 0.0347        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 1.29          |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.0145        |
|    lagrangian_multiplier | 0.0484        |
|    learning_rate         | 0.0003        |
|    loss                  | 86.4          |
|    n_updates             | 60            |
|    policy_gradient_loss  | -0.0057       |
|    std                   | 1             |
|    value_loss            | 655           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.019592]  |
| time/                    |              |
|    fps                   | 100          |
|    iterations            | 28           |
|    time_elapsed          | 573          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.0076509947 |
|    clip_fraction         | 0.0681       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 229          |
|    entropy_loss          | -2.73        |
|    explained_variance    | -0.021       |
|    lagrangian_multiplier | 0.074        |
|    learning_rate         | 0.0003       |
|    loss                  | 76.9         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 0.953        |
|    value_loss            | 619          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.6808779] |
| time/                    |              |
|    fps                   | 131          |
|    iterations            | 4            |
|    time_elapsed          | 62           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.0062822574 |
|    clip_fraction         | 0.0361       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.148        |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0254       |
|    lagrangian_multiplier | 0.0564       |
|    learning_rate         | 0.0003       |
|    loss                  | 36.3         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00544     |
|    std                   | 1.01         |
|    value_loss            | 372          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3366841] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 8            |
|    time_elapsed          | 126          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.008355587  |
|    clip_fraction         | 0.0692       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.19         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0204      |
|    lagrangian_multiplier | 0.0512       |
|    learning_rate         | 0.0003       |
|    loss                  | 88.3         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00987     |
|    std                   | 0.995        |
|    value_loss            | 905          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4869313] |
| time/                    |              |
|    fps                   | 100          |
|    iterations            | 29           |
|    time_elapsed          | 589          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.0054561235 |
|    clip_fraction         | 0.0456       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 232          |
|    entropy_loss          | -2.74        |
|    explained_variance    | -0.00666     |
|    lagrangian_multiplier | 0.0699       |
|    learning_rate         | 0.0003       |
|    loss                  | 53.9         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00893     |
|    std                   | 0.957        |
|    value_loss            | 396          |
-------------------------------------------
------------------------------------------
| reward                   | [-0.754416] |
| time/                    |             |
|    fps                   | 130         |
|    iterations            | 5           |
|    time_elapsed          | 78          |
|    total_timesteps       | 10240       |
| train/                   |             |
|    approx_kl             | 0.00486489  |
|    clip_fraction         | 0.0378      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.172       |
|    entropy_loss          | -2.87       |
|    explained_variance    | 0.00432     |
|    lagrangian_multiplier | 0.0671      |
|    learning_rate         | 0.0003      |
|    loss                  | 49.7        |
|    n_updates             | 40          |
|    policy_gradient_loss  | -0.00742    |
|    std                   | 1.02        |
|    value_loss            | 516         |
------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.7147039] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 9            |
|    time_elapsed          | 142          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.004374353  |
|    clip_fraction         | 0.0395       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.14         |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0601       |
|    lagrangian_multiplier | 0.0506       |
|    learning_rate         | 0.0003       |
|    loss                  | 68.6         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00756     |
|    std                   | 1            |
|    value_loss            | 551          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.96153444] |
| time/                    |               |
|    fps                   | 101           |
|    iterations            | 30            |
|    time_elapsed          | 605           |
|    total_timesteps       | 61440         |
| train/                   |               |
|    approx_kl             | 0.008527417   |
|    clip_fraction         | 0.112         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 207           |
|    entropy_loss          | -2.76         |
|    explained_variance    | 0.00793       |
|    lagrangian_multiplier | 0.0688        |
|    learning_rate         | 0.0003        |
|    loss                  | 51.8          |
|    n_updates             | 290           |
|    policy_gradient_loss  | -0.011        |
|    std                   | 0.968         |
|    value_loss            | 389           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.4087917] |
| time/                    |              |
|    fps                   | 130          |
|    iterations            | 6            |
|    time_elapsed          | 94           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0064691333 |
|    clip_fraction         | 0.0619       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.111        |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.0828      |
|    lagrangian_multiplier | 0.0566       |
|    learning_rate         | 0.0003       |
|    loss                  | 57.8         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00704     |
|    std                   | 1.01         |
|    value_loss            | 548          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7311299] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 10           |
|    time_elapsed          | 158          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0066970065 |
|    clip_fraction         | 0.0644       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 5.15         |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0379       |
|    lagrangian_multiplier | 0.0535       |
|    learning_rate         | 0.0003       |
|    loss                  | 106          |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00858     |
|    std                   | 0.999        |
|    value_loss            | 786          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6395332] |
| time/                    |              |
|    fps                   | 23           |
|    iterations            | 30           |
|    time_elapsed          | 2603         |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0067953872 |
|    clip_fraction         | 0.0554       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00384      |
|    entropy_loss          | -2.97        |
|    explained_variance    | 0.0343       |
|    lagrangian_multiplier | 0.0782       |
|    learning_rate         | 0.0003       |
|    loss                  | 9.06         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00638     |
|    std                   | 1.07         |
|    value_loss            | 111          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7449217] |
| time/                    |              |
|    fps                   | 102          |
|    iterations            | 31           |
|    time_elapsed          | 621          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0053843274 |
|    clip_fraction         | 0.0556       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 191          |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.0238       |
|    lagrangian_multiplier | 0.0798       |
|    learning_rate         | 0.0003       |
|    loss                  | 56.2         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.0101      |
|    std                   | 0.966        |
|    value_loss            | 471          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.020021]  |
| time/                    |              |
|    fps                   | 130          |
|    iterations            | 7            |
|    time_elapsed          | 110          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.0056644822 |
|    clip_fraction         | 0.05         |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.166        |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.00355     |
|    lagrangian_multiplier | 0.0586       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.7         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00773     |
|    std                   | 1.01         |
|    value_loss            | 681          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8649042] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 11           |
|    time_elapsed          | 175          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0068664104 |
|    clip_fraction         | 0.0805       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.152        |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0881      |
|    lagrangian_multiplier | 0.0451       |
|    learning_rate         | 0.0003       |
|    loss                  | 51.3         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.0112      |
|    std                   | 1.01         |
|    value_loss            | 412          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7376947] |
| time/                    |              |
|    fps                   | 102          |
|    iterations            | 32           |
|    time_elapsed          | 637          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.007606258  |
|    clip_fraction         | 0.0543       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 216          |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0237      |
|    lagrangian_multiplier | 0.0729       |
|    learning_rate         | 0.0003       |
|    loss                  | 152          |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00718     |
|    std                   | 0.959        |
|    value_loss            | 1.24e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2118647] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 8            |
|    time_elapsed          | 126          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.0058270553 |
|    clip_fraction         | 0.0493       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.253        |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.000173    |
|    lagrangian_multiplier | 0.0645       |
|    learning_rate         | 0.0003       |
|    loss                  | 45           |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00761     |
|    std                   | 1            |
|    value_loss            | 442          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8864853] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 12           |
|    time_elapsed          | 191          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.007788265  |
|    clip_fraction         | 0.0758       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.59         |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.0714      |
|    lagrangian_multiplier | 0.0479       |
|    learning_rate         | 0.0003       |
|    loss                  | 38.4         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00886     |
|    std                   | 1.03         |
|    value_loss            | 300          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.4414504] |
| time/                    |              |
|    fps                   | 103          |
|    iterations            | 33           |
|    time_elapsed          | 653          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.006356404  |
|    clip_fraction         | 0.054        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 185          |
|    entropy_loss          | -2.75        |
|    explained_variance    | 0.016        |
|    lagrangian_multiplier | 0.0693       |
|    learning_rate         | 0.0003       |
|    loss                  | 48.7         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00714     |
|    std                   | 0.956        |
|    value_loss            | 368          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.036165]  |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 9            |
|    time_elapsed          | 142          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0040923166 |
|    clip_fraction         | 0.0205       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.102        |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.129       |
|    lagrangian_multiplier | 0.0658       |
|    learning_rate         | 0.0003       |
|    loss                  | 24           |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.0033      |
|    std                   | 0.987        |
|    value_loss            | 253          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0862601] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 13           |
|    time_elapsed          | 207          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0056267288 |
|    clip_fraction         | 0.0489       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.169        |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0669       |
|    lagrangian_multiplier | 0.0509       |
|    learning_rate         | 0.0003       |
|    loss                  | 27.1         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00595     |
|    std                   | 1.02         |
|    value_loss            | 270          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9827882] |
| time/                    |              |
|    fps                   | 103          |
|    iterations            | 34           |
|    time_elapsed          | 669          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.009699804  |
|    clip_fraction         | 0.0936       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 232          |
|    entropy_loss          | -2.72        |
|    explained_variance    | 0.0296       |
|    lagrangian_multiplier | 0.0651       |
|    learning_rate         | 0.0003       |
|    loss                  | 83.9         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.0141      |
|    std                   | 0.937        |
|    value_loss            | 657          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.0152156] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 10           |
|    time_elapsed          | 158          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.005148608  |
|    clip_fraction         | 0.0409       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0639       |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0834      |
|    lagrangian_multiplier | 0.0686       |
|    learning_rate         | 0.0003       |
|    loss                  | 42.5         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00581     |
|    std                   | 0.986        |
|    value_loss            | 405          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4134169] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 14           |
|    time_elapsed          | 223          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0071554407 |
|    clip_fraction         | 0.0625       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 7.82         |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0247       |
|    lagrangian_multiplier | 0.0515       |
|    learning_rate         | 0.0003       |
|    loss                  | 54           |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00756     |
|    std                   | 1.03         |
|    value_loss            | 478          |
-------------------------------------------
------------------------------------------
| reward                   | [-2.212643] |
| time/                    |             |
|    fps                   | 104         |
|    iterations            | 35          |
|    time_elapsed          | 686         |
|    total_timesteps       | 71680       |
| train/                   |             |
|    approx_kl             | 0.006912307 |
|    clip_fraction         | 0.0774      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 235         |
|    entropy_loss          | -2.7        |
|    explained_variance    | -0.00936    |
|    lagrangian_multiplier | 0.0664      |
|    learning_rate         | 0.0003      |
|    loss                  | 107         |
|    n_updates             | 340         |
|    policy_gradient_loss  | -0.0111     |
|    std                   | 0.935       |
|    value_loss            | 770         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.5935904] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 11           |
|    time_elapsed          | 173          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.007025563  |
|    clip_fraction         | 0.0491       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.122        |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0563      |
|    lagrangian_multiplier | 0.0541       |
|    learning_rate         | 0.0003       |
|    loss                  | 34.2         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00661     |
|    std                   | 0.975        |
|    value_loss            | 309          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5945419] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 15           |
|    time_elapsed          | 239          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.006343404  |
|    clip_fraction         | 0.0528       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.217        |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.0415      |
|    lagrangian_multiplier | 0.0481       |
|    learning_rate         | 0.0003       |
|    loss                  | 49.9         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.0075      |
|    std                   | 1.02         |
|    value_loss            | 421          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.91144955] |
| time/                    |               |
|    fps                   | 23            |
|    iterations            | 31            |
|    time_elapsed          | 2682          |
|    total_timesteps       | 63488         |
| train/                   |               |
|    approx_kl             | 0.005005151   |
|    clip_fraction         | 0.0317        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00539       |
|    entropy_loss          | -2.96         |
|    explained_variance    | 0.0516        |
|    lagrangian_multiplier | 0.0814        |
|    learning_rate         | 0.0003        |
|    loss                  | 32.1          |
|    n_updates             | 300           |
|    policy_gradient_loss  | -0.00487      |
|    std                   | 1.06          |
|    value_loss            | 413           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.1668015] |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 36           |
|    time_elapsed          | 702          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.00983978   |
|    clip_fraction         | 0.101        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 249          |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.0196      |
|    lagrangian_multiplier | 0.0601       |
|    learning_rate         | 0.0003       |
|    loss                  | 219          |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0176      |
|    std                   | 0.951        |
|    value_loss            | 1.76e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6072156] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 12           |
|    time_elapsed          | 189          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.007013851  |
|    clip_fraction         | 0.0709       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.223        |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.028       |
|    lagrangian_multiplier | 0.0609       |
|    learning_rate         | 0.0003       |
|    loss                  | 65.1         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00738     |
|    std                   | 0.974        |
|    value_loss            | 698          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0790181] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 16           |
|    time_elapsed          | 255          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.006449255  |
|    clip_fraction         | 0.0675       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.253        |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.041       |
|    lagrangian_multiplier | 0.0549       |
|    learning_rate         | 0.0003       |
|    loss                  | 34.3         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00956     |
|    std                   | 1.03         |
|    value_loss            | 306          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.9405043] |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 37           |
|    time_elapsed          | 718          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.007556959  |
|    clip_fraction         | 0.0498       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 253          |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.0328      |
|    lagrangian_multiplier | 0.0619       |
|    learning_rate         | 0.0003       |
|    loss                  | 111          |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00964     |
|    std                   | 0.946        |
|    value_loss            | 739          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5053257] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 13           |
|    time_elapsed          | 205          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0055822134 |
|    clip_fraction         | 0.052        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.219        |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.027       |
|    lagrangian_multiplier | 0.0489       |
|    learning_rate         | 0.0003       |
|    loss                  | 71.7         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00909     |
|    std                   | 0.96         |
|    value_loss            | 574          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.97344923] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 17            |
|    time_elapsed          | 271           |
|    total_timesteps       | 34816         |
| train/                   |               |
|    approx_kl             | 0.004092902   |
|    clip_fraction         | 0.0343        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 2.64          |
|    entropy_loss          | -2.9          |
|    explained_variance    | -0.0392       |
|    lagrangian_multiplier | 0.0564        |
|    learning_rate         | 0.0003        |
|    loss                  | 39.9          |
|    n_updates             | 160           |
|    policy_gradient_loss  | -0.00454      |
|    std                   | 1.03          |
|    value_loss            | 372           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.6146351] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 14           |
|    time_elapsed          | 221          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.0064431857 |
|    clip_fraction         | 0.0624       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.247        |
|    entropy_loss          | -2.76        |
|    explained_variance    | -0.0172      |
|    lagrangian_multiplier | 0.0622       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.7         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00598     |
|    std                   | 0.967        |
|    value_loss            | 475          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2349004] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 18           |
|    time_elapsed          | 287          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0066235517 |
|    clip_fraction         | 0.0593       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 5.09         |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.0148      |
|    lagrangian_multiplier | 0.0509       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.4         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00747     |
|    std                   | 1.02         |
|    value_loss            | 399          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.6999822] |
| time/                    |              |
|    fps                   | 103          |
|    iterations            | 38           |
|    time_elapsed          | 753          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0063789585 |
|    clip_fraction         | 0.0647       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 195          |
|    entropy_loss          | -2.72        |
|    explained_variance    | -0.0288      |
|    lagrangian_multiplier | 0.0647       |
|    learning_rate         | 0.0003       |
|    loss                  | 155          |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.0107      |
|    std                   | 0.942        |
|    value_loss            | 1.45e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.7388012] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 15           |
|    time_elapsed          | 237          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.007359628  |
|    clip_fraction         | 0.0611       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.179        |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0117      |
|    lagrangian_multiplier | 0.0624       |
|    learning_rate         | 0.0003       |
|    loss                  | 37.2         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00834     |
|    std                   | 0.971        |
|    value_loss            | 463          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9137427] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 19           |
|    time_elapsed          | 303          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0051316596 |
|    clip_fraction         | 0.0561       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.19         |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.00461     |
|    lagrangian_multiplier | 0.0484       |
|    learning_rate         | 0.0003       |
|    loss                  | 40.2         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00719     |
|    std                   | 1.01         |
|    value_loss            | 311          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.156672] |
| time/                    |             |
|    fps                   | 103         |
|    iterations            | 39          |
|    time_elapsed          | 769         |
|    total_timesteps       | 79872       |
| train/                   |             |
|    approx_kl             | 0.008437282 |
|    clip_fraction         | 0.0797      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 245         |
|    entropy_loss          | -2.71       |
|    explained_variance    | -0.0232     |
|    lagrangian_multiplier | 0.0591      |
|    learning_rate         | 0.0003      |
|    loss                  | 208         |
|    n_updates             | 380         |
|    policy_gradient_loss  | -0.0114     |
|    std                   | 0.936       |
|    value_loss            | 1.73e+03    |
------------------------------------------
-------------------------------------------
| reward                   | [-1.4420292] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 16           |
|    time_elapsed          | 253          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.005495956  |
|    clip_fraction         | 0.0404       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0853       |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0393      |
|    lagrangian_multiplier | 0.0642       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.7         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00604     |
|    std                   | 0.981        |
|    value_loss            | 574          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.4510977] |
| time/                    |              |
|    fps                   | 23           |
|    iterations            | 32           |
|    time_elapsed          | 2758         |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.004756744  |
|    clip_fraction         | 0.0464       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00342      |
|    entropy_loss          | -2.96        |
|    explained_variance    | 0.0519       |
|    lagrangian_multiplier | 0.0755       |
|    learning_rate         | 0.0003       |
|    loss                  | 22.5         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00464     |
|    std                   | 1.06         |
|    value_loss            | 314          |
-------------------------------------------
-----------------------------------------
| reward                   | [-1.56395] |
| time/                    |            |
|    fps                   | 128        |
|    iterations            | 20         |
|    time_elapsed          | 319        |
|    total_timesteps       | 40960      |
| train/                   |            |
|    approx_kl             | 0.00603295 |
|    clip_fraction         | 0.0372     |
|    clip_range            | 0.2        |
|    cost_value_loss       | 0.177      |
|    entropy_loss          | -2.84      |
|    explained_variance    | -0.0389    |
|    lagrangian_multiplier | 0.0514     |
|    learning_rate         | 0.0003     |
|    loss                  | 34.5       |
|    n_updates             | 190        |
|    policy_gradient_loss  | -0.0069    |
|    std                   | 0.992      |
|    value_loss            | 307        |
-----------------------------------------
-------------------------------------------
| reward                   | [-2.6977096] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 17           |
|    time_elapsed          | 269          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.004926286  |
|    clip_fraction         | 0.0373       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0708       |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.00745     |
|    lagrangian_multiplier | 0.0679       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.1         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00403     |
|    std                   | 0.972        |
|    value_loss            | 774          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4423996] |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 40           |
|    time_elapsed          | 785          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.005628042  |
|    clip_fraction         | 0.0539       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 219          |
|    entropy_loss          | -2.71        |
|    explained_variance    | -0.0183      |
|    lagrangian_multiplier | 0.0799       |
|    learning_rate         | 0.0003       |
|    loss                  | 91.1         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00785     |
|    std                   | 0.944        |
|    value_loss            | 927          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0011408] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 21           |
|    time_elapsed          | 335          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.003995847  |
|    clip_fraction         | 0.0302       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.196        |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.00267     |
|    lagrangian_multiplier | 0.0505       |
|    learning_rate         | 0.0003       |
|    loss                  | 43           |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00572     |
|    std                   | 0.991        |
|    value_loss            | 386          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-4.329652]  |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 18           |
|    time_elapsed          | 285          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0048608603 |
|    clip_fraction         | 0.0313       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0963       |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0296       |
|    lagrangian_multiplier | 0.0656       |
|    learning_rate         | 0.0003       |
|    loss                  | 120          |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00475     |
|    std                   | 0.973        |
|    value_loss            | 1.18e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-4.8518763] |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 41           |
|    time_elapsed          | 801          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.008684063  |
|    clip_fraction         | 0.0877       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 247          |
|    entropy_loss          | -2.71        |
|    explained_variance    | -0.00505     |
|    lagrangian_multiplier | 0.0681       |
|    learning_rate         | 0.0003       |
|    loss                  | 103          |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.0133      |
|    std                   | 0.938        |
|    value_loss            | 945          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8102584] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 22           |
|    time_elapsed          | 351          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.004526811  |
|    clip_fraction         | 0.0425       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0625       |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0446      |
|    lagrangian_multiplier | 0.0413       |
|    learning_rate         | 0.0003       |
|    loss                  | 32.7         |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00562     |
|    std                   | 0.988        |
|    value_loss            | 259          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.5308814] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 19           |
|    time_elapsed          | 301          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.005967128  |
|    clip_fraction         | 0.0521       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0941       |
|    entropy_loss          | -2.8         |
|    explained_variance    | 0.0334       |
|    lagrangian_multiplier | 0.0636       |
|    learning_rate         | 0.0003       |
|    loss                  | 159          |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.0057      |
|    std                   | 0.986        |
|    value_loss            | 1.6e+03      |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.5677098] |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 42           |
|    time_elapsed          | 818          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.007939774  |
|    clip_fraction         | 0.0833       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 242          |
|    entropy_loss          | -2.7         |
|    explained_variance    | 0.00214      |
|    lagrangian_multiplier | 0.059        |
|    learning_rate         | 0.0003       |
|    loss                  | 229          |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.0104      |
|    std                   | 0.93         |
|    value_loss            | 2.38e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.38780016] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 23            |
|    time_elapsed          | 367           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.005564753   |
|    clip_fraction         | 0.0519        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0497        |
|    entropy_loss          | -2.81         |
|    explained_variance    | -0.0782       |
|    lagrangian_multiplier | 0.0527        |
|    learning_rate         | 0.0003        |
|    loss                  | 30.5          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00499      |
|    std                   | 0.982         |
|    value_loss            | 246           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.6564896] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 20           |
|    time_elapsed          | 317          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.008095595  |
|    clip_fraction         | 0.0714       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0753       |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0391       |
|    lagrangian_multiplier | 0.0648       |
|    learning_rate         | 0.0003       |
|    loss                  | 153          |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.0105      |
|    std                   | 0.985        |
|    value_loss            | 1.36e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8597741] |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 43           |
|    time_elapsed          | 834          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.00769932   |
|    clip_fraction         | 0.0786       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 249          |
|    entropy_loss          | -2.67        |
|    explained_variance    | -0.0112      |
|    lagrangian_multiplier | 0.0704       |
|    learning_rate         | 0.0003       |
|    loss                  | 223          |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.0106      |
|    std                   | 0.916        |
|    value_loss            | 2.12e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5884986] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 24           |
|    time_elapsed          | 383          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.004478452  |
|    clip_fraction         | 0.0393       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0499       |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0104      |
|    lagrangian_multiplier | 0.0576       |
|    learning_rate         | 0.0003       |
|    loss                  | 30.4         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00598     |
|    std                   | 0.975        |
|    value_loss            | 286          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.4516838] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 21           |
|    time_elapsed          | 333          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.006974066  |
|    clip_fraction         | 0.0576       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.065        |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0263      |
|    lagrangian_multiplier | 0.0674       |
|    learning_rate         | 0.0003       |
|    loss                  | 123          |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00797     |
|    std                   | 0.985        |
|    value_loss            | 1.14e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.47943833] |
| time/                    |               |
|    fps                   | 105           |
|    iterations            | 44            |
|    time_elapsed          | 850           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.008145918   |
|    clip_fraction         | 0.0633        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 239           |
|    entropy_loss          | -2.64         |
|    explained_variance    | -0.00967      |
|    lagrangian_multiplier | 0.0608        |
|    learning_rate         | 0.0003        |
|    loss                  | 209           |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.013        |
|    std                   | 0.899         |
|    value_loss            | 1.76e+03      |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.60415393] |
| time/                    |               |
|    fps                   | 23            |
|    iterations            | 33            |
|    time_elapsed          | 2839          |
|    total_timesteps       | 67584         |
| train/                   |               |
|    approx_kl             | 0.004414625   |
|    clip_fraction         | 0.0391        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00324       |
|    entropy_loss          | -2.98         |
|    explained_variance    | 0.044         |
|    lagrangian_multiplier | 0.0808        |
|    learning_rate         | 0.0003        |
|    loss                  | 14.6          |
|    n_updates             | 320           |
|    policy_gradient_loss  | -0.00342      |
|    std                   | 1.08          |
|    value_loss            | 157           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.8091716] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 25           |
|    time_elapsed          | 399          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.0047270684 |
|    clip_fraction         | 0.0452       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0557       |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0441      |
|    lagrangian_multiplier | 0.0418       |
|    learning_rate         | 0.0003       |
|    loss                  | 34.2         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00527     |
|    std                   | 0.974        |
|    value_loss            | 242          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.545284]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 22           |
|    time_elapsed          | 349          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.0058285804 |
|    clip_fraction         | 0.0548       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0452       |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0283       |
|    lagrangian_multiplier | 0.07         |
|    learning_rate         | 0.0003       |
|    loss                  | 86.9         |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00512     |
|    std                   | 0.991        |
|    value_loss            | 869          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5843776] |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 45           |
|    time_elapsed          | 870          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.008657002  |
|    clip_fraction         | 0.079        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 240          |
|    entropy_loss          | -2.62        |
|    explained_variance    | -0.0114      |
|    lagrangian_multiplier | 0.0656       |
|    learning_rate         | 0.0003       |
|    loss                  | 143          |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.0138      |
|    std                   | 0.898        |
|    value_loss            | 1.33e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9294822] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 26           |
|    time_elapsed          | 415          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0050720666 |
|    clip_fraction         | 0.0465       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0716       |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0306      |
|    lagrangian_multiplier | 0.0447       |
|    learning_rate         | 0.0003       |
|    loss                  | 38.4         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00498     |
|    std                   | 0.982        |
|    value_loss            | 306          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.55674547] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 23            |
|    time_elapsed          | 365           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.0063520563  |
|    clip_fraction         | 0.0436        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0893        |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.0289        |
|    lagrangian_multiplier | 0.0589        |
|    learning_rate         | 0.0003        |
|    loss                  | 84.2          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00672      |
|    std                   | 0.997         |
|    value_loss            | 868           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.62990236] |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 46            |
|    time_elapsed          | 886           |
|    total_timesteps       | 94208         |
| train/                   |               |
|    approx_kl             | 0.0074921297  |
|    clip_fraction         | 0.0776        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 235           |
|    entropy_loss          | -2.61         |
|    explained_variance    | -0.0076       |
|    lagrangian_multiplier | 0.0676        |
|    learning_rate         | 0.0003        |
|    loss                  | 188           |
|    n_updates             | 450           |
|    policy_gradient_loss  | -0.0126       |
|    std                   | 0.891         |
|    value_loss            | 2.07e+03      |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.1825156] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 27           |
|    time_elapsed          | 431          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.0061207553 |
|    clip_fraction         | 0.0451       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.113        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.00387      |
|    lagrangian_multiplier | 0.0436       |
|    learning_rate         | 0.0003       |
|    loss                  | 43.9         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00472     |
|    std                   | 0.994        |
|    value_loss            | 335          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2165186] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 24           |
|    time_elapsed          | 381          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.008153391  |
|    clip_fraction         | 0.0879       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.106        |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0287       |
|    lagrangian_multiplier | 0.0652       |
|    learning_rate         | 0.0003       |
|    loss                  | 181          |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.0126      |
|    std                   | 0.983        |
|    value_loss            | 1.76e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1994905] |
| time/                    |              |
|    fps                   | 106          |
|    iterations            | 47           |
|    time_elapsed          | 902          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.007234279  |
|    clip_fraction         | 0.0628       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 241          |
|    entropy_loss          | -2.61        |
|    explained_variance    | -0.0216      |
|    lagrangian_multiplier | 0.07         |
|    learning_rate         | 0.0003       |
|    loss                  | 166          |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.00954     |
|    std                   | 0.893        |
|    value_loss            | 1.69e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.40753275] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 28            |
|    time_elapsed          | 447           |
|    total_timesteps       | 57344         |
| train/                   |               |
|    approx_kl             | 0.0044945134  |
|    clip_fraction         | 0.0355        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0667        |
|    entropy_loss          | -2.82         |
|    explained_variance    | -0.0496       |
|    lagrangian_multiplier | 0.0312        |
|    learning_rate         | 0.0003        |
|    loss                  | 61.6          |
|    n_updates             | 270           |
|    policy_gradient_loss  | -0.00485      |
|    std                   | 0.986         |
|    value_loss            | 346           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.2767114] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 25           |
|    time_elapsed          | 397          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.0063083568 |
|    clip_fraction         | 0.0576       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0761       |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.00751     |
|    lagrangian_multiplier | 0.0434       |
|    learning_rate         | 0.0003       |
|    loss                  | 278          |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00821     |
|    std                   | 0.977        |
|    value_loss            | 2.17e+03     |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.0589395] |
| time/                    |              |
|    fps                   | 106          |
|    iterations            | 48           |
|    time_elapsed          | 918          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.008416307  |
|    clip_fraction         | 0.0728       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 239          |
|    entropy_loss          | -2.61        |
|    explained_variance    | -0.0158      |
|    lagrangian_multiplier | 0.0636       |
|    learning_rate         | 0.0003       |
|    loss                  | 188          |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.0135      |
|    std                   | 0.89         |
|    value_loss            | 2.03e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.87148595] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 29            |
|    time_elapsed          | 463           |
|    total_timesteps       | 59392         |
| train/                   |               |
|    approx_kl             | 0.006254997   |
|    clip_fraction         | 0.0512        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0815        |
|    entropy_loss          | -2.8          |
|    explained_variance    | -0.0232       |
|    lagrangian_multiplier | 0.044         |
|    learning_rate         | 0.0003        |
|    loss                  | 73.3          |
|    n_updates             | 280           |
|    policy_gradient_loss  | -0.00666      |
|    std                   | 0.975         |
|    value_loss            | 558           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.41569495] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 26            |
|    time_elapsed          | 413           |
|    total_timesteps       | 53248         |
| train/                   |               |
|    approx_kl             | 0.008616633   |
|    clip_fraction         | 0.0712        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.109         |
|    entropy_loss          | -2.79         |
|    explained_variance    | 0.0169        |
|    lagrangian_multiplier | 0.0645        |
|    learning_rate         | 0.0003        |
|    loss                  | 151           |
|    n_updates             | 250           |
|    policy_gradient_loss  | -0.00945      |
|    std                   | 0.977         |
|    value_loss            | 1.56e+03      |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.90067416] |
| time/                    |               |
|    fps                   | 107           |
|    iterations            | 49            |
|    time_elapsed          | 934           |
|    total_timesteps       | 100352        |
| train/                   |               |
|    approx_kl             | 0.0075435326  |
|    clip_fraction         | 0.07          |
|    clip_range            | 0.2           |
|    cost_value_loss       | 229           |
|    entropy_loss          | -2.6          |
|    explained_variance    | -0.00335      |
|    lagrangian_multiplier | 0.0728        |
|    learning_rate         | 0.0003        |
|    loss                  | 215           |
|    n_updates             | 480           |
|    policy_gradient_loss  | -0.0103       |
|    std                   | 0.889         |
|    value_loss            | 2.58e+03      |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.99128747] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 30            |
|    time_elapsed          | 479           |
|    total_timesteps       | 61440         |
| train/                   |               |
|    approx_kl             | 0.006500137   |
|    clip_fraction         | 0.0574        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.307         |
|    entropy_loss          | -2.8          |
|    explained_variance    | 0.0154        |
|    lagrangian_multiplier | 0.043         |
|    learning_rate         | 0.0003        |
|    loss                  | 71.6          |
|    n_updates             | 290           |
|    policy_gradient_loss  | -0.00842      |
|    std                   | 0.985         |
|    value_loss            | 591           |
--------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 103, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 90, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
-------------------------------------------
| reward                   | [-0.2802865] |
| time/                    |              |
|    fps                   | 23           |
|    iterations            | 34           |
|    time_elapsed          | 2921         |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0051424475 |
|    clip_fraction         | 0.0448       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00368      |
|    entropy_loss          | -2.98        |
|    explained_variance    | 0.0395       |
|    lagrangian_multiplier | 0.0774       |
|    learning_rate         | 0.0003       |
|    loss                  | 14.3         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00425     |
|    std                   | 1.08         |
|    value_loss            | 190          |
-------------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñá‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñà‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÅ‚ñÅ‚ñà‚ñà‚ñÑ‚ñÉ‚ñá
wandb:             train/approx_kl ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñá‚ñÇ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÇ‚ñÖ‚ñÉ‚ñà‚ñà‚ñÖ‚ñÉ‚ñÜ‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ
wandb:         train/clip_fraction ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÇ‚ñà‚ñÉ‚ñÖ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñá‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñÖ‚ñá‚ñÜ‚ñà‚ñá‚ñÖ‚ñá‚ñá‚ñà‚ñÖ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñà‚ñà‚ñÖ‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:          train/entropy_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:    train/explained_variance ‚ñá‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñà‚ñá‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÜ‚ñá‚ñÇ‚ñÜ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÑ
wandb: train/lagrangian_multiplier ‚ñÉ‚ñÖ‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñÅ‚ñÑ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñá‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÖ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÇ‚ñÜ‚ñÉ‚ñÑ‚ñÜ‚ñÇ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÜ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñá‚ñá‚ñà‚ñÖ‚ñà‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÉ‚ñÅ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÖ
wandb:                   train/std ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            train/value_loss ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñá
wandb: 
wandb: Run summary:
wandb:                      reward -0.90067
wandb:             train/approx_kl 0.00754
wandb:         train/clip_fraction 0.06997
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 228.84399
wandb:          train/entropy_loss -2.60199
wandb:    train/explained_variance -0.00335
wandb: train/lagrangian_multiplier 0.07282
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 215.19417
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.0103
wandb:                   train/std 0.88872
wandb:            train/value_loss 2583.51826
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/nwc579oy
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231223_084249-nwc579oy/logs
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: error: airl.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-1.5841503] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 27           |
|    time_elapsed          | 429          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.0072580175 |
|    clip_fraction         | 0.0604       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0914       |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.0302       |
|    lagrangian_multiplier | 0.0688       |
|    learning_rate         | 0.0003       |
|    loss                  | 170          |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00515     |
|    std                   | 0.968        |
|    value_loss            | 1.73e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5559656] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 31           |
|    time_elapsed          | 495          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.0062341797 |
|    clip_fraction         | 0.0516       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 1.35         |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.00753     |
|    lagrangian_multiplier | 0.0446       |
|    learning_rate         | 0.0003       |
|    loss                  | 74.8         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.0083      |
|    std                   | 0.991        |
|    value_loss            | 654          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6468903] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 28           |
|    time_elapsed          | 445          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.0070887124 |
|    clip_fraction         | 0.0709       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.104        |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0436      |
|    lagrangian_multiplier | 0.0557       |
|    learning_rate         | 0.0003       |
|    loss                  | 176          |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00852     |
|    std                   | 0.978        |
|    value_loss            | 1.66e+03     |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114474.12
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.9772333] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 32           |
|    time_elapsed          | 511          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.0058686985 |
|    clip_fraction         | 0.0424       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0616       |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0293      |
|    lagrangian_multiplier | 0.0415       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.9         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00531     |
|    std                   | 0.98         |
|    value_loss            | 329          |
-------------------------------------------
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231223_085858-eurs7c1s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/eurs7c1s
-------------------------------------------
| reward                   | [-1.8665351] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 29           |
|    time_elapsed          | 461          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.0076738764 |
|    clip_fraction         | 0.0679       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.142        |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.00472     |
|    lagrangian_multiplier | 0.0657       |
|    learning_rate         | 0.0003       |
|    loss                  | 105          |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.0086      |
|    std                   | 0.974        |
|    value_loss            | 1.01e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2686441] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 33           |
|    time_elapsed          | 527          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0051446157 |
|    clip_fraction         | 0.0555       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0503       |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0258      |
|    lagrangian_multiplier | 0.0503       |
|    learning_rate         | 0.0003       |
|    loss                  | 29           |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00582     |
|    std                   | 0.986        |
|    value_loss            | 261          |
-------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.24832565] |
| time/              |               |
|    fps             | 133           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
-------------------------------------------
| reward                   | [-2.1036572] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 30           |
|    time_elapsed          | 477          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0074844537 |
|    clip_fraction         | 0.0697       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.107        |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0544      |
|    lagrangian_multiplier | 0.059        |
|    learning_rate         | 0.0003       |
|    loss                  | 130          |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00991     |
|    std                   | 0.965        |
|    value_loss            | 1.34e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1850272] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 34           |
|    time_elapsed          | 543          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.005438597  |
|    clip_fraction         | 0.0386       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0558       |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0434      |
|    lagrangian_multiplier | 0.0426       |
|    learning_rate         | 0.0003       |
|    loss                  | 30.2         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00296     |
|    std                   | 0.976        |
|    value_loss            | 236          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.94715756] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 31            |
|    time_elapsed          | 493           |
|    total_timesteps       | 63488         |
| train/                   |               |
|    approx_kl             | 0.0067100897  |
|    clip_fraction         | 0.073         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.107         |
|    entropy_loss          | -2.77         |
|    explained_variance    | 0.0389        |
|    lagrangian_multiplier | 0.062         |
|    learning_rate         | 0.0003        |
|    loss                  | 196           |
|    n_updates             | 300           |
|    policy_gradient_loss  | -0.0123       |
|    std                   | 0.971         |
|    value_loss            | 1.8e+03       |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.52089214] |
| time/                    |               |
|    fps                   | 23            |
|    iterations            | 35            |
|    time_elapsed          | 2997          |
|    total_timesteps       | 71680         |
| train/                   |               |
|    approx_kl             | 0.006686531   |
|    clip_fraction         | 0.0514        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00506       |
|    entropy_loss          | -2.97         |
|    explained_variance    | 0.0303        |
|    lagrangian_multiplier | 0.0766        |
|    learning_rate         | 0.0003        |
|    loss                  | 23.4          |
|    n_updates             | 340           |
|    policy_gradient_loss  | -0.00559      |
|    std                   | 1.06          |
|    value_loss            | 301           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.607907]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 35           |
|    time_elapsed          | 559          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0054316754 |
|    clip_fraction         | 0.0461       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.1          |
|    entropy_loss          | -2.78        |
|    explained_variance    | 0.00142      |
|    lagrangian_multiplier | 0.0596       |
|    learning_rate         | 0.0003       |
|    loss                  | 39.4         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00628     |
|    std                   | 0.974        |
|    value_loss            | 346          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3818533] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 32           |
|    time_elapsed          | 509          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.008944494  |
|    clip_fraction         | 0.0915       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0997       |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0363      |
|    lagrangian_multiplier | 0.0588       |
|    learning_rate         | 0.0003       |
|    loss                  | 81.6         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00987     |
|    std                   | 0.977        |
|    value_loss            | 835          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7436722] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 36           |
|    time_elapsed          | 575          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.006509834  |
|    clip_fraction         | 0.0735       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0868       |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0389      |
|    lagrangian_multiplier | 0.0587       |
|    learning_rate         | 0.0003       |
|    loss                  | 24.4         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00908     |
|    std                   | 0.997        |
|    value_loss            | 263          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.2052853] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 33           |
|    time_elapsed          | 525          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.0068854643 |
|    clip_fraction         | 0.0606       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.115        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0242       |
|    lagrangian_multiplier | 0.067        |
|    learning_rate         | 0.0003       |
|    loss                  | 104          |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00824     |
|    std                   | 0.99         |
|    value_loss            | 1.03e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.47386268] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 37            |
|    time_elapsed          | 591           |
|    total_timesteps       | 75776         |
| train/                   |               |
|    approx_kl             | 0.006408636   |
|    clip_fraction         | 0.061         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 1.67          |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.0243       |
|    lagrangian_multiplier | 0.046         |
|    learning_rate         | 0.0003        |
|    loss                  | 44            |
|    n_updates             | 360           |
|    policy_gradient_loss  | -0.01         |
|    std                   | 1.01          |
|    value_loss            | 390           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.32907483] |
| time/                    |               |
|    fps                   | 48            |
|    iterations            | 2             |
|    time_elapsed          | 85            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.004779161   |
|    clip_fraction         | 0.0445        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 73.9          |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.0129        |
|    lagrangian_multiplier | 0.0572        |
|    learning_rate         | 0.0003        |
|    loss                  | 56.6          |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.0069       |
|    std                   | 1             |
|    value_loss            | 434           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.2652187] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 34           |
|    time_elapsed          | 541          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0064820135 |
|    clip_fraction         | 0.0718       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.113        |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0448      |
|    lagrangian_multiplier | 0.0631       |
|    learning_rate         | 0.0003       |
|    loss                  | 88.4         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.0118      |
|    std                   | 0.991        |
|    value_loss            | 1.06e+03     |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.0202184] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 38           |
|    time_elapsed          | 607          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.00839856   |
|    clip_fraction         | 0.0733       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0413       |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0606      |
|    lagrangian_multiplier | 0.049        |
|    learning_rate         | 0.0003       |
|    loss                  | 28.8         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00701     |
|    std                   | 1.01         |
|    value_loss            | 202          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.37245518] |
| time/                    |               |
|    fps                   | 60            |
|    iterations            | 3             |
|    time_elapsed          | 101           |
|    total_timesteps       | 6144          |
| train/                   |               |
|    approx_kl             | 0.005599193   |
|    clip_fraction         | 0.059         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 36            |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.00959      |
|    lagrangian_multiplier | 0.0549        |
|    learning_rate         | 0.0003        |
|    loss                  | 59            |
|    n_updates             | 20            |
|    policy_gradient_loss  | -0.00606      |
|    std                   | 1             |
|    value_loss            | 448           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.9018894] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 35           |
|    time_elapsed          | 557          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0072334534 |
|    clip_fraction         | 0.0759       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0937       |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.00676      |
|    lagrangian_multiplier | 0.0658       |
|    learning_rate         | 0.0003       |
|    loss                  | 110          |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.0108      |
|    std                   | 0.993        |
|    value_loss            | 1.06e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1574886] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 39           |
|    time_elapsed          | 623          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.007070871  |
|    clip_fraction         | 0.0615       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.102        |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0527      |
|    lagrangian_multiplier | 0.0498       |
|    learning_rate         | 0.0003       |
|    loss                  | 27.6         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00694     |
|    std                   | 1            |
|    value_loss            | 226          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.83720565] |
| time/                    |               |
|    fps                   | 69            |
|    iterations            | 4             |
|    time_elapsed          | 117           |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.005734775   |
|    clip_fraction         | 0.0501        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 52.8          |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.00462      |
|    lagrangian_multiplier | 0.0604        |
|    learning_rate         | 0.0003        |
|    loss                  | 44.6          |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00568      |
|    std                   | 1             |
|    value_loss            | 396           |
--------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.28073955] |
| time/                    |               |
|    fps                   | 23            |
|    iterations            | 36            |
|    time_elapsed          | 3074          |
|    total_timesteps       | 73728         |
| train/                   |               |
|    approx_kl             | 0.007852107   |
|    clip_fraction         | 0.0539        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0029        |
|    entropy_loss          | -2.95         |
|    explained_variance    | 0.0308        |
|    lagrangian_multiplier | 0.0796        |
|    learning_rate         | 0.0003        |
|    loss                  | 29.4          |
|    n_updates             | 350           |
|    policy_gradient_loss  | -0.00475      |
|    std                   | 1.05          |
|    value_loss            | 372           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.8708026] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 36           |
|    time_elapsed          | 573          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.0061908704 |
|    clip_fraction         | 0.0748       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.105        |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.039       |
|    lagrangian_multiplier | 0.0576       |
|    learning_rate         | 0.0003       |
|    loss                  | 165          |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00646     |
|    std                   | 0.993        |
|    value_loss            | 1.38e+03     |
-------------------------------------------
------------------------------------------
| reward                   | [-0.836888] |
| time/                    |             |
|    fps                   | 128         |
|    iterations            | 40          |
|    time_elapsed          | 639         |
|    total_timesteps       | 81920       |
| train/                   |             |
|    approx_kl             | 0.006302825 |
|    clip_fraction         | 0.0506      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0651      |
|    entropy_loss          | -2.84       |
|    explained_variance    | -0.0324     |
|    lagrangian_multiplier | 0.0441      |
|    learning_rate         | 0.0003      |
|    loss                  | 44          |
|    n_updates             | 390         |
|    policy_gradient_loss  | -0.00734    |
|    std                   | 1           |
|    value_loss            | 311         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.0034771] |
| time/                    |              |
|    fps                   | 76           |
|    iterations            | 5            |
|    time_elapsed          | 134          |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0062383143 |
|    clip_fraction         | 0.046        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.342        |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.00781     |
|    lagrangian_multiplier | 0.0585       |
|    learning_rate         | 0.0003       |
|    loss                  | 25.7         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00652     |
|    std                   | 1.02         |
|    value_loss            | 241          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.2573342] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 37           |
|    time_elapsed          | 589          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.00925624   |
|    clip_fraction         | 0.0765       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0818       |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0451      |
|    lagrangian_multiplier | 0.0609       |
|    learning_rate         | 0.0003       |
|    loss                  | 111          |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.011       |
|    std                   | 0.993        |
|    value_loss            | 1.29e+03     |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.72963077] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 41            |
|    time_elapsed          | 655           |
|    total_timesteps       | 83968         |
| train/                   |               |
|    approx_kl             | 0.0051145754  |
|    clip_fraction         | 0.0656        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0693        |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.0259       |
|    lagrangian_multiplier | 0.0477        |
|    learning_rate         | 0.0003        |
|    loss                  | 29.4          |
|    n_updates             | 400           |
|    policy_gradient_loss  | -0.00706      |
|    std                   | 1             |
|    value_loss            | 253           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.83057463] |
| time/                    |               |
|    fps                   | 81            |
|    iterations            | 6             |
|    time_elapsed          | 150           |
|    total_timesteps       | 12288         |
| train/                   |               |
|    approx_kl             | 0.0052115056  |
|    clip_fraction         | 0.0432        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 72.1          |
|    entropy_loss          | -2.88         |
|    explained_variance    | -0.0235       |
|    lagrangian_multiplier | 0.0671        |
|    learning_rate         | 0.0003        |
|    loss                  | 73.9          |
|    n_updates             | 50            |
|    policy_gradient_loss  | -0.00791      |
|    std                   | 1.02          |
|    value_loss            | 656           |
--------------------------------------------
-------------------------------------------
| reward                   | [-3.4080396] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 38           |
|    time_elapsed          | 605          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.008427203  |
|    clip_fraction         | 0.0593       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.096        |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0445      |
|    lagrangian_multiplier | 0.0583       |
|    learning_rate         | 0.0003       |
|    loss                  | 104          |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00888     |
|    std                   | 0.979        |
|    value_loss            | 1.09e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9688955] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 42           |
|    time_elapsed          | 671          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0068722107 |
|    clip_fraction         | 0.0704       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0545       |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0303      |
|    lagrangian_multiplier | 0.0488       |
|    learning_rate         | 0.0003       |
|    loss                  | 26.7         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00659     |
|    std                   | 0.984        |
|    value_loss            | 227          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1191077] |
| time/                    |              |
|    fps                   | 85           |
|    iterations            | 7            |
|    time_elapsed          | 166          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.005959021  |
|    clip_fraction         | 0.0381       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 62.5         |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0144       |
|    lagrangian_multiplier | 0.0632       |
|    learning_rate         | 0.0003       |
|    loss                  | 45.9         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00605     |
|    std                   | 1.01         |
|    value_loss            | 371          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7269636] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 39           |
|    time_elapsed          | 621          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.00755213   |
|    clip_fraction         | 0.0817       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.086        |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0138      |
|    lagrangian_multiplier | 0.0669       |
|    learning_rate         | 0.0003       |
|    loss                  | 92           |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.0108      |
|    std                   | 0.978        |
|    value_loss            | 934          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.32972708] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 43            |
|    time_elapsed          | 688           |
|    total_timesteps       | 88064         |
| train/                   |               |
|    approx_kl             | 0.0055459132  |
|    clip_fraction         | 0.0545        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0203        |
|    entropy_loss          | -2.79         |
|    explained_variance    | -0.1          |
|    lagrangian_multiplier | 0.0458        |
|    learning_rate         | 0.0003        |
|    loss                  | 19.2          |
|    n_updates             | 420           |
|    policy_gradient_loss  | -0.00483      |
|    std                   | 0.97          |
|    value_loss            | 147           |
--------------------------------------------
------------------------------------------
| reward                   | [-1.339152] |
| time/                    |             |
|    fps                   | 89          |
|    iterations            | 8           |
|    time_elapsed          | 183         |
|    total_timesteps       | 16384       |
| train/                   |             |
|    approx_kl             | 0.005459424 |
|    clip_fraction         | 0.0399      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 74.3        |
|    entropy_loss          | -2.86       |
|    explained_variance    | -0.0385     |
|    lagrangian_multiplier | 0.0559      |
|    learning_rate         | 0.0003      |
|    loss                  | 47.1        |
|    n_updates             | 70          |
|    policy_gradient_loss  | -0.00734    |
|    std                   | 1.01        |
|    value_loss            | 335         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.2312646] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 40           |
|    time_elapsed          | 637          |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.0053788256 |
|    clip_fraction         | 0.0659       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0908       |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0198      |
|    lagrangian_multiplier | 0.0557       |
|    learning_rate         | 0.0003       |
|    loss                  | 89.9         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00662     |
|    std                   | 0.988        |
|    value_loss            | 899          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-0.9155401] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 44           |
|    time_elapsed          | 704          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.005434594  |
|    clip_fraction         | 0.0512       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0193       |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.129       |
|    lagrangian_multiplier | 0.0512       |
|    learning_rate         | 0.0003       |
|    loss                  | 15.2         |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.00579     |
|    std                   | 0.97         |
|    value_loss            | 118          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3131007] |
| time/                    |              |
|    fps                   | 24           |
|    iterations            | 37           |
|    time_elapsed          | 3150         |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.0067950934 |
|    clip_fraction         | 0.0553       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00486      |
|    entropy_loss          | -2.94        |
|    explained_variance    | 0.0461       |
|    lagrangian_multiplier | 0.0775       |
|    learning_rate         | 0.0003       |
|    loss                  | 22.7         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00419     |
|    std                   | 1.05         |
|    value_loss            | 243          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3605754] |
| time/                    |              |
|    fps                   | 92           |
|    iterations            | 9            |
|    time_elapsed          | 199          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0059404187 |
|    clip_fraction         | 0.0542       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 24.6         |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0223       |
|    lagrangian_multiplier | 0.055        |
|    learning_rate         | 0.0003       |
|    loss                  | 34           |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00535     |
|    std                   | 0.995        |
|    value_loss            | 305          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.476488] |
| time/                    |             |
|    fps                   | 128         |
|    iterations            | 41          |
|    time_elapsed          | 653         |
|    total_timesteps       | 83968       |
| train/                   |             |
|    approx_kl             | 0.00700991  |
|    clip_fraction         | 0.075       |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0956      |
|    entropy_loss          | -2.83       |
|    explained_variance    | 0.0296      |
|    lagrangian_multiplier | 0.0592      |
|    learning_rate         | 0.0003      |
|    loss                  | 34.6        |
|    n_updates             | 400         |
|    policy_gradient_loss  | -0.01       |
|    std                   | 0.999       |
|    value_loss            | 335         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.24928622] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 45            |
|    time_elapsed          | 720           |
|    total_timesteps       | 92160         |
| train/                   |               |
|    approx_kl             | 0.003881662   |
|    clip_fraction         | 0.0341        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0214        |
|    entropy_loss          | -2.78         |
|    explained_variance    | -0.0686       |
|    lagrangian_multiplier | 0.0517        |
|    learning_rate         | 0.0003        |
|    loss                  | 16            |
|    n_updates             | 440           |
|    policy_gradient_loss  | -0.00266      |
|    std                   | 0.972         |
|    value_loss            | 138           |
--------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1734598] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 42           |
|    time_elapsed          | 669          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0076612337 |
|    clip_fraction         | 0.0739       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.095        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.00892      |
|    lagrangian_multiplier | 0.0764       |
|    learning_rate         | 0.0003       |
|    loss                  | 48.9         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00932     |
|    std                   | 0.995        |
|    value_loss            | 543          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.66938967] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 46            |
|    time_elapsed          | 735           |
|    total_timesteps       | 94208         |
| train/                   |               |
|    approx_kl             | 0.0029583783  |
|    clip_fraction         | 0.0353        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0212        |
|    entropy_loss          | -2.78         |
|    explained_variance    | -0.0898       |
|    lagrangian_multiplier | 0.0554        |
|    learning_rate         | 0.0003        |
|    loss                  | 16.3          |
|    n_updates             | 450           |
|    policy_gradient_loss  | -0.00305      |
|    std                   | 0.972         |
|    value_loss            | 140           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.6974365] |
| time/                    |              |
|    fps                   | 90           |
|    iterations            | 10           |
|    time_elapsed          | 225          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0071474463 |
|    clip_fraction         | 0.069        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 130          |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0138       |
|    lagrangian_multiplier | 0.0493       |
|    learning_rate         | 0.0003       |
|    loss                  | 68.6         |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00915     |
|    std                   | 0.993        |
|    value_loss            | 427          |
-------------------------------------------
------------------------------------------
| reward                   | [-2.127037] |
| time/                    |             |
|    fps                   | 128         |
|    iterations            | 43          |
|    time_elapsed          | 685         |
|    total_timesteps       | 88064       |
| train/                   |             |
|    approx_kl             | 0.006965546 |
|    clip_fraction         | 0.0774      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.112       |
|    entropy_loss          | -2.84       |
|    explained_variance    | -0.0206     |
|    lagrangian_multiplier | 0.0675      |
|    learning_rate         | 0.0003      |
|    loss                  | 40.2        |
|    n_updates             | 420         |
|    policy_gradient_loss  | -0.00883    |
|    std                   | 1           |
|    value_loss            | 389         |
------------------------------------------
-------------------------------------------
| reward                   | [-0.3975094] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 47           |
|    time_elapsed          | 751          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.006258012  |
|    clip_fraction         | 0.0544       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0203       |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0979      |
|    lagrangian_multiplier | 0.0907       |
|    learning_rate         | 0.0003       |
|    loss                  | 4.56         |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.00577     |
|    std                   | 0.963        |
|    value_loss            | 55           |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.89730453] |
| time/                    |               |
|    fps                   | 93            |
|    iterations            | 11            |
|    time_elapsed          | 241           |
|    total_timesteps       | 22528         |
| train/                   |               |
|    approx_kl             | 0.0057016276  |
|    clip_fraction         | 0.0534        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 90.2          |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.0201        |
|    lagrangian_multiplier | 0.0494        |
|    learning_rate         | 0.0003        |
|    loss                  | 59.4          |
|    n_updates             | 100           |
|    policy_gradient_loss  | -0.00836      |
|    std                   | 0.993         |
|    value_loss            | 337           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.40484625] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 44            |
|    time_elapsed          | 701           |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.007464177   |
|    clip_fraction         | 0.0879        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0844        |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.0563       |
|    lagrangian_multiplier | 0.0633        |
|    learning_rate         | 0.0003        |
|    loss                  | 55.6          |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.0104       |
|    std                   | 1.01          |
|    value_loss            | 589           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.44736126] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 48            |
|    time_elapsed          | 767           |
|    total_timesteps       | 98304         |
| train/                   |               |
|    approx_kl             | 0.006091634   |
|    clip_fraction         | 0.0479        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0291        |
|    entropy_loss          | -2.75         |
|    explained_variance    | -0.0538       |
|    lagrangian_multiplier | 0.0511        |
|    learning_rate         | 0.0003        |
|    loss                  | 32.5          |
|    n_updates             | 470           |
|    policy_gradient_loss  | -0.00478      |
|    std                   | 0.949         |
|    value_loss            | 265           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.0349307] |
| time/                    |              |
|    fps                   | 95           |
|    iterations            | 12           |
|    time_elapsed          | 258          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0054075606 |
|    clip_fraction         | 0.0515       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 71.2         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.00977     |
|    lagrangian_multiplier | 0.0533       |
|    learning_rate         | 0.0003       |
|    loss                  | 64.8         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.0092      |
|    std                   | 1.01         |
|    value_loss            | 490          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7319975] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 45           |
|    time_elapsed          | 717          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.00700532   |
|    clip_fraction         | 0.0703       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0864       |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.00838      |
|    lagrangian_multiplier | 0.0604       |
|    learning_rate         | 0.0003       |
|    loss                  | 78.6         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00891     |
|    std                   | 1.01         |
|    value_loss            | 844          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7719408] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 49           |
|    time_elapsed          | 783          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.0045824926 |
|    clip_fraction         | 0.0493       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0206       |
|    entropy_loss          | -2.75        |
|    explained_variance    | -0.0585      |
|    lagrangian_multiplier | 0.0746       |
|    learning_rate         | 0.0003       |
|    loss                  | 9.23         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.00551     |
|    std                   | 0.963        |
|    value_loss            | 130          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 103, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 90, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
--------------------------------------------
| reward                   | [-0.41054213] |
| time/                    |               |
|    fps                   | 24            |
|    iterations            | 38            |
|    time_elapsed          | 3226          |
|    total_timesteps       | 77824         |
| train/                   |               |
|    approx_kl             | 0.0071071982  |
|    clip_fraction         | 0.0735        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0196        |
|    entropy_loss          | -2.94         |
|    explained_variance    | 0.0319        |
|    lagrangian_multiplier | 0.0708        |
|    learning_rate         | 0.0003        |
|    loss                  | 63.4          |
|    n_updates             | 370           |
|    policy_gradient_loss  | -0.005        |
|    std                   | 1.05          |
|    value_loss            | 660           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.230708]  |
| time/                    |              |
|    fps                   | 96           |
|    iterations            | 13           |
|    time_elapsed          | 274          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0068798447 |
|    clip_fraction         | 0.0554       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 170          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.00931     |
|    lagrangian_multiplier | 0.0601       |
|    learning_rate         | 0.0003       |
|    loss                  | 70.2         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00712     |
|    std                   | 1.01         |
|    value_loss            | 565          |
-------------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÅ‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñà‚ñá‚ñá‚ñÜ
wandb:             train/approx_kl ‚ñÜ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñà‚ñÇ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÉ
wandb:         train/clip_fraction ‚ñÜ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÖ‚ñá‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÖ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÑ‚ñÜ‚ñÅ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:          train/entropy_loss ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñà
wandb:    train/explained_variance ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñá‚ñÉ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÑ
wandb: train/lagrangian_multiplier ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñÜ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñá‚ñÖ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñà‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÜ
wandb:                   train/std ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ
wandb:            train/value_loss ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñà‚ñÖ‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ
wandb: 
wandb: Run summary:
wandb:                      reward -0.77194
wandb:             train/approx_kl 0.00458
wandb:         train/clip_fraction 0.04932
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 0.02059
wandb:          train/entropy_loss -2.7495
wandb:    train/explained_variance -0.05855
wandb: train/lagrangian_multiplier 0.07458
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 9.23271
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00551
wandb:                   train/std 0.9628
wandb:            train/value_loss 130.04506
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/7amm0ztn
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231223_005025-7amm0ztn/logs
srun: error: dqn.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-0.577712]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 46           |
|    time_elapsed          | 733          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0057981974 |
|    clip_fraction         | 0.0621       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.105        |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0213      |
|    lagrangian_multiplier | 0.0723       |
|    learning_rate         | 0.0003       |
|    loss                  | 48.1         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.00828     |
|    std                   | 0.996        |
|    value_loss            | 516          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0176773] |
| time/                    |              |
|    fps                   | 98           |
|    iterations            | 14           |
|    time_elapsed          | 290          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.006803815  |
|    clip_fraction         | 0.056        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 170          |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.013       |
|    lagrangian_multiplier | 0.056        |
|    learning_rate         | 0.0003       |
|    loss                  | 78.9         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00749     |
|    std                   | 1.01         |
|    value_loss            | 582          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4561678] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 47           |
|    time_elapsed          | 749          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.006079711  |
|    clip_fraction         | 0.0575       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0756       |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0275      |
|    lagrangian_multiplier | 0.0461       |
|    learning_rate         | 0.0003       |
|    loss                  | 54           |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.00908     |
|    std                   | 0.992        |
|    value_loss            | 424          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.243961] |
| time/                    |             |
|    fps                   | 100         |
|    iterations            | 15          |
|    time_elapsed          | 306         |
|    total_timesteps       | 30720       |
| train/                   |             |
|    approx_kl             | 0.005076684 |
|    clip_fraction         | 0.0449      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 121         |
|    entropy_loss          | -2.86       |
|    explained_variance    | -0.0127     |
|    lagrangian_multiplier | 0.0636      |
|    learning_rate         | 0.0003      |
|    loss                  | 60.3        |
|    n_updates             | 140         |
|    policy_gradient_loss  | -0.00681    |
|    std                   | 1.01        |
|    value_loss            | 457         |
------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114474.13
--------------------------------------------
| reward                   | [-0.99874187] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 48            |
|    time_elapsed          | 765           |
|    total_timesteps       | 98304         |
| train/                   |               |
|    approx_kl             | 0.006594382   |
|    clip_fraction         | 0.0752        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.076         |
|    entropy_loss          | -2.82         |
|    explained_variance    | -0.02         |
|    lagrangian_multiplier | 0.0524        |
|    learning_rate         | 0.0003        |
|    loss                  | 64.1          |
|    n_updates             | 470           |
|    policy_gradient_loss  | -0.00773      |
|    std                   | 0.993         |
|    value_loss            | 603           |
--------------------------------------------
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231223_010413-axzppts0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/axzppts0
-------------------------------------------
| reward                   | [-1.3308903] |
| time/                    |              |
|    fps                   | 101          |
|    iterations            | 16           |
|    time_elapsed          | 323          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0050538788 |
|    clip_fraction         | 0.0503       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 118          |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.00666      |
|    lagrangian_multiplier | 0.0587       |
|    learning_rate         | 0.0003       |
|    loss                  | 81.4         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00764     |
|    std                   | 1            |
|    value_loss            | 602          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4805624] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 49           |
|    time_elapsed          | 781          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.008094311  |
|    clip_fraction         | 0.0787       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.107        |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0413      |
|    lagrangian_multiplier | 0.0585       |
|    learning_rate         | 0.0003       |
|    loss                  | 73.6         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.0114      |
|    std                   | 0.982        |
|    value_loss            | 629          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 103, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 90, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
Using cpu device
--------------------------------------
| reward             | [-0.27584392] |
| time/              |               |
|    fps             | 132           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñá‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñá‚ñÜ‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ
wandb:             train/approx_kl ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñá‚ñÜ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÜ
wandb:         train/clip_fraction ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñá
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñà‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ
wandb:          train/entropy_loss ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ
wandb:    train/explained_variance ‚ñÜ‚ñá‚ñá‚ñá‚ñÉ‚ñÜ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñà‚ñÖ‚ñà‚ñà‚ñÜ‚ñá‚ñà‚ñÖ‚ñÑ‚ñà‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñÖ
wandb: train/lagrangian_multiplier ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÅ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñÑ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÇ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñá‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÅ‚ñÜ‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÇ
wandb:                   train/std ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÖ‚ñÉ
wandb:            train/value_loss ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:                      reward -1.48056
wandb:             train/approx_kl 0.00809
wandb:         train/clip_fraction 0.07871
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 0.10696
wandb:          train/entropy_loss -2.80921
wandb:    train/explained_variance -0.04133
wandb: train/lagrangian_multiplier 0.05852
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 73.61829
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.01139
wandb:                   train/std 0.98239
wandb:            train/value_loss 628.97098
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/u1qsee1p
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231223_005126-u1qsee1p/logs
srun: error: ddpg.ist.berkeley.edu: task 0: Exited with exit code 1
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114474.14
-------------------------------------------
| reward                   | [-1.0116833] |
| time/                    |              |
|    fps                   | 102          |
|    iterations            | 17           |
|    time_elapsed          | 339          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.008897462  |
|    clip_fraction         | 0.075        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 181          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0238      |
|    lagrangian_multiplier | 0.0582       |
|    learning_rate         | 0.0003       |
|    loss                  | 66.6         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.0138      |
|    std                   | 1            |
|    value_loss            | 433          |
-------------------------------------------
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231223_010440-17l5mhtw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/17l5mhtw
--------------------------------------------
| reward                   | [-0.49851245] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 2             |
|    time_elapsed          | 31            |
|    total_timesteps       | 4096          |
| train/                   |               |
|    approx_kl             | 0.005530065   |
|    clip_fraction         | 0.0587        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 120           |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.00168      |
|    lagrangian_multiplier | 0.0541        |
|    learning_rate         | 0.0003        |
|    loss                  | 110           |
|    n_updates             | 10            |
|    policy_gradient_loss  | -0.00683      |
|    std                   | 1             |
|    value_loss            | 901           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.0883379] |
| time/                    |              |
|    fps                   | 24           |
|    iterations            | 39           |
|    time_elapsed          | 3305         |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0047267852 |
|    clip_fraction         | 0.0448       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.316        |
|    entropy_loss          | -2.95        |
|    explained_variance    | 0.0271       |
|    lagrangian_multiplier | 0.0792       |
|    learning_rate         | 0.0003       |
|    loss                  | 20.5         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00815     |
|    std                   | 1.06         |
|    value_loss            | 238          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1233617] |
| time/                    |              |
|    fps                   | 103          |
|    iterations            | 18           |
|    time_elapsed          | 356          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0076609096 |
|    clip_fraction         | 0.0647       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 69.5         |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.00955      |
|    lagrangian_multiplier | 0.0453       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.8         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.0108      |
|    std                   | 1.02         |
|    value_loss            | 379          |
-------------------------------------------
Using cpu device
-------------------------------------
| reward             | [-0.4300694] |
| time/              |              |
|    fps             | 135          |
|    iterations      | 1            |
|    time_elapsed    | 15           |
|    total_timesteps | 2048         |
-------------------------------------
-------------------------------------------
| reward                   | [-1.0873196] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 3            |
|    time_elapsed          | 48           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0062274644 |
|    clip_fraction         | 0.0472       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 121          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0336      |
|    lagrangian_multiplier | 0.0541       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.8         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00408     |
|    std                   | 1            |
|    value_loss            | 455          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.2620914] |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 19           |
|    time_elapsed          | 372          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.006176245  |
|    clip_fraction         | 0.0576       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 175          |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.00393     |
|    lagrangian_multiplier | 0.0517       |
|    learning_rate         | 0.0003       |
|    loss                  | 57.1         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00793     |
|    std                   | 1.02         |
|    value_loss            | 297          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.2449896] |
| time/                    |              |
|    fps                   | 131          |
|    iterations            | 2            |
|    time_elapsed          | 31           |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.004416258  |
|    clip_fraction         | 0.0332       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 12.9         |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0011      |
|    lagrangian_multiplier | 0.0653       |
|    learning_rate         | 0.0003       |
|    loss                  | 41.3         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00326     |
|    std                   | 0.993        |
|    value_loss            | 488          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.77500516] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 4             |
|    time_elapsed          | 64            |
|    total_timesteps       | 8192          |
| train/                   |               |
|    approx_kl             | 0.004328467   |
|    clip_fraction         | 0.0369        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 216           |
|    entropy_loss          | -2.84         |
|    explained_variance    | 0.00836       |
|    lagrangian_multiplier | 0.0565        |
|    learning_rate         | 0.0003        |
|    loss                  | 149           |
|    n_updates             | 30            |
|    policy_gradient_loss  | -0.00536      |
|    std                   | 0.997         |
|    value_loss            | 1.01e+03      |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.241789]  |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 20           |
|    time_elapsed          | 388          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0048983814 |
|    clip_fraction         | 0.0292       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 175          |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0259       |
|    lagrangian_multiplier | 0.0561       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.8         |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.00725     |
|    std                   | 1            |
|    value_loss            | 397          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6729723] |
| time/                    |              |
|    fps                   | 130          |
|    iterations            | 3            |
|    time_elapsed          | 47           |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.0063445778 |
|    clip_fraction         | 0.073        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 135          |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0227       |
|    lagrangian_multiplier | 0.0489       |
|    learning_rate         | 0.0003       |
|    loss                  | 75.3         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00766     |
|    std                   | 0.996        |
|    value_loss            | 524          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2131416] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 5            |
|    time_elapsed          | 80           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0043155923 |
|    clip_fraction         | 0.041        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 120          |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0223       |
|    lagrangian_multiplier | 0.0538       |
|    learning_rate         | 0.0003       |
|    loss                  | 60.6         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00538     |
|    std                   | 1            |
|    value_loss            | 396          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2374799] |
| time/                    |              |
|    fps                   | 129          |
|    iterations            | 4            |
|    time_elapsed          | 63           |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.004208762  |
|    clip_fraction         | 0.0362       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 25.6         |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.00675      |
|    lagrangian_multiplier | 0.0693       |
|    learning_rate         | 0.0003       |
|    loss                  | 43.7         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00406     |
|    std                   | 0.99         |
|    value_loss            | 416          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.84451926] |
| time/                    |               |
|    fps                   | 106           |
|    iterations            | 21            |
|    time_elapsed          | 405           |
|    total_timesteps       | 43008         |
| train/                   |               |
|    approx_kl             | 0.007842114   |
|    clip_fraction         | 0.0994        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 154           |
|    entropy_loss          | -2.86         |
|    explained_variance    | 0.024         |
|    lagrangian_multiplier | 0.0617        |
|    learning_rate         | 0.0003        |
|    loss                  | 38.7          |
|    n_updates             | 200           |
|    policy_gradient_loss  | -0.011        |
|    std                   | 1.02          |
|    value_loss            | 301           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.61186475] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 6             |
|    time_elapsed          | 96            |
|    total_timesteps       | 12288         |
| train/                   |               |
|    approx_kl             | 0.0058883526  |
|    clip_fraction         | 0.043         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 7.12          |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.0393       |
|    lagrangian_multiplier | 0.0673        |
|    learning_rate         | 0.0003        |
|    loss                  | 63            |
|    n_updates             | 50            |
|    policy_gradient_loss  | -0.00734      |
|    std                   | 1             |
|    value_loss            | 718           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.771673]  |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 5            |
|    time_elapsed          | 79           |
|    total_timesteps       | 10240        |
| train/                   |              |
|    approx_kl             | 0.0039219083 |
|    clip_fraction         | 0.0361       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 180          |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.000946     |
|    lagrangian_multiplier | 0.0744       |
|    learning_rate         | 0.0003       |
|    loss                  | 63.5         |
|    n_updates             | 40           |
|    policy_gradient_loss  | -0.00488     |
|    std                   | 1            |
|    value_loss            | 542          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.3182523] |
| time/                    |              |
|    fps                   | 106          |
|    iterations            | 22           |
|    time_elapsed          | 421          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.0062235496 |
|    clip_fraction         | 0.0501       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 47.9         |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0215       |
|    lagrangian_multiplier | 0.0593       |
|    learning_rate         | 0.0003       |
|    loss                  | 33.4         |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00824     |
|    std                   | 1.03         |
|    value_loss            | 253          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8625988] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 7            |
|    time_elapsed          | 113          |
|    total_timesteps       | 14336        |
| train/                   |              |
|    approx_kl             | 0.0063569713 |
|    clip_fraction         | 0.058        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 154          |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0101      |
|    lagrangian_multiplier | 0.0663       |
|    learning_rate         | 0.0003       |
|    loss                  | 59.7         |
|    n_updates             | 60           |
|    policy_gradient_loss  | -0.00648     |
|    std                   | 0.998        |
|    value_loss            | 520          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3331035] |
| time/                    |              |
|    fps                   | 24           |
|    iterations            | 40           |
|    time_elapsed          | 3384         |
|    total_timesteps       | 81920        |
| train/                   |              |
|    approx_kl             | 0.00442275   |
|    clip_fraction         | 0.0364       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00488      |
|    entropy_loss          | -2.95        |
|    explained_variance    | 0.0104       |
|    lagrangian_multiplier | 0.0791       |
|    learning_rate         | 0.0003       |
|    loss                  | 13.3         |
|    n_updates             | 390          |
|    policy_gradient_loss  | -0.00427     |
|    std                   | 1.06         |
|    value_loss            | 170          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8953276] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 6            |
|    time_elapsed          | 95           |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.005598177  |
|    clip_fraction         | 0.0423       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 143          |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0263       |
|    lagrangian_multiplier | 0.0875       |
|    learning_rate         | 0.0003       |
|    loss                  | 64.2         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00738     |
|    std                   | 1            |
|    value_loss            | 670          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.53420585] |
| time/                    |               |
|    fps                   | 107           |
|    iterations            | 23            |
|    time_elapsed          | 438           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.006062191   |
|    clip_fraction         | 0.0382        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 124           |
|    entropy_loss          | -2.92         |
|    explained_variance    | 0.0282        |
|    lagrangian_multiplier | 0.0469        |
|    learning_rate         | 0.0003        |
|    loss                  | 66.3          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00888      |
|    std                   | 1.05          |
|    value_loss            | 348           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.81583613] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 8             |
|    time_elapsed          | 129           |
|    total_timesteps       | 16384         |
| train/                   |               |
|    approx_kl             | 0.006893082   |
|    clip_fraction         | 0.048         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 59.8          |
|    entropy_loss          | -2.81         |
|    explained_variance    | -0.00356      |
|    lagrangian_multiplier | 0.0623        |
|    learning_rate         | 0.0003        |
|    loss                  | 52.1          |
|    n_updates             | 70            |
|    policy_gradient_loss  | -0.00663      |
|    std                   | 0.981         |
|    value_loss            | 441           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.91392386] |
| time/                    |               |
|    fps                   | 128           |
|    iterations            | 7             |
|    time_elapsed          | 111           |
|    total_timesteps       | 14336         |
| train/                   |               |
|    approx_kl             | 0.006379583   |
|    clip_fraction         | 0.0531        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 128           |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.0698        |
|    lagrangian_multiplier | 0.0738        |
|    learning_rate         | 0.0003        |
|    loss                  | 45.3          |
|    n_updates             | 60            |
|    policy_gradient_loss  | -0.00597      |
|    std                   | 1.01          |
|    value_loss            | 394           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.62648183] |
| time/                    |               |
|    fps                   | 108           |
|    iterations            | 24            |
|    time_elapsed          | 454           |
|    total_timesteps       | 49152         |
| train/                   |               |
|    approx_kl             | 0.005296384   |
|    clip_fraction         | 0.0376        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 88.9          |
|    entropy_loss          | -2.95         |
|    explained_variance    | -0.00799      |
|    lagrangian_multiplier | 0.0589        |
|    learning_rate         | 0.0003        |
|    loss                  | 54.8          |
|    n_updates             | 230           |
|    policy_gradient_loss  | -0.00602      |
|    std                   | 1.06          |
|    value_loss            | 459           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.9367597] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 9            |
|    time_elapsed          | 145          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.005126318  |
|    clip_fraction         | 0.0433       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 107          |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.00935     |
|    lagrangian_multiplier | 0.076        |
|    learning_rate         | 0.0003       |
|    loss                  | 95           |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00669     |
|    std                   | 0.969        |
|    value_loss            | 989          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6579856] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 8            |
|    time_elapsed          | 127          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.005625005  |
|    clip_fraction         | 0.0489       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 142          |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.000883     |
|    lagrangian_multiplier | 0.0507       |
|    learning_rate         | 0.0003       |
|    loss                  | 69.6         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00696     |
|    std                   | 1.01         |
|    value_loss            | 367          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0639081] |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 25           |
|    time_elapsed          | 470          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.0069226846 |
|    clip_fraction         | 0.0667       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 122          |
|    entropy_loss          | -2.95        |
|    explained_variance    | -0.00182     |
|    lagrangian_multiplier | 0.057        |
|    learning_rate         | 0.0003       |
|    loss                  | 42.7         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00759     |
|    std                   | 1.05         |
|    value_loss            | 292          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.200433] |
| time/                    |             |
|    fps                   | 126         |
|    iterations            | 10          |
|    time_elapsed          | 162         |
|    total_timesteps       | 20480       |
| train/                   |             |
|    approx_kl             | 0.004529452 |
|    clip_fraction         | 0.0392      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.0345      |
|    entropy_loss          | -2.79       |
|    explained_variance    | -0.0484     |
|    lagrangian_multiplier | 0.062       |
|    learning_rate         | 0.0003      |
|    loss                  | 40.1        |
|    n_updates             | 90          |
|    policy_gradient_loss  | -0.0053     |
|    std                   | 0.98        |
|    value_loss            | 348         |
------------------------------------------
-------------------------------------------
| reward                   | [-0.6928862] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 9            |
|    time_elapsed          | 143          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.004651778  |
|    clip_fraction         | 0.0426       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 117          |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.00208      |
|    lagrangian_multiplier | 0.039        |
|    learning_rate         | 0.0003       |
|    loss                  | 106          |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00545     |
|    std                   | 1.01         |
|    value_loss            | 575          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.72165614] |
| time/                    |               |
|    fps                   | 109           |
|    iterations            | 26            |
|    time_elapsed          | 487           |
|    total_timesteps       | 53248         |
| train/                   |               |
|    approx_kl             | 0.007109914   |
|    clip_fraction         | 0.0613        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 133           |
|    entropy_loss          | -2.92         |
|    explained_variance    | 0.0399        |
|    lagrangian_multiplier | 0.0585        |
|    learning_rate         | 0.0003        |
|    loss                  | 59            |
|    n_updates             | 250           |
|    policy_gradient_loss  | -0.00751      |
|    std                   | 1.04          |
|    value_loss            | 471           |
--------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
--------------------------------------------
| reward                   | [-0.70167077] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 11            |
|    time_elapsed          | 178           |
|    total_timesteps       | 22528         |
| train/                   |               |
|    approx_kl             | 0.0061850776  |
|    clip_fraction         | 0.0743        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 123           |
|    entropy_loss          | -2.8          |
|    explained_variance    | -0.00219      |
|    lagrangian_multiplier | 0.0628        |
|    learning_rate         | 0.0003        |
|    loss                  | 44.9          |
|    n_updates             | 100           |
|    policy_gradient_loss  | -0.00965      |
|    std                   | 0.981         |
|    value_loss            | 350           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.9559688] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 10           |
|    time_elapsed          | 159          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.006084722  |
|    clip_fraction         | 0.0519       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 171          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0113      |
|    lagrangian_multiplier | 0.0456       |
|    learning_rate         | 0.0003       |
|    loss                  | 114          |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00752     |
|    std                   | 1.01         |
|    value_loss            | 738          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.2716986] |
| time/                    |              |
|    fps                   | 109          |
|    iterations            | 27           |
|    time_elapsed          | 503          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.0061441963 |
|    clip_fraction         | 0.055        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 162          |
|    entropy_loss          | -2.9         |
|    explained_variance    | 0.0368       |
|    lagrangian_multiplier | 0.0635       |
|    learning_rate         | 0.0003       |
|    loss                  | 66.5         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00876     |
|    std                   | 1.03         |
|    value_loss            | 519          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8067794] |
| time/                    |              |
|    fps                   | 24           |
|    iterations            | 41           |
|    time_elapsed          | 3461         |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.0052866903 |
|    clip_fraction         | 0.0524       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00494      |
|    entropy_loss          | -2.95        |
|    explained_variance    | 0.0265       |
|    lagrangian_multiplier | 0.0777       |
|    learning_rate         | 0.0003       |
|    loss                  | 20.4         |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00585     |
|    std                   | 1.06         |
|    value_loss            | 240          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8825188] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 12           |
|    time_elapsed          | 194          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.006679593  |
|    clip_fraction         | 0.0594       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 57.3         |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.022       |
|    lagrangian_multiplier | 0.0604       |
|    learning_rate         | 0.0003       |
|    loss                  | 39.7         |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00943     |
|    std                   | 0.976        |
|    value_loss            | 370          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2456547] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 11           |
|    time_elapsed          | 175          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.005414564  |
|    clip_fraction         | 0.0488       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 209          |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.00488     |
|    lagrangian_multiplier | 0.0454       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.4         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00685     |
|    std                   | 1.01         |
|    value_loss            | 326          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7876547] |
| time/                    |              |
|    fps                   | 110          |
|    iterations            | 28           |
|    time_elapsed          | 520          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.005153424  |
|    clip_fraction         | 0.0395       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.21         |
|    entropy_loss          | -2.9         |
|    explained_variance    | -0.0091      |
|    lagrangian_multiplier | 0.0734       |
|    learning_rate         | 0.0003       |
|    loss                  | 21.7         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00497     |
|    std                   | 1.03         |
|    value_loss            | 250          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.37128168] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 13            |
|    time_elapsed          | 210           |
|    total_timesteps       | 26624         |
| train/                   |               |
|    approx_kl             | 0.005793457   |
|    clip_fraction         | 0.0645        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 43.1          |
|    entropy_loss          | -2.8          |
|    explained_variance    | 0.00773       |
|    lagrangian_multiplier | 0.0684        |
|    learning_rate         | 0.0003        |
|    loss                  | 40.5          |
|    n_updates             | 120           |
|    policy_gradient_loss  | -0.00964      |
|    std                   | 0.99          |
|    value_loss            | 386           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.6332552] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 12           |
|    time_elapsed          | 192          |
|    total_timesteps       | 24576        |
| train/                   |              |
|    approx_kl             | 0.0047612335 |
|    clip_fraction         | 0.0401       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 181          |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0195       |
|    lagrangian_multiplier | 0.0559       |
|    learning_rate         | 0.0003       |
|    loss                  | 78           |
|    n_updates             | 110          |
|    policy_gradient_loss  | -0.00608     |
|    std                   | 0.995        |
|    value_loss            | 444          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3554583] |
| time/                    |              |
|    fps                   | 110          |
|    iterations            | 29           |
|    time_elapsed          | 536          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.007813398  |
|    clip_fraction         | 0.0661       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 218          |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.0175       |
|    lagrangian_multiplier | 0.0682       |
|    learning_rate         | 0.0003       |
|    loss                  | 70.3         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00951     |
|    std                   | 1.02         |
|    value_loss            | 539          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.902044] |
| time/                    |             |
|    fps                   | 126         |
|    iterations            | 14          |
|    time_elapsed          | 227         |
|    total_timesteps       | 28672       |
| train/                   |             |
|    approx_kl             | 0.005587468 |
|    clip_fraction         | 0.0487      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 21.1        |
|    entropy_loss          | -2.81       |
|    explained_variance    | 0.0216      |
|    lagrangian_multiplier | 0.0676      |
|    learning_rate         | 0.0003      |
|    loss                  | 20.4        |
|    n_updates             | 130         |
|    policy_gradient_loss  | -0.00819    |
|    std                   | 0.984       |
|    value_loss            | 342         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.3633163] |
| time/                    |              |
|    fps                   | 128          |
|    iterations            | 13           |
|    time_elapsed          | 207          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.006337609  |
|    clip_fraction         | 0.0625       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 79           |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.00552     |
|    lagrangian_multiplier | 0.0411       |
|    learning_rate         | 0.0003       |
|    loss                  | 55.1         |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00789     |
|    std                   | 0.998        |
|    value_loss            | 316          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0663317] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 30           |
|    time_elapsed          | 552          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0076135276 |
|    clip_fraction         | 0.0762       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 188          |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0261       |
|    lagrangian_multiplier | 0.0557       |
|    learning_rate         | 0.0003       |
|    loss                  | 92.2         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00872     |
|    std                   | 1.02         |
|    value_loss            | 594          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.0681795] |
| time/                    |              |
|    fps                   | 126          |
|    iterations            | 15           |
|    time_elapsed          | 243          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.005483317  |
|    clip_fraction         | 0.0514       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.09         |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0255      |
|    lagrangian_multiplier | 0.0731       |
|    learning_rate         | 0.0003       |
|    loss                  | 16.4         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.0065      |
|    std                   | 0.991        |
|    value_loss            | 157          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.6899054] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 14           |
|    time_elapsed          | 224          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.004265287  |
|    clip_fraction         | 0.0474       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 67.8         |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0295      |
|    lagrangian_multiplier | 0.0337       |
|    learning_rate         | 0.0003       |
|    loss                  | 127          |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.0056      |
|    std                   | 1.01         |
|    value_loss            | 762          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2932585] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 31           |
|    time_elapsed          | 569          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.008980558  |
|    clip_fraction         | 0.083        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 120          |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.0289      |
|    lagrangian_multiplier | 0.0594       |
|    learning_rate         | 0.0003       |
|    loss                  | 85.1         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.0146      |
|    std                   | 1.04         |
|    value_loss            | 593          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.36248425] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 16            |
|    time_elapsed          | 259           |
|    total_timesteps       | 32768         |
| train/                   |               |
|    approx_kl             | 0.004808369   |
|    clip_fraction         | 0.0386        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 11.1          |
|    entropy_loss          | -2.83         |
|    explained_variance    | 0.0131        |
|    lagrangian_multiplier | 0.0649        |
|    learning_rate         | 0.0003        |
|    loss                  | 27.4          |
|    n_updates             | 150           |
|    policy_gradient_loss  | -0.00551      |
|    std                   | 1             |
|    value_loss            | 253           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.0772173] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 15           |
|    time_elapsed          | 240          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0052757235 |
|    clip_fraction         | 0.0374       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 99.5         |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0162       |
|    lagrangian_multiplier | 0.0463       |
|    learning_rate         | 0.0003       |
|    loss                  | 91.3         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00557     |
|    std                   | 1.02         |
|    value_loss            | 612          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7809567] |
| time/                    |              |
|    fps                   | 111          |
|    iterations            | 32           |
|    time_elapsed          | 585          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.009673486  |
|    clip_fraction         | 0.0929       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 131          |
|    entropy_loss          | -2.92        |
|    explained_variance    | 0.0169       |
|    lagrangian_multiplier | 0.0633       |
|    learning_rate         | 0.0003       |
|    loss                  | 38.5         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00999     |
|    std                   | 1.05         |
|    value_loss            | 244          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9972385] |
| time/                    |              |
|    fps                   | 24           |
|    iterations            | 42           |
|    time_elapsed          | 3540         |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0061889486 |
|    clip_fraction         | 0.0366       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00395      |
|    entropy_loss          | -2.94        |
|    explained_variance    | 0.0347       |
|    lagrangian_multiplier | 0.0828       |
|    learning_rate         | 0.0003       |
|    loss                  | 26           |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00456     |
|    std                   | 1.05         |
|    value_loss            | 252          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.38184184] |
| time/                    |               |
|    fps                   | 126           |
|    iterations            | 17            |
|    time_elapsed          | 276           |
|    total_timesteps       | 34816         |
| train/                   |               |
|    approx_kl             | 0.005503011   |
|    clip_fraction         | 0.0485        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 30.7          |
|    entropy_loss          | -2.83         |
|    explained_variance    | 0.0794        |
|    lagrangian_multiplier | 0.0708        |
|    learning_rate         | 0.0003        |
|    loss                  | 22.6          |
|    n_updates             | 160           |
|    policy_gradient_loss  | -0.00941      |
|    std                   | 0.994         |
|    value_loss            | 197           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.442811]  |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 16           |
|    time_elapsed          | 256          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0045487443 |
|    clip_fraction         | 0.0246       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 45.8         |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0368       |
|    lagrangian_multiplier | 0.0571       |
|    learning_rate         | 0.0003       |
|    loss                  | 76.3         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00389     |
|    std                   | 1.03         |
|    value_loss            | 651          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.87074137] |
| time/                    |               |
|    fps                   | 112           |
|    iterations            | 33            |
|    time_elapsed          | 601           |
|    total_timesteps       | 67584         |
| train/                   |               |
|    approx_kl             | 0.0071106357  |
|    clip_fraction         | 0.0632        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 134           |
|    entropy_loss          | -2.92         |
|    explained_variance    | 0.0309        |
|    lagrangian_multiplier | 0.0562        |
|    learning_rate         | 0.0003        |
|    loss                  | 70.2          |
|    n_updates             | 320           |
|    policy_gradient_loss  | -0.00977      |
|    std                   | 1.04          |
|    value_loss            | 521           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.6864307] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 18           |
|    time_elapsed          | 292          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0044904915 |
|    clip_fraction         | 0.0305       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.284        |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0511       |
|    lagrangian_multiplier | 0.0732       |
|    learning_rate         | 0.0003       |
|    loss                  | 4.65         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00386     |
|    std                   | 0.994        |
|    value_loss            | 59.1         |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-2.5535722] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 17           |
|    time_elapsed          | 272          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.005360117  |
|    clip_fraction         | 0.053        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 220          |
|    entropy_loss          | -2.88        |
|    explained_variance    | 0.0484       |
|    lagrangian_multiplier | 0.0734       |
|    learning_rate         | 0.0003       |
|    loss                  | 95.9         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00769     |
|    std                   | 1.02         |
|    value_loss            | 973          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2567265] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 19           |
|    time_elapsed          | 308          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.0038732623 |
|    clip_fraction         | 0.0375       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0114       |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.000725     |
|    lagrangian_multiplier | 0.0761       |
|    learning_rate         | 0.0003       |
|    loss                  | 7.7          |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00465     |
|    std                   | 0.98         |
|    value_loss            | 88.7         |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.60100573] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 18            |
|    time_elapsed          | 288           |
|    total_timesteps       | 36864         |
| train/                   |               |
|    approx_kl             | 0.0051311534  |
|    clip_fraction         | 0.0512        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 105           |
|    entropy_loss          | -2.87         |
|    explained_variance    | 0.0423        |
|    lagrangian_multiplier | 0.0569        |
|    learning_rate         | 0.0003        |
|    loss                  | 70.2          |
|    n_updates             | 170           |
|    policy_gradient_loss  | -0.00719      |
|    std                   | 1.02          |
|    value_loss            | 705           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.69463253] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 20            |
|    time_elapsed          | 325           |
|    total_timesteps       | 40960         |
| train/                   |               |
|    approx_kl             | 0.004709792   |
|    clip_fraction         | 0.0391        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0146        |
|    entropy_loss          | -2.79         |
|    explained_variance    | 0.0283        |
|    lagrangian_multiplier | 0.0703        |
|    learning_rate         | 0.0003        |
|    loss                  | 13.2          |
|    n_updates             | 190           |
|    policy_gradient_loss  | -0.00542      |
|    std                   | 0.976         |
|    value_loss            | 148           |
--------------------------------------------
-------------------------------------------
| reward                   | [-2.4249709] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 19           |
|    time_elapsed          | 304          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.004963111  |
|    clip_fraction         | 0.0343       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 73.6         |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0116       |
|    lagrangian_multiplier | 0.0461       |
|    learning_rate         | 0.0003       |
|    loss                  | 89.1         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.0072      |
|    std                   | 1.02         |
|    value_loss            | 610          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1252171] |
| time/                    |              |
|    fps                   | 107          |
|    iterations            | 34           |
|    time_elapsed          | 649          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.005004823  |
|    clip_fraction         | 0.024        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0308       |
|    entropy_loss          | -2.9         |
|    explained_variance    | -0.00837     |
|    lagrangian_multiplier | 0.0665       |
|    learning_rate         | 0.0003       |
|    loss                  | 13.2         |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00256     |
|    std                   | 1.03         |
|    value_loss            | 145          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7318391] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 21           |
|    time_elapsed          | 341          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.004138825  |
|    clip_fraction         | 0.0265       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 8.99         |
|    entropy_loss          | -2.79        |
|    explained_variance    | 0.0148       |
|    lagrangian_multiplier | 0.0681       |
|    learning_rate         | 0.0003       |
|    loss                  | 12.1         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00388     |
|    std                   | 0.978        |
|    value_loss            | 127          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5987858] |
| time/                    |              |
|    fps                   | 24           |
|    iterations            | 43           |
|    time_elapsed          | 3616         |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0041734045 |
|    clip_fraction         | 0.0246       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00319      |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.0536       |
|    lagrangian_multiplier | 0.0757       |
|    learning_rate         | 0.0003       |
|    loss                  | 14.7         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00138     |
|    std                   | 1.02         |
|    value_loss            | 180          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.64762133] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 20            |
|    time_elapsed          | 320           |
|    total_timesteps       | 40960         |
| train/                   |               |
|    approx_kl             | 0.004508692   |
|    clip_fraction         | 0.03          |
|    clip_range            | 0.2           |
|    cost_value_loss       | 153           |
|    entropy_loss          | -2.87         |
|    explained_variance    | 0.0304        |
|    lagrangian_multiplier | 0.0766        |
|    learning_rate         | 0.0003        |
|    loss                  | 65.6          |
|    n_updates             | 190           |
|    policy_gradient_loss  | -0.00493      |
|    std                   | 1.02          |
|    value_loss            | 696           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.1950765] |
| time/                    |              |
|    fps                   | 107          |
|    iterations            | 35           |
|    time_elapsed          | 666          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.004807974  |
|    clip_fraction         | 0.0483       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 27.7         |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.00144      |
|    lagrangian_multiplier | 0.0716       |
|    learning_rate         | 0.0003       |
|    loss                  | 23.6         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00495     |
|    std                   | 1.03         |
|    value_loss            | 247          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.147218]  |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 22           |
|    time_elapsed          | 358          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.0069202287 |
|    clip_fraction         | 0.0538       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0103       |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0157      |
|    lagrangian_multiplier | 0.0804       |
|    learning_rate         | 0.0003       |
|    loss                  | 5.03         |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00665     |
|    std                   | 0.989        |
|    value_loss            | 67.6         |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6638165] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 21           |
|    time_elapsed          | 336          |
|    total_timesteps       | 43008        |
| train/                   |              |
|    approx_kl             | 0.0037625232 |
|    clip_fraction         | 0.0183       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 19.8         |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.0478       |
|    lagrangian_multiplier | 0.065        |
|    learning_rate         | 0.0003       |
|    loss                  | 23.5         |
|    n_updates             | 200          |
|    policy_gradient_loss  | -0.00536     |
|    std                   | 1.01         |
|    value_loss            | 256          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.0416412] |
| time/                    |              |
|    fps                   | 107          |
|    iterations            | 36           |
|    time_elapsed          | 683          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.006463397  |
|    clip_fraction         | 0.0611       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 75.1         |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.0236      |
|    lagrangian_multiplier | 0.0695       |
|    learning_rate         | 0.0003       |
|    loss                  | 21.5         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0074      |
|    std                   | 1.03         |
|    value_loss            | 194          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.56987685] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 23            |
|    time_elapsed          | 374           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.005524714   |
|    clip_fraction         | 0.0656        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 79.5          |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.0536        |
|    lagrangian_multiplier | 0.0624        |
|    learning_rate         | 0.0003        |
|    loss                  | 25.9          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.01         |
|    std                   | 0.998         |
|    value_loss            | 219           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.33528417] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 22            |
|    time_elapsed          | 352           |
|    total_timesteps       | 45056         |
| train/                   |               |
|    approx_kl             | 0.0058897845  |
|    clip_fraction         | 0.0445        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 3.11          |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.0264       |
|    lagrangian_multiplier | 0.044         |
|    learning_rate         | 0.0003        |
|    loss                  | 32.5          |
|    n_updates             | 210           |
|    policy_gradient_loss  | -0.00537      |
|    std                   | 0.998         |
|    value_loss            | 230           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.7646879] |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 37           |
|    time_elapsed          | 699          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.007347145  |
|    clip_fraction         | 0.0715       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 43.6         |
|    entropy_loss          | -2.89        |
|    explained_variance    | -0.036       |
|    lagrangian_multiplier | 0.0723       |
|    learning_rate         | 0.0003       |
|    loss                  | 27.6         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.0118      |
|    std                   | 1.03         |
|    value_loss            | 265          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.29790235] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 24            |
|    time_elapsed          | 390           |
|    total_timesteps       | 49152         |
| train/                   |               |
|    approx_kl             | 0.003779404   |
|    clip_fraction         | 0.0391        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.816         |
|    entropy_loss          | -2.82         |
|    explained_variance    | 0.0622        |
|    lagrangian_multiplier | 0.0752        |
|    learning_rate         | 0.0003        |
|    loss                  | 7.69          |
|    n_updates             | 230           |
|    policy_gradient_loss  | -0.00467      |
|    std                   | 0.99          |
|    value_loss            | 99.1          |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.8597272] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 23           |
|    time_elapsed          | 369          |
|    total_timesteps       | 47104        |
| train/                   |              |
|    approx_kl             | 0.006971274  |
|    clip_fraction         | 0.0688       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 83.4         |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0309       |
|    lagrangian_multiplier | 0.0474       |
|    learning_rate         | 0.0003       |
|    loss                  | 96.8         |
|    n_updates             | 220          |
|    policy_gradient_loss  | -0.0113      |
|    std                   | 1            |
|    value_loss            | 515          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.363354]  |
| time/                    |              |
|    fps                   | 108          |
|    iterations            | 38           |
|    time_elapsed          | 715          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0053577004 |
|    clip_fraction         | 0.0539       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 63.2         |
|    entropy_loss          | -2.9         |
|    explained_variance    | -0.0237      |
|    lagrangian_multiplier | 0.0667       |
|    learning_rate         | 0.0003       |
|    loss                  | 34.4         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.0108      |
|    std                   | 1.04         |
|    value_loss            | 407          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.34499663] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 25            |
|    time_elapsed          | 407           |
|    total_timesteps       | 51200         |
| train/                   |               |
|    approx_kl             | 0.0041169645  |
|    clip_fraction         | 0.0308        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0044        |
|    entropy_loss          | -2.81         |
|    explained_variance    | -0.00564      |
|    lagrangian_multiplier | 0.0783        |
|    learning_rate         | 0.0003        |
|    loss                  | 11.2          |
|    n_updates             | 240           |
|    policy_gradient_loss  | -0.00278      |
|    std                   | 0.979         |
|    value_loss            | 133           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.71337473] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 24            |
|    time_elapsed          | 385           |
|    total_timesteps       | 49152         |
| train/                   |               |
|    approx_kl             | 0.0049220473  |
|    clip_fraction         | 0.04          |
|    clip_range            | 0.2           |
|    cost_value_loss       | 71.3          |
|    entropy_loss          | -2.85         |
|    explained_variance    | 0.0483        |
|    lagrangian_multiplier | 0.0556        |
|    learning_rate         | 0.0003        |
|    loss                  | 42.1          |
|    n_updates             | 230           |
|    policy_gradient_loss  | -0.00702      |
|    std                   | 1.01          |
|    value_loss            | 284           |
--------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.1197995] |
| time/                    |              |
|    fps                   | 24           |
|    iterations            | 44           |
|    time_elapsed          | 3692         |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.0069394913 |
|    clip_fraction         | 0.0733       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00601      |
|    entropy_loss          | -2.89        |
|    explained_variance    | 0.00236      |
|    lagrangian_multiplier | 0.0688       |
|    learning_rate         | 0.0003       |
|    loss                  | 29.6         |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.00669     |
|    std                   | 1.03         |
|    value_loss            | 283          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.55068624] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 26            |
|    time_elapsed          | 423           |
|    total_timesteps       | 53248         |
| train/                   |               |
|    approx_kl             | 0.003568293   |
|    clip_fraction         | 0.023         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00448       |
|    entropy_loss          | -2.79         |
|    explained_variance    | -0.0218       |
|    lagrangian_multiplier | 0.0813        |
|    learning_rate         | 0.0003        |
|    loss                  | 10.2          |
|    n_updates             | 250           |
|    policy_gradient_loss  | -0.00339      |
|    std                   | 0.978         |
|    value_loss            | 127           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.9366215] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 25           |
|    time_elapsed          | 401          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.006490155  |
|    clip_fraction         | 0.0557       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 9.25         |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0093      |
|    lagrangian_multiplier | 0.0374       |
|    learning_rate         | 0.0003       |
|    loss                  | 33.2         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00497     |
|    std                   | 1.01         |
|    value_loss            | 209          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.72878474] |
| time/                    |               |
|    fps                   | 107           |
|    iterations            | 39            |
|    time_elapsed          | 745           |
|    total_timesteps       | 79872         |
| train/                   |               |
|    approx_kl             | 0.006135769   |
|    clip_fraction         | 0.0588        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 87.7          |
|    entropy_loss          | -2.91         |
|    explained_variance    | 0.0345        |
|    lagrangian_multiplier | 0.0557        |
|    learning_rate         | 0.0003        |
|    loss                  | 96.8          |
|    n_updates             | 380           |
|    policy_gradient_loss  | -0.00716      |
|    std                   | 1.03          |
|    value_loss            | 708           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.68960714] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 27            |
|    time_elapsed          | 440           |
|    total_timesteps       | 55296         |
| train/                   |               |
|    approx_kl             | 0.006539599   |
|    clip_fraction         | 0.066         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.00559       |
|    entropy_loss          | -2.81         |
|    explained_variance    | -0.179        |
|    lagrangian_multiplier | 0.0756        |
|    learning_rate         | 0.0003        |
|    loss                  | 6.23          |
|    n_updates             | 260           |
|    policy_gradient_loss  | -0.00494      |
|    std                   | 0.989         |
|    value_loss            | 75.2          |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.029068]  |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 26           |
|    time_elapsed          | 417          |
|    total_timesteps       | 53248        |
| train/                   |              |
|    approx_kl             | 0.0053348662 |
|    clip_fraction         | 0.0465       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 41.3         |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0269      |
|    lagrangian_multiplier | 0.059        |
|    learning_rate         | 0.0003       |
|    loss                  | 43.3         |
|    n_updates             | 250          |
|    policy_gradient_loss  | -0.00912     |
|    std                   | 1.02         |
|    value_loss            | 382          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.73477674] |
| time/                    |               |
|    fps                   | 107           |
|    iterations            | 40            |
|    time_elapsed          | 762           |
|    total_timesteps       | 81920         |
| train/                   |               |
|    approx_kl             | 0.0068950066  |
|    clip_fraction         | 0.0653        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 61            |
|    entropy_loss          | -2.89         |
|    explained_variance    | 0.0276        |
|    lagrangian_multiplier | 0.0613        |
|    learning_rate         | 0.0003        |
|    loss                  | 32            |
|    n_updates             | 390           |
|    policy_gradient_loss  | -0.0103       |
|    std                   | 1.03          |
|    value_loss            | 388           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.69283]   |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 28           |
|    time_elapsed          | 456          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.0073145395 |
|    clip_fraction         | 0.064        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.00642      |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.0215       |
|    lagrangian_multiplier | 0.0815       |
|    learning_rate         | 0.0003       |
|    loss                  | 7.6          |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.00537     |
|    std                   | 0.998        |
|    value_loss            | 102          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.39970374] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 27            |
|    time_elapsed          | 433           |
|    total_timesteps       | 55296         |
| train/                   |               |
|    approx_kl             | 0.007369517   |
|    clip_fraction         | 0.0703        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 24.7          |
|    entropy_loss          | -2.88         |
|    explained_variance    | -0.0289       |
|    lagrangian_multiplier | 0.0387        |
|    learning_rate         | 0.0003        |
|    loss                  | 46            |
|    n_updates             | 260           |
|    policy_gradient_loss  | -0.00699      |
|    std                   | 1.03          |
|    value_loss            | 293           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.9344647] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 29           |
|    time_elapsed          | 472          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.0058032544 |
|    clip_fraction         | 0.0598       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 4.98         |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.00451     |
|    lagrangian_multiplier | 0.0675       |
|    learning_rate         | 0.0003       |
|    loss                  | 19.9         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.00895     |
|    std                   | 1            |
|    value_loss            | 199          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.43994123] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 28            |
|    time_elapsed          | 449           |
|    total_timesteps       | 57344         |
| train/                   |               |
|    approx_kl             | 0.0027566347  |
|    clip_fraction         | 0.0204        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 2.15          |
|    entropy_loss          | -2.9          |
|    explained_variance    | -0.0599       |
|    lagrangian_multiplier | 0.0462        |
|    learning_rate         | 0.0003        |
|    loss                  | 38.1          |
|    n_updates             | 270           |
|    policy_gradient_loss  | -0.00157      |
|    std                   | 1.04          |
|    value_loss            | 280           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3832054] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 30           |
|    time_elapsed          | 489          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0075237844 |
|    clip_fraction         | 0.0808       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 3.71         |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.0466       |
|    lagrangian_multiplier | 0.0694       |
|    learning_rate         | 0.0003       |
|    loss                  | 14.6         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00687     |
|    std                   | 1            |
|    value_loss            | 148          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.93629146] |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 41            |
|    time_elapsed          | 804           |
|    total_timesteps       | 83968         |
| train/                   |               |
|    approx_kl             | 0.005737126   |
|    clip_fraction         | 0.0506        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 106           |
|    entropy_loss          | -2.91         |
|    explained_variance    | -0.0482       |
|    lagrangian_multiplier | 0.0681        |
|    learning_rate         | 0.0003        |
|    loss                  | 37.2          |
|    n_updates             | 400           |
|    policy_gradient_loss  | -0.0104       |
|    std                   | 1.05          |
|    value_loss            | 326           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.68024373] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 29            |
|    time_elapsed          | 465           |
|    total_timesteps       | 59392         |
| train/                   |               |
|    approx_kl             | 0.007737196   |
|    clip_fraction         | 0.068         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 8.3           |
|    entropy_loss          | -2.88         |
|    explained_variance    | -0.0576       |
|    lagrangian_multiplier | 0.0309        |
|    learning_rate         | 0.0003        |
|    loss                  | 36.7          |
|    n_updates             | 280           |
|    policy_gradient_loss  | -0.00934      |
|    std                   | 1.01          |
|    value_loss            | 234           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3591889] |
| time/                    |              |
|    fps                   | 24           |
|    iterations            | 45           |
|    time_elapsed          | 3769         |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.0054338216 |
|    clip_fraction         | 0.0656       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 19           |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.0416       |
|    lagrangian_multiplier | 0.0681       |
|    learning_rate         | 0.0003       |
|    loss                  | 46.7         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00885     |
|    std                   | 1.04         |
|    value_loss            | 514          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.3679376] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 31           |
|    time_elapsed          | 505          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.005099927  |
|    clip_fraction         | 0.0422       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 2.28         |
|    entropy_loss          | -2.83        |
|    explained_variance    | -0.0292      |
|    lagrangian_multiplier | 0.0659       |
|    learning_rate         | 0.0003       |
|    loss                  | 23.9         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.0046      |
|    std                   | 0.996        |
|    value_loss            | 212          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5229377] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 30           |
|    time_elapsed          | 481          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.005473463  |
|    clip_fraction         | 0.0528       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 12.3         |
|    entropy_loss          | -2.86        |
|    explained_variance    | 0.00919      |
|    lagrangian_multiplier | 0.0366       |
|    learning_rate         | 0.0003       |
|    loss                  | 34.3         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00673     |
|    std                   | 1.01         |
|    value_loss            | 233          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.227688] |
| time/                    |             |
|    fps                   | 125         |
|    iterations            | 32          |
|    time_elapsed          | 521         |
|    total_timesteps       | 65536       |
| train/                   |             |
|    approx_kl             | 0.006255218 |
|    clip_fraction         | 0.0707      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 92          |
|    entropy_loss          | -2.83       |
|    explained_variance    | -0.0102     |
|    lagrangian_multiplier | 0.0471      |
|    learning_rate         | 0.0003      |
|    loss                  | 88.1        |
|    n_updates             | 310         |
|    policy_gradient_loss  | -0.01       |
|    std                   | 0.994       |
|    value_loss            | 583         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.89098793] |
| time/                    |               |
|    fps                   | 127           |
|    iterations            | 31            |
|    time_elapsed          | 497           |
|    total_timesteps       | 63488         |
| train/                   |               |
|    approx_kl             | 0.0061481725  |
|    clip_fraction         | 0.0539        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 81.6          |
|    entropy_loss          | -2.86         |
|    explained_variance    | -0.0241       |
|    lagrangian_multiplier | 0.052         |
|    learning_rate         | 0.0003        |
|    loss                  | 57            |
|    n_updates             | 300           |
|    policy_gradient_loss  | -0.00634      |
|    std                   | 1.01          |
|    value_loss            | 370           |
--------------------------------------------
------------------------------------------
| reward                   | [-2.420521] |
| time/                    |             |
|    fps                   | 102         |
|    iterations            | 42          |
|    time_elapsed          | 840         |
|    total_timesteps       | 86016       |
| train/                   |             |
|    approx_kl             | 0.007414348 |
|    clip_fraction         | 0.0591      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 111         |
|    entropy_loss          | -2.92       |
|    explained_variance    | -0.0276     |
|    lagrangian_multiplier | 0.0537      |
|    learning_rate         | 0.0003      |
|    loss                  | 42.9        |
|    n_updates             | 410         |
|    policy_gradient_loss  | -0.00969    |
|    std                   | 1.04        |
|    value_loss            | 250         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.8866794] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 33           |
|    time_elapsed          | 538          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.006043534  |
|    clip_fraction         | 0.0543       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 12.8         |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.00247     |
|    lagrangian_multiplier | 0.0598       |
|    learning_rate         | 0.0003       |
|    loss                  | 26.3         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00745     |
|    std                   | 0.99         |
|    value_loss            | 278          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4001654] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 32           |
|    time_elapsed          | 513          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.0062258425 |
|    clip_fraction         | 0.0463       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 16           |
|    entropy_loss          | -2.87        |
|    explained_variance    | 0.0183       |
|    lagrangian_multiplier | 0.0413       |
|    learning_rate         | 0.0003       |
|    loss                  | 25.1         |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.00573     |
|    std                   | 1.02         |
|    value_loss            | 164          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
-------------------------------------------
| reward                   | [-1.3147136] |
| time/                    |              |
|    fps                   | 102          |
|    iterations            | 43           |
|    time_elapsed          | 856          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0058830734 |
|    clip_fraction         | 0.0667       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 146          |
|    entropy_loss          | -2.93        |
|    explained_variance    | 0.0115       |
|    lagrangian_multiplier | 0.0627       |
|    learning_rate         | 0.0003       |
|    loss                  | 64           |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.0093      |
|    std                   | 1.05         |
|    value_loss            | 565          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.31112763] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 34            |
|    time_elapsed          | 554           |
|    total_timesteps       | 69632         |
| train/                   |               |
|    approx_kl             | 0.0056701642  |
|    clip_fraction         | 0.043         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 71.3          |
|    entropy_loss          | -2.81         |
|    explained_variance    | -0.0309       |
|    lagrangian_multiplier | 0.0668        |
|    learning_rate         | 0.0003        |
|    loss                  | 32.4          |
|    n_updates             | 330           |
|    policy_gradient_loss  | -0.00616      |
|    std                   | 0.983         |
|    value_loss            | 269           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.1119627] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 33           |
|    time_elapsed          | 529          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.005441621  |
|    clip_fraction         | 0.0572       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 177          |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.00935     |
|    lagrangian_multiplier | 0.0538       |
|    learning_rate         | 0.0003       |
|    loss                  | 91.4         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00791     |
|    std                   | 1.02         |
|    value_loss            | 543          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6737413] |
| time/                    |              |
|    fps                   | 103          |
|    iterations            | 44           |
|    time_elapsed          | 873          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.0062412857 |
|    clip_fraction         | 0.0583       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 88.2         |
|    entropy_loss          | -2.94        |
|    explained_variance    | 0.00484      |
|    lagrangian_multiplier | 0.0786       |
|    learning_rate         | 0.0003       |
|    loss                  | 32.7         |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.00819     |
|    std                   | 1.05         |
|    value_loss            | 341          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6094879] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 35           |
|    time_elapsed          | 570          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0062203347 |
|    clip_fraction         | 0.0597       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 133          |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.00221     |
|    lagrangian_multiplier | 0.0657       |
|    learning_rate         | 0.0003       |
|    loss                  | 51.3         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00855     |
|    std                   | 0.985        |
|    value_loss            | 294          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7804096] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 34           |
|    time_elapsed          | 545          |
|    total_timesteps       | 69632        |
| train/                   |              |
|    approx_kl             | 0.0069222497 |
|    clip_fraction         | 0.0579       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 186          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.033       |
|    lagrangian_multiplier | 0.0511       |
|    learning_rate         | 0.0003       |
|    loss                  | 103          |
|    n_updates             | 330          |
|    policy_gradient_loss  | -0.00913     |
|    std                   | 1.01         |
|    value_loss            | 632          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.174171] |
| time/                    |             |
|    fps                   | 103         |
|    iterations            | 45          |
|    time_elapsed          | 889         |
|    total_timesteps       | 92160       |
| train/                   |             |
|    approx_kl             | 0.007376236 |
|    clip_fraction         | 0.0732      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 126         |
|    entropy_loss          | -2.93       |
|    explained_variance    | -0.0185     |
|    lagrangian_multiplier | 0.0722      |
|    learning_rate         | 0.0003      |
|    loss                  | 27.9        |
|    n_updates             | 440         |
|    policy_gradient_loss  | -0.0101     |
|    std                   | 1.05        |
|    value_loss            | 300         |
------------------------------------------
--------------------------------------------
| reward                   | [-0.31420085] |
| time/                    |               |
|    fps                   | 24            |
|    iterations            | 46            |
|    time_elapsed          | 3845          |
|    total_timesteps       | 94208         |
| train/                   |               |
|    approx_kl             | 0.0074378224  |
|    clip_fraction         | 0.0764        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 27.6          |
|    entropy_loss          | -2.93         |
|    explained_variance    | 0.0351        |
|    lagrangian_multiplier | 0.0739        |
|    learning_rate         | 0.0003        |
|    loss                  | 46.2          |
|    n_updates             | 450           |
|    policy_gradient_loss  | -0.0111       |
|    std                   | 1.05          |
|    value_loss            | 448           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.46850494] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 36            |
|    time_elapsed          | 587           |
|    total_timesteps       | 73728         |
| train/                   |               |
|    approx_kl             | 0.0071122795  |
|    clip_fraction         | 0.0662        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 150           |
|    entropy_loss          | -2.83         |
|    explained_variance    | 0.0233        |
|    lagrangian_multiplier | 0.0625        |
|    learning_rate         | 0.0003        |
|    loss                  | 50.2          |
|    n_updates             | 350           |
|    policy_gradient_loss  | -0.00972      |
|    std                   | 1.01          |
|    value_loss            | 319           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.679831]  |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 35           |
|    time_elapsed          | 561          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0057862713 |
|    clip_fraction         | 0.0747       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 132          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0405      |
|    lagrangian_multiplier | 0.0592       |
|    learning_rate         | 0.0003       |
|    loss                  | 69.5         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.00853     |
|    std                   | 1.02         |
|    value_loss            | 539          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.67684186] |
| time/                    |               |
|    fps                   | 103           |
|    iterations            | 46            |
|    time_elapsed          | 905           |
|    total_timesteps       | 94208         |
| train/                   |               |
|    approx_kl             | 0.007947626   |
|    clip_fraction         | 0.0759        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 1.38          |
|    entropy_loss          | -2.93         |
|    explained_variance    | -0.0138       |
|    lagrangian_multiplier | 0.0786        |
|    learning_rate         | 0.0003        |
|    loss                  | 9.06          |
|    n_updates             | 450           |
|    policy_gradient_loss  | -0.0105       |
|    std                   | 1.05          |
|    value_loss            | 107           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.6729207] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 37           |
|    time_elapsed          | 603          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.0063039353 |
|    clip_fraction         | 0.0492       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 55.5         |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0406       |
|    lagrangian_multiplier | 0.0603       |
|    learning_rate         | 0.0003       |
|    loss                  | 23.4         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.00604     |
|    std                   | 1            |
|    value_loss            | 200          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8159396] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 36           |
|    time_elapsed          | 577          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.007586524  |
|    clip_fraction         | 0.0674       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 186          |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.0171      |
|    lagrangian_multiplier | 0.0527       |
|    learning_rate         | 0.0003       |
|    loss                  | 91.1         |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.00661     |
|    std                   | 1.02         |
|    value_loss            | 582          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.42730087] |
| time/                    |               |
|    fps                   | 104           |
|    iterations            | 47            |
|    time_elapsed          | 922           |
|    total_timesteps       | 96256         |
| train/                   |               |
|    approx_kl             | 0.005662732   |
|    clip_fraction         | 0.0647        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0295        |
|    entropy_loss          | -2.93         |
|    explained_variance    | 0.00558       |
|    lagrangian_multiplier | 0.0781        |
|    learning_rate         | 0.0003        |
|    loss                  | 7.06          |
|    n_updates             | 460           |
|    policy_gradient_loss  | -0.00637      |
|    std                   | 1.05          |
|    value_loss            | 82.5          |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.5107761] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 38           |
|    time_elapsed          | 620          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.0071937153 |
|    clip_fraction         | 0.0823       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 107          |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.00419     |
|    lagrangian_multiplier | 0.0526       |
|    learning_rate         | 0.0003       |
|    loss                  | 51.3         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.0122      |
|    std                   | 1            |
|    value_loss            | 316          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.7403779] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 37           |
|    time_elapsed          | 593          |
|    total_timesteps       | 75776        |
| train/                   |              |
|    approx_kl             | 0.0069363117 |
|    clip_fraction         | 0.0669       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 143          |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.0206      |
|    lagrangian_multiplier | 0.0469       |
|    learning_rate         | 0.0003       |
|    loss                  | 97.9         |
|    n_updates             | 360          |
|    policy_gradient_loss  | -0.012       |
|    std                   | 1.01         |
|    value_loss            | 523          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5522442] |
| time/                    |              |
|    fps                   | 104          |
|    iterations            | 48           |
|    time_elapsed          | 938          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.0061522983 |
|    clip_fraction         | 0.0583       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 23.7         |
|    entropy_loss          | -2.92        |
|    explained_variance    | 0.0649       |
|    lagrangian_multiplier | 0.0371       |
|    learning_rate         | 0.0003       |
|    loss                  | 111          |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00521     |
|    std                   | 1.04         |
|    value_loss            | 682          |
-------------------------------------------
------------------------------------------
| reward                   | [-0.600433] |
| time/                    |             |
|    fps                   | 125         |
|    iterations            | 39          |
|    time_elapsed          | 636         |
|    total_timesteps       | 79872       |
| train/                   |             |
|    approx_kl             | 0.005130005 |
|    clip_fraction         | 0.0531      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 77.3        |
|    entropy_loss          | -2.85       |
|    explained_variance    | -0.0151     |
|    lagrangian_multiplier | 0.062       |
|    learning_rate         | 0.0003      |
|    loss                  | 30.9        |
|    n_updates             | 380         |
|    policy_gradient_loss  | -0.00778    |
|    std                   | 1.01        |
|    value_loss            | 267         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.8009593] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 38           |
|    time_elapsed          | 610          |
|    total_timesteps       | 77824        |
| train/                   |              |
|    approx_kl             | 0.00601943   |
|    clip_fraction         | 0.0562       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 50.4         |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0363      |
|    lagrangian_multiplier | 0.0445       |
|    learning_rate         | 0.0003       |
|    loss                  | 62.8         |
|    n_updates             | 370          |
|    policy_gradient_loss  | -0.00931     |
|    std                   | 1            |
|    value_loss            | 466          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.338959]  |
| time/                    |              |
|    fps                   | 105          |
|    iterations            | 49           |
|    time_elapsed          | 954          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.0050644786 |
|    clip_fraction         | 0.0454       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0224       |
|    entropy_loss          | -2.91        |
|    explained_variance    | 0.00997      |
|    lagrangian_multiplier | 0.0749       |
|    learning_rate         | 0.0003       |
|    loss                  | 40.4         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.00566     |
|    std                   | 1.04         |
|    value_loss            | 403          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 103, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 90, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                      reward ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñá‚ñÅ‚ñÖ‚ñá‚ñÖ‚ñà‚ñá‚ñÑ
wandb:             train/approx_kl ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñá‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñá‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÅ
wandb:         train/clip_fraction ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÅ‚ñà‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÉ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñà‚ñà‚ñà‚ñá‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÅ‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÅ‚ñÇ‚ñÅ
wandb:          train/entropy_loss ‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ
wandb:    train/explained_variance ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñà‚ñÑ
wandb: train/lagrangian_multiplier ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñá‚ñà‚ñÅ‚ñá
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñá‚ñÜ‚ñÉ‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñá‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñà‚ñÉ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ
wandb:                   train/std ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñá
wandb:            train/value_loss ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñá‚ñÉ‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÅ‚ñà‚ñÖ
wandb: 
wandb: Run summary:
wandb:                      reward -1.33896
wandb:             train/approx_kl 0.00506
wandb:         train/clip_fraction 0.04536
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 0.02237
wandb:          train/entropy_loss -2.91052
wandb:    train/explained_variance 0.00997
wandb: train/lagrangian_multiplier 0.07487
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 40.41628
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00566
wandb:                   train/std 1.04176
wandb:            train/value_loss 402.64995
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/eurs7c1s
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231223_085858-eurs7c1s/logs
srun: error: airl.ist.berkeley.edu: task 0: Exited with exit code 1
--------------------------------------------
| reward                   | [-0.31608278] |
| time/                    |               |
|    fps                   | 24            |
|    iterations            | 47            |
|    time_elapsed          | 3921          |
|    total_timesteps       | 96256         |
| train/                   |               |
|    approx_kl             | 0.005483443   |
|    clip_fraction         | 0.0395        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 52.3          |
|    entropy_loss          | -2.95         |
|    explained_variance    | 0.0434        |
|    lagrangian_multiplier | 0.0614        |
|    learning_rate         | 0.0003        |
|    loss                  | 61.6          |
|    n_updates             | 460           |
|    policy_gradient_loss  | -0.00639      |
|    std                   | 1.06          |
|    value_loss            | 488           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.73157805] |
| time/                    |               |
|    fps                   | 125           |
|    iterations            | 40            |
|    time_elapsed          | 652           |
|    total_timesteps       | 81920         |
| train/                   |               |
|    approx_kl             | 0.0058865184  |
|    clip_fraction         | 0.0716        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 64.5          |
|    entropy_loss          | -2.86         |
|    explained_variance    | -0.0547       |
|    lagrangian_multiplier | 0.068         |
|    learning_rate         | 0.0003        |
|    loss                  | 21.8          |
|    n_updates             | 390           |
|    policy_gradient_loss  | -0.0114       |
|    std                   | 1.02          |
|    value_loss            | 144           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.6240923] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 39           |
|    time_elapsed          | 626          |
|    total_timesteps       | 79872        |
| train/                   |              |
|    approx_kl             | 0.0052818726 |
|    clip_fraction         | 0.0353       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 50.5         |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0366      |
|    lagrangian_multiplier | 0.0542       |
|    learning_rate         | 0.0003       |
|    loss                  | 64.3         |
|    n_updates             | 380          |
|    policy_gradient_loss  | -0.00498     |
|    std                   | 1.01         |
|    value_loss            | 573          |
-------------------------------------------
srun: Job 114474 step creation still disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=114474.15
wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231223_091523-zienkr4y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppol-highway-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4
wandb: üöÄ View run at https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/zienkr4y
------------------------------------------
| reward                   | [-2.474478] |
| time/                    |             |
|    fps                   | 127         |
|    iterations            | 40          |
|    time_elapsed          | 642         |
|    total_timesteps       | 81920       |
| train/                   |             |
|    approx_kl             | 0.004118939 |
|    clip_fraction         | 0.0301      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 151         |
|    entropy_loss          | -2.86       |
|    explained_variance    | -0.0329     |
|    lagrangian_multiplier | 0.0558      |
|    learning_rate         | 0.0003      |
|    loss                  | 111         |
|    n_updates             | 390         |
|    policy_gradient_loss  | -0.00641    |
|    std                   | 1.01        |
|    value_loss            | 832         |
------------------------------------------
-------------------------------------------
| reward                   | [-0.3832331] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 41           |
|    time_elapsed          | 669          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.010035001  |
|    clip_fraction         | 0.108        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 160          |
|    entropy_loss          | -2.87        |
|    explained_variance    | -0.0238      |
|    lagrangian_multiplier | 0.0527       |
|    learning_rate         | 0.0003       |
|    loss                  | 125          |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.0134      |
|    std                   | 1.02         |
|    value_loss            | 1.03e+03     |
-------------------------------------------
Using cpu device
--------------------------------------
| reward             | [-0.44064862] |
| time/              |               |
|    fps             | 134           |
|    iterations      | 1             |
|    time_elapsed    | 15            |
|    total_timesteps | 2048          |
--------------------------------------
-------------------------------------------
| reward                   | [-1.9910103] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 41           |
|    time_elapsed          | 658          |
|    total_timesteps       | 83968        |
| train/                   |              |
|    approx_kl             | 0.005469937  |
|    clip_fraction         | 0.0543       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 63.2         |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0488      |
|    lagrangian_multiplier | 0.0457       |
|    learning_rate         | 0.0003       |
|    loss                  | 95           |
|    n_updates             | 400          |
|    policy_gradient_loss  | -0.00734     |
|    std                   | 1            |
|    value_loss            | 615          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5044991] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 42           |
|    time_elapsed          | 685          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.0051465495 |
|    clip_fraction         | 0.0455       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 156          |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.00502     |
|    lagrangian_multiplier | 0.0618       |
|    learning_rate         | 0.0003       |
|    loss                  | 78.5         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00816     |
|    std                   | 1.03         |
|    value_loss            | 687          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9617151] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 42           |
|    time_elapsed          | 674          |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.005045022  |
|    clip_fraction         | 0.0425       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 98.1         |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0348      |
|    lagrangian_multiplier | 0.0487       |
|    learning_rate         | 0.0003       |
|    loss                  | 104          |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.00728     |
|    std                   | 1.02         |
|    value_loss            | 750          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.104395]  |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 43           |
|    time_elapsed          | 701          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0076911473 |
|    clip_fraction         | 0.0668       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 150          |
|    entropy_loss          | -2.91        |
|    explained_variance    | -0.0008      |
|    lagrangian_multiplier | 0.0578       |
|    learning_rate         | 0.0003       |
|    loss                  | 85.4         |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.0099      |
|    std                   | 1.04         |
|    value_loss            | 707          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.982144]  |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 43           |
|    time_elapsed          | 690          |
|    total_timesteps       | 88064        |
| train/                   |              |
|    approx_kl             | 0.0046222853 |
|    clip_fraction         | 0.0391       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 83.9         |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.0203      |
|    lagrangian_multiplier | 0.0466       |
|    learning_rate         | 0.0003       |
|    loss                  | 106          |
|    n_updates             | 420          |
|    policy_gradient_loss  | -0.00537     |
|    std                   | 1.01         |
|    value_loss            | 695          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.3840091] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 44           |
|    time_elapsed          | 718          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.0074719368 |
|    clip_fraction         | 0.0701       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 215          |
|    entropy_loss          | -2.92        |
|    explained_variance    | -0.0242      |
|    lagrangian_multiplier | 0.0625       |
|    learning_rate         | 0.0003       |
|    loss                  | 128          |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.0102      |
|    std                   | 1.04         |
|    value_loss            | 1.27e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8464249] |
| time/                    |              |
|    fps                   | 24           |
|    iterations            | 48           |
|    time_elapsed          | 3998         |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.004659167  |
|    clip_fraction         | 0.0434       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 35.7         |
|    entropy_loss          | -2.97        |
|    explained_variance    | 0.0135       |
|    lagrangian_multiplier | 0.072        |
|    learning_rate         | 0.0003       |
|    loss                  | 39.8         |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00872     |
|    std                   | 1.08         |
|    value_loss            | 304          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2478124] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 44           |
|    time_elapsed          | 707          |
|    total_timesteps       | 90112        |
| train/                   |              |
|    approx_kl             | 0.005251622  |
|    clip_fraction         | 0.0417       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 132          |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0178      |
|    lagrangian_multiplier | 0.0393       |
|    learning_rate         | 0.0003       |
|    loss                  | 155          |
|    n_updates             | 430          |
|    policy_gradient_loss  | -0.0077      |
|    std                   | 1            |
|    value_loss            | 924          |
-------------------------------------------
------------------------------------------
| reward                   | [-2.406453] |
| time/                    |             |
|    fps                   | 125         |
|    iterations            | 45          |
|    time_elapsed          | 734         |
|    total_timesteps       | 92160       |
| train/                   |             |
|    approx_kl             | 0.007760062 |
|    clip_fraction         | 0.0637      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 178         |
|    entropy_loss          | -2.93       |
|    explained_variance    | 0.0212      |
|    lagrangian_multiplier | 0.0566      |
|    learning_rate         | 0.0003      |
|    loss                  | 131         |
|    n_updates             | 440         |
|    policy_gradient_loss  | -0.00836    |
|    std                   | 1.06        |
|    value_loss            | 993         |
------------------------------------------
-------------------------------------------
| reward                   | [-3.3234904] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 45           |
|    time_elapsed          | 723          |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.005028806  |
|    clip_fraction         | 0.0513       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 124          |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.0372      |
|    lagrangian_multiplier | 0.056        |
|    learning_rate         | 0.0003       |
|    loss                  | 59.9         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00933     |
|    std                   | 1.01         |
|    value_loss            | 367          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.46945]   |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 46           |
|    time_elapsed          | 751          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0072179995 |
|    clip_fraction         | 0.059        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 144          |
|    entropy_loss          | -2.95        |
|    explained_variance    | 0.0026       |
|    lagrangian_multiplier | 0.0579       |
|    learning_rate         | 0.0003       |
|    loss                  | 98           |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.00997     |
|    std                   | 1.06         |
|    value_loss            | 933          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3051918] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 46           |
|    time_elapsed          | 739          |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.0056354143 |
|    clip_fraction         | 0.0578       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 123          |
|    entropy_loss          | -2.86        |
|    explained_variance    | -0.00821     |
|    lagrangian_multiplier | 0.0519       |
|    learning_rate         | 0.0003       |
|    loss                  | 87.6         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.0104      |
|    std                   | 1.02         |
|    value_loss            | 596          |
-------------------------------------------
-------------------------------------------
| reward                   | [-3.2191842] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 47           |
|    time_elapsed          | 767          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.0076285657 |
|    clip_fraction         | 0.0931       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 147          |
|    entropy_loss          | -2.97        |
|    explained_variance    | -0.0219      |
|    lagrangian_multiplier | 0.0488       |
|    learning_rate         | 0.0003       |
|    loss                  | 168          |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.0114      |
|    std                   | 1.07         |
|    value_loss            | 1.33e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5028983] |
| time/                    |              |
|    fps                   | 40           |
|    iterations            | 2            |
|    time_elapsed          | 101          |
|    total_timesteps       | 4096         |
| train/                   |              |
|    approx_kl             | 0.0052601146 |
|    clip_fraction         | 0.0506       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.443        |
|    entropy_loss          | -2.84        |
|    explained_variance    | 0.00779      |
|    lagrangian_multiplier | 0.0632       |
|    learning_rate         | 0.0003       |
|    loss                  | 34.7         |
|    n_updates             | 10           |
|    policy_gradient_loss  | -0.00599     |
|    std                   | 1            |
|    value_loss            | 346          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.5822361] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 47           |
|    time_elapsed          | 755          |
|    total_timesteps       | 96256        |
| train/                   |              |
|    approx_kl             | 0.007086047  |
|    clip_fraction         | 0.0628       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 37.6         |
|    entropy_loss          | -2.88        |
|    explained_variance    | -0.0111      |
|    lagrangian_multiplier | 0.0372       |
|    learning_rate         | 0.0003       |
|    loss                  | 44.6         |
|    n_updates             | 460          |
|    policy_gradient_loss  | -0.00957     |
|    std                   | 1.03         |
|    value_loss            | 301          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6407716] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 48           |
|    time_elapsed          | 783          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.007682873  |
|    clip_fraction         | 0.0915       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 229          |
|    entropy_loss          | -2.99        |
|    explained_variance    | -0.0132      |
|    lagrangian_multiplier | 0.0525       |
|    learning_rate         | 0.0003       |
|    loss                  | 237          |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.0116      |
|    std                   | 1.08         |
|    value_loss            | 1.94e+03     |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.9696528] |
| time/                    |              |
|    fps                   | 52           |
|    iterations            | 3            |
|    time_elapsed          | 117          |
|    total_timesteps       | 6144         |
| train/                   |              |
|    approx_kl             | 0.005842624  |
|    clip_fraction         | 0.0483       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.0887       |
|    entropy_loss          | -2.85        |
|    explained_variance    | 0.0186       |
|    lagrangian_multiplier | 0.0585       |
|    learning_rate         | 0.0003       |
|    loss                  | 52.9         |
|    n_updates             | 20           |
|    policy_gradient_loss  | -0.00444     |
|    std                   | 1            |
|    value_loss            | 495          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7082011] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 48           |
|    time_elapsed          | 771          |
|    total_timesteps       | 98304        |
| train/                   |              |
|    approx_kl             | 0.004994965  |
|    clip_fraction         | 0.0523       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 66.4         |
|    entropy_loss          | -2.91        |
|    explained_variance    | -0.00739     |
|    lagrangian_multiplier | 0.039        |
|    learning_rate         | 0.0003       |
|    loss                  | 47.8         |
|    n_updates             | 470          |
|    policy_gradient_loss  | -0.00642     |
|    std                   | 1.04         |
|    value_loss            | 343          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.6453378] |
| time/                    |              |
|    fps                   | 125          |
|    iterations            | 49           |
|    time_elapsed          | 800          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.009859955  |
|    clip_fraction         | 0.0858       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 170          |
|    entropy_loss          | -2.99        |
|    explained_variance    | -0.0152      |
|    lagrangian_multiplier | 0.0551       |
|    learning_rate         | 0.0003       |
|    loss                  | 264          |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.0109      |
|    std                   | 1.08         |
|    value_loss            | 1.95e+03     |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 103, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 90, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
-------------------------------------------
| reward                   | [-0.469633]  |
| time/                    |              |
|    fps                   | 24           |
|    iterations            | 49           |
|    time_elapsed          | 4074         |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.0058916677 |
|    clip_fraction         | 0.0442       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 26.5         |
|    entropy_loss          | -2.98        |
|    explained_variance    | 0.0368       |
|    lagrangian_multiplier | 0.0701       |
|    learning_rate         | 0.0003       |
|    loss                  | 22.3         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.0078      |
|    std                   | 1.07         |
|    value_loss            | 225          |
-------------------------------------------
wandb: 
wandb: Run history:
wandb:                      reward ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÖ‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÖ
wandb:             train/approx_kl ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñà
wandb:         train/clip_fraction ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÖ‚ñÇ‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñà‚ñá
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñà‚ñÜ
wandb:          train/entropy_loss ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:    train/explained_variance ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÅ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ
wandb: train/lagrangian_multiplier ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñà‚ñÑ‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñÖ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÉ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñà
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÉ‚ñà‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ
wandb:                   train/std ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà
wandb:            train/value_loss ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:                      reward -1.64534
wandb:             train/approx_kl 0.00986
wandb:         train/clip_fraction 0.08584
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 169.5158
wandb:          train/entropy_loss -2.9891
wandb:    train/explained_variance -0.01524
wandb: train/lagrangian_multiplier 0.05512
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 264.27881
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.01091
wandb:                   train/std 1.0785
wandb:            train/value_loss 1952.41122
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/axzppts0
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231223_010413-axzppts0/logs
srun: error: dqn.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-0.7563327] |
| time/                    |              |
|    fps                   | 127          |
|    iterations            | 49           |
|    time_elapsed          | 787          |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.004675211  |
|    clip_fraction         | 0.0504       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 47.2         |
|    entropy_loss          | -2.94        |
|    explained_variance    | -0.0547      |
|    lagrangian_multiplier | 0.0362       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.2         |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.00659     |
|    std                   | 1.06         |
|    value_loss            | 275          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 103, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 90, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÇ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñá‚ñÖ‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÜ‚ñÅ‚ñÖ‚ñà‚ñá
wandb:             train/approx_kl ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñà‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñÑ
wandb:         train/clip_fraction ‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñà‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñÖ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñÖ‚ñÇ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñá‚ñÉ‚ñÑ‚ñÇ‚ñà‚ñÉ‚ñÜ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñá‚ñá‚ñá‚ñÜ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñÇ
wandb:          train/entropy_loss ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÅ
wandb:    train/explained_variance ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÜ‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñÉ‚ñá‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÅ
wandb: train/lagrangian_multiplier ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñà‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñá‚ñÖ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÅ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÇ‚ñÜ‚ñÉ‚ñÑ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÅ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ
wandb:                   train/std ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:            train/value_loss ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñà‚ñÉ‚ñÇ‚ñÉ‚ñÇ
wandb: 
wandb: Run summary:
wandb:                      reward -0.75633
wandb:             train/approx_kl 0.00468
wandb:         train/clip_fraction 0.05039
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 47.17561
wandb:          train/entropy_loss -2.93705
wandb:    train/explained_variance -0.05468
wandb: train/lagrangian_multiplier 0.03618
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 47.24238
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.00659
wandb:                   train/std 1.05937
wandb:            train/value_loss 274.81052
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/17l5mhtw
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231223_010440-17l5mhtw/logs
srun: error: ddpg.ist.berkeley.edu: task 0: Exited with exit code 1
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 103, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 90, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñá‚ñÜ‚ñÖ‚ñá‚ñÇ‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñà‚ñà‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñÉ‚ñà‚ñÜ‚ñà‚ñá‚ñÅ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñà‚ñÖ‚ñÑ‚ñá‚ñá‚ñá
wandb:             train/approx_kl ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÅ‚ñá‚ñÇ‚ñÜ‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñÜ‚ñá‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÖ
wandb:         train/clip_fraction ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÅ‚ñá‚ñÇ‚ñÑ‚ñÇ‚ñÜ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñà‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñá‚ñÜ‚ñÉ‚ñÑ‚ñÑ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñà‚ñÜ‚ñÖ
wandb:          train/entropy_loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ
wandb:    train/explained_variance ‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñá‚ñÅ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñá‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÇ‚ñÖ‚ñÖ‚ñÉ‚ñÑ
wandb: train/lagrangian_multiplier ‚ñá‚ñá‚ñÉ‚ñÜ‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÖ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÇ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÇ‚ñÜ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÅ‚ñÜ‚ñÖ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÉ
wandb:                   train/std ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñÜ
wandb:            train/value_loss ‚ñÜ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÇ
wandb: 
wandb: Run summary:
wandb:                      reward -0.46963
wandb:             train/approx_kl 0.00589
wandb:         train/clip_fraction 0.04419
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 26.45333
wandb:          train/entropy_loss -2.98178
wandb:    train/explained_variance 0.03681
wandb: train/lagrangian_multiplier 0.0701
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 22.25004
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.0078
wandb:                   train/std 1.07475
wandb:            train/value_loss 225.14252
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/tjc7kab3
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231223_000944-tjc7kab3/logs
srun: error: gail.ist.berkeley.edu: task 0: Exited with exit code 1
-------------------------------------------
| reward                   | [-1.4026635] |
| time/                    |              |
|    fps                   | 40           |
|    iterations            | 4            |
|    time_elapsed          | 204          |
|    total_timesteps       | 8192         |
| train/                   |              |
|    approx_kl             | 0.0060135727 |
|    clip_fraction         | 0.0614       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.301        |
|    entropy_loss          | -2.85        |
|    explained_variance    | -0.00445     |
|    lagrangian_multiplier | 0.0516       |
|    learning_rate         | 0.0003       |
|    loss                  | 99.1         |
|    n_updates             | 30           |
|    policy_gradient_loss  | -0.00759     |
|    std                   | 1            |
|    value_loss            | 797          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.76003206] |
| time/                    |               |
|    fps                   | 45            |
|    iterations            | 5             |
|    time_elapsed          | 227           |
|    total_timesteps       | 10240         |
| train/                   |               |
|    approx_kl             | 0.007240139   |
|    clip_fraction         | 0.0707        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.155         |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.0407       |
|    lagrangian_multiplier | 0.0512        |
|    learning_rate         | 0.0003        |
|    loss                  | 50.6          |
|    n_updates             | 40            |
|    policy_gradient_loss  | -0.00762      |
|    std                   | 0.998         |
|    value_loss            | 425           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.4135283] |
| time/                    |              |
|    fps                   | 50           |
|    iterations            | 6            |
|    time_elapsed          | 243          |
|    total_timesteps       | 12288        |
| train/                   |              |
|    approx_kl             | 0.0052386997 |
|    clip_fraction         | 0.0525       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.223        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0172       |
|    lagrangian_multiplier | 0.0628       |
|    learning_rate         | 0.0003       |
|    loss                  | 45.4         |
|    n_updates             | 50           |
|    policy_gradient_loss  | -0.00842     |
|    std                   | 0.999        |
|    value_loss            | 471          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.372134] |
| time/                    |             |
|    fps                   | 45          |
|    iterations            | 7           |
|    time_elapsed          | 315         |
|    total_timesteps       | 14336       |
| train/                   |             |
|    approx_kl             | 0.00586546  |
|    clip_fraction         | 0.0489      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.243       |
|    entropy_loss          | -2.83       |
|    explained_variance    | -0.0118     |
|    lagrangian_multiplier | 0.0561      |
|    learning_rate         | 0.0003      |
|    loss                  | 49.4        |
|    n_updates             | 60          |
|    policy_gradient_loss  | -0.00664    |
|    std                   | 0.992       |
|    value_loss            | 443         |
------------------------------------------
-------------------------------------------
| reward                   | [-0.7117554] |
| time/                    |              |
|    fps                   | 49           |
|    iterations            | 8            |
|    time_elapsed          | 331          |
|    total_timesteps       | 16384        |
| train/                   |              |
|    approx_kl             | 0.005026899  |
|    clip_fraction         | 0.0419       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.307        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.00634      |
|    lagrangian_multiplier | 0.0532       |
|    learning_rate         | 0.0003       |
|    loss                  | 55.8         |
|    n_updates             | 70           |
|    policy_gradient_loss  | -0.00563     |
|    std                   | 1            |
|    value_loss            | 495          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3416181] |
| time/                    |              |
|    fps                   | 52           |
|    iterations            | 9            |
|    time_elapsed          | 348          |
|    total_timesteps       | 18432        |
| train/                   |              |
|    approx_kl             | 0.0046643186 |
|    clip_fraction         | 0.0281       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.459        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.018        |
|    lagrangian_multiplier | 0.0595       |
|    learning_rate         | 0.0003       |
|    loss                  | 47.1         |
|    n_updates             | 80           |
|    policy_gradient_loss  | -0.00685     |
|    std                   | 0.992        |
|    value_loss            | 435          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5676057] |
| time/                    |              |
|    fps                   | 56           |
|    iterations            | 10           |
|    time_elapsed          | 364          |
|    total_timesteps       | 20480        |
| train/                   |              |
|    approx_kl             | 0.0059409463 |
|    clip_fraction         | 0.0518       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.129        |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0183      |
|    lagrangian_multiplier | 0.0567       |
|    learning_rate         | 0.0003       |
|    loss                  | 55           |
|    n_updates             | 90           |
|    policy_gradient_loss  | -0.00631     |
|    std                   | 0.99         |
|    value_loss            | 429          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1987904] |
| time/                    |              |
|    fps                   | 59           |
|    iterations            | 11           |
|    time_elapsed          | 381          |
|    total_timesteps       | 22528        |
| train/                   |              |
|    approx_kl             | 0.0049240342 |
|    clip_fraction         | 0.0506       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.497        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.0139       |
|    lagrangian_multiplier | 0.0606       |
|    learning_rate         | 0.0003       |
|    loss                  | 77.8         |
|    n_updates             | 100          |
|    policy_gradient_loss  | -0.00783     |
|    std                   | 1            |
|    value_loss            | 695          |
-------------------------------------------
------------------------------------------
| reward                   | [-1.416535] |
| time/                    |             |
|    fps                   | 61          |
|    iterations            | 12          |
|    time_elapsed          | 397         |
|    total_timesteps       | 24576       |
| train/                   |             |
|    approx_kl             | 0.006338847 |
|    clip_fraction         | 0.0481      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.206       |
|    entropy_loss          | -2.82       |
|    explained_variance    | -0.0135     |
|    lagrangian_multiplier | 0.0514      |
|    learning_rate         | 0.0003      |
|    loss                  | 41.6        |
|    n_updates             | 110         |
|    policy_gradient_loss  | -0.00764    |
|    std                   | 0.983       |
|    value_loss            | 378         |
------------------------------------------
-------------------------------------------
| reward                   | [-1.2211477] |
| time/                    |              |
|    fps                   | 64           |
|    iterations            | 13           |
|    time_elapsed          | 414          |
|    total_timesteps       | 26624        |
| train/                   |              |
|    approx_kl             | 0.0058751544 |
|    clip_fraction         | 0.0506       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.179        |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0149      |
|    lagrangian_multiplier | 0.062        |
|    learning_rate         | 0.0003       |
|    loss                  | 25           |
|    n_updates             | 120          |
|    policy_gradient_loss  | -0.00583     |
|    std                   | 0.98         |
|    value_loss            | 235          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.9046876] |
| time/                    |              |
|    fps                   | 66           |
|    iterations            | 14           |
|    time_elapsed          | 430          |
|    total_timesteps       | 28672        |
| train/                   |              |
|    approx_kl             | 0.006622342  |
|    clip_fraction         | 0.0691       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.164        |
|    entropy_loss          | -2.82        |
|    explained_variance    | -0.0102      |
|    lagrangian_multiplier | 0.0506       |
|    learning_rate         | 0.0003       |
|    loss                  | 42.7         |
|    n_updates             | 130          |
|    policy_gradient_loss  | -0.00816     |
|    std                   | 0.998        |
|    value_loss            | 410          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4722675] |
| time/                    |              |
|    fps                   | 68           |
|    iterations            | 15           |
|    time_elapsed          | 447          |
|    total_timesteps       | 30720        |
| train/                   |              |
|    approx_kl             | 0.0059927017 |
|    clip_fraction         | 0.055        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.342        |
|    entropy_loss          | -2.82        |
|    explained_variance    | 0.00802      |
|    lagrangian_multiplier | 0.0602       |
|    learning_rate         | 0.0003       |
|    loss                  | 59.4         |
|    n_updates             | 140          |
|    policy_gradient_loss  | -0.00647     |
|    std                   | 0.989        |
|    value_loss            | 546          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2659979] |
| time/                    |              |
|    fps                   | 70           |
|    iterations            | 16           |
|    time_elapsed          | 463          |
|    total_timesteps       | 32768        |
| train/                   |              |
|    approx_kl             | 0.0046165884 |
|    clip_fraction         | 0.0371       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.109        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0162       |
|    lagrangian_multiplier | 0.0693       |
|    learning_rate         | 0.0003       |
|    loss                  | 28.6         |
|    n_updates             | 150          |
|    policy_gradient_loss  | -0.00576     |
|    std                   | 0.98         |
|    value_loss            | 271          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.4957365] |
| time/                    |              |
|    fps                   | 72           |
|    iterations            | 17           |
|    time_elapsed          | 479          |
|    total_timesteps       | 34816        |
| train/                   |              |
|    approx_kl             | 0.004615901  |
|    clip_fraction         | 0.0374       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.204        |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.00899     |
|    lagrangian_multiplier | 0.0446       |
|    learning_rate         | 0.0003       |
|    loss                  | 58.9         |
|    n_updates             | 160          |
|    policy_gradient_loss  | -0.00468     |
|    std                   | 0.986        |
|    value_loss            | 445          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.5533947] |
| time/                    |              |
|    fps                   | 74           |
|    iterations            | 18           |
|    time_elapsed          | 496          |
|    total_timesteps       | 36864        |
| train/                   |              |
|    approx_kl             | 0.0074470225 |
|    clip_fraction         | 0.0533       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.301        |
|    entropy_loss          | -2.81        |
|    explained_variance    | 0.0203       |
|    lagrangian_multiplier | 0.055        |
|    learning_rate         | 0.0003       |
|    loss                  | 44.3         |
|    n_updates             | 170          |
|    policy_gradient_loss  | -0.00638     |
|    std                   | 0.983        |
|    value_loss            | 410          |
-------------------------------------------
-------------------------------------------
| reward                   | [-2.3847044] |
| time/                    |              |
|    fps                   | 75           |
|    iterations            | 19           |
|    time_elapsed          | 512          |
|    total_timesteps       | 38912        |
| train/                   |              |
|    approx_kl             | 0.007242492  |
|    clip_fraction         | 0.0581       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.25         |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0255      |
|    lagrangian_multiplier | 0.0561       |
|    learning_rate         | 0.0003       |
|    loss                  | 60.6         |
|    n_updates             | 180          |
|    policy_gradient_loss  | -0.00861     |
|    std                   | 0.98         |
|    value_loss            | 518          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.8752925] |
| time/                    |              |
|    fps                   | 77           |
|    iterations            | 20           |
|    time_elapsed          | 529          |
|    total_timesteps       | 40960        |
| train/                   |              |
|    approx_kl             | 0.0062263263 |
|    clip_fraction         | 0.0649       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.297        |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0127      |
|    lagrangian_multiplier | 0.0536       |
|    learning_rate         | 0.0003       |
|    loss                  | 155          |
|    n_updates             | 190          |
|    policy_gradient_loss  | -0.0115      |
|    std                   | 0.987        |
|    value_loss            | 1.34e+03     |
-------------------------------------------
------------------------------------------
| reward                   | [-1.810411] |
| time/                    |             |
|    fps                   | 78          |
|    iterations            | 21          |
|    time_elapsed          | 545         |
|    total_timesteps       | 43008       |
| train/                   |             |
|    approx_kl             | 0.006048662 |
|    clip_fraction         | 0.0614      |
|    clip_range            | 0.2         |
|    cost_value_loss       | 0.354       |
|    entropy_loss          | -2.8        |
|    explained_variance    | 0.00417     |
|    lagrangian_multiplier | 0.0634      |
|    learning_rate         | 0.0003      |
|    loss                  | 51.7        |
|    n_updates             | 200         |
|    policy_gradient_loss  | -0.00826    |
|    std                   | 0.979       |
|    value_loss            | 537         |
------------------------------------------
-------------------------------------------
| reward                   | [-0.5027572] |
| time/                    |              |
|    fps                   | 80           |
|    iterations            | 22           |
|    time_elapsed          | 562          |
|    total_timesteps       | 45056        |
| train/                   |              |
|    approx_kl             | 0.004688057  |
|    clip_fraction         | 0.033        |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.236        |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0246      |
|    lagrangian_multiplier | 0.0553       |
|    learning_rate         | 0.0003       |
|    loss                  | 41.5         |
|    n_updates             | 210          |
|    policy_gradient_loss  | -0.00617     |
|    std                   | 0.972        |
|    value_loss            | 342          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.78929573] |
| time/                    |               |
|    fps                   | 81            |
|    iterations            | 23            |
|    time_elapsed          | 578           |
|    total_timesteps       | 47104         |
| train/                   |               |
|    approx_kl             | 0.0056683533  |
|    clip_fraction         | 0.0523        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.169         |
|    entropy_loss          | -2.78         |
|    explained_variance    | -0.00795      |
|    lagrangian_multiplier | 0.0594        |
|    learning_rate         | 0.0003        |
|    loss                  | 74.2          |
|    n_updates             | 220           |
|    policy_gradient_loss  | -0.00822      |
|    std                   | 0.977         |
|    value_loss            | 693           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.2110736] |
| time/                    |              |
|    fps                   | 79           |
|    iterations            | 24           |
|    time_elapsed          | 616          |
|    total_timesteps       | 49152        |
| train/                   |              |
|    approx_kl             | 0.0065371594 |
|    clip_fraction         | 0.0629       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.221        |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0525      |
|    lagrangian_multiplier | 0.0628       |
|    learning_rate         | 0.0003       |
|    loss                  | 28.6         |
|    n_updates             | 230          |
|    policy_gradient_loss  | -0.00929     |
|    std                   | 0.986        |
|    value_loss            | 277          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.7720274] |
| time/                    |              |
|    fps                   | 80           |
|    iterations            | 25           |
|    time_elapsed          | 632          |
|    total_timesteps       | 51200        |
| train/                   |              |
|    approx_kl             | 0.006389899  |
|    clip_fraction         | 0.0562       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.383        |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.0189      |
|    lagrangian_multiplier | 0.0558       |
|    learning_rate         | 0.0003       |
|    loss                  | 85.6         |
|    n_updates             | 240          |
|    policy_gradient_loss  | -0.00795     |
|    std                   | 0.983        |
|    value_loss            | 893          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.34500995] |
| time/                    |               |
|    fps                   | 82            |
|    iterations            | 26            |
|    time_elapsed          | 649           |
|    total_timesteps       | 53248         |
| train/                   |               |
|    approx_kl             | 0.006377316   |
|    clip_fraction         | 0.0575        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.178         |
|    entropy_loss          | -2.79         |
|    explained_variance    | -0.0236       |
|    lagrangian_multiplier | 0.0495        |
|    learning_rate         | 0.0003        |
|    loss                  | 67.8          |
|    n_updates             | 250           |
|    policy_gradient_loss  | -0.0064       |
|    std                   | 0.969         |
|    value_loss            | 577           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.1966192] |
| time/                    |              |
|    fps                   | 83           |
|    iterations            | 27           |
|    time_elapsed          | 665          |
|    total_timesteps       | 55296        |
| train/                   |              |
|    approx_kl             | 0.0059193596 |
|    clip_fraction         | 0.0401       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.279        |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0187      |
|    lagrangian_multiplier | 0.0609       |
|    learning_rate         | 0.0003       |
|    loss                  | 39.4         |
|    n_updates             | 260          |
|    policy_gradient_loss  | -0.00774     |
|    std                   | 0.973        |
|    value_loss            | 416          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.1479017] |
| time/                    |              |
|    fps                   | 84           |
|    iterations            | 28           |
|    time_elapsed          | 682          |
|    total_timesteps       | 57344        |
| train/                   |              |
|    approx_kl             | 0.0062983083 |
|    clip_fraction         | 0.0609       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.188        |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0189      |
|    lagrangian_multiplier | 0.0613       |
|    learning_rate         | 0.0003       |
|    loss                  | 31.2         |
|    n_updates             | 270          |
|    policy_gradient_loss  | -0.0113      |
|    std                   | 0.98         |
|    value_loss            | 277          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2510916] |
| time/                    |              |
|    fps                   | 84           |
|    iterations            | 29           |
|    time_elapsed          | 698          |
|    total_timesteps       | 59392        |
| train/                   |              |
|    approx_kl             | 0.0074753584 |
|    clip_fraction         | 0.0673       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.229        |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0234      |
|    lagrangian_multiplier | 0.0554       |
|    learning_rate         | 0.0003       |
|    loss                  | 76.6         |
|    n_updates             | 280          |
|    policy_gradient_loss  | -0.0084      |
|    std                   | 0.978        |
|    value_loss            | 677          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3526351] |
| time/                    |              |
|    fps                   | 85           |
|    iterations            | 30           |
|    time_elapsed          | 715          |
|    total_timesteps       | 61440        |
| train/                   |              |
|    approx_kl             | 0.0056108423 |
|    clip_fraction         | 0.0563       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.185        |
|    entropy_loss          | -2.8         |
|    explained_variance    | -0.0305      |
|    lagrangian_multiplier | 0.05         |
|    learning_rate         | 0.0003       |
|    loss                  | 39.6         |
|    n_updates             | 290          |
|    policy_gradient_loss  | -0.00916     |
|    std                   | 0.982        |
|    value_loss            | 321          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.8740902] |
| time/                    |              |
|    fps                   | 86           |
|    iterations            | 31           |
|    time_elapsed          | 732          |
|    total_timesteps       | 63488        |
| train/                   |              |
|    approx_kl             | 0.004303101  |
|    clip_fraction         | 0.0357       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.134        |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0284      |
|    lagrangian_multiplier | 0.056        |
|    learning_rate         | 0.0003       |
|    loss                  | 43.4         |
|    n_updates             | 300          |
|    policy_gradient_loss  | -0.00515     |
|    std                   | 0.968        |
|    value_loss            | 403          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.4678101] |
| time/                    |              |
|    fps                   | 87           |
|    iterations            | 32           |
|    time_elapsed          | 748          |
|    total_timesteps       | 65536        |
| train/                   |              |
|    approx_kl             | 0.007869219  |
|    clip_fraction         | 0.0812       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.239        |
|    entropy_loss          | -2.77        |
|    explained_variance    | 0.00468      |
|    lagrangian_multiplier | 0.068        |
|    learning_rate         | 0.0003       |
|    loss                  | 33           |
|    n_updates             | 310          |
|    policy_gradient_loss  | -0.0113      |
|    std                   | 0.966        |
|    value_loss            | 351          |
-------------------------------------------
-------------------------------------------
| reward                   | [-0.6979734] |
| time/                    |              |
|    fps                   | 87           |
|    iterations            | 33           |
|    time_elapsed          | 775          |
|    total_timesteps       | 67584        |
| train/                   |              |
|    approx_kl             | 0.005149452  |
|    clip_fraction         | 0.0408       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.117        |
|    entropy_loss          | -2.77        |
|    explained_variance    | -0.0218      |
|    lagrangian_multiplier | 0.0549       |
|    learning_rate         | 0.0003       |
|    loss                  | 27.2         |
|    n_updates             | 320          |
|    policy_gradient_loss  | -0.00598     |
|    std                   | 0.968        |
|    value_loss            | 242          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.98215264] |
| time/                    |               |
|    fps                   | 87            |
|    iterations            | 34            |
|    time_elapsed          | 792           |
|    total_timesteps       | 69632         |
| train/                   |               |
|    approx_kl             | 0.0060632853  |
|    clip_fraction         | 0.0563        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.0985        |
|    entropy_loss          | -2.76         |
|    explained_variance    | -0.0149       |
|    lagrangian_multiplier | 0.0612        |
|    learning_rate         | 0.0003        |
|    loss                  | 25.1          |
|    n_updates             | 330           |
|    policy_gradient_loss  | -0.00772      |
|    std                   | 0.962         |
|    value_loss            | 227           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.7812216] |
| time/                    |              |
|    fps                   | 88           |
|    iterations            | 35           |
|    time_elapsed          | 808          |
|    total_timesteps       | 71680        |
| train/                   |              |
|    approx_kl             | 0.0094166435 |
|    clip_fraction         | 0.0988       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.364        |
|    entropy_loss          | -2.78        |
|    explained_variance    | -0.0192      |
|    lagrangian_multiplier | 0.0624       |
|    learning_rate         | 0.0003       |
|    loss                  | 45.1         |
|    n_updates             | 340          |
|    policy_gradient_loss  | -0.0134      |
|    std                   | 0.983        |
|    value_loss            | 474          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.2090323] |
| time/                    |              |
|    fps                   | 86           |
|    iterations            | 36           |
|    time_elapsed          | 857          |
|    total_timesteps       | 73728        |
| train/                   |              |
|    approx_kl             | 0.007879854  |
|    clip_fraction         | 0.0934       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.398        |
|    entropy_loss          | -2.81        |
|    explained_variance    | -0.00473     |
|    lagrangian_multiplier | 0.0683       |
|    learning_rate         | 0.0003       |
|    loss                  | 23           |
|    n_updates             | 350          |
|    policy_gradient_loss  | -0.0136      |
|    std                   | 0.991        |
|    value_loss            | 294          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.41069624] |
| time/                    |               |
|    fps                   | 80            |
|    iterations            | 37            |
|    time_elapsed          | 944           |
|    total_timesteps       | 75776         |
| train/                   |               |
|    approx_kl             | 0.007014289   |
|    clip_fraction         | 0.0665        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.252         |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.00174      |
|    lagrangian_multiplier | 0.0584        |
|    learning_rate         | 0.0003        |
|    loss                  | 32.1          |
|    n_updates             | 360           |
|    policy_gradient_loss  | -0.00972      |
|    std                   | 1             |
|    value_loss            | 298           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.53540397] |
| time/                    |               |
|    fps                   | 78            |
|    iterations            | 38            |
|    time_elapsed          | 995           |
|    total_timesteps       | 77824         |
| train/                   |               |
|    approx_kl             | 0.007208231   |
|    clip_fraction         | 0.0729        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.165         |
|    entropy_loss          | -2.86         |
|    explained_variance    | -0.039        |
|    lagrangian_multiplier | 0.0527        |
|    learning_rate         | 0.0003        |
|    loss                  | 27.4          |
|    n_updates             | 370           |
|    policy_gradient_loss  | -0.0112       |
|    std                   | 1.02          |
|    value_loss            | 239           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.47234678] |
| time/                    |               |
|    fps                   | 78            |
|    iterations            | 39            |
|    time_elapsed          | 1012          |
|    total_timesteps       | 79872         |
| train/                   |               |
|    approx_kl             | 0.0055412585  |
|    clip_fraction         | 0.0465        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.103         |
|    entropy_loss          | -2.87         |
|    explained_variance    | 0.0279        |
|    lagrangian_multiplier | 0.0678        |
|    learning_rate         | 0.0003        |
|    loss                  | 50.5          |
|    n_updates             | 380           |
|    policy_gradient_loss  | -0.00479      |
|    std                   | 1.02          |
|    value_loss            | 586           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.95638776] |
| time/                    |               |
|    fps                   | 78            |
|    iterations            | 40            |
|    time_elapsed          | 1040          |
|    total_timesteps       | 81920         |
| train/                   |               |
|    approx_kl             | 0.0051166518  |
|    clip_fraction         | 0.0465        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.146         |
|    entropy_loss          | -2.85         |
|    explained_variance    | -0.0228       |
|    lagrangian_multiplier | 0.0548        |
|    learning_rate         | 0.0003        |
|    loss                  | 22.3          |
|    n_updates             | 390           |
|    policy_gradient_loss  | -0.0071       |
|    std                   | 1.01          |
|    value_loss            | 213           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.72578245] |
| time/                    |               |
|    fps                   | 74            |
|    iterations            | 41            |
|    time_elapsed          | 1120          |
|    total_timesteps       | 83968         |
| train/                   |               |
|    approx_kl             | 0.005409804   |
|    clip_fraction         | 0.0422        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.225         |
|    entropy_loss          | -2.84         |
|    explained_variance    | -0.0215       |
|    lagrangian_multiplier | 0.0415        |
|    learning_rate         | 0.0003        |
|    loss                  | 39.7          |
|    n_updates             | 400           |
|    policy_gradient_loss  | -0.00538      |
|    std                   | 1             |
|    value_loss            | 287           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.3789392] |
| time/                    |              |
|    fps                   | 75           |
|    iterations            | 42           |
|    time_elapsed          | 1137         |
|    total_timesteps       | 86016        |
| train/                   |              |
|    approx_kl             | 0.008208247  |
|    clip_fraction         | 0.0814       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.494        |
|    entropy_loss          | -2.84        |
|    explained_variance    | -0.0252      |
|    lagrangian_multiplier | 0.062        |
|    learning_rate         | 0.0003       |
|    loss                  | 69.6         |
|    n_updates             | 410          |
|    policy_gradient_loss  | -0.0107      |
|    std                   | 1.01         |
|    value_loss            | 731          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.46916917] |
| time/                    |               |
|    fps                   | 76            |
|    iterations            | 43            |
|    time_elapsed          | 1153          |
|    total_timesteps       | 88064         |
| train/                   |               |
|    approx_kl             | 0.006507486   |
|    clip_fraction         | 0.063         |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.399         |
|    entropy_loss          | -2.83         |
|    explained_variance    | -0.0285       |
|    lagrangian_multiplier | 0.0594        |
|    learning_rate         | 0.0003        |
|    loss                  | 53.9          |
|    n_updates             | 420           |
|    policy_gradient_loss  | -0.00901      |
|    std                   | 0.99          |
|    value_loss            | 492           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.92590237] |
| time/                    |               |
|    fps                   | 76            |
|    iterations            | 44            |
|    time_elapsed          | 1170          |
|    total_timesteps       | 90112         |
| train/                   |               |
|    approx_kl             | 0.0068717087  |
|    clip_fraction         | 0.0795        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.209         |
|    entropy_loss          | -2.8          |
|    explained_variance    | -0.0307       |
|    lagrangian_multiplier | 0.04          |
|    learning_rate         | 0.0003        |
|    loss                  | 74.7          |
|    n_updates             | 430           |
|    policy_gradient_loss  | -0.00996      |
|    std                   | 0.983         |
|    value_loss            | 490           |
--------------------------------------------
-------------------------------------------
| reward                   | [-0.5809364] |
| time/                    |              |
|    fps                   | 77           |
|    iterations            | 45           |
|    time_elapsed          | 1186         |
|    total_timesteps       | 92160        |
| train/                   |              |
|    approx_kl             | 0.005712903  |
|    clip_fraction         | 0.0532       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.105        |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.0204      |
|    lagrangian_multiplier | 0.0413       |
|    learning_rate         | 0.0003       |
|    loss                  | 33.6         |
|    n_updates             | 440          |
|    policy_gradient_loss  | -0.00467     |
|    std                   | 0.978        |
|    value_loss            | 233          |
-------------------------------------------
-------------------------------------------
| reward                   | [-1.3214004] |
| time/                    |              |
|    fps                   | 78           |
|    iterations            | 46           |
|    time_elapsed          | 1203         |
|    total_timesteps       | 94208        |
| train/                   |              |
|    approx_kl             | 0.006317134  |
|    clip_fraction         | 0.0735       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.472        |
|    entropy_loss          | -2.79        |
|    explained_variance    | -0.018       |
|    lagrangian_multiplier | 0.0616       |
|    learning_rate         | 0.0003       |
|    loss                  | 79.4         |
|    n_updates             | 450          |
|    policy_gradient_loss  | -0.0103      |
|    std                   | 0.983        |
|    value_loss            | 771          |
-------------------------------------------
--------------------------------------------
| reward                   | [-0.44875768] |
| time/                    |               |
|    fps                   | 78            |
|    iterations            | 47            |
|    time_elapsed          | 1219          |
|    total_timesteps       | 96256         |
| train/                   |               |
|    approx_kl             | 0.0065560937  |
|    clip_fraction         | 0.0555        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.351         |
|    entropy_loss          | -2.79         |
|    explained_variance    | -0.0126       |
|    lagrangian_multiplier | 0.0509        |
|    learning_rate         | 0.0003        |
|    loss                  | 50.4          |
|    n_updates             | 460           |
|    policy_gradient_loss  | -0.01         |
|    std                   | 0.985         |
|    value_loss            | 381           |
--------------------------------------------
--------------------------------------------
| reward                   | [-0.80885243] |
| time/                    |               |
|    fps                   | 79            |
|    iterations            | 48            |
|    time_elapsed          | 1236          |
|    total_timesteps       | 98304         |
| train/                   |               |
|    approx_kl             | 0.006182908   |
|    clip_fraction         | 0.0658        |
|    clip_range            | 0.2           |
|    cost_value_loss       | 0.204         |
|    entropy_loss          | -2.81         |
|    explained_variance    | -0.00831      |
|    lagrangian_multiplier | 0.0506        |
|    learning_rate         | 0.0003        |
|    loss                  | 34.7          |
|    n_updates             | 470           |
|    policy_gradient_loss  | -0.0114       |
|    std                   | 0.995         |
|    value_loss            | 395           |
--------------------------------------------
-------------------------------------------
| reward                   | [-1.9701245] |
| time/                    |              |
|    fps                   | 80           |
|    iterations            | 49           |
|    time_elapsed          | 1252         |
|    total_timesteps       | 100352       |
| train/                   |              |
|    approx_kl             | 0.006656665  |
|    clip_fraction         | 0.0708       |
|    clip_range            | 0.2          |
|    cost_value_loss       | 0.182        |
|    entropy_loss          | -2.83        |
|    explained_variance    | 0.00287      |
|    lagrangian_multiplier | 0.0494       |
|    learning_rate         | 0.0003       |
|    loss                  | 109          |
|    n_updates             | 480          |
|    policy_gradient_loss  | -0.0108      |
|    std                   | 1.01         |
|    value_loss            | 732          |
-------------------------------------------
Traceback (most recent call last):
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 103, in <module>
    train()
  File "/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/pyrallis/argparsing.py", line 158, in wrapper_inner
    response = fn(cfg, *args, **kwargs)
  File "/nas/ucb/mason/ethically-compliant-rl/PPOL_New/train_ppol.py", line 90, in train
    check_build_path(path)
TypeError: 'staticmethod' object is not callable
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                      reward ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÇ‚ñÇ
wandb:             train/approx_kl ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÅ‚ñá‚ñÉ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÉ‚ñÇ‚ñà‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÖ
wandb:         train/clip_fraction ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñá‚ñÇ‚ñÑ‚ñà‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñá‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñÜ
wandb:            train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cost_value_loss ‚ñá‚ñÅ‚ñÖ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñÇ‚ñà‚ñÉ‚ñÇ‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÇ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñà‚ñÜ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÉ
wandb:          train/entropy_loss ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ
wandb:    train/explained_variance ‚ñÜ‚ñá‚ñÖ‚ñÅ‚ñá‚ñÜ‚ñá‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÅ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ
wandb: train/lagrangian_multiplier ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñÇ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñÑ‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÉ
wandb:         train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÜ
wandb:             train/n_updates ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  train/policy_gradient_loss ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÖ‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñÑ‚ñá‚ñÉ‚ñá‚ñÖ‚ñÅ‚ñÑ‚ñÉ‚ñà‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñà‚ñÑ‚ñÉ‚ñÉ
wandb:                   train/std ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ
wandb:            train/value_loss ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñà‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÑ
wandb: 
wandb: Run summary:
wandb:                      reward -1.97012
wandb:             train/approx_kl 0.00666
wandb:         train/clip_fraction 0.07075
wandb:            train/clip_range 0.2
wandb:       train/cost_value_loss 0.18185
wandb:          train/entropy_loss -2.82575
wandb:    train/explained_variance 0.00287
wandb: train/lagrangian_multiplier 0.04937
wandb:         train/learning_rate 0.0003
wandb:                  train/loss 108.52411
wandb:             train/n_updates 480
wandb:  train/policy_gradient_loss -0.01081
wandb:                   train/std 1.00511
wandb:            train/value_loss 731.5754
wandb: 
wandb: üöÄ View run ppol-highway-parking at: https://wandb.ai/ecrl/New-PPOL-SpeedLimit%3D4/runs/zienkr4y
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231223_091523-zienkr4y/logs
srun: error: airl.ist.berkeley.edu: task 0: Exited with exit code 1
