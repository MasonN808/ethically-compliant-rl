wandb: Currently logged in as: mason-nakamura1 (ecrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.2
wandb: Run data is saved locally in /nas/ucb/mason/ethically-compliant-rl/wandb/run-20231120_142135-25w8qhij
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo-KLpenalty-beta(dynamic)-parking
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ecrl/PPO-Penalty-Report5
wandb: üöÄ View run at https://wandb.ai/ecrl/PPO-Penalty-Report5/runs/25w8qhij
Using cpu device
-------------------------------------
| reward             | [-0.4385915] |
| time/              |              |
|    fps             | 162          |
|    iterations      | 1            |
|    time_elapsed    | 12           |
|    total_timesteps | 2048         |
-------------------------------------
-----------------------------------------------
| reward                      | [-0.41521704] |
| time/                       |               |
|    fps                      | 153           |
|    iterations               | 2             |
|    time_elapsed             | 26            |
|    total_timesteps          | 4096          |
| train/                      |               |
|    approx_kl                | 7.011262      |
|    approx_ln(kl)            | 1.9475178     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.83         |
|    explained_variance       | 0.0381        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | inf           |
|    ln(policy_gradient_loss) | inf           |
|    loss                     | inf           |
|    n_updates                | 10            |
|    policy_gradient_loss     | inf           |
|    std                      | 0.964         |
|    value_loss               | 61.1          |
-----------------------------------------------
-----------------------------------------------
| reward                      | [-0.32496628] |
| time/                       |               |
|    fps                      | 150           |
|    iterations               | 3             |
|    time_elapsed             | 40            |
|    total_timesteps          | 6144          |
| train/                      |               |
|    approx_kl                | 11.299578     |
|    approx_ln(kl)            | 2.4247653     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.7          |
|    explained_variance       | -0.594        |
|    learning_rate            | 0.001         |
|    ln(loss)                 | inf           |
|    ln(policy_gradient_loss) | inf           |
|    loss                     | inf           |
|    n_updates                | 20            |
|    policy_gradient_loss     | inf           |
|    std                      | 0.908         |
|    value_loss               | 16.8          |
-----------------------------------------------
----------------------------------------------
| reward                      | [-0.3978556] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 4            |
|    time_elapsed             | 54           |
|    total_timesteps          | 8192         |
| train/                      |              |
|    approx_kl                | 10.3391485   |
|    approx_ln(kl)            | 2.3359375    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.64        |
|    explained_variance       | 0.631        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | inf          |
|    loss                     | inf          |
|    n_updates                | 30           |
|    policy_gradient_loss     | inf          |
|    std                      | 0.896        |
|    value_loss               | 48.9         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: overflow encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
----------------------------------------------
| reward                      | [-1.8769535] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 5            |
|    time_elapsed             | 68           |
|    total_timesteps          | 10240        |
| train/                      |              |
|    approx_kl                | 12.89696     |
|    approx_ln(kl)            | 2.5569916    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.61        |
|    explained_variance       | 0.724        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | inf          |
|    loss                     | inf          |
|    n_updates                | 40           |
|    policy_gradient_loss     | inf          |
|    std                      | 0.893        |
|    value_loss               | 302          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: invalid value encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
----------------------------------------------
| reward                      | [-0.6986061] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 6            |
|    time_elapsed             | 81           |
|    total_timesteps          | 12288        |
| train/                      |              |
|    approx_kl                | 13.329311    |
|    approx_ln(kl)            | 2.5899656    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.61        |
|    explained_variance       | 0.891        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 50           |
|    policy_gradient_loss     | nan          |
|    std                      | 0.889        |
|    value_loss               | 138          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: invalid value encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
----------------------------------------------
| reward                      | [-0.6037258] |
| time/                       |              |
|    fps                      | 150          |
|    iterations               | 7            |
|    time_elapsed             | 94           |
|    total_timesteps          | 14336        |
| train/                      |              |
|    approx_kl                | 29.064014    |
|    approx_ln(kl)            | 3.3695009    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.6         |
|    explained_variance       | 0.901        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 60           |
|    policy_gradient_loss     | nan          |
|    std                      | 0.885        |
|    value_loss               | 188          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: invalid value encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
----------------------------------------------
| reward                      | [-0.5522698] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 8            |
|    time_elapsed             | 108          |
|    total_timesteps          | 16384        |
| train/                      |              |
|    approx_kl                | 7.607854     |
|    approx_ln(kl)            | 2.0291812    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.55        |
|    explained_variance       | 0.188        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 70           |
|    policy_gradient_loss     | nan          |
|    std                      | 0.855        |
|    value_loss               | 19.2         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: invalid value encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
----------------------------------------------
| reward                      | [-3.7369342] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 9            |
|    time_elapsed             | 121          |
|    total_timesteps          | 18432        |
| train/                      |              |
|    approx_kl                | 6.2478848    |
|    approx_ln(kl)            | 1.832243     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.52        |
|    explained_variance       | 0.742        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 80           |
|    policy_gradient_loss     | nan          |
|    std                      | 0.847        |
|    value_loss               | 3.8          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: invalid value encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
----------------------------------------------
| reward                      | [-2.0761056] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 10           |
|    time_elapsed             | 135          |
|    total_timesteps          | 20480        |
| train/                      |              |
|    approx_kl                | 16.122023    |
|    approx_ln(kl)            | 2.7801862    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.5         |
|    explained_variance       | 0.73         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 90           |
|    policy_gradient_loss     | nan          |
|    std                      | 0.843        |
|    value_loss               | 71.6         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: invalid value encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
---------------------------------------------
| reward                      | [-0.616228] |
| time/                       |             |
|    fps                      | 151         |
|    iterations               | 11          |
|    time_elapsed             | 148         |
|    total_timesteps          | 22528       |
| train/                      |             |
|    approx_kl                | 8.233736    |
|    approx_ln(kl)            | 2.10824     |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.5        |
|    explained_variance       | 0.91        |
|    learning_rate            | 0.001       |
|    ln(loss)                 | inf         |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | inf         |
|    n_updates                | 100         |
|    policy_gradient_loss     | nan         |
|    std                      | 0.84        |
|    value_loss               | 38.6        |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: invalid value encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
----------------------------------------------
| reward                      | [-1.3033439] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 12           |
|    time_elapsed             | 161          |
|    total_timesteps          | 24576        |
| train/                      |              |
|    approx_kl                | 17.959438    |
|    approx_ln(kl)            | 2.888116     |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.48        |
|    explained_variance       | 0.936        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 110          |
|    policy_gradient_loss     | nan          |
|    std                      | 0.829        |
|    value_loss               | 149          |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: invalid value encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
----------------------------------------------
| reward                      | [-0.5745712] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 13           |
|    time_elapsed             | 175          |
|    total_timesteps          | 26624        |
| train/                      |              |
|    approx_kl                | 18.349428    |
|    approx_ln(kl)            | 2.9095984    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.46        |
|    explained_variance       | 0.338        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 120          |
|    policy_gradient_loss     | nan          |
|    std                      | 0.819        |
|    value_loss               | 34.5         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: invalid value encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
----------------------------------------------
| reward                      | [-1.9600174] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 14           |
|    time_elapsed             | 189          |
|    total_timesteps          | 28672        |
| train/                      |              |
|    approx_kl                | 21.289143    |
|    approx_ln(kl)            | 3.0581973    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.44        |
|    explained_variance       | 0.547        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 130          |
|    policy_gradient_loss     | nan          |
|    std                      | 0.825        |
|    value_loss               | 17.9         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: invalid value encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
---------------------------------------------
| reward                      | [-2.106706] |
| time/                       |             |
|    fps                      | 151         |
|    iterations               | 15          |
|    time_elapsed             | 203         |
|    total_timesteps          | 30720       |
| train/                      |             |
|    approx_kl                | 8.732523    |
|    approx_ln(kl)            | 2.1670544   |
|    clip_range               | 0.2         |
|    entropy_loss             | -2.45       |
|    explained_variance       | 0.481       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | inf         |
|    ln(policy_gradient_loss) | nan         |
|    loss                     | inf         |
|    n_updates                | 140         |
|    policy_gradient_loss     | nan         |
|    std                      | 0.824       |
|    value_loss               | 55          |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: invalid value encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
----------------------------------------------
| reward                      | [-2.6190114] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 16           |
|    time_elapsed             | 216          |
|    total_timesteps          | 32768        |
| train/                      |              |
|    approx_kl                | 18.411503    |
|    approx_ln(kl)            | 2.9129755    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.45        |
|    explained_variance       | 0.0302       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 150          |
|    policy_gradient_loss     | nan          |
|    std                      | 0.824        |
|    value_loss               | 83.9         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: invalid value encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
-----------------------------------------------
| reward                      | [-0.41760266] |
| time/                       |               |
|    fps                      | 151           |
|    iterations               | 17            |
|    time_elapsed             | 230           |
|    total_timesteps          | 34816         |
| train/                      |               |
|    approx_kl                | 16.788189     |
|    approx_ln(kl)            | 2.8206756     |
|    clip_range               | 0.2           |
|    entropy_loss             | -2.44         |
|    explained_variance       | -1.29         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | inf           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | inf           |
|    n_updates                | 160           |
|    policy_gradient_loss     | nan           |
|    std                      | 0.818         |
|    value_loss               | 38.4          |
-----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: invalid value encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
----------------------------------------------
| reward                      | [-3.2728405] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 18           |
|    time_elapsed             | 243          |
|    total_timesteps          | 36864        |
| train/                      |              |
|    approx_kl                | 25.665077    |
|    approx_ln(kl)            | 3.2451313    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.43        |
|    explained_variance       | -0.324       |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 170          |
|    policy_gradient_loss     | nan          |
|    std                      | 0.81         |
|    value_loss               | 38.6         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: invalid value encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
----------------------------------------------
| reward                      | [-3.3245716] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 19           |
|    time_elapsed             | 257          |
|    total_timesteps          | 38912        |
| train/                      |              |
|    approx_kl                | 52.791107    |
|    approx_ln(kl)            | 3.9663427    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.4         |
|    explained_variance       | 0.57         |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 180          |
|    policy_gradient_loss     | nan          |
|    std                      | 0.791        |
|    value_loss               | 32.7         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: invalid value encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
----------------------------------------------
| reward                      | [-3.8399968] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 20           |
|    time_elapsed             | 270          |
|    total_timesteps          | 40960        |
| train/                      |              |
|    approx_kl                | 86.898926    |
|    approx_ln(kl)            | 4.4647455    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.36        |
|    explained_variance       | 0.876        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 190          |
|    policy_gradient_loss     | nan          |
|    std                      | 0.779        |
|    value_loss               | 53.6         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: invalid value encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
----------------------------------------------
| reward                      | [-0.3599188] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 21           |
|    time_elapsed             | 284          |
|    total_timesteps          | 43008        |
| train/                      |              |
|    approx_kl                | 47.792557    |
|    approx_ln(kl)            | 3.86687      |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.27        |
|    explained_variance       | 0.899        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 200          |
|    policy_gradient_loss     | nan          |
|    std                      | 0.719        |
|    value_loss               | 48.3         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: invalid value encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
----------------------------------------------
| reward                      | [-0.8911818] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 22           |
|    time_elapsed             | 298          |
|    total_timesteps          | 45056        |
| train/                      |              |
|    approx_kl                | 28.204353    |
|    approx_ln(kl)            | 3.3394763    |
|    clip_range               | 0.2          |
|    entropy_loss             | -2.09        |
|    explained_variance       | 0.964        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 210          |
|    policy_gradient_loss     | nan          |
|    std                      | 0.649        |
|    value_loss               | 56.6         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: invalid value encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
----------------------------------------------
| reward                      | [-1.7184278] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 23           |
|    time_elapsed             | 311          |
|    total_timesteps          | 47104        |
| train/                      |              |
|    approx_kl                | 22.408943    |
|    approx_ln(kl)            | 3.10946      |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.92        |
|    explained_variance       | 0.973        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 220          |
|    policy_gradient_loss     | nan          |
|    std                      | 0.603        |
|    value_loss               | 11.4         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/stable_baselines3/stable_baselines3/ppo_penalty/ppo_penalty.py:234: RuntimeWarning: invalid value encountered in multiply
  policy_loss = -th.mean(advantages * ratio - self.beta * approx_kl_div)
----------------------------------------------
| reward                      | [-2.0773177] |
| time/                       |              |
|    fps                      | 151          |
|    iterations               | 24           |
|    time_elapsed             | 324          |
|    total_timesteps          | 49152        |
| train/                      |              |
|    approx_kl                | 24.86382     |
|    approx_ln(kl)            | 3.2134137    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.8         |
|    explained_variance       | 0.985        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 230          |
|    policy_gradient_loss     | nan          |
|    std                      | 0.58         |
|    value_loss               | 29.5         |
----------------------------------------------
----------------------------------------------
| reward                      | [-0.2813454] |
| time/                       |              |
|    fps                      | 127          |
|    iterations               | 25           |
|    time_elapsed             | 400          |
|    total_timesteps          | 51200        |
| train/                      |              |
|    approx_kl                | 36.77787     |
|    approx_ln(kl)            | 3.6048963    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.73        |
|    explained_variance       | 0.985        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | inf          |
|    loss                     | inf          |
|    n_updates                | 240          |
|    policy_gradient_loss     | inf          |
|    std                      | 0.557        |
|    value_loss               | 42.4         |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.37376365] |
| time/                       |               |
|    fps                      | 105           |
|    iterations               | 26            |
|    time_elapsed             | 503           |
|    total_timesteps          | 53248         |
| train/                      |               |
|    approx_kl                | 32.933064     |
|    approx_ln(kl)            | 3.494477      |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.61         |
|    explained_variance       | 0.983         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | inf           |
|    ln(policy_gradient_loss) | inf           |
|    loss                     | inf           |
|    n_updates                | 250           |
|    policy_gradient_loss     | inf           |
|    std                      | 0.52          |
|    value_loss               | 20.8          |
-----------------------------------------------
----------------------------------------------
| reward                      | [-0.6700008] |
| time/                       |              |
|    fps                      | 106          |
|    iterations               | 27           |
|    time_elapsed             | 517          |
|    total_timesteps          | 55296        |
| train/                      |              |
|    approx_kl                | 3.7806153    |
|    approx_ln(kl)            | 1.3298868    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.52        |
|    explained_variance       | 0.595        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | inf          |
|    loss                     | inf          |
|    n_updates                | 260          |
|    policy_gradient_loss     | inf          |
|    std                      | 0.515        |
|    value_loss               | 13.3         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)
----------------------------------------------
| reward                      | [-1.1770796] |
| time/                       |              |
|    fps                      | 108          |
|    iterations               | 28           |
|    time_elapsed             | 530          |
|    total_timesteps          | 57344        |
| train/                      |              |
|    approx_kl                | 3.2431684    |
|    approx_ln(kl)            | 1.1765507    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.5         |
|    explained_variance       | 0.455        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 270          |
|    policy_gradient_loss     | nan          |
|    std                      | 0.506        |
|    value_loss               | 8.47         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)
----------------------------------------------
| reward                      | [-2.4358556] |
| time/                       |              |
|    fps                      | 109          |
|    iterations               | 29           |
|    time_elapsed             | 544          |
|    total_timesteps          | 59392        |
| train/                      |              |
|    approx_kl                | 2.6419244    |
|    approx_ln(kl)            | 0.97150755   |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.47        |
|    explained_variance       | 0.389        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 280          |
|    policy_gradient_loss     | nan          |
|    std                      | 0.502        |
|    value_loss               | 40.6         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)
-----------------------------------------------
| reward                      | [-0.37794328] |
| time/                       |               |
|    fps                      | 110           |
|    iterations               | 30            |
|    time_elapsed             | 557           |
|    total_timesteps          | 61440         |
| train/                      |               |
|    approx_kl                | 33.959633     |
|    approx_ln(kl)            | 3.5251725     |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.45         |
|    explained_variance       | 0.924         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | inf           |
|    ln(policy_gradient_loss) | nan           |
|    loss                     | inf           |
|    n_updates                | 290           |
|    policy_gradient_loss     | nan           |
|    std                      | 0.499         |
|    value_loss               | 36.2          |
-----------------------------------------------
----------------------------------------------
| reward                      | [-3.4935482] |
| time/                       |              |
|    fps                      | 111          |
|    iterations               | 31           |
|    time_elapsed             | 571          |
|    total_timesteps          | 63488        |
| train/                      |              |
|    approx_kl                | 4.6961336    |
|    approx_ln(kl)            | 1.5467396    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.44        |
|    explained_variance       | 0.956        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | inf          |
|    loss                     | inf          |
|    n_updates                | 300          |
|    policy_gradient_loss     | inf          |
|    std                      | 0.495        |
|    value_loss               | 34.3         |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.30648854] |
| time/                       |               |
|    fps                      | 111           |
|    iterations               | 32            |
|    time_elapsed             | 585           |
|    total_timesteps          | 65536         |
| train/                      |               |
|    approx_kl                | 73.59306      |
|    approx_ln(kl)            | 4.2985506     |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.42         |
|    explained_variance       | 0.992         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | inf           |
|    ln(policy_gradient_loss) | inf           |
|    loss                     | inf           |
|    n_updates                | 310           |
|    policy_gradient_loss     | inf           |
|    std                      | 0.489         |
|    value_loss               | 6.66          |
-----------------------------------------------
--------------------------------------------
| reward                      | [-0.70544] |
| time/                       |            |
|    fps                      | 112        |
|    iterations               | 33         |
|    time_elapsed             | 598        |
|    total_timesteps          | 67584      |
| train/                      |            |
|    approx_kl                | 8.365837   |
|    approx_ln(kl)            | 2.1241565  |
|    clip_range               | 0.2        |
|    entropy_loss             | -1.41      |
|    explained_variance       | 0.846      |
|    learning_rate            | 0.001      |
|    ln(loss)                 | inf        |
|    ln(policy_gradient_loss) | inf        |
|    loss                     | inf        |
|    n_updates                | 320        |
|    policy_gradient_loss     | inf        |
|    std                      | 0.488      |
|    value_loss               | 39.8       |
--------------------------------------------
----------------------------------------------
| reward                      | [-0.7114903] |
| time/                       |              |
|    fps                      | 113          |
|    iterations               | 34           |
|    time_elapsed             | 612          |
|    total_timesteps          | 69632        |
| train/                      |              |
|    approx_kl                | 23.261475    |
|    approx_ln(kl)            | 3.1467986    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.4         |
|    explained_variance       | 0.315        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | inf          |
|    loss                     | inf          |
|    n_updates                | 330          |
|    policy_gradient_loss     | inf          |
|    std                      | 0.484        |
|    value_loss               | 40.3         |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.80964506] |
| time/                       |               |
|    fps                      | 114           |
|    iterations               | 35            |
|    time_elapsed             | 626           |
|    total_timesteps          | 71680         |
| train/                      |               |
|    approx_kl                | 15.181768     |
|    approx_ln(kl)            | 2.7200952     |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.38         |
|    explained_variance       | 0.104         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | inf           |
|    ln(policy_gradient_loss) | inf           |
|    loss                     | inf           |
|    n_updates                | 340           |
|    policy_gradient_loss     | inf           |
|    std                      | 0.478         |
|    value_loss               | 11.9          |
-----------------------------------------------
---------------------------------------------
| reward                      | [-0.592429] |
| time/                       |             |
|    fps                      | 115         |
|    iterations               | 36          |
|    time_elapsed             | 640         |
|    total_timesteps          | 73728       |
| train/                      |             |
|    approx_kl                | 33.298676   |
|    approx_ln(kl)            | 3.5055177   |
|    clip_range               | 0.2         |
|    entropy_loss             | -1.34       |
|    explained_variance       | 0.941       |
|    learning_rate            | 0.001       |
|    ln(loss)                 | inf         |
|    ln(policy_gradient_loss) | inf         |
|    loss                     | inf         |
|    n_updates                | 350         |
|    policy_gradient_loss     | inf         |
|    std                      | 0.464       |
|    value_loss               | 2.01        |
---------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)
----------------------------------------------
| reward                      | [-0.3504712] |
| time/                       |              |
|    fps                      | 115          |
|    iterations               | 37           |
|    time_elapsed             | 653          |
|    total_timesteps          | 75776        |
| train/                      |              |
|    approx_kl                | 48.880424    |
|    approx_ln(kl)            | 3.889377     |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.29        |
|    explained_variance       | 0.583        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 360          |
|    policy_gradient_loss     | nan          |
|    std                      | 0.455        |
|    value_loss               | 18.1         |
----------------------------------------------
/nas/ucb/mason/ethically-compliant-rl/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)
----------------------------------------------
| reward                      | [-3.6399102] |
| time/                       |              |
|    fps                      | 116          |
|    iterations               | 38           |
|    time_elapsed             | 666          |
|    total_timesteps          | 77824        |
| train/                      |              |
|    approx_kl                | 89.71243     |
|    approx_ln(kl)            | 4.496609     |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.21        |
|    explained_variance       | 0.974        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | nan          |
|    loss                     | inf          |
|    n_updates                | 370          |
|    policy_gradient_loss     | nan          |
|    std                      | 0.441        |
|    value_loss               | 0.748        |
----------------------------------------------
----------------------------------------------
| reward                      | [-3.8582556] |
| time/                       |              |
|    fps                      | 117          |
|    iterations               | 39           |
|    time_elapsed             | 680          |
|    total_timesteps          | 79872        |
| train/                      |              |
|    approx_kl                | 36.48259     |
|    approx_ln(kl)            | 3.5968351    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.18        |
|    explained_variance       | 0.854        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | inf          |
|    loss                     | inf          |
|    n_updates                | 380          |
|    policy_gradient_loss     | inf          |
|    std                      | 0.43         |
|    value_loss               | 39.7         |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.50355893] |
| time/                       |               |
|    fps                      | 118           |
|    iterations               | 40            |
|    time_elapsed             | 693           |
|    total_timesteps          | 81920         |
| train/                      |               |
|    approx_kl                | 12.948124     |
|    approx_ln(kl)            | 2.560951      |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.15         |
|    explained_variance       | 0.884         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | inf           |
|    ln(policy_gradient_loss) | inf           |
|    loss                     | inf           |
|    n_updates                | 390           |
|    policy_gradient_loss     | inf           |
|    std                      | 0.429         |
|    value_loss               | 40.2          |
-----------------------------------------------
-----------------------------------------------
| reward                      | [-0.48984522] |
| time/                       |               |
|    fps                      | 118           |
|    iterations               | 41            |
|    time_elapsed             | 707           |
|    total_timesteps          | 83968         |
| train/                      |               |
|    approx_kl                | 33.15883      |
|    approx_ln(kl)            | 3.501309      |
|    clip_range               | 0.2           |
|    entropy_loss             | -1.14         |
|    explained_variance       | 0.954         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | inf           |
|    ln(policy_gradient_loss) | inf           |
|    loss                     | inf           |
|    n_updates                | 400           |
|    policy_gradient_loss     | inf           |
|    std                      | 0.424         |
|    value_loss               | 29            |
-----------------------------------------------
----------------------------------------------
| reward                      | [-0.4537175] |
| time/                       |              |
|    fps                      | 119          |
|    iterations               | 42           |
|    time_elapsed             | 720          |
|    total_timesteps          | 86016        |
| train/                      |              |
|    approx_kl                | 87.744865    |
|    approx_ln(kl)            | 4.4744334    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.1         |
|    explained_variance       | 0.901        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | inf          |
|    loss                     | inf          |
|    n_updates                | 410          |
|    policy_gradient_loss     | inf          |
|    std                      | 0.419        |
|    value_loss               | 0.877        |
----------------------------------------------
----------------------------------------------
| reward                      | [-0.5918792] |
| time/                       |              |
|    fps                      | 119          |
|    iterations               | 43           |
|    time_elapsed             | 733          |
|    total_timesteps          | 88064        |
| train/                      |              |
|    approx_kl                | 34.901115    |
|    approx_ln(kl)            | 3.5525188    |
|    clip_range               | 0.2          |
|    entropy_loss             | -1.03        |
|    explained_variance       | 0.977        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | inf          |
|    loss                     | inf          |
|    n_updates                | 420          |
|    policy_gradient_loss     | inf          |
|    std                      | 0.396        |
|    value_loss               | 1.01         |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.26353577] |
| time/                       |               |
|    fps                      | 120           |
|    iterations               | 44            |
|    time_elapsed             | 747           |
|    total_timesteps          | 90112         |
| train/                      |               |
|    approx_kl                | 37.87979      |
|    approx_ln(kl)            | 3.6344178     |
|    clip_range               | 0.2           |
|    entropy_loss             | -0.938        |
|    explained_variance       | 0.951         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | inf           |
|    ln(policy_gradient_loss) | inf           |
|    loss                     | inf           |
|    n_updates                | 430           |
|    policy_gradient_loss     | inf           |
|    std                      | 0.383         |
|    value_loss               | 0.57          |
-----------------------------------------------
-----------------------------------------------
| reward                      | [-0.49436572] |
| time/                       |               |
|    fps                      | 121           |
|    iterations               | 45            |
|    time_elapsed             | 760           |
|    total_timesteps          | 92160         |
| train/                      |               |
|    approx_kl                | 24.641954     |
|    approx_ln(kl)            | 3.2044504     |
|    clip_range               | 0.2           |
|    entropy_loss             | -0.916        |
|    explained_variance       | 0.974         |
|    learning_rate            | 0.001         |
|    ln(loss)                 | inf           |
|    ln(policy_gradient_loss) | inf           |
|    loss                     | inf           |
|    n_updates                | 440           |
|    policy_gradient_loss     | inf           |
|    std                      | 0.382         |
|    value_loss               | 32.3          |
-----------------------------------------------
----------------------------------------------
| reward                      | [-0.4348989] |
| time/                       |              |
|    fps                      | 121          |
|    iterations               | 46           |
|    time_elapsed             | 774          |
|    total_timesteps          | 94208        |
| train/                      |              |
|    approx_kl                | 31.764997    |
|    approx_ln(kl)            | 3.458365     |
|    clip_range               | 0.2          |
|    entropy_loss             | -0.911       |
|    explained_variance       | 0.987        |
|    learning_rate            | 0.001        |
|    ln(loss)                 | inf          |
|    ln(policy_gradient_loss) | inf          |
|    loss                     | inf          |
|    n_updates                | 450          |
|    policy_gradient_loss     | inf          |
|    std                      | 0.381        |
|    value_loss               | 30.8         |
----------------------------------------------
-----------------------------------------------
| reward                      | [-0.52430236] |
| time/                       |               |
|    fps                      | 122           |
|    iterations               | 47            |
|    time_elapsed             | 787           |
|    total_timesteps          | 96256         |
| train/                      |               |
|    approx_kl                | 25.176619     |
|    approx_ln(kl)            | 3.2259157     |
|    clip_range               | 0.2           |
|    entropy_loss             | -0.898        |
|    explained_variance       | 0.99          |
|    learning_rate            | 0.001         |
|    ln(loss)                 | inf           |
|    ln(policy_gradient_loss) | inf           |
|    loss                     | inf           |
|    n_updates                | 460           |
|    policy_gradient_loss     | inf           |
|    std                      | 0.374         |
|    value_loss               | 15.8          |
-----------------------------------------------
slurmstepd: error: *** STEP 97090.0 ON ddpg.ist.berkeley.edu CANCELLED AT 2023-11-20T14:34:52 ***
slurmstepd: error: *** JOB 97090 ON ddpg.ist.berkeley.edu CANCELLED AT 2023-11-20T14:34:52 ***
